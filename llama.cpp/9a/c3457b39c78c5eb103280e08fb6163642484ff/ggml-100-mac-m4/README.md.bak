### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.36 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.14 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.08 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.28 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.32 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.87 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.83 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  192.51 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.92 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.85 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 252.97 sec*proc (29 tests)

Total Test time (real) = 252.99 sec

real	4m13.096s
user	8m40.517s
sys	0m7.147s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.16 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.71 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.41 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   31.01 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.91 sec*proc (29 tests)

Total Test time (real) =  54.92 sec

real	0m54.929s
user	1m17.688s
sys	0m6.351s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.123 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.686 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.229 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.236 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.239 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.240 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.241 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.241 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.242 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.243 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.244 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.248 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.249 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.249 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.252 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.255 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.256 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.257 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.257 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.258 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.259 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.888 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.997 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.999 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.000 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.001 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.001 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.001 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.002 I llama_model_loader: - type  f32:  124 tensors
0.00.027.003 I llama_model_loader: - type  f16:   73 tensors
0.00.027.003 I print_info: file format = GGUF V3 (latest)
0.00.027.004 I print_info: file type   = F16
0.00.027.007 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.655 I load: special tokens cache size = 5
0.00.033.920 I load: token to piece cache size = 0.2032 MB
0.00.033.924 I print_info: arch             = bert
0.00.033.924 I print_info: vocab_only       = 0
0.00.033.924 I print_info: n_ctx_train      = 512
0.00.033.925 I print_info: n_embd           = 384
0.00.033.925 I print_info: n_layer          = 12
0.00.033.928 I print_info: n_head           = 12
0.00.033.929 I print_info: n_head_kv        = 12
0.00.033.930 I print_info: n_rot            = 32
0.00.033.930 I print_info: n_swa            = 0
0.00.033.930 I print_info: n_embd_head_k    = 32
0.00.033.930 I print_info: n_embd_head_v    = 32
0.00.033.931 I print_info: n_gqa            = 1
0.00.033.932 I print_info: n_embd_k_gqa     = 384
0.00.033.933 I print_info: n_embd_v_gqa     = 384
0.00.033.934 I print_info: f_norm_eps       = 1.0e-12
0.00.033.935 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.033.935 I print_info: f_clamp_kqv      = 0.0e+00
0.00.033.935 I print_info: f_max_alibi_bias = 0.0e+00
0.00.033.935 I print_info: f_logit_scale    = 0.0e+00
0.00.033.936 I print_info: n_ff             = 1536
0.00.033.937 I print_info: n_expert         = 0
0.00.033.937 I print_info: n_expert_used    = 0
0.00.033.937 I print_info: causal attn      = 0
0.00.033.937 I print_info: pooling type     = 2
0.00.033.937 I print_info: rope type        = 2
0.00.033.938 I print_info: rope scaling     = linear
0.00.033.939 I print_info: freq_base_train  = 10000.0
0.00.033.939 I print_info: freq_scale_train = 1
0.00.033.939 I print_info: n_ctx_orig_yarn  = 512
0.00.033.940 I print_info: rope_finetuned   = unknown
0.00.033.940 I print_info: ssm_d_conv       = 0
0.00.033.940 I print_info: ssm_d_inner      = 0
0.00.033.940 I print_info: ssm_d_state      = 0
0.00.033.940 I print_info: ssm_dt_rank      = 0
0.00.033.941 I print_info: ssm_dt_b_c_rms   = 0
0.00.033.941 I print_info: model type       = 33M
0.00.033.941 I print_info: model params     = 33.21 M
0.00.033.944 I print_info: general.name     = Bge Small
0.00.033.945 I print_info: vocab type       = WPM
0.00.033.945 I print_info: n_vocab          = 30522
0.00.033.945 I print_info: n_merges         = 0
0.00.033.946 I print_info: BOS token        = 101 '[CLS]'
0.00.033.946 I print_info: UNK token        = 100 '[UNK]'
0.00.033.946 I print_info: SEP token        = 102 '[SEP]'
0.00.033.947 I print_info: PAD token        = 0 '[PAD]'
0.00.033.947 I print_info: MASK token       = 103 '[MASK]'
0.00.033.947 I print_info: LF token         = 0 '[PAD]'
0.00.033.947 I print_info: max token length = 21
0.00.033.954 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.037.182 I load_tensors: offloading 12 repeating layers to GPU
0.00.037.184 I load_tensors: offloading output layer to GPU
0.00.037.184 I load_tensors: offloaded 13/13 layers to GPU
0.00.037.210 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.211 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.502 I llama_init_from_model: n_seq_max     = 1
0.00.037.503 I llama_init_from_model: n_ctx         = 512
0.00.037.503 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.504 I llama_init_from_model: n_batch       = 2048
0.00.037.504 I llama_init_from_model: n_ubatch      = 2048
0.00.037.504 I llama_init_from_model: flash_attn    = 0
0.00.037.505 I llama_init_from_model: freq_base     = 10000.0
0.00.037.505 I llama_init_from_model: freq_scale    = 1
0.00.037.506 I ggml_metal_init: allocating
0.00.037.510 I ggml_metal_init: found device: Apple M4
0.00.037.517 I ggml_metal_init: picking default device: Apple M4
0.00.038.210 I ggml_metal_init: using embedded metal library
0.00.042.287 I ggml_metal_init: GPU name:   Apple M4
0.00.042.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.290 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.291 I ggml_metal_init: simdgroup reduction   = true
0.00.042.291 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.292 I ggml_metal_init: has residency sets    = true
0.00.042.292 I ggml_metal_init: has bfloat            = true
0.00.042.292 I ggml_metal_init: use bfloat            = true
0.00.042.292 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.054.341 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.055.049 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.055.052 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.055.053 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.056.329 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.056.330 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.056.330 I llama_init_from_model: graph nodes  = 429
0.00.056.331 I llama_init_from_model: graph splits = 2
0.00.056.332 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.332 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.062.178 I 
0.00.062.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.062.869 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.067.983 I llama_perf_context_print:        load time =      46.48 ms
0.00.067.984 I llama_perf_context_print: prompt eval time =       4.97 ms /     9 tokens (    0.55 ms per token,  1809.77 tokens per second)
0.00.067.987 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.067.988 I llama_perf_context_print:       total time =       5.81 ms /    10 tokens
0.00.068.133 I ggml_metal_free: deallocating

real	0m0.247s
user	0m0.049s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.746 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.529 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.533 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.535 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.535 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.535 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.536 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.536 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.537 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.537 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.538 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.538 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.538 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.540 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.541 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.541 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.541 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.542 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.542 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.022 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.705 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.706 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.706 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.706 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.707 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.707 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.707 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.708 I llama_model_loader: - type  f32:  124 tensors
0.00.015.708 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.709 I print_info: file format = GGUF V3 (latest)
0.00.015.709 I print_info: file type   = Q8_0
0.00.015.710 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.209 I load: special tokens cache size = 5
0.00.019.494 I load: token to piece cache size = 0.2032 MB
0.00.019.497 I print_info: arch             = bert
0.00.019.497 I print_info: vocab_only       = 0
0.00.019.498 I print_info: n_ctx_train      = 512
0.00.019.498 I print_info: n_embd           = 384
0.00.019.498 I print_info: n_layer          = 12
0.00.019.501 I print_info: n_head           = 12
0.00.019.504 I print_info: n_head_kv        = 12
0.00.019.505 I print_info: n_rot            = 32
0.00.019.505 I print_info: n_swa            = 0
0.00.019.505 I print_info: n_embd_head_k    = 32
0.00.019.505 I print_info: n_embd_head_v    = 32
0.00.019.505 I print_info: n_gqa            = 1
0.00.019.510 I print_info: n_embd_k_gqa     = 384
0.00.019.511 I print_info: n_embd_v_gqa     = 384
0.00.019.511 I print_info: f_norm_eps       = 1.0e-12
0.00.019.512 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.512 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.512 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.512 I print_info: f_logit_scale    = 0.0e+00
0.00.019.513 I print_info: n_ff             = 1536
0.00.019.513 I print_info: n_expert         = 0
0.00.019.513 I print_info: n_expert_used    = 0
0.00.019.513 I print_info: causal attn      = 0
0.00.019.513 I print_info: pooling type     = 2
0.00.019.513 I print_info: rope type        = 2
0.00.019.514 I print_info: rope scaling     = linear
0.00.019.515 I print_info: freq_base_train  = 10000.0
0.00.019.515 I print_info: freq_scale_train = 1
0.00.019.515 I print_info: n_ctx_orig_yarn  = 512
0.00.019.515 I print_info: rope_finetuned   = unknown
0.00.019.515 I print_info: ssm_d_conv       = 0
0.00.019.517 I print_info: ssm_d_inner      = 0
0.00.019.517 I print_info: ssm_d_state      = 0
0.00.019.517 I print_info: ssm_dt_rank      = 0
0.00.019.517 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.518 I print_info: model type       = 33M
0.00.019.518 I print_info: model params     = 33.21 M
0.00.019.518 I print_info: general.name     = Bge Small
0.00.019.519 I print_info: vocab type       = WPM
0.00.019.519 I print_info: n_vocab          = 30522
0.00.019.519 I print_info: n_merges         = 0
0.00.019.519 I print_info: BOS token        = 101 '[CLS]'
0.00.019.519 I print_info: UNK token        = 100 '[UNK]'
0.00.019.520 I print_info: SEP token        = 102 '[SEP]'
0.00.019.520 I print_info: PAD token        = 0 '[PAD]'
0.00.019.520 I print_info: MASK token       = 103 '[MASK]'
0.00.019.520 I print_info: LF token         = 0 '[PAD]'
0.00.019.520 I print_info: max token length = 21
0.00.019.521 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.021.214 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.215 I load_tensors: offloading output layer to GPU
0.00.021.215 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.222 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.224 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.393 I llama_init_from_model: n_seq_max     = 1
0.00.021.393 I llama_init_from_model: n_ctx         = 512
0.00.021.394 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.394 I llama_init_from_model: n_batch       = 2048
0.00.021.394 I llama_init_from_model: n_ubatch      = 2048
0.00.021.394 I llama_init_from_model: flash_attn    = 0
0.00.021.395 I llama_init_from_model: freq_base     = 10000.0
0.00.021.395 I llama_init_from_model: freq_scale    = 1
0.00.021.395 I ggml_metal_init: allocating
0.00.021.399 I ggml_metal_init: found device: Apple M4
0.00.021.402 I ggml_metal_init: picking default device: Apple M4
0.00.021.919 I ggml_metal_init: using embedded metal library
0.00.024.444 I ggml_metal_init: GPU name:   Apple M4
0.00.024.446 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.447 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.447 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.447 I ggml_metal_init: simdgroup reduction   = true
0.00.024.448 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.448 I ggml_metal_init: has residency sets    = true
0.00.024.448 I ggml_metal_init: has bfloat            = true
0.00.024.448 I ggml_metal_init: use bfloat            = true
0.00.024.449 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.449 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.721 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.333 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.335 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.336 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.277 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.279 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.279 I llama_init_from_model: graph nodes  = 429
0.00.036.279 I llama_init_from_model: graph splits = 2
0.00.036.280 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.359 I 
0.00.040.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.885 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.284 I llama_perf_context_print:        load time =      30.61 ms
0.00.045.285 I llama_perf_context_print: prompt eval time =       4.28 ms /     9 tokens (    0.48 ms per token,  2103.79 tokens per second)
0.00.045.286 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.286 I llama_perf_context_print:       total time =       4.93 ms /    10 tokens
0.00.045.491 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.286 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.547 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.510 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.515 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.518 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.519 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.520 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.521 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.522 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.523 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.523 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.524 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.525 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.528 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.529 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.529 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.530 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.531 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.789 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.892 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.616 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.619 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.620 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.620 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.620 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.621 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.621 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.621 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.622 I llama_model_loader: - type  f32:   40 tensors
0.00.048.622 I llama_model_loader: - type  f16:   30 tensors
0.00.048.623 I print_info: file format = GGUF V3 (latest)
0.00.048.624 I print_info: file type   = F16
0.00.048.625 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.053.041 W load: empty token at index 5
0.00.058.233 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.731 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.766 I load: special tokens cache size = 5
0.00.323.792 I load: token to piece cache size = 1.5060 MB
0.00.323.800 I print_info: arch             = jina-bert-v2
0.00.323.801 I print_info: vocab_only       = 0
0.00.323.805 I print_info: n_ctx_train      = 8192
0.00.323.806 I print_info: n_embd           = 384
0.00.323.807 I print_info: n_layer          = 4
0.00.323.816 I print_info: n_head           = 12
0.00.323.817 I print_info: n_head_kv        = 12
0.00.323.817 I print_info: n_rot            = 32
0.00.323.817 I print_info: n_swa            = 0
0.00.323.817 I print_info: n_embd_head_k    = 32
0.00.323.817 I print_info: n_embd_head_v    = 32
0.00.323.818 I print_info: n_gqa            = 1
0.00.323.819 I print_info: n_embd_k_gqa     = 384
0.00.323.820 I print_info: n_embd_v_gqa     = 384
0.00.323.821 I print_info: f_norm_eps       = 1.0e-12
0.00.323.823 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.323.823 I print_info: f_clamp_kqv      = 0.0e+00
0.00.323.824 I print_info: f_max_alibi_bias = 8.0e+00
0.00.323.824 I print_info: f_logit_scale    = 0.0e+00
0.00.323.824 I print_info: n_ff             = 1536
0.00.323.825 I print_info: n_expert         = 0
0.00.323.826 I print_info: n_expert_used    = 0
0.00.323.826 I print_info: causal attn      = 0
0.00.323.827 I print_info: pooling type     = -1
0.00.323.827 I print_info: rope type        = -1
0.00.323.827 I print_info: rope scaling     = linear
0.00.323.828 I print_info: freq_base_train  = 10000.0
0.00.323.828 I print_info: freq_scale_train = 1
0.00.323.828 I print_info: n_ctx_orig_yarn  = 8192
0.00.323.828 I print_info: rope_finetuned   = unknown
0.00.323.828 I print_info: ssm_d_conv       = 0
0.00.323.828 I print_info: ssm_d_inner      = 0
0.00.323.829 I print_info: ssm_d_state      = 0
0.00.323.829 I print_info: ssm_dt_rank      = 0
0.00.323.829 I print_info: ssm_dt_b_c_rms   = 0
0.00.323.829 I print_info: model type       = 33M
0.00.323.830 I print_info: model params     = 32.90 M
0.00.323.830 I print_info: general.name     = Jina Bert Implementation
0.00.323.831 I print_info: vocab type       = BPE
0.00.323.832 I print_info: n_vocab          = 61056
0.00.323.832 I print_info: n_merges         = 39382
0.00.323.832 I print_info: BOS token        = 0 '<s>'
0.00.323.832 I print_info: EOS token        = 2 '</s>'
0.00.323.837 I print_info: UNK token        = 3 '<unk>'
0.00.323.837 I print_info: SEP token        = 2 '</s>'
0.00.323.838 I print_info: PAD token        = 1 '<pad>'
0.00.323.838 I print_info: MASK token       = 4 '<mask>'
0.00.323.843 I print_info: EOG token        = 2 '</s>'
0.00.323.844 I print_info: max token length = 45
0.00.323.845 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.325.828 I load_tensors: offloading 4 repeating layers to GPU
0.00.325.829 I load_tensors: offloading output layer to GPU
0.00.325.829 I load_tensors: offloaded 5/5 layers to GPU
0.00.325.852 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.325.854 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.326.235 I llama_init_from_model: n_seq_max     = 1
0.00.326.236 I llama_init_from_model: n_ctx         = 8192
0.00.326.236 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.326.236 I llama_init_from_model: n_batch       = 2048
0.00.326.236 I llama_init_from_model: n_ubatch      = 2048
0.00.326.236 I llama_init_from_model: flash_attn    = 0
0.00.326.237 I llama_init_from_model: freq_base     = 10000.0
0.00.326.237 I llama_init_from_model: freq_scale    = 1
0.00.326.238 I ggml_metal_init: allocating
0.00.326.242 I ggml_metal_init: found device: Apple M4
0.00.326.245 I ggml_metal_init: picking default device: Apple M4
0.00.327.144 I ggml_metal_init: using embedded metal library
0.00.330.078 I ggml_metal_init: GPU name:   Apple M4
0.00.330.079 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.330.080 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.330.080 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.330.080 I ggml_metal_init: simdgroup reduction   = true
0.00.330.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.330.081 I ggml_metal_init: has residency sets    = true
0.00.330.081 I ggml_metal_init: has bfloat            = true
0.00.330.081 I ggml_metal_init: use bfloat            = true
0.00.330.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.330.082 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.339.581 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.342.817 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.342.819 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.342.823 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.349.706 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.349.708 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.349.708 I llama_init_from_model: graph nodes  = 154
0.00.349.709 I llama_init_from_model: graph splits = 2
0.00.349.710 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.349.710 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.583 I 
0.00.356.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.718 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.356.719 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.356.722 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.356.722 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.356.726 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.356.726 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.357.278 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.360.803 I llama_perf_context_print:        load time =     335.03 ms
0.00.360.804 I llama_perf_context_print: prompt eval time =       3.52 ms /    62 tokens (    0.06 ms per token, 17628.66 tokens per second)
0.00.360.805 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.360.805 I llama_perf_context_print:       total time =       4.22 ms /    63 tokens
0.00.361.003 I ggml_metal_free: deallocating

real	0m1.062s
user	0m0.333s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.237 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.398 I main: llama backend init
0.00.000.407 I main: load the model and apply lora adapter, if any
0.00.058.612 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.072.595 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.072.610 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.072.616 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.072.618 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.072.618 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.072.619 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.072.620 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.072.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.072.624 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.072.625 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.072.625 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.072.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.072.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.072.629 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.072.632 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.072.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.072.634 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.081.755 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.083.797 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.091.101 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.091.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.091.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.091.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.091.106 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.091.107 I llama_model_loader: - type  f32:  194 tensors
0.00.091.107 I llama_model_loader: - type  f16:   98 tensors
0.00.091.108 I print_info: file format = GGUF V3 (latest)
0.00.091.110 I print_info: file type   = all F32 (guessed)
0.00.091.111 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.104.290 I load: special tokens cache size = 25
0.00.112.482 I load: token to piece cache size = 0.2984 MB
0.00.112.486 I print_info: arch             = gptneox
0.00.112.486 I print_info: vocab_only       = 0
0.00.112.486 I print_info: n_ctx_train      = 2048
0.00.112.486 I print_info: n_embd           = 2048
0.00.112.486 I print_info: n_layer          = 24
0.00.112.489 I print_info: n_head           = 16
0.00.112.490 I print_info: n_head_kv        = 16
0.00.112.491 I print_info: n_rot            = 32
0.00.112.491 I print_info: n_swa            = 0
0.00.112.491 I print_info: n_embd_head_k    = 128
0.00.112.491 I print_info: n_embd_head_v    = 128
0.00.112.492 I print_info: n_gqa            = 1
0.00.112.493 I print_info: n_embd_k_gqa     = 2048
0.00.112.494 I print_info: n_embd_v_gqa     = 2048
0.00.112.494 I print_info: f_norm_eps       = 1.0e-05
0.00.112.494 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.112.495 I print_info: f_clamp_kqv      = 0.0e+00
0.00.112.495 I print_info: f_max_alibi_bias = 0.0e+00
0.00.112.495 I print_info: f_logit_scale    = 0.0e+00
0.00.112.496 I print_info: n_ff             = 8192
0.00.112.496 I print_info: n_expert         = 0
0.00.112.496 I print_info: n_expert_used    = 0
0.00.112.496 I print_info: causal attn      = 1
0.00.112.496 I print_info: pooling type     = 0
0.00.112.496 I print_info: rope type        = 2
0.00.112.497 I print_info: rope scaling     = linear
0.00.112.497 I print_info: freq_base_train  = 10000.0
0.00.112.498 I print_info: freq_scale_train = 1
0.00.112.498 I print_info: n_ctx_orig_yarn  = 2048
0.00.112.498 I print_info: rope_finetuned   = unknown
0.00.112.498 I print_info: ssm_d_conv       = 0
0.00.112.498 I print_info: ssm_d_inner      = 0
0.00.112.499 I print_info: ssm_d_state      = 0
0.00.112.499 I print_info: ssm_dt_rank      = 0
0.00.112.499 I print_info: ssm_dt_b_c_rms   = 0
0.00.112.499 I print_info: model type       = 1.4B
0.00.112.500 I print_info: model params     = 1.41 B
0.00.112.500 I print_info: general.name     = 1.4B
0.00.112.500 I print_info: vocab type       = BPE
0.00.112.500 I print_info: n_vocab          = 50304
0.00.112.501 I print_info: n_merges         = 50009
0.00.112.501 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.112.501 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.112.501 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.112.502 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.112.502 I print_info: LF token         = 187 ''
0.00.112.502 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.112.503 I print_info: max token length = 1024
0.00.112.503 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.156.437 I load_tensors: offloading 24 repeating layers to GPU
0.00.156.440 I load_tensors: offloading output layer to GPU
0.00.156.441 I load_tensors: offloaded 25/25 layers to GPU
0.00.156.469 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.156.470 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.156.937 I llama_init_from_model: n_seq_max     = 1
0.00.156.938 I llama_init_from_model: n_ctx         = 2048
0.00.156.938 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.156.938 I llama_init_from_model: n_batch       = 2048
0.00.156.939 I llama_init_from_model: n_ubatch      = 512
0.00.156.939 I llama_init_from_model: flash_attn    = 0
0.00.156.939 I llama_init_from_model: freq_base     = 10000.0
0.00.156.940 I llama_init_from_model: freq_scale    = 1
0.00.156.941 I ggml_metal_init: allocating
0.00.156.974 I ggml_metal_init: found device: Apple M4
0.00.156.983 I ggml_metal_init: picking default device: Apple M4
0.00.157.633 I ggml_metal_init: using embedded metal library
0.00.167.319 I ggml_metal_init: GPU name:   Apple M4
0.00.167.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.167.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.167.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.167.322 I ggml_metal_init: simdgroup reduction   = true
0.00.167.322 I ggml_metal_init: simdgroup matrix mul. = true
0.00.167.323 I ggml_metal_init: has residency sets    = true
0.00.167.323 I ggml_metal_init: has bfloat            = true
0.00.167.323 I ggml_metal_init: use bfloat            = true
0.00.167.323 I ggml_metal_init: hasUnifiedMemory      = true
0.00.167.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.192.550 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.220.461 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.220.467 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.220.489 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.223.898 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.223.900 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.223.900 I llama_init_from_model: graph nodes  = 967
0.00.223.900 I llama_init_from_model: graph splits = 2
0.00.223.906 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.224.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.224.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.289.661 I main: llama threadpool init, n_threads = 4
0.00.289.704 I 
0.00.289.736 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.289.738 I 
0.00.289.925 I sampler seed: 1234
0.00.289.930 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.289.954 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.289.956 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.289.956 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.118.104 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.02.118.105 I llama_perf_context_print:        load time =     230.20 ms
0.02.118.106 I llama_perf_context_print: prompt eval time =      43.58 ms /     7 tokens (    6.23 ms per token,   160.63 tokens per second)
0.02.118.107 I llama_perf_context_print:        eval time =    1781.65 ms /    63 runs   (   28.28 ms per token,    35.36 tokens per second)
0.02.118.107 I llama_perf_context_print:       total time =    1829.28 ms /    70 tokens
0.02.118.343 I ggml_metal_free: deallocating

real	0m2.462s
user	0m0.132s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.667 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.101 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.928 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.936 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.948 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.949 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.952 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.955 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.955 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.956 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.957 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.959 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.961 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.962 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.968 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.273 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.176 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.703 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.703 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.704 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.704 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.705 I llama_model_loader: - type  f32:  194 tensors
0.00.056.705 I llama_model_loader: - type  f16:   98 tensors
0.00.056.706 I print_info: file format = GGUF V3 (latest)
0.00.056.707 I print_info: file type   = all F32 (guessed)
0.00.056.708 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.806 I load: special tokens cache size = 25
0.00.076.744 I load: token to piece cache size = 0.2984 MB
0.00.076.747 I print_info: arch             = gptneox
0.00.076.747 I print_info: vocab_only       = 0
0.00.076.747 I print_info: n_ctx_train      = 2048
0.00.076.748 I print_info: n_embd           = 2048
0.00.076.748 I print_info: n_layer          = 24
0.00.076.751 I print_info: n_head           = 16
0.00.076.752 I print_info: n_head_kv        = 16
0.00.076.752 I print_info: n_rot            = 32
0.00.076.753 I print_info: n_swa            = 0
0.00.076.753 I print_info: n_embd_head_k    = 128
0.00.076.753 I print_info: n_embd_head_v    = 128
0.00.076.754 I print_info: n_gqa            = 1
0.00.076.754 I print_info: n_embd_k_gqa     = 2048
0.00.076.757 I print_info: n_embd_v_gqa     = 2048
0.00.076.757 I print_info: f_norm_eps       = 1.0e-05
0.00.076.758 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.758 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.758 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.758 I print_info: f_logit_scale    = 0.0e+00
0.00.076.759 I print_info: n_ff             = 8192
0.00.076.759 I print_info: n_expert         = 0
0.00.076.760 I print_info: n_expert_used    = 0
0.00.076.760 I print_info: causal attn      = 1
0.00.076.761 I print_info: pooling type     = 0
0.00.076.761 I print_info: rope type        = 2
0.00.076.761 I print_info: rope scaling     = linear
0.00.076.761 I print_info: freq_base_train  = 10000.0
0.00.076.762 I print_info: freq_scale_train = 1
0.00.076.762 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.762 I print_info: rope_finetuned   = unknown
0.00.076.762 I print_info: ssm_d_conv       = 0
0.00.076.762 I print_info: ssm_d_inner      = 0
0.00.076.763 I print_info: ssm_d_state      = 0
0.00.076.763 I print_info: ssm_dt_rank      = 0
0.00.076.763 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.763 I print_info: model type       = 1.4B
0.00.076.763 I print_info: model params     = 1.41 B
0.00.076.763 I print_info: general.name     = 1.4B
0.00.076.768 I print_info: vocab type       = BPE
0.00.076.768 I print_info: n_vocab          = 50304
0.00.076.769 I print_info: n_merges         = 50009
0.00.076.769 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.770 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.770 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.770 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.770 I print_info: LF token         = 187 ''
0.00.076.771 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.771 I print_info: max token length = 1024
0.00.076.771 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.461.318 I load_tensors: offloading 24 repeating layers to GPU
0.01.461.322 I load_tensors: offloading output layer to GPU
0.01.461.322 I load_tensors: offloaded 25/25 layers to GPU
0.01.461.346 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.461.348 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.462.396 I llama_init_from_model: n_seq_max     = 1
0.01.462.398 I llama_init_from_model: n_ctx         = 128
0.01.462.398 I llama_init_from_model: n_ctx_per_seq = 128
0.01.462.399 I llama_init_from_model: n_batch       = 128
0.01.462.399 I llama_init_from_model: n_ubatch      = 128
0.01.462.399 I llama_init_from_model: flash_attn    = 0
0.01.462.400 I llama_init_from_model: freq_base     = 10000.0
0.01.462.400 I llama_init_from_model: freq_scale    = 1
0.01.462.401 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.462.402 I ggml_metal_init: allocating
0.01.462.448 I ggml_metal_init: found device: Apple M4
0.01.462.455 I ggml_metal_init: picking default device: Apple M4
0.01.463.661 I ggml_metal_init: using embedded metal library
0.01.468.197 I ggml_metal_init: GPU name:   Apple M4
0.01.468.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.468.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.468.201 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.468.202 I ggml_metal_init: simdgroup reduction   = true
0.01.468.202 I ggml_metal_init: simdgroup matrix mul. = true
0.01.468.202 I ggml_metal_init: has residency sets    = true
0.01.468.202 I ggml_metal_init: has bfloat            = true
0.01.468.203 I ggml_metal_init: use bfloat            = true
0.01.468.204 I ggml_metal_init: hasUnifiedMemory      = true
0.01.468.205 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.486.688 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.488.950 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.488.956 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.488.971 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.490.886 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.490.888 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.490.888 I llama_init_from_model: graph nodes  = 967
0.01.490.888 I llama_init_from_model: graph splits = 2
0.01.490.890 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.490.890 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.525.291 I 
0.01.525.334 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.525.338 I perplexity: tokenizing the input ..
0.01.530.964 I perplexity: tokenization took 5.625 ms
0.01.530.968 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.649.633 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.650.975 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.651.007 I llama_perf_context_print:        load time =    1501.18 ms
0.01.651.008 I llama_perf_context_print: prompt eval time =     118.36 ms /   128 tokens (    0.92 ms per token,  1081.48 tokens per second)
0.01.651.009 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.651.009 I llama_perf_context_print:       total time =     125.72 ms /   129 tokens
0.01.651.386 I ggml_metal_free: deallocating

real	0m1.837s
user	0m0.102s
sys	0m0.254s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.009.850 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.419 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.423 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.425 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.426 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.427 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.427 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.427 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.429 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.429 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.429 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.430 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.430 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.430 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.433 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.433 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.271 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.262 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.055 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.057 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.057 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.057 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.058 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.058 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.058 I llama_model_loader: - type  f32:  194 tensors
0.00.028.059 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.059 I print_info: file format = GGUF V3 (latest)
0.00.028.060 I print_info: file type   = Q8_0
0.00.028.061 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.050 I load: special tokens cache size = 25
0.00.042.239 I load: token to piece cache size = 0.2984 MB
0.00.042.244 I print_info: arch             = gptneox
0.00.042.245 I print_info: vocab_only       = 0
0.00.042.247 I print_info: n_ctx_train      = 2048
0.00.042.247 I print_info: n_embd           = 2048
0.00.042.251 I print_info: n_layer          = 24
0.00.042.257 I print_info: n_head           = 16
0.00.042.258 I print_info: n_head_kv        = 16
0.00.042.258 I print_info: n_rot            = 32
0.00.042.258 I print_info: n_swa            = 0
0.00.042.259 I print_info: n_embd_head_k    = 128
0.00.042.259 I print_info: n_embd_head_v    = 128
0.00.042.259 I print_info: n_gqa            = 1
0.00.042.260 I print_info: n_embd_k_gqa     = 2048
0.00.042.261 I print_info: n_embd_v_gqa     = 2048
0.00.042.261 I print_info: f_norm_eps       = 1.0e-05
0.00.042.261 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.262 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.262 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.262 I print_info: f_logit_scale    = 0.0e+00
0.00.042.263 I print_info: n_ff             = 8192
0.00.042.263 I print_info: n_expert         = 0
0.00.042.263 I print_info: n_expert_used    = 0
0.00.042.263 I print_info: causal attn      = 1
0.00.042.263 I print_info: pooling type     = 0
0.00.042.264 I print_info: rope type        = 2
0.00.042.265 I print_info: rope scaling     = linear
0.00.042.265 I print_info: freq_base_train  = 10000.0
0.00.042.266 I print_info: freq_scale_train = 1
0.00.042.266 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.266 I print_info: rope_finetuned   = unknown
0.00.042.266 I print_info: ssm_d_conv       = 0
0.00.042.266 I print_info: ssm_d_inner      = 0
0.00.042.266 I print_info: ssm_d_state      = 0
0.00.042.266 I print_info: ssm_dt_rank      = 0
0.00.042.267 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.267 I print_info: model type       = 1.4B
0.00.042.267 I print_info: model params     = 1.41 B
0.00.042.267 I print_info: general.name     = 1.4B
0.00.042.268 I print_info: vocab type       = BPE
0.00.042.269 I print_info: n_vocab          = 50304
0.00.042.269 I print_info: n_merges         = 50009
0.00.042.269 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.269 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.269 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.269 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.270 I print_info: LF token         = 187 ''
0.00.042.270 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.270 I print_info: max token length = 1024
0.00.042.271 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.074.616 I load_tensors: offloading 24 repeating layers to GPU
0.01.074.621 I load_tensors: offloading output layer to GPU
0.01.074.623 I load_tensors: offloaded 25/25 layers to GPU
0.01.074.647 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.074.648 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.075.405 I llama_init_from_model: n_seq_max     = 1
0.01.075.406 I llama_init_from_model: n_ctx         = 2048
0.01.075.407 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.075.407 I llama_init_from_model: n_batch       = 2048
0.01.075.407 I llama_init_from_model: n_ubatch      = 512
0.01.075.408 I llama_init_from_model: flash_attn    = 0
0.01.075.408 I llama_init_from_model: freq_base     = 10000.0
0.01.075.409 I llama_init_from_model: freq_scale    = 1
0.01.075.410 I ggml_metal_init: allocating
0.01.075.421 I ggml_metal_init: found device: Apple M4
0.01.075.429 I ggml_metal_init: picking default device: Apple M4
0.01.076.639 I ggml_metal_init: using embedded metal library
0.01.081.923 I ggml_metal_init: GPU name:   Apple M4
0.01.081.927 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.081.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.081.929 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.081.929 I ggml_metal_init: simdgroup reduction   = true
0.01.081.929 I ggml_metal_init: simdgroup matrix mul. = true
0.01.081.930 I ggml_metal_init: has residency sets    = true
0.01.081.930 I ggml_metal_init: has bfloat            = true
0.01.081.930 I ggml_metal_init: use bfloat            = true
0.01.081.931 I ggml_metal_init: hasUnifiedMemory      = true
0.01.081.933 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.098.643 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.144.290 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.144.300 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.144.329 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.148.656 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.148.658 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.148.658 I llama_init_from_model: graph nodes  = 967
0.01.148.659 I llama_init_from_model: graph splits = 2
0.01.148.662 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.148.787 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.148.787 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.204.473 I main: llama threadpool init, n_threads = 4
0.01.204.507 I 
0.01.204.526 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.204.526 I 
0.01.204.773 I sampler seed: 1234
0.01.204.786 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.204.803 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.204.805 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.204.805 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.309.357 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48864.42 tokens per second)
0.02.309.358 I llama_perf_context_print:        load time =    1193.85 ms
0.02.309.359 I llama_perf_context_print: prompt eval time =      49.10 ms /     7 tokens (    7.01 ms per token,   142.57 tokens per second)
0.02.309.360 I llama_perf_context_print:        eval time =    1052.98 ms /    63 runs   (   16.71 ms per token,    59.83 tokens per second)
0.02.309.360 I llama_perf_context_print:       total time =    1105.66 ms /    70 tokens
0.02.309.628 I ggml_metal_free: deallocating

real	0m2.326s
user	0m0.108s
sys	0m0.259s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.273 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.502 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.683 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.689 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.691 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.692 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.692 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.692 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.693 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.694 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.694 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.695 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.695 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.695 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.695 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.696 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.698 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.698 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.699 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.516 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.499 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.341 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.342 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.343 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.343 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.344 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.344 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.344 I llama_model_loader: - type  f32:  194 tensors
0.00.026.345 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.345 I print_info: file format = GGUF V3 (latest)
0.00.026.346 I print_info: file type   = Q8_0
0.00.026.347 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.273 I load: special tokens cache size = 25
0.00.040.573 I load: token to piece cache size = 0.2984 MB
0.00.040.577 I print_info: arch             = gptneox
0.00.040.578 I print_info: vocab_only       = 0
0.00.040.578 I print_info: n_ctx_train      = 2048
0.00.040.578 I print_info: n_embd           = 2048
0.00.040.578 I print_info: n_layer          = 24
0.00.040.582 I print_info: n_head           = 16
0.00.040.583 I print_info: n_head_kv        = 16
0.00.040.583 I print_info: n_rot            = 32
0.00.040.583 I print_info: n_swa            = 0
0.00.040.583 I print_info: n_embd_head_k    = 128
0.00.040.587 I print_info: n_embd_head_v    = 128
0.00.040.588 I print_info: n_gqa            = 1
0.00.040.589 I print_info: n_embd_k_gqa     = 2048
0.00.040.589 I print_info: n_embd_v_gqa     = 2048
0.00.040.590 I print_info: f_norm_eps       = 1.0e-05
0.00.040.590 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.591 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.591 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.591 I print_info: f_logit_scale    = 0.0e+00
0.00.040.592 I print_info: n_ff             = 8192
0.00.040.592 I print_info: n_expert         = 0
0.00.040.592 I print_info: n_expert_used    = 0
0.00.040.592 I print_info: causal attn      = 1
0.00.040.592 I print_info: pooling type     = 0
0.00.040.592 I print_info: rope type        = 2
0.00.040.593 I print_info: rope scaling     = linear
0.00.040.593 I print_info: freq_base_train  = 10000.0
0.00.040.593 I print_info: freq_scale_train = 1
0.00.040.594 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.594 I print_info: rope_finetuned   = unknown
0.00.040.594 I print_info: ssm_d_conv       = 0
0.00.040.594 I print_info: ssm_d_inner      = 0
0.00.040.594 I print_info: ssm_d_state      = 0
0.00.040.595 I print_info: ssm_dt_rank      = 0
0.00.040.595 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.595 I print_info: model type       = 1.4B
0.00.040.596 I print_info: model params     = 1.41 B
0.00.040.596 I print_info: general.name     = 1.4B
0.00.040.597 I print_info: vocab type       = BPE
0.00.040.598 I print_info: n_vocab          = 50304
0.00.040.598 I print_info: n_merges         = 50009
0.00.040.599 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.599 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.599 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.599 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.600 I print_info: LF token         = 187 ''
0.00.040.600 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.600 I print_info: max token length = 1024
0.00.040.601 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.920.216 I load_tensors: offloading 24 repeating layers to GPU
0.00.920.222 I load_tensors: offloading output layer to GPU
0.00.920.222 I load_tensors: offloaded 25/25 layers to GPU
0.00.920.252 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.920.255 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.921.804 I llama_init_from_model: n_seq_max     = 1
0.00.921.806 I llama_init_from_model: n_ctx         = 128
0.00.921.806 I llama_init_from_model: n_ctx_per_seq = 128
0.00.921.807 I llama_init_from_model: n_batch       = 128
0.00.921.807 I llama_init_from_model: n_ubatch      = 128
0.00.921.808 I llama_init_from_model: flash_attn    = 0
0.00.921.809 I llama_init_from_model: freq_base     = 10000.0
0.00.921.809 I llama_init_from_model: freq_scale    = 1
0.00.921.810 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.921.811 I ggml_metal_init: allocating
0.00.921.890 I ggml_metal_init: found device: Apple M4
0.00.921.900 I ggml_metal_init: picking default device: Apple M4
0.00.923.351 I ggml_metal_init: using embedded metal library
0.00.928.801 I ggml_metal_init: GPU name:   Apple M4
0.00.928.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.928.805 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.928.806 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.928.806 I ggml_metal_init: simdgroup reduction   = true
0.00.928.806 I ggml_metal_init: simdgroup matrix mul. = true
0.00.928.807 I ggml_metal_init: has residency sets    = true
0.00.928.807 I ggml_metal_init: has bfloat            = true
0.00.928.807 I ggml_metal_init: use bfloat            = true
0.00.928.808 I ggml_metal_init: hasUnifiedMemory      = true
0.00.928.809 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.944.190 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.947.553 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.947.563 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.947.605 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.950.586 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.950.587 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.950.588 I llama_init_from_model: graph nodes  = 967
0.00.950.588 I llama_init_from_model: graph splits = 2
0.00.950.591 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.950.591 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.978.358 I 
0.00.978.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.978.430 I perplexity: tokenizing the input ..
0.00.985.415 I perplexity: tokenization took 6.983 ms
0.00.985.419 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.122.825 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.124.248 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.124.276 I llama_perf_context_print:        load time =     967.85 ms
0.01.124.277 I llama_perf_context_print: prompt eval time =     137.18 ms /   128 tokens (    1.07 ms per token,   933.09 tokens per second)
0.01.124.278 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.124.278 I llama_perf_context_print:       total time =     145.92 ms /   129 tokens
0.01.124.665 I ggml_metal_free: deallocating

real	0m1.140s
user	0m0.075s
sys	0m0.177s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.011.674 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.730 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.735 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.738 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.743 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.744 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.744 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.745 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.745 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.746 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.746 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.746 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.750 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.750 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.750 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.585 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.345 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.347 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.347 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.347 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.348 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.348 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.029.349 I llama_model_loader: - type  f32:  194 tensors
0.00.029.349 I llama_model_loader: - type q4_0:   97 tensors
0.00.029.349 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.350 I print_info: file format = GGUF V3 (latest)
0.00.029.354 I print_info: file type   = Q4_0
0.00.029.355 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.037.647 I load: special tokens cache size = 25
0.00.043.924 I load: token to piece cache size = 0.2984 MB
0.00.043.928 I print_info: arch             = gptneox
0.00.043.929 I print_info: vocab_only       = 0
0.00.043.929 I print_info: n_ctx_train      = 2048
0.00.043.929 I print_info: n_embd           = 2048
0.00.043.929 I print_info: n_layer          = 24
0.00.043.933 I print_info: n_head           = 16
0.00.043.934 I print_info: n_head_kv        = 16
0.00.043.935 I print_info: n_rot            = 32
0.00.043.935 I print_info: n_swa            = 0
0.00.043.935 I print_info: n_embd_head_k    = 128
0.00.043.935 I print_info: n_embd_head_v    = 128
0.00.043.939 I print_info: n_gqa            = 1
0.00.043.939 I print_info: n_embd_k_gqa     = 2048
0.00.043.940 I print_info: n_embd_v_gqa     = 2048
0.00.043.941 I print_info: f_norm_eps       = 1.0e-05
0.00.043.941 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.941 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.941 I print_info: f_logit_scale    = 0.0e+00
0.00.043.942 I print_info: n_ff             = 8192
0.00.043.942 I print_info: n_expert         = 0
0.00.043.942 I print_info: n_expert_used    = 0
0.00.043.942 I print_info: causal attn      = 1
0.00.043.942 I print_info: pooling type     = 0
0.00.043.943 I print_info: rope type        = 2
0.00.043.947 I print_info: rope scaling     = linear
0.00.043.948 I print_info: freq_base_train  = 10000.0
0.00.043.948 I print_info: freq_scale_train = 1
0.00.043.949 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.949 I print_info: rope_finetuned   = unknown
0.00.043.949 I print_info: ssm_d_conv       = 0
0.00.043.950 I print_info: ssm_d_inner      = 0
0.00.043.950 I print_info: ssm_d_state      = 0
0.00.043.950 I print_info: ssm_dt_rank      = 0
0.00.043.951 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.951 I print_info: model type       = 1.4B
0.00.043.951 I print_info: model params     = 1.41 B
0.00.043.951 I print_info: general.name     = 1.4B
0.00.043.952 I print_info: vocab type       = BPE
0.00.043.952 I print_info: n_vocab          = 50304
0.00.043.952 I print_info: n_merges         = 50009
0.00.043.952 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.952 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.953 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.953 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.953 I print_info: LF token         = 187 ''
0.00.043.953 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.953 I print_info: max token length = 1024
0.00.043.954 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.956 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.966 I load_tensors: offloading output layer to GPU
0.00.628.967 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.998 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.629.000 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.630.414 I llama_init_from_model: n_seq_max     = 1
0.00.630.416 I llama_init_from_model: n_ctx         = 2048
0.00.630.417 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.630.417 I llama_init_from_model: n_batch       = 2048
0.00.630.418 I llama_init_from_model: n_ubatch      = 512
0.00.630.418 I llama_init_from_model: flash_attn    = 0
0.00.630.419 I llama_init_from_model: freq_base     = 10000.0
0.00.630.420 I llama_init_from_model: freq_scale    = 1
0.00.630.422 I ggml_metal_init: allocating
0.00.630.481 I ggml_metal_init: found device: Apple M4
0.00.630.493 I ggml_metal_init: picking default device: Apple M4
0.00.632.290 I ggml_metal_init: using embedded metal library
0.00.638.139 I ggml_metal_init: GPU name:   Apple M4
0.00.638.179 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.194 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.195 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.196 I ggml_metal_init: simdgroup reduction   = true
0.00.638.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.198 I ggml_metal_init: has residency sets    = true
0.00.638.198 I ggml_metal_init: has bfloat            = true
0.00.638.199 I ggml_metal_init: use bfloat            = true
0.00.638.205 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.956 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.440 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.716.448 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.716.473 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.442 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.720.443 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.720.444 I llama_init_from_model: graph nodes  = 967
0.00.720.444 I llama_init_from_model: graph splits = 2
0.00.720.449 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.561 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.562 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.830 I main: llama threadpool init, n_threads = 4
0.00.774.873 I 
0.00.774.899 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.900 I 
0.00.775.074 I sampler seed: 1234
0.00.775.078 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.089 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.089 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.089 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.455.117 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51263.54 tokens per second)
0.01.455.118 I llama_perf_context_print:        load time =     762.47 ms
0.01.455.118 I llama_perf_context_print: prompt eval time =      48.93 ms /     7 tokens (    6.99 ms per token,   143.07 tokens per second)
0.01.455.119 I llama_perf_context_print:        eval time =     628.23 ms /    63 runs   (    9.97 ms per token,   100.28 tokens per second)
0.01.455.119 I llama_perf_context_print:       total time =     680.97 ms /    70 tokens
0.01.455.320 I ggml_metal_free: deallocating

real	0m1.476s
user	0m0.112s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.275 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.320 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.601 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.607 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.611 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.612 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.612 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.613 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.613 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.614 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.614 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.614 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.617 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.617 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.617 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.618 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.620 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.139 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.141 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.142 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.142 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.143 I llama_model_loader: - type  f32:  194 tensors
0.00.026.143 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.143 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.144 I print_info: file format = GGUF V3 (latest)
0.00.026.144 I print_info: file type   = Q4_0
0.00.026.150 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.561 I load: special tokens cache size = 25
0.00.040.502 I load: token to piece cache size = 0.2984 MB
0.00.040.511 I print_info: arch             = gptneox
0.00.040.512 I print_info: vocab_only       = 0
0.00.040.512 I print_info: n_ctx_train      = 2048
0.00.040.512 I print_info: n_embd           = 2048
0.00.040.512 I print_info: n_layer          = 24
0.00.040.517 I print_info: n_head           = 16
0.00.040.518 I print_info: n_head_kv        = 16
0.00.040.519 I print_info: n_rot            = 32
0.00.040.519 I print_info: n_swa            = 0
0.00.040.519 I print_info: n_embd_head_k    = 128
0.00.040.519 I print_info: n_embd_head_v    = 128
0.00.040.520 I print_info: n_gqa            = 1
0.00.040.521 I print_info: n_embd_k_gqa     = 2048
0.00.040.521 I print_info: n_embd_v_gqa     = 2048
0.00.040.522 I print_info: f_norm_eps       = 1.0e-05
0.00.040.522 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.523 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.523 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.523 I print_info: f_logit_scale    = 0.0e+00
0.00.040.524 I print_info: n_ff             = 8192
0.00.040.524 I print_info: n_expert         = 0
0.00.040.524 I print_info: n_expert_used    = 0
0.00.040.525 I print_info: causal attn      = 1
0.00.040.525 I print_info: pooling type     = 0
0.00.040.525 I print_info: rope type        = 2
0.00.040.525 I print_info: rope scaling     = linear
0.00.040.526 I print_info: freq_base_train  = 10000.0
0.00.040.527 I print_info: freq_scale_train = 1
0.00.040.527 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.528 I print_info: rope_finetuned   = unknown
0.00.040.528 I print_info: ssm_d_conv       = 0
0.00.040.528 I print_info: ssm_d_inner      = 0
0.00.040.528 I print_info: ssm_d_state      = 0
0.00.040.529 I print_info: ssm_dt_rank      = 0
0.00.040.529 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.529 I print_info: model type       = 1.4B
0.00.040.529 I print_info: model params     = 1.41 B
0.00.040.529 I print_info: general.name     = 1.4B
0.00.040.530 I print_info: vocab type       = BPE
0.00.040.530 I print_info: n_vocab          = 50304
0.00.040.530 I print_info: n_merges         = 50009
0.00.040.530 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.531 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.531 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.531 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.531 I print_info: LF token         = 187 ''
0.00.040.531 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.532 I print_info: max token length = 1024
0.00.040.532 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.607.864 I load_tensors: offloading 24 repeating layers to GPU
0.00.607.878 I load_tensors: offloading output layer to GPU
0.00.607.879 I load_tensors: offloaded 25/25 layers to GPU
0.00.607.913 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.607.914 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.609.483 I llama_init_from_model: n_seq_max     = 1
0.00.609.487 I llama_init_from_model: n_ctx         = 128
0.00.609.487 I llama_init_from_model: n_ctx_per_seq = 128
0.00.609.488 I llama_init_from_model: n_batch       = 128
0.00.609.488 I llama_init_from_model: n_ubatch      = 128
0.00.609.489 I llama_init_from_model: flash_attn    = 0
0.00.609.491 I llama_init_from_model: freq_base     = 10000.0
0.00.609.491 I llama_init_from_model: freq_scale    = 1
0.00.609.492 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.609.495 I ggml_metal_init: allocating
0.00.609.578 I ggml_metal_init: found device: Apple M4
0.00.609.592 I ggml_metal_init: picking default device: Apple M4
0.00.611.476 I ggml_metal_init: using embedded metal library
0.00.617.555 I ggml_metal_init: GPU name:   Apple M4
0.00.617.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.564 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.564 I ggml_metal_init: simdgroup reduction   = true
0.00.617.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.565 I ggml_metal_init: has residency sets    = true
0.00.617.565 I ggml_metal_init: has bfloat            = true
0.00.617.565 I ggml_metal_init: use bfloat            = true
0.00.617.567 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.579 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.237 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.639.828 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.639.835 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.639.868 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.642.969 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.642.971 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.642.971 I llama_init_from_model: graph nodes  = 967
0.00.642.972 I llama_init_from_model: graph splits = 2
0.00.642.975 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.642.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.618 I 
0.00.671.694 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.702 I perplexity: tokenizing the input ..
0.00.677.783 I perplexity: tokenization took 6.079 ms
0.00.677.793 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.556 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.807.922 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.807.944 I llama_perf_context_print:        load time =     661.29 ms
0.00.807.945 I llama_perf_context_print: prompt eval time =     128.36 ms /   128 tokens (    1.00 ms per token,   997.18 tokens per second)
0.00.807.945 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.946 I llama_perf_context_print:       total time =     136.33 ms /   129 tokens
0.00.808.334 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.078s
sys	0m0.140s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.103 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.929 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.933 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.935 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.935 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.936 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.936 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.937 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.937 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.938 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.938 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.938 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.939 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.941 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.946 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.746 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.731 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.522 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.524 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.524 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.524 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.524 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.525 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.525 I llama_model_loader: - type  f32:  194 tensors
0.00.025.525 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.525 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.526 I print_info: file format = GGUF V3 (latest)
0.00.025.526 I print_info: file type   = Q4_1
0.00.025.527 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.327 I load: special tokens cache size = 25
0.00.039.294 I load: token to piece cache size = 0.2984 MB
0.00.039.296 I print_info: arch             = gptneox
0.00.039.297 I print_info: vocab_only       = 0
0.00.039.297 I print_info: n_ctx_train      = 2048
0.00.039.297 I print_info: n_embd           = 2048
0.00.039.297 I print_info: n_layer          = 24
0.00.039.300 I print_info: n_head           = 16
0.00.039.301 I print_info: n_head_kv        = 16
0.00.039.301 I print_info: n_rot            = 32
0.00.039.301 I print_info: n_swa            = 0
0.00.039.303 I print_info: n_embd_head_k    = 128
0.00.039.303 I print_info: n_embd_head_v    = 128
0.00.039.304 I print_info: n_gqa            = 1
0.00.039.304 I print_info: n_embd_k_gqa     = 2048
0.00.039.310 I print_info: n_embd_v_gqa     = 2048
0.00.039.312 I print_info: f_norm_eps       = 1.0e-05
0.00.039.313 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.313 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.313 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.313 I print_info: f_logit_scale    = 0.0e+00
0.00.039.314 I print_info: n_ff             = 8192
0.00.039.314 I print_info: n_expert         = 0
0.00.039.314 I print_info: n_expert_used    = 0
0.00.039.314 I print_info: causal attn      = 1
0.00.039.315 I print_info: pooling type     = 0
0.00.039.316 I print_info: rope type        = 2
0.00.039.318 I print_info: rope scaling     = linear
0.00.039.318 I print_info: freq_base_train  = 10000.0
0.00.039.318 I print_info: freq_scale_train = 1
0.00.039.319 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.319 I print_info: rope_finetuned   = unknown
0.00.039.323 I print_info: ssm_d_conv       = 0
0.00.039.324 I print_info: ssm_d_inner      = 0
0.00.039.324 I print_info: ssm_d_state      = 0
0.00.039.324 I print_info: ssm_dt_rank      = 0
0.00.039.325 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.325 I print_info: model type       = 1.4B
0.00.039.325 I print_info: model params     = 1.41 B
0.00.039.326 I print_info: general.name     = 1.4B
0.00.039.326 I print_info: vocab type       = BPE
0.00.039.326 I print_info: n_vocab          = 50304
0.00.039.327 I print_info: n_merges         = 50009
0.00.039.327 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.327 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.327 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.327 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.328 I print_info: LF token         = 187 ''
0.00.039.328 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.328 I print_info: max token length = 1024
0.00.039.329 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.644.728 I load_tensors: offloading 24 repeating layers to GPU
0.00.644.743 I load_tensors: offloading output layer to GPU
0.00.644.744 I load_tensors: offloaded 25/25 layers to GPU
0.00.644.778 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.644.779 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.646.471 I llama_init_from_model: n_seq_max     = 1
0.00.646.474 I llama_init_from_model: n_ctx         = 2048
0.00.646.474 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.646.475 I llama_init_from_model: n_batch       = 2048
0.00.646.475 I llama_init_from_model: n_ubatch      = 512
0.00.646.476 I llama_init_from_model: flash_attn    = 0
0.00.646.478 I llama_init_from_model: freq_base     = 10000.0
0.00.646.479 I llama_init_from_model: freq_scale    = 1
0.00.646.481 I ggml_metal_init: allocating
0.00.646.556 I ggml_metal_init: found device: Apple M4
0.00.646.569 I ggml_metal_init: picking default device: Apple M4
0.00.648.445 I ggml_metal_init: using embedded metal library
0.00.654.982 I ggml_metal_init: GPU name:   Apple M4
0.00.654.986 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.654.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.654.988 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.654.989 I ggml_metal_init: simdgroup reduction   = true
0.00.654.989 I ggml_metal_init: simdgroup matrix mul. = true
0.00.654.989 I ggml_metal_init: has residency sets    = true
0.00.654.989 I ggml_metal_init: has bfloat            = true
0.00.654.989 I ggml_metal_init: use bfloat            = true
0.00.654.990 I ggml_metal_init: hasUnifiedMemory      = true
0.00.654.992 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.718 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.726.728 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.726.738 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.726.759 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.730.841 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.730.843 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.730.844 I llama_init_from_model: graph nodes  = 967
0.00.730.844 I llama_init_from_model: graph splits = 2
0.00.730.848 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.730.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.730.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.016 I main: llama threadpool init, n_threads = 4
0.00.784.059 I 
0.00.784.081 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.083 I 
0.00.784.257 I sampler seed: 1234
0.00.784.262 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.273 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.273 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.273 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.522.818 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.01.522.819 I llama_perf_context_print:        load time =     774.21 ms
0.01.522.820 I llama_perf_context_print: prompt eval time =      49.09 ms /     7 tokens (    7.01 ms per token,   142.61 tokens per second)
0.01.522.820 I llama_perf_context_print:        eval time =     686.64 ms /    63 runs   (   10.90 ms per token,    91.75 tokens per second)
0.01.522.821 I llama_perf_context_print:       total time =     739.50 ms /    70 tokens
0.01.523.039 I ggml_metal_free: deallocating

real	0m1.539s
user	0m0.108s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.816 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.182 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.189 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.189 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.193 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.194 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.194 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.195 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.195 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.196 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.196 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.197 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.199 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.199 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.960 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.017 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.727 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.728 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.728 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.729 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.729 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.729 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.730 I llama_model_loader: - type  f32:  194 tensors
0.00.024.730 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.731 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.731 I print_info: file format = GGUF V3 (latest)
0.00.024.732 I print_info: file type   = Q4_1
0.00.024.733 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.622 I load: special tokens cache size = 25
0.00.038.772 I load: token to piece cache size = 0.2984 MB
0.00.038.776 I print_info: arch             = gptneox
0.00.038.777 I print_info: vocab_only       = 0
0.00.038.777 I print_info: n_ctx_train      = 2048
0.00.038.777 I print_info: n_embd           = 2048
0.00.038.777 I print_info: n_layer          = 24
0.00.038.782 I print_info: n_head           = 16
0.00.038.783 I print_info: n_head_kv        = 16
0.00.038.783 I print_info: n_rot            = 32
0.00.038.783 I print_info: n_swa            = 0
0.00.038.784 I print_info: n_embd_head_k    = 128
0.00.038.785 I print_info: n_embd_head_v    = 128
0.00.038.786 I print_info: n_gqa            = 1
0.00.038.787 I print_info: n_embd_k_gqa     = 2048
0.00.038.787 I print_info: n_embd_v_gqa     = 2048
0.00.038.788 I print_info: f_norm_eps       = 1.0e-05
0.00.038.788 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.788 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.789 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.789 I print_info: f_logit_scale    = 0.0e+00
0.00.038.789 I print_info: n_ff             = 8192
0.00.038.789 I print_info: n_expert         = 0
0.00.038.790 I print_info: n_expert_used    = 0
0.00.038.790 I print_info: causal attn      = 1
0.00.038.792 I print_info: pooling type     = 0
0.00.038.792 I print_info: rope type        = 2
0.00.038.792 I print_info: rope scaling     = linear
0.00.038.793 I print_info: freq_base_train  = 10000.0
0.00.038.799 I print_info: freq_scale_train = 1
0.00.038.799 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.799 I print_info: rope_finetuned   = unknown
0.00.038.799 I print_info: ssm_d_conv       = 0
0.00.038.801 I print_info: ssm_d_inner      = 0
0.00.038.801 I print_info: ssm_d_state      = 0
0.00.038.801 I print_info: ssm_dt_rank      = 0
0.00.038.801 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.802 I print_info: model type       = 1.4B
0.00.038.802 I print_info: model params     = 1.41 B
0.00.038.802 I print_info: general.name     = 1.4B
0.00.038.802 I print_info: vocab type       = BPE
0.00.038.803 I print_info: n_vocab          = 50304
0.00.038.803 I print_info: n_merges         = 50009
0.00.038.803 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.803 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.803 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.803 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.804 I print_info: LF token         = 187 ''
0.00.038.804 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.804 I print_info: max token length = 1024
0.00.038.805 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.642.505 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.520 I load_tensors: offloading output layer to GPU
0.00.642.521 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.556 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.642.557 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.644.281 I llama_init_from_model: n_seq_max     = 1
0.00.644.283 I llama_init_from_model: n_ctx         = 128
0.00.644.284 I llama_init_from_model: n_ctx_per_seq = 128
0.00.644.284 I llama_init_from_model: n_batch       = 128
0.00.644.284 I llama_init_from_model: n_ubatch      = 128
0.00.644.285 I llama_init_from_model: flash_attn    = 0
0.00.644.287 I llama_init_from_model: freq_base     = 10000.0
0.00.644.287 I llama_init_from_model: freq_scale    = 1
0.00.644.288 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.644.290 I ggml_metal_init: allocating
0.00.644.371 I ggml_metal_init: found device: Apple M4
0.00.644.385 I ggml_metal_init: picking default device: Apple M4
0.00.646.237 I ggml_metal_init: using embedded metal library
0.00.652.936 I ggml_metal_init: GPU name:   Apple M4
0.00.652.942 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.943 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.944 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.948 I ggml_metal_init: simdgroup reduction   = true
0.00.652.948 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.948 I ggml_metal_init: has residency sets    = true
0.00.652.948 I ggml_metal_init: has bfloat            = true
0.00.652.949 I ggml_metal_init: use bfloat            = true
0.00.652.950 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.953 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.481 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.674.029 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.674.033 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.674.064 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.301 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.677.303 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.677.303 I llama_init_from_model: graph nodes  = 967
0.00.677.303 I llama_init_from_model: graph splits = 2
0.00.677.306 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.677.307 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.360 I 
0.00.702.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.441 I perplexity: tokenizing the input ..
0.00.710.058 I perplexity: tokenization took 7.614 ms
0.00.710.066 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.846.429 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.847.781 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.847.805 I llama_perf_context_print:        load time =     693.53 ms
0.00.847.806 I llama_perf_context_print: prompt eval time =     135.50 ms /   128 tokens (    1.06 ms per token,   944.67 tokens per second)
0.00.847.807 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.807 I llama_perf_context_print:       total time =     145.45 ms /   129 tokens
0.00.848.209 I ggml_metal_free: deallocating

real	0m0.862s
user	0m0.080s
sys	0m0.122s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.114 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.646 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.648 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.649 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.653 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.654 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.655 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.655 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.656 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.656 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.656 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.657 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.658 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.662 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.365 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.364 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.044 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.046 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.046 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.046 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.046 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.047 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.047 I llama_model_loader: - type  f32:  194 tensors
0.00.026.047 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.047 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.048 I print_info: file format = GGUF V3 (latest)
0.00.026.048 I print_info: file type   = Q5_0
0.00.026.050 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.749 I load: special tokens cache size = 25
0.00.039.845 I load: token to piece cache size = 0.2984 MB
0.00.039.848 I print_info: arch             = gptneox
0.00.039.848 I print_info: vocab_only       = 0
0.00.039.848 I print_info: n_ctx_train      = 2048
0.00.039.848 I print_info: n_embd           = 2048
0.00.039.849 I print_info: n_layer          = 24
0.00.039.852 I print_info: n_head           = 16
0.00.039.853 I print_info: n_head_kv        = 16
0.00.039.853 I print_info: n_rot            = 32
0.00.039.853 I print_info: n_swa            = 0
0.00.039.855 I print_info: n_embd_head_k    = 128
0.00.039.855 I print_info: n_embd_head_v    = 128
0.00.039.856 I print_info: n_gqa            = 1
0.00.039.857 I print_info: n_embd_k_gqa     = 2048
0.00.039.862 I print_info: n_embd_v_gqa     = 2048
0.00.039.863 I print_info: f_norm_eps       = 1.0e-05
0.00.039.863 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.863 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.863 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.864 I print_info: f_logit_scale    = 0.0e+00
0.00.039.865 I print_info: n_ff             = 8192
0.00.039.865 I print_info: n_expert         = 0
0.00.039.865 I print_info: n_expert_used    = 0
0.00.039.865 I print_info: causal attn      = 1
0.00.039.865 I print_info: pooling type     = 0
0.00.039.867 I print_info: rope type        = 2
0.00.039.867 I print_info: rope scaling     = linear
0.00.039.868 I print_info: freq_base_train  = 10000.0
0.00.039.868 I print_info: freq_scale_train = 1
0.00.039.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.868 I print_info: rope_finetuned   = unknown
0.00.039.871 I print_info: ssm_d_conv       = 0
0.00.039.871 I print_info: ssm_d_inner      = 0
0.00.039.871 I print_info: ssm_d_state      = 0
0.00.039.872 I print_info: ssm_dt_rank      = 0
0.00.039.872 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.872 I print_info: model type       = 1.4B
0.00.039.872 I print_info: model params     = 1.41 B
0.00.039.872 I print_info: general.name     = 1.4B
0.00.039.873 I print_info: vocab type       = BPE
0.00.039.873 I print_info: n_vocab          = 50304
0.00.039.873 I print_info: n_merges         = 50009
0.00.039.874 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.874 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.875 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.875 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.875 I print_info: LF token         = 187 ''
0.00.039.876 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.876 I print_info: max token length = 1024
0.00.039.876 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.685.639 I load_tensors: offloading 24 repeating layers to GPU
0.00.685.655 I load_tensors: offloading output layer to GPU
0.00.685.656 I load_tensors: offloaded 25/25 layers to GPU
0.00.685.689 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.685.690 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.687.482 I llama_init_from_model: n_seq_max     = 1
0.00.687.484 I llama_init_from_model: n_ctx         = 2048
0.00.687.485 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.687.485 I llama_init_from_model: n_batch       = 2048
0.00.687.486 I llama_init_from_model: n_ubatch      = 512
0.00.687.486 I llama_init_from_model: flash_attn    = 0
0.00.687.489 I llama_init_from_model: freq_base     = 10000.0
0.00.687.490 I llama_init_from_model: freq_scale    = 1
0.00.687.492 I ggml_metal_init: allocating
0.00.687.563 I ggml_metal_init: found device: Apple M4
0.00.687.576 I ggml_metal_init: picking default device: Apple M4
0.00.689.479 I ggml_metal_init: using embedded metal library
0.00.696.185 I ggml_metal_init: GPU name:   Apple M4
0.00.696.190 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.192 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.192 I ggml_metal_init: simdgroup reduction   = true
0.00.696.192 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.193 I ggml_metal_init: has residency sets    = true
0.00.696.193 I ggml_metal_init: has bfloat            = true
0.00.696.193 I ggml_metal_init: use bfloat            = true
0.00.696.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.196 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.279 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.767.975 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.767.983 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.768.009 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.772.934 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.772.936 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.772.936 I llama_init_from_model: graph nodes  = 967
0.00.772.937 I llama_init_from_model: graph splits = 2
0.00.772.944 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.773.067 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.773.068 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.829.964 I main: llama threadpool init, n_threads = 4
0.00.830.011 I 
0.00.830.034 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.830.035 I 
0.00.830.209 I sampler seed: 1234
0.00.830.213 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.830.234 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.830.235 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.830.235 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.625.694 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51976.57 tokens per second)
0.01.625.694 I llama_perf_context_print:        load time =     819.15 ms
0.01.625.695 I llama_perf_context_print: prompt eval time =      42.86 ms /     7 tokens (    6.12 ms per token,   163.32 tokens per second)
0.01.625.696 I llama_perf_context_print:        eval time =     749.70 ms /    63 runs   (   11.90 ms per token,    84.03 tokens per second)
0.01.625.696 I llama_perf_context_print:       total time =     796.43 ms /    70 tokens
0.01.625.976 I ggml_metal_free: deallocating

real	0m1.643s
user	0m0.108s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.124 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.154 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.495 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.501 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.502 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.508 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.508 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.511 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.512 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.513 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.513 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.514 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.516 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.518 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.060 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.349 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.522 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.522 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.523 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.524 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.032.525 I llama_model_loader: - type  f32:  194 tensors
0.00.032.525 I llama_model_loader: - type q5_0:   97 tensors
0.00.032.526 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.526 I print_info: file format = GGUF V3 (latest)
0.00.032.527 I print_info: file type   = Q5_0
0.00.032.528 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.049.721 I load: special tokens cache size = 25
0.00.060.225 I load: token to piece cache size = 0.2984 MB
0.00.060.228 I print_info: arch             = gptneox
0.00.060.228 I print_info: vocab_only       = 0
0.00.060.228 I print_info: n_ctx_train      = 2048
0.00.060.228 I print_info: n_embd           = 2048
0.00.060.229 I print_info: n_layer          = 24
0.00.060.231 I print_info: n_head           = 16
0.00.060.232 I print_info: n_head_kv        = 16
0.00.060.232 I print_info: n_rot            = 32
0.00.060.233 I print_info: n_swa            = 0
0.00.060.233 I print_info: n_embd_head_k    = 128
0.00.060.233 I print_info: n_embd_head_v    = 128
0.00.060.234 I print_info: n_gqa            = 1
0.00.060.234 I print_info: n_embd_k_gqa     = 2048
0.00.060.235 I print_info: n_embd_v_gqa     = 2048
0.00.060.235 I print_info: f_norm_eps       = 1.0e-05
0.00.060.236 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.239 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.239 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.240 I print_info: f_logit_scale    = 0.0e+00
0.00.060.240 I print_info: n_ff             = 8192
0.00.060.241 I print_info: n_expert         = 0
0.00.060.241 I print_info: n_expert_used    = 0
0.00.060.241 I print_info: causal attn      = 1
0.00.060.241 I print_info: pooling type     = 0
0.00.060.241 I print_info: rope type        = 2
0.00.060.241 I print_info: rope scaling     = linear
0.00.060.242 I print_info: freq_base_train  = 10000.0
0.00.060.242 I print_info: freq_scale_train = 1
0.00.060.242 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.242 I print_info: rope_finetuned   = unknown
0.00.060.243 I print_info: ssm_d_conv       = 0
0.00.060.245 I print_info: ssm_d_inner      = 0
0.00.060.245 I print_info: ssm_d_state      = 0
0.00.060.245 I print_info: ssm_dt_rank      = 0
0.00.060.245 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.245 I print_info: model type       = 1.4B
0.00.060.246 I print_info: model params     = 1.41 B
0.00.060.246 I print_info: general.name     = 1.4B
0.00.060.246 I print_info: vocab type       = BPE
0.00.060.246 I print_info: n_vocab          = 50304
0.00.060.247 I print_info: n_merges         = 50009
0.00.060.247 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.247 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.247 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.247 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.248 I print_info: LF token         = 187 ''
0.00.060.248 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.248 I print_info: max token length = 1024
0.00.060.249 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.716.000 I load_tensors: offloading 24 repeating layers to GPU
0.00.716.010 I load_tensors: offloading output layer to GPU
0.00.716.012 I load_tensors: offloaded 25/25 layers to GPU
0.00.716.045 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.716.046 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.717.824 I llama_init_from_model: n_seq_max     = 1
0.00.717.831 I llama_init_from_model: n_ctx         = 128
0.00.717.832 I llama_init_from_model: n_ctx_per_seq = 128
0.00.717.832 I llama_init_from_model: n_batch       = 128
0.00.717.833 I llama_init_from_model: n_ubatch      = 128
0.00.717.833 I llama_init_from_model: flash_attn    = 0
0.00.717.834 I llama_init_from_model: freq_base     = 10000.0
0.00.717.835 I llama_init_from_model: freq_scale    = 1
0.00.717.835 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.717.841 I ggml_metal_init: allocating
0.00.717.930 I ggml_metal_init: found device: Apple M4
0.00.717.944 I ggml_metal_init: picking default device: Apple M4
0.00.719.747 I ggml_metal_init: using embedded metal library
0.00.726.210 I ggml_metal_init: GPU name:   Apple M4
0.00.726.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.726.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.726.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.726.216 I ggml_metal_init: simdgroup reduction   = true
0.00.726.216 I ggml_metal_init: simdgroup matrix mul. = true
0.00.726.216 I ggml_metal_init: has residency sets    = true
0.00.726.216 I ggml_metal_init: has bfloat            = true
0.00.726.217 I ggml_metal_init: use bfloat            = true
0.00.726.218 I ggml_metal_init: hasUnifiedMemory      = true
0.00.726.219 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.743.396 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.746.935 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.746.938 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.746.963 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.568 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.750.570 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.750.570 I llama_init_from_model: graph nodes  = 967
0.00.750.571 I llama_init_from_model: graph splits = 2
0.00.750.574 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.750.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.480 I 
0.00.778.556 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.564 I perplexity: tokenizing the input ..
0.00.785.911 I perplexity: tokenization took 7.345 ms
0.00.785.918 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.922.415 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.923.842 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.923.862 I llama_perf_context_print:        load time =     768.32 ms
0.00.923.863 I llama_perf_context_print: prompt eval time =     135.51 ms /   128 tokens (    1.06 ms per token,   944.60 tokens per second)
0.00.923.866 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.923.867 I llama_perf_context_print:       total time =     145.39 ms /   129 tokens
0.00.924.216 I ggml_metal_free: deallocating

real	0m0.986s
user	0m0.098s
sys	0m0.139s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.673 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.162 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.164 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.164 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.165 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.165 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.165 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.166 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.167 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.167 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.167 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.170 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.172 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.172 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.172 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.974 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.969 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.785 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.786 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.786 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.787 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.787 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.787 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.788 I llama_model_loader: - type  f32:  194 tensors
0.00.025.788 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.789 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.789 I print_info: file format = GGUF V3 (latest)
0.00.025.790 I print_info: file type   = Q5_1
0.00.025.791 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.552 I load: special tokens cache size = 25
0.00.039.545 I load: token to piece cache size = 0.2984 MB
0.00.039.547 I print_info: arch             = gptneox
0.00.039.548 I print_info: vocab_only       = 0
0.00.039.548 I print_info: n_ctx_train      = 2048
0.00.039.548 I print_info: n_embd           = 2048
0.00.039.548 I print_info: n_layer          = 24
0.00.039.551 I print_info: n_head           = 16
0.00.039.552 I print_info: n_head_kv        = 16
0.00.039.552 I print_info: n_rot            = 32
0.00.039.552 I print_info: n_swa            = 0
0.00.039.552 I print_info: n_embd_head_k    = 128
0.00.039.554 I print_info: n_embd_head_v    = 128
0.00.039.555 I print_info: n_gqa            = 1
0.00.039.555 I print_info: n_embd_k_gqa     = 2048
0.00.039.556 I print_info: n_embd_v_gqa     = 2048
0.00.039.557 I print_info: f_norm_eps       = 1.0e-05
0.00.039.557 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.557 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.558 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.558 I print_info: f_logit_scale    = 0.0e+00
0.00.039.558 I print_info: n_ff             = 8192
0.00.039.559 I print_info: n_expert         = 0
0.00.039.559 I print_info: n_expert_used    = 0
0.00.039.559 I print_info: causal attn      = 1
0.00.039.559 I print_info: pooling type     = 0
0.00.039.559 I print_info: rope type        = 2
0.00.039.560 I print_info: rope scaling     = linear
0.00.039.561 I print_info: freq_base_train  = 10000.0
0.00.039.561 I print_info: freq_scale_train = 1
0.00.039.561 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.561 I print_info: rope_finetuned   = unknown
0.00.039.561 I print_info: ssm_d_conv       = 0
0.00.039.562 I print_info: ssm_d_inner      = 0
0.00.039.562 I print_info: ssm_d_state      = 0
0.00.039.562 I print_info: ssm_dt_rank      = 0
0.00.039.563 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.563 I print_info: model type       = 1.4B
0.00.039.564 I print_info: model params     = 1.41 B
0.00.039.564 I print_info: general.name     = 1.4B
0.00.039.565 I print_info: vocab type       = BPE
0.00.039.565 I print_info: n_vocab          = 50304
0.00.039.565 I print_info: n_merges         = 50009
0.00.039.565 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.566 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.566 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.566 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.566 I print_info: LF token         = 187 ''
0.00.039.566 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.567 I print_info: max token length = 1024
0.00.039.568 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.616.447 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.451 I load_tensors: offloading output layer to GPU
0.00.616.453 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.477 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.616.479 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.617.899 I llama_init_from_model: n_seq_max     = 1
0.00.617.901 I llama_init_from_model: n_ctx         = 2048
0.00.617.901 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.617.902 I llama_init_from_model: n_batch       = 2048
0.00.617.902 I llama_init_from_model: n_ubatch      = 512
0.00.617.903 I llama_init_from_model: flash_attn    = 0
0.00.617.904 I llama_init_from_model: freq_base     = 10000.0
0.00.617.904 I llama_init_from_model: freq_scale    = 1
0.00.617.906 I ggml_metal_init: allocating
0.00.617.922 I ggml_metal_init: found device: Apple M4
0.00.617.931 I ggml_metal_init: picking default device: Apple M4
0.00.619.436 I ggml_metal_init: using embedded metal library
0.00.625.359 I ggml_metal_init: GPU name:   Apple M4
0.00.625.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.363 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.364 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.365 I ggml_metal_init: simdgroup reduction   = true
0.00.625.365 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.365 I ggml_metal_init: has residency sets    = true
0.00.625.365 I ggml_metal_init: has bfloat            = true
0.00.625.366 I ggml_metal_init: use bfloat            = true
0.00.625.367 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.631 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.737 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.694.743 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.694.811 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.928 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.698.930 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.698.930 I llama_init_from_model: graph nodes  = 967
0.00.698.931 I llama_init_from_model: graph splits = 2
0.00.698.936 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.699.061 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.699.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.147 I main: llama threadpool init, n_threads = 4
0.00.755.193 I 
0.00.755.215 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.219 I 
0.00.755.370 I sampler seed: 1234
0.00.755.374 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.755.385 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.755.385 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.755.385 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.594.846 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51523.95 tokens per second)
0.01.594.846 I llama_perf_context_print:        load time =     745.78 ms
0.01.594.847 I llama_perf_context_print: prompt eval time =      42.20 ms /     7 tokens (    6.03 ms per token,   165.88 tokens per second)
0.01.594.848 I llama_perf_context_print:        eval time =     794.24 ms /    63 runs   (   12.61 ms per token,    79.32 tokens per second)
0.01.594.848 I llama_perf_context_print:       total time =     840.39 ms /    70 tokens
0.01.595.115 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.108s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.690 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.177 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.020.182 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.184 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.185 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.190 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.191 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.193 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.195 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.195 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.196 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.196 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.197 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.201 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.202 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.202 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.833 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.783 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.366 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.367 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.368 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.368 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.368 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.369 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.369 I llama_model_loader: - type  f32:  194 tensors
0.00.028.370 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.370 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.370 I print_info: file format = GGUF V3 (latest)
0.00.028.371 I print_info: file type   = Q5_1
0.00.028.373 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.036.189 I load: special tokens cache size = 25
0.00.042.311 I load: token to piece cache size = 0.2984 MB
0.00.042.315 I print_info: arch             = gptneox
0.00.042.315 I print_info: vocab_only       = 0
0.00.042.315 I print_info: n_ctx_train      = 2048
0.00.042.316 I print_info: n_embd           = 2048
0.00.042.316 I print_info: n_layer          = 24
0.00.042.320 I print_info: n_head           = 16
0.00.042.321 I print_info: n_head_kv        = 16
0.00.042.321 I print_info: n_rot            = 32
0.00.042.325 I print_info: n_swa            = 0
0.00.042.325 I print_info: n_embd_head_k    = 128
0.00.042.326 I print_info: n_embd_head_v    = 128
0.00.042.326 I print_info: n_gqa            = 1
0.00.042.328 I print_info: n_embd_k_gqa     = 2048
0.00.042.328 I print_info: n_embd_v_gqa     = 2048
0.00.042.329 I print_info: f_norm_eps       = 1.0e-05
0.00.042.329 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.329 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.329 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.330 I print_info: f_logit_scale    = 0.0e+00
0.00.042.330 I print_info: n_ff             = 8192
0.00.042.330 I print_info: n_expert         = 0
0.00.042.331 I print_info: n_expert_used    = 0
0.00.042.331 I print_info: causal attn      = 1
0.00.042.331 I print_info: pooling type     = 0
0.00.042.331 I print_info: rope type        = 2
0.00.042.331 I print_info: rope scaling     = linear
0.00.042.332 I print_info: freq_base_train  = 10000.0
0.00.042.332 I print_info: freq_scale_train = 1
0.00.042.332 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.332 I print_info: rope_finetuned   = unknown
0.00.042.332 I print_info: ssm_d_conv       = 0
0.00.042.332 I print_info: ssm_d_inner      = 0
0.00.042.333 I print_info: ssm_d_state      = 0
0.00.042.333 I print_info: ssm_dt_rank      = 0
0.00.042.333 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.334 I print_info: model type       = 1.4B
0.00.042.334 I print_info: model params     = 1.41 B
0.00.042.335 I print_info: general.name     = 1.4B
0.00.042.335 I print_info: vocab type       = BPE
0.00.042.335 I print_info: n_vocab          = 50304
0.00.042.335 I print_info: n_merges         = 50009
0.00.042.335 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.336 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.336 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.336 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.336 I print_info: LF token         = 187 ''
0.00.042.336 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.336 I print_info: max token length = 1024
0.00.042.337 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.620.946 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.964 I load_tensors: offloading output layer to GPU
0.00.620.965 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.000 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.621.002 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.622.726 I llama_init_from_model: n_seq_max     = 1
0.00.622.728 I llama_init_from_model: n_ctx         = 128
0.00.622.728 I llama_init_from_model: n_ctx_per_seq = 128
0.00.622.728 I llama_init_from_model: n_batch       = 128
0.00.622.729 I llama_init_from_model: n_ubatch      = 128
0.00.622.729 I llama_init_from_model: flash_attn    = 0
0.00.622.730 I llama_init_from_model: freq_base     = 10000.0
0.00.622.731 I llama_init_from_model: freq_scale    = 1
0.00.622.732 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.733 I ggml_metal_init: allocating
0.00.622.766 I ggml_metal_init: found device: Apple M4
0.00.622.777 I ggml_metal_init: picking default device: Apple M4
0.00.624.207 I ggml_metal_init: using embedded metal library
0.00.630.337 I ggml_metal_init: GPU name:   Apple M4
0.00.630.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.343 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.344 I ggml_metal_init: simdgroup reduction   = true
0.00.630.344 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.344 I ggml_metal_init: has residency sets    = true
0.00.630.344 I ggml_metal_init: has bfloat            = true
0.00.630.345 I ggml_metal_init: use bfloat            = true
0.00.630.346 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.356 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.770 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.243 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.651.251 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.651.283 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.654.351 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.654.353 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.654.354 I llama_init_from_model: graph nodes  = 967
0.00.654.354 I llama_init_from_model: graph splits = 2
0.00.654.358 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.654.358 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.663 I 
0.00.684.744 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.752 I perplexity: tokenizing the input ..
0.00.691.762 I perplexity: tokenization took 7.008 ms
0.00.691.767 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.276 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.839.605 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.839.627 I llama_perf_context_print:        load time =     675.96 ms
0.00.839.628 I llama_perf_context_print: prompt eval time =     145.96 ms /   128 tokens (    1.14 ms per token,   876.94 tokens per second)
0.00.839.628 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.839.629 I llama_perf_context_print:       total time =     154.97 ms /   129 tokens
0.00.840.010 I ggml_metal_free: deallocating

real	0m0.854s
user	0m0.078s
sys	0m0.131s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.948 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.412 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.418 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.419 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.420 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.420 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.421 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.421 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.422 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.423 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.423 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.423 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.424 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.424 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.424 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.426 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.426 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.426 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.183 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.177 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.811 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.812 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.812 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.812 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.813 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.813 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.813 I llama_model_loader: - type  f32:  194 tensors
0.00.024.814 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.814 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.814 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.815 I print_info: file format = GGUF V3 (latest)
0.00.024.815 I print_info: file type   = Q2_K - Medium
0.00.024.816 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.503 I load: special tokens cache size = 25
0.00.038.490 I load: token to piece cache size = 0.2984 MB
0.00.038.493 I print_info: arch             = gptneox
0.00.038.493 I print_info: vocab_only       = 0
0.00.038.493 I print_info: n_ctx_train      = 2048
0.00.038.493 I print_info: n_embd           = 2048
0.00.038.493 I print_info: n_layer          = 24
0.00.038.496 I print_info: n_head           = 16
0.00.038.496 I print_info: n_head_kv        = 16
0.00.038.497 I print_info: n_rot            = 32
0.00.038.497 I print_info: n_swa            = 0
0.00.038.497 I print_info: n_embd_head_k    = 128
0.00.038.497 I print_info: n_embd_head_v    = 128
0.00.038.498 I print_info: n_gqa            = 1
0.00.038.498 I print_info: n_embd_k_gqa     = 2048
0.00.038.502 I print_info: n_embd_v_gqa     = 2048
0.00.038.502 I print_info: f_norm_eps       = 1.0e-05
0.00.038.502 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.503 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.503 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.503 I print_info: f_logit_scale    = 0.0e+00
0.00.038.504 I print_info: n_ff             = 8192
0.00.038.504 I print_info: n_expert         = 0
0.00.038.504 I print_info: n_expert_used    = 0
0.00.038.504 I print_info: causal attn      = 1
0.00.038.504 I print_info: pooling type     = 0
0.00.038.506 I print_info: rope type        = 2
0.00.038.506 I print_info: rope scaling     = linear
0.00.038.507 I print_info: freq_base_train  = 10000.0
0.00.038.507 I print_info: freq_scale_train = 1
0.00.038.507 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.507 I print_info: rope_finetuned   = unknown
0.00.038.507 I print_info: ssm_d_conv       = 0
0.00.038.507 I print_info: ssm_d_inner      = 0
0.00.038.508 I print_info: ssm_d_state      = 0
0.00.038.508 I print_info: ssm_dt_rank      = 0
0.00.038.508 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.508 I print_info: model type       = 1.4B
0.00.038.508 I print_info: model params     = 1.41 B
0.00.038.509 I print_info: general.name     = 1.4B
0.00.038.509 I print_info: vocab type       = BPE
0.00.038.509 I print_info: n_vocab          = 50304
0.00.038.509 I print_info: n_merges         = 50009
0.00.038.510 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.510 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.510 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.510 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: LF token         = 187 ''
0.00.038.515 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: max token length = 1024
0.00.038.516 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.340.897 I load_tensors: offloading 24 repeating layers to GPU
0.00.340.912 I load_tensors: offloading output layer to GPU
0.00.340.913 I load_tensors: offloaded 25/25 layers to GPU
0.00.340.948 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.340.950 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.342.581 I llama_init_from_model: n_seq_max     = 1
0.00.342.586 I llama_init_from_model: n_ctx         = 2048
0.00.342.586 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.342.587 I llama_init_from_model: n_batch       = 2048
0.00.342.587 I llama_init_from_model: n_ubatch      = 512
0.00.342.587 I llama_init_from_model: flash_attn    = 0
0.00.342.589 I llama_init_from_model: freq_base     = 10000.0
0.00.342.590 I llama_init_from_model: freq_scale    = 1
0.00.342.592 I ggml_metal_init: allocating
0.00.342.700 I ggml_metal_init: found device: Apple M4
0.00.342.713 I ggml_metal_init: picking default device: Apple M4
0.00.344.590 I ggml_metal_init: using embedded metal library
0.00.350.159 I ggml_metal_init: GPU name:   Apple M4
0.00.350.172 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.350.173 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.350.174 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.350.174 I ggml_metal_init: simdgroup reduction   = true
0.00.350.175 I ggml_metal_init: simdgroup matrix mul. = true
0.00.350.175 I ggml_metal_init: has residency sets    = true
0.00.350.175 I ggml_metal_init: has bfloat            = true
0.00.350.175 I ggml_metal_init: use bfloat            = true
0.00.350.180 I ggml_metal_init: hasUnifiedMemory      = true
0.00.350.185 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.371.756 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.432.217 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.432.226 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.432.253 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.437.103 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.437.105 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.437.106 I llama_init_from_model: graph nodes  = 967
0.00.437.106 I llama_init_from_model: graph splits = 2
0.00.437.110 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.437.235 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.437.236 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.498.874 I main: llama threadpool init, n_threads = 4
0.00.498.917 I 
0.00.498.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.498.941 I 
0.00.499.120 I sampler seed: 1234
0.00.499.124 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.499.135 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.499.137 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.499.137 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.182.399 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56393.96 tokens per second)
0.01.182.400 I llama_perf_context_print:        load time =     488.22 ms
0.01.182.400 I llama_perf_context_print: prompt eval time =      44.30 ms /     7 tokens (    6.33 ms per token,   158.02 tokens per second)
0.01.182.401 I llama_perf_context_print:        eval time =     636.17 ms /    63 runs   (   10.10 ms per token,    99.03 tokens per second)
0.01.182.402 I llama_perf_context_print:       total time =     684.23 ms /    70 tokens
0.01.182.627 I ggml_metal_free: deallocating

real	0m1.202s
user	0m0.112s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.142 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.026 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.032 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.035 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.035 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.036 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.036 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.036 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.037 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.038 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.038 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.039 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.040 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.040 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.042 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.648 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.604 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.194 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.195 I llama_model_loader: - type  f32:  194 tensors
0.00.025.195 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.195 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.195 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.196 I print_info: file format = GGUF V3 (latest)
0.00.025.196 I print_info: file type   = Q2_K - Medium
0.00.025.197 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.915 I load: special tokens cache size = 25
0.00.038.937 I load: token to piece cache size = 0.2984 MB
0.00.038.940 I print_info: arch             = gptneox
0.00.038.940 I print_info: vocab_only       = 0
0.00.038.941 I print_info: n_ctx_train      = 2048
0.00.038.941 I print_info: n_embd           = 2048
0.00.038.941 I print_info: n_layer          = 24
0.00.038.945 I print_info: n_head           = 16
0.00.038.946 I print_info: n_head_kv        = 16
0.00.038.946 I print_info: n_rot            = 32
0.00.038.946 I print_info: n_swa            = 0
0.00.038.946 I print_info: n_embd_head_k    = 128
0.00.038.947 I print_info: n_embd_head_v    = 128
0.00.038.947 I print_info: n_gqa            = 1
0.00.038.948 I print_info: n_embd_k_gqa     = 2048
0.00.038.949 I print_info: n_embd_v_gqa     = 2048
0.00.038.951 I print_info: f_norm_eps       = 1.0e-05
0.00.038.951 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.951 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.952 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.952 I print_info: f_logit_scale    = 0.0e+00
0.00.038.954 I print_info: n_ff             = 8192
0.00.038.954 I print_info: n_expert         = 0
0.00.038.955 I print_info: n_expert_used    = 0
0.00.038.955 I print_info: causal attn      = 1
0.00.038.955 I print_info: pooling type     = 0
0.00.038.955 I print_info: rope type        = 2
0.00.038.955 I print_info: rope scaling     = linear
0.00.038.956 I print_info: freq_base_train  = 10000.0
0.00.038.956 I print_info: freq_scale_train = 1
0.00.038.956 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.956 I print_info: rope_finetuned   = unknown
0.00.038.957 I print_info: ssm_d_conv       = 0
0.00.038.957 I print_info: ssm_d_inner      = 0
0.00.038.957 I print_info: ssm_d_state      = 0
0.00.038.957 I print_info: ssm_dt_rank      = 0
0.00.038.957 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.957 I print_info: model type       = 1.4B
0.00.038.958 I print_info: model params     = 1.41 B
0.00.038.958 I print_info: general.name     = 1.4B
0.00.038.959 I print_info: vocab type       = BPE
0.00.038.959 I print_info: n_vocab          = 50304
0.00.038.959 I print_info: n_merges         = 50009
0.00.038.959 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.959 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.960 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.960 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.960 I print_info: LF token         = 187 ''
0.00.038.960 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.961 I print_info: max token length = 1024
0.00.038.961 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.340.953 I load_tensors: offloading 24 repeating layers to GPU
0.00.340.966 I load_tensors: offloading output layer to GPU
0.00.340.967 I load_tensors: offloaded 25/25 layers to GPU
0.00.340.996 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.340.997 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.342.613 I llama_init_from_model: n_seq_max     = 1
0.00.342.619 I llama_init_from_model: n_ctx         = 128
0.00.342.619 I llama_init_from_model: n_ctx_per_seq = 128
0.00.342.620 I llama_init_from_model: n_batch       = 128
0.00.342.620 I llama_init_from_model: n_ubatch      = 128
0.00.342.620 I llama_init_from_model: flash_attn    = 0
0.00.342.622 I llama_init_from_model: freq_base     = 10000.0
0.00.342.623 I llama_init_from_model: freq_scale    = 1
0.00.342.623 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.342.625 I ggml_metal_init: allocating
0.00.342.697 I ggml_metal_init: found device: Apple M4
0.00.342.710 I ggml_metal_init: picking default device: Apple M4
0.00.344.461 I ggml_metal_init: using embedded metal library
0.00.349.862 I ggml_metal_init: GPU name:   Apple M4
0.00.349.873 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.349.874 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.349.874 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.349.875 I ggml_metal_init: simdgroup reduction   = true
0.00.349.875 I ggml_metal_init: simdgroup matrix mul. = true
0.00.349.875 I ggml_metal_init: has residency sets    = true
0.00.349.876 I ggml_metal_init: has bfloat            = true
0.00.349.879 I ggml_metal_init: use bfloat            = true
0.00.349.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.349.887 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.370.732 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.374.331 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.374.338 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.374.374 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.377.757 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.377.759 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.377.760 I llama_init_from_model: graph nodes  = 967
0.00.377.760 I llama_init_from_model: graph splits = 2
0.00.377.764 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.377.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.409.909 I 
0.00.409.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.410.002 I perplexity: tokenizing the input ..
0.00.416.958 I perplexity: tokenization took 6.952 ms
0.00.416.963 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.560.622 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.561.956 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.561.985 I llama_perf_context_print:        load time =     399.76 ms
0.00.561.986 I llama_perf_context_print: prompt eval time =     142.78 ms /   128 tokens (    1.12 ms per token,   896.51 tokens per second)
0.00.561.987 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.561.988 I llama_perf_context_print:       total time =     152.08 ms /   129 tokens
0.00.562.374 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.080s
sys	0m0.091s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.778 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.130 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.135 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.137 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.137 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.138 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.138 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.138 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.139 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.140 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.140 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.141 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.142 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.142 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.143 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.146 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.147 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.918 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.947 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.705 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.706 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.706 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.706 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.707 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.707 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.708 I llama_model_loader: - type  f32:  194 tensors
0.00.025.708 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.708 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.708 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.709 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.709 I print_info: file format = GGUF V3 (latest)
0.00.025.710 I print_info: file type   = Q3_K - Medium
0.00.025.710 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.483 I load: special tokens cache size = 25
0.00.039.631 I load: token to piece cache size = 0.2984 MB
0.00.039.634 I print_info: arch             = gptneox
0.00.039.634 I print_info: vocab_only       = 0
0.00.039.634 I print_info: n_ctx_train      = 2048
0.00.039.634 I print_info: n_embd           = 2048
0.00.039.634 I print_info: n_layer          = 24
0.00.039.637 I print_info: n_head           = 16
0.00.039.638 I print_info: n_head_kv        = 16
0.00.039.638 I print_info: n_rot            = 32
0.00.039.638 I print_info: n_swa            = 0
0.00.039.638 I print_info: n_embd_head_k    = 128
0.00.039.638 I print_info: n_embd_head_v    = 128
0.00.039.639 I print_info: n_gqa            = 1
0.00.039.640 I print_info: n_embd_k_gqa     = 2048
0.00.039.640 I print_info: n_embd_v_gqa     = 2048
0.00.039.641 I print_info: f_norm_eps       = 1.0e-05
0.00.039.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.642 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.642 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.642 I print_info: f_logit_scale    = 0.0e+00
0.00.039.643 I print_info: n_ff             = 8192
0.00.039.643 I print_info: n_expert         = 0
0.00.039.643 I print_info: n_expert_used    = 0
0.00.039.644 I print_info: causal attn      = 1
0.00.039.645 I print_info: pooling type     = 0
0.00.039.645 I print_info: rope type        = 2
0.00.039.646 I print_info: rope scaling     = linear
0.00.039.647 I print_info: freq_base_train  = 10000.0
0.00.039.647 I print_info: freq_scale_train = 1
0.00.039.647 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.649 I print_info: rope_finetuned   = unknown
0.00.039.649 I print_info: ssm_d_conv       = 0
0.00.039.650 I print_info: ssm_d_inner      = 0
0.00.039.650 I print_info: ssm_d_state      = 0
0.00.039.650 I print_info: ssm_dt_rank      = 0
0.00.039.650 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.650 I print_info: model type       = 1.4B
0.00.039.651 I print_info: model params     = 1.41 B
0.00.039.651 I print_info: general.name     = 1.4B
0.00.039.651 I print_info: vocab type       = BPE
0.00.039.651 I print_info: n_vocab          = 50304
0.00.039.652 I print_info: n_merges         = 50009
0.00.039.653 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.653 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.653 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.653 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.655 I print_info: LF token         = 187 ''
0.00.039.655 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.655 I print_info: max token length = 1024
0.00.039.656 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.447.503 I load_tensors: offloading 24 repeating layers to GPU
0.00.447.518 I load_tensors: offloading output layer to GPU
0.00.447.519 I load_tensors: offloaded 25/25 layers to GPU
0.00.447.554 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.447.555 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.449.133 I llama_init_from_model: n_seq_max     = 1
0.00.449.135 I llama_init_from_model: n_ctx         = 2048
0.00.449.136 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.449.136 I llama_init_from_model: n_batch       = 2048
0.00.449.137 I llama_init_from_model: n_ubatch      = 512
0.00.449.137 I llama_init_from_model: flash_attn    = 0
0.00.449.140 I llama_init_from_model: freq_base     = 10000.0
0.00.449.141 I llama_init_from_model: freq_scale    = 1
0.00.449.143 I ggml_metal_init: allocating
0.00.449.222 I ggml_metal_init: found device: Apple M4
0.00.449.234 I ggml_metal_init: picking default device: Apple M4
0.00.451.172 I ggml_metal_init: using embedded metal library
0.00.457.545 I ggml_metal_init: GPU name:   Apple M4
0.00.457.550 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.457.551 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.457.552 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.457.553 I ggml_metal_init: simdgroup reduction   = true
0.00.457.553 I ggml_metal_init: simdgroup matrix mul. = true
0.00.457.553 I ggml_metal_init: has residency sets    = true
0.00.457.554 I ggml_metal_init: has bfloat            = true
0.00.457.554 I ggml_metal_init: use bfloat            = true
0.00.457.555 I ggml_metal_init: hasUnifiedMemory      = true
0.00.457.556 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.476.661 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.532.707 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.532.716 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.532.747 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.537.230 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.537.233 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.537.233 I llama_init_from_model: graph nodes  = 967
0.00.537.233 I llama_init_from_model: graph splits = 2
0.00.537.238 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.537.378 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.537.379 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.511 I main: llama threadpool init, n_threads = 4
0.00.596.559 I 
0.00.596.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.582 I 
0.00.596.753 I sampler seed: 1234
0.00.596.758 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.596.782 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.596.783 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.596.784 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.342.922 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.342.923 I llama_perf_context_print:        load time =     586.04 ms
0.01.342.924 I llama_perf_context_print: prompt eval time =      50.14 ms /     7 tokens (    7.16 ms per token,   139.60 tokens per second)
0.01.342.924 I llama_perf_context_print:        eval time =     692.98 ms /    63 runs   (   11.00 ms per token,    90.91 tokens per second)
0.01.342.925 I llama_perf_context_print:       total time =     747.10 ms /    70 tokens
0.01.343.204 I ggml_metal_free: deallocating

real	0m1.360s
user	0m0.109s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.819 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.529 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.536 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.540 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.540 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.541 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.541 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.542 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.543 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.544 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.544 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.544 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.546 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.549 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.549 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.205 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.213 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.109 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.110 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.112 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.112 I llama_model_loader: - type  f32:  194 tensors
0.00.024.113 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.113 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.113 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.113 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.114 I print_info: file format = GGUF V3 (latest)
0.00.024.115 I print_info: file type   = Q3_K - Medium
0.00.024.116 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.947 I load: special tokens cache size = 25
0.00.038.139 I load: token to piece cache size = 0.2984 MB
0.00.038.144 I print_info: arch             = gptneox
0.00.038.144 I print_info: vocab_only       = 0
0.00.038.144 I print_info: n_ctx_train      = 2048
0.00.038.144 I print_info: n_embd           = 2048
0.00.038.145 I print_info: n_layer          = 24
0.00.038.149 I print_info: n_head           = 16
0.00.038.150 I print_info: n_head_kv        = 16
0.00.038.150 I print_info: n_rot            = 32
0.00.038.150 I print_info: n_swa            = 0
0.00.038.152 I print_info: n_embd_head_k    = 128
0.00.038.153 I print_info: n_embd_head_v    = 128
0.00.038.153 I print_info: n_gqa            = 1
0.00.038.154 I print_info: n_embd_k_gqa     = 2048
0.00.038.154 I print_info: n_embd_v_gqa     = 2048
0.00.038.155 I print_info: f_norm_eps       = 1.0e-05
0.00.038.155 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.155 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.156 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.156 I print_info: f_logit_scale    = 0.0e+00
0.00.038.156 I print_info: n_ff             = 8192
0.00.038.157 I print_info: n_expert         = 0
0.00.038.157 I print_info: n_expert_used    = 0
0.00.038.157 I print_info: causal attn      = 1
0.00.038.157 I print_info: pooling type     = 0
0.00.038.157 I print_info: rope type        = 2
0.00.038.157 I print_info: rope scaling     = linear
0.00.038.157 I print_info: freq_base_train  = 10000.0
0.00.038.158 I print_info: freq_scale_train = 1
0.00.038.158 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.158 I print_info: rope_finetuned   = unknown
0.00.038.158 I print_info: ssm_d_conv       = 0
0.00.038.158 I print_info: ssm_d_inner      = 0
0.00.038.159 I print_info: ssm_d_state      = 0
0.00.038.159 I print_info: ssm_dt_rank      = 0
0.00.038.159 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.159 I print_info: model type       = 1.4B
0.00.038.159 I print_info: model params     = 1.41 B
0.00.038.159 I print_info: general.name     = 1.4B
0.00.038.160 I print_info: vocab type       = BPE
0.00.038.160 I print_info: n_vocab          = 50304
0.00.038.162 I print_info: n_merges         = 50009
0.00.038.162 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.162 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.162 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.163 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.163 I print_info: LF token         = 187 ''
0.00.038.163 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.163 I print_info: max token length = 1024
0.00.038.164 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.459.180 I load_tensors: offloading 24 repeating layers to GPU
0.00.459.192 I load_tensors: offloading output layer to GPU
0.00.459.193 I load_tensors: offloaded 25/25 layers to GPU
0.00.459.225 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.459.226 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.460.786 I llama_init_from_model: n_seq_max     = 1
0.00.460.793 I llama_init_from_model: n_ctx         = 128
0.00.460.793 I llama_init_from_model: n_ctx_per_seq = 128
0.00.460.794 I llama_init_from_model: n_batch       = 128
0.00.460.794 I llama_init_from_model: n_ubatch      = 128
0.00.460.794 I llama_init_from_model: flash_attn    = 0
0.00.460.795 I llama_init_from_model: freq_base     = 10000.0
0.00.460.796 I llama_init_from_model: freq_scale    = 1
0.00.460.797 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.460.801 I ggml_metal_init: allocating
0.00.460.902 I ggml_metal_init: found device: Apple M4
0.00.460.917 I ggml_metal_init: picking default device: Apple M4
0.00.462.706 I ggml_metal_init: using embedded metal library
0.00.468.315 I ggml_metal_init: GPU name:   Apple M4
0.00.468.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.468.326 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.468.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.468.328 I ggml_metal_init: simdgroup reduction   = true
0.00.468.328 I ggml_metal_init: simdgroup matrix mul. = true
0.00.468.329 I ggml_metal_init: has residency sets    = true
0.00.468.329 I ggml_metal_init: has bfloat            = true
0.00.468.329 I ggml_metal_init: use bfloat            = true
0.00.468.331 I ggml_metal_init: hasUnifiedMemory      = true
0.00.468.335 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.488.192 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.491.817 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.491.827 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.491.863 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.495.152 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.495.154 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.495.154 I llama_init_from_model: graph nodes  = 967
0.00.495.155 I llama_init_from_model: graph splits = 2
0.00.495.158 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.495.158 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.522.190 I 
0.00.522.256 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.522.264 I perplexity: tokenizing the input ..
0.00.529.592 I perplexity: tokenization took 7.326 ms
0.00.529.599 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.662.708 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.664.043 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.664.064 I llama_perf_context_print:        load time =     513.36 ms
0.00.664.066 I llama_perf_context_print: prompt eval time =     132.21 ms /   128 tokens (    1.03 ms per token,   968.19 tokens per second)
0.00.664.067 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.664.069 I llama_perf_context_print:       total time =     141.88 ms /   129 tokens
0.00.664.476 I ggml_metal_free: deallocating

real	0m0.678s
user	0m0.079s
sys	0m0.121s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.011.266 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.748 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.753 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.755 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.755 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.760 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.761 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.761 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.762 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.762 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.763 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.763 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.765 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.765 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.766 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.770 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.771 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.601 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.570 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.391 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.392 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.392 I llama_model_loader: - type  f32:  194 tensors
0.00.027.392 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.393 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.393 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.393 I print_info: file format = GGUF V3 (latest)
0.00.027.394 I print_info: file type   = Q4_K - Medium
0.00.027.395 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.383 I load: special tokens cache size = 25
0.00.041.544 I load: token to piece cache size = 0.2984 MB
0.00.041.546 I print_info: arch             = gptneox
0.00.041.546 I print_info: vocab_only       = 0
0.00.041.546 I print_info: n_ctx_train      = 2048
0.00.041.547 I print_info: n_embd           = 2048
0.00.041.547 I print_info: n_layer          = 24
0.00.041.549 I print_info: n_head           = 16
0.00.041.550 I print_info: n_head_kv        = 16
0.00.041.550 I print_info: n_rot            = 32
0.00.041.551 I print_info: n_swa            = 0
0.00.041.551 I print_info: n_embd_head_k    = 128
0.00.041.553 I print_info: n_embd_head_v    = 128
0.00.041.553 I print_info: n_gqa            = 1
0.00.041.554 I print_info: n_embd_k_gqa     = 2048
0.00.041.555 I print_info: n_embd_v_gqa     = 2048
0.00.041.555 I print_info: f_norm_eps       = 1.0e-05
0.00.041.556 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.556 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.556 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.556 I print_info: f_logit_scale    = 0.0e+00
0.00.041.557 I print_info: n_ff             = 8192
0.00.041.557 I print_info: n_expert         = 0
0.00.041.557 I print_info: n_expert_used    = 0
0.00.041.558 I print_info: causal attn      = 1
0.00.041.559 I print_info: pooling type     = 0
0.00.041.561 I print_info: rope type        = 2
0.00.041.561 I print_info: rope scaling     = linear
0.00.041.561 I print_info: freq_base_train  = 10000.0
0.00.041.562 I print_info: freq_scale_train = 1
0.00.041.562 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.562 I print_info: rope_finetuned   = unknown
0.00.041.562 I print_info: ssm_d_conv       = 0
0.00.041.563 I print_info: ssm_d_inner      = 0
0.00.041.563 I print_info: ssm_d_state      = 0
0.00.041.563 I print_info: ssm_dt_rank      = 0
0.00.041.563 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.563 I print_info: model type       = 1.4B
0.00.041.564 I print_info: model params     = 1.41 B
0.00.041.564 I print_info: general.name     = 1.4B
0.00.041.565 I print_info: vocab type       = BPE
0.00.041.565 I print_info: n_vocab          = 50304
0.00.041.565 I print_info: n_merges         = 50009
0.00.041.565 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.567 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.567 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.567 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.567 I print_info: LF token         = 187 ''
0.00.041.567 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.568 I print_info: max token length = 1024
0.00.041.568 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.385 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.401 I load_tensors: offloading output layer to GPU
0.00.538.401 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.436 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.437 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.540.234 I llama_init_from_model: n_seq_max     = 1
0.00.540.246 I llama_init_from_model: n_ctx         = 2048
0.00.540.247 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.540.247 I llama_init_from_model: n_batch       = 2048
0.00.540.247 I llama_init_from_model: n_ubatch      = 512
0.00.540.248 I llama_init_from_model: flash_attn    = 0
0.00.540.251 I llama_init_from_model: freq_base     = 10000.0
0.00.540.251 I llama_init_from_model: freq_scale    = 1
0.00.540.257 I ggml_metal_init: allocating
0.00.540.346 I ggml_metal_init: found device: Apple M4
0.00.540.360 I ggml_metal_init: picking default device: Apple M4
0.00.542.220 I ggml_metal_init: using embedded metal library
0.00.548.981 I ggml_metal_init: GPU name:   Apple M4
0.00.548.986 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.548.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.548.987 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.548.988 I ggml_metal_init: simdgroup reduction   = true
0.00.548.988 I ggml_metal_init: simdgroup matrix mul. = true
0.00.548.988 I ggml_metal_init: has residency sets    = true
0.00.548.989 I ggml_metal_init: has bfloat            = true
0.00.548.989 I ggml_metal_init: use bfloat            = true
0.00.548.990 I ggml_metal_init: hasUnifiedMemory      = true
0.00.548.992 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.566.559 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.029 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.627.038 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.627.111 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.631.564 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.631.566 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.631.566 I llama_init_from_model: graph nodes  = 967
0.00.631.566 I llama_init_from_model: graph splits = 2
0.00.631.573 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.631.696 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.631.697 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.013 I main: llama threadpool init, n_threads = 4
0.00.690.060 I 
0.00.690.081 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.083 I 
0.00.690.258 I sampler seed: 1234
0.00.690.262 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.304 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.307 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.308 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.450.775 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49894.59 tokens per second)
0.01.450.776 I llama_perf_context_print:        load time =     678.04 ms
0.01.450.777 I llama_perf_context_print: prompt eval time =      57.37 ms /     7 tokens (    8.20 ms per token,   122.01 tokens per second)
0.01.450.778 I llama_perf_context_print:        eval time =     700.06 ms /    63 runs   (   11.11 ms per token,    89.99 tokens per second)
0.01.450.779 I llama_perf_context_print:       total time =     761.46 ms /    70 tokens
0.01.451.019 I ggml_metal_free: deallocating

real	0m1.468s
user	0m0.110s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.956 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.962 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.964 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.965 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.965 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.965 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.966 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.966 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.967 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.970 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.970 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.971 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.971 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.973 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.973 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.973 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.768 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.773 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.616 I llama_model_loader: - type  f32:  194 tensors
0.00.024.616 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.617 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.617 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.617 I print_info: file format = GGUF V3 (latest)
0.00.024.618 I print_info: file type   = Q4_K - Medium
0.00.024.619 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.896 I load: special tokens cache size = 25
0.00.039.096 I load: token to piece cache size = 0.2984 MB
0.00.039.100 I print_info: arch             = gptneox
0.00.039.101 I print_info: vocab_only       = 0
0.00.039.101 I print_info: n_ctx_train      = 2048
0.00.039.101 I print_info: n_embd           = 2048
0.00.039.101 I print_info: n_layer          = 24
0.00.039.106 I print_info: n_head           = 16
0.00.039.107 I print_info: n_head_kv        = 16
0.00.039.109 I print_info: n_rot            = 32
0.00.039.110 I print_info: n_swa            = 0
0.00.039.110 I print_info: n_embd_head_k    = 128
0.00.039.110 I print_info: n_embd_head_v    = 128
0.00.039.110 I print_info: n_gqa            = 1
0.00.039.111 I print_info: n_embd_k_gqa     = 2048
0.00.039.112 I print_info: n_embd_v_gqa     = 2048
0.00.039.112 I print_info: f_norm_eps       = 1.0e-05
0.00.039.113 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.113 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.115 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.115 I print_info: f_logit_scale    = 0.0e+00
0.00.039.116 I print_info: n_ff             = 8192
0.00.039.116 I print_info: n_expert         = 0
0.00.039.116 I print_info: n_expert_used    = 0
0.00.039.116 I print_info: causal attn      = 1
0.00.039.116 I print_info: pooling type     = 0
0.00.039.116 I print_info: rope type        = 2
0.00.039.117 I print_info: rope scaling     = linear
0.00.039.117 I print_info: freq_base_train  = 10000.0
0.00.039.118 I print_info: freq_scale_train = 1
0.00.039.118 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.118 I print_info: rope_finetuned   = unknown
0.00.039.119 I print_info: ssm_d_conv       = 0
0.00.039.119 I print_info: ssm_d_inner      = 0
0.00.039.119 I print_info: ssm_d_state      = 0
0.00.039.119 I print_info: ssm_dt_rank      = 0
0.00.039.119 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.120 I print_info: model type       = 1.4B
0.00.039.120 I print_info: model params     = 1.41 B
0.00.039.121 I print_info: general.name     = 1.4B
0.00.039.121 I print_info: vocab type       = BPE
0.00.039.121 I print_info: n_vocab          = 50304
0.00.039.121 I print_info: n_merges         = 50009
0.00.039.122 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.124 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.124 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.124 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.125 I print_info: LF token         = 187 ''
0.00.039.125 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.125 I print_info: max token length = 1024
0.00.039.126 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.519.741 I load_tensors: offloading 24 repeating layers to GPU
0.00.519.756 I load_tensors: offloading output layer to GPU
0.00.519.757 I load_tensors: offloaded 25/25 layers to GPU
0.00.519.789 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.519.790 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.521.402 I llama_init_from_model: n_seq_max     = 1
0.00.521.404 I llama_init_from_model: n_ctx         = 128
0.00.521.405 I llama_init_from_model: n_ctx_per_seq = 128
0.00.521.405 I llama_init_from_model: n_batch       = 128
0.00.521.406 I llama_init_from_model: n_ubatch      = 128
0.00.521.406 I llama_init_from_model: flash_attn    = 0
0.00.521.409 I llama_init_from_model: freq_base     = 10000.0
0.00.521.409 I llama_init_from_model: freq_scale    = 1
0.00.521.410 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.521.412 I ggml_metal_init: allocating
0.00.521.462 I ggml_metal_init: found device: Apple M4
0.00.521.475 I ggml_metal_init: picking default device: Apple M4
0.00.523.215 I ggml_metal_init: using embedded metal library
0.00.529.801 I ggml_metal_init: GPU name:   Apple M4
0.00.529.807 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.529.808 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.529.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.529.809 I ggml_metal_init: simdgroup reduction   = true
0.00.529.810 I ggml_metal_init: simdgroup matrix mul. = true
0.00.529.810 I ggml_metal_init: has residency sets    = true
0.00.529.810 I ggml_metal_init: has bfloat            = true
0.00.529.810 I ggml_metal_init: use bfloat            = true
0.00.529.812 I ggml_metal_init: hasUnifiedMemory      = true
0.00.529.822 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.548.270 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.551.829 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.551.836 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.551.867 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.555.103 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.555.105 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.555.105 I llama_init_from_model: graph nodes  = 967
0.00.555.106 I llama_init_from_model: graph splits = 2
0.00.555.108 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.555.109 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.585.659 I 
0.00.585.744 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.585.751 I perplexity: tokenizing the input ..
0.00.593.018 I perplexity: tokenization took 7.264 ms
0.00.593.033 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.736.755 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.738.078 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.738.103 I llama_perf_context_print:        load time =     576.72 ms
0.00.738.104 I llama_perf_context_print: prompt eval time =     142.86 ms /   128 tokens (    1.12 ms per token,   895.96 tokens per second)
0.00.738.105 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.738.105 I llama_perf_context_print:       total time =     152.45 ms /   129 tokens
0.00.738.492 I ggml_metal_free: deallocating

real	0m0.753s
user	0m0.080s
sys	0m0.124s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.011.377 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.798 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.809 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.810 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.811 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.814 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.815 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.815 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.816 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.818 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.818 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.819 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.571 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.554 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.270 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.270 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.270 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.270 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.271 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.271 I llama_model_loader: - type  f32:  194 tensors
0.00.027.272 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.272 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.273 I print_info: file format = GGUF V3 (latest)
0.00.027.273 I print_info: file type   = Q5_K - Medium
0.00.027.274 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.010 I load: special tokens cache size = 25
0.00.041.096 I load: token to piece cache size = 0.2984 MB
0.00.041.098 I print_info: arch             = gptneox
0.00.041.098 I print_info: vocab_only       = 0
0.00.041.099 I print_info: n_ctx_train      = 2048
0.00.041.099 I print_info: n_embd           = 2048
0.00.041.099 I print_info: n_layer          = 24
0.00.041.102 I print_info: n_head           = 16
0.00.041.103 I print_info: n_head_kv        = 16
0.00.041.103 I print_info: n_rot            = 32
0.00.041.103 I print_info: n_swa            = 0
0.00.041.103 I print_info: n_embd_head_k    = 128
0.00.041.103 I print_info: n_embd_head_v    = 128
0.00.041.104 I print_info: n_gqa            = 1
0.00.041.105 I print_info: n_embd_k_gqa     = 2048
0.00.041.106 I print_info: n_embd_v_gqa     = 2048
0.00.041.107 I print_info: f_norm_eps       = 1.0e-05
0.00.041.108 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.108 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.108 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.110 I print_info: f_logit_scale    = 0.0e+00
0.00.041.110 I print_info: n_ff             = 8192
0.00.041.111 I print_info: n_expert         = 0
0.00.041.111 I print_info: n_expert_used    = 0
0.00.041.111 I print_info: causal attn      = 1
0.00.041.111 I print_info: pooling type     = 0
0.00.041.111 I print_info: rope type        = 2
0.00.041.112 I print_info: rope scaling     = linear
0.00.041.112 I print_info: freq_base_train  = 10000.0
0.00.041.112 I print_info: freq_scale_train = 1
0.00.041.112 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.113 I print_info: rope_finetuned   = unknown
0.00.041.117 I print_info: ssm_d_conv       = 0
0.00.041.117 I print_info: ssm_d_inner      = 0
0.00.041.117 I print_info: ssm_d_state      = 0
0.00.041.117 I print_info: ssm_dt_rank      = 0
0.00.041.118 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.118 I print_info: model type       = 1.4B
0.00.041.119 I print_info: model params     = 1.41 B
0.00.041.120 I print_info: general.name     = 1.4B
0.00.041.120 I print_info: vocab type       = BPE
0.00.041.120 I print_info: n_vocab          = 50304
0.00.041.120 I print_info: n_merges         = 50009
0.00.041.121 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.121 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.121 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.121 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.121 I print_info: LF token         = 187 ''
0.00.041.122 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.122 I print_info: max token length = 1024
0.00.041.122 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.311 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.325 I load_tensors: offloading output layer to GPU
0.00.628.326 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.358 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.628.360 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.629.732 I llama_init_from_model: n_seq_max     = 1
0.00.629.736 I llama_init_from_model: n_ctx         = 2048
0.00.629.737 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.629.738 I llama_init_from_model: n_batch       = 2048
0.00.629.738 I llama_init_from_model: n_ubatch      = 512
0.00.629.738 I llama_init_from_model: flash_attn    = 0
0.00.629.741 I llama_init_from_model: freq_base     = 10000.0
0.00.629.741 I llama_init_from_model: freq_scale    = 1
0.00.629.744 I ggml_metal_init: allocating
0.00.629.800 I ggml_metal_init: found device: Apple M4
0.00.629.813 I ggml_metal_init: picking default device: Apple M4
0.00.631.873 I ggml_metal_init: using embedded metal library
0.00.638.441 I ggml_metal_init: GPU name:   Apple M4
0.00.638.446 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.446 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.447 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.448 I ggml_metal_init: simdgroup reduction   = true
0.00.638.448 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.448 I ggml_metal_init: has residency sets    = true
0.00.638.449 I ggml_metal_init: has bfloat            = true
0.00.638.449 I ggml_metal_init: use bfloat            = true
0.00.638.449 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.451 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.272 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.045 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.705.051 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.705.072 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.709.236 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.709.237 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.709.238 I llama_init_from_model: graph nodes  = 967
0.00.709.238 I llama_init_from_model: graph splits = 2
0.00.709.244 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.709.363 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.709.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.495 I main: llama threadpool init, n_threads = 4
0.00.774.534 I 
0.00.774.554 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.554 I 
0.00.774.703 I sampler seed: 1234
0.00.774.708 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.719 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.719 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.719 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.614.241 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49615.65 tokens per second)
0.01.614.241 I llama_perf_context_print:        load time =     762.41 ms
0.01.614.242 I llama_perf_context_print: prompt eval time =      51.14 ms /     7 tokens (    7.31 ms per token,   136.88 tokens per second)
0.01.614.243 I llama_perf_context_print:        eval time =     785.72 ms /    63 runs   (   12.47 ms per token,    80.18 tokens per second)
0.01.614.243 I llama_perf_context_print:       total time =     840.45 ms /    70 tokens
0.01.614.456 I ggml_metal_free: deallocating

real	0m1.633s
user	0m0.110s
sys	0m0.225s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.040 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.930 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.936 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.938 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.938 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.939 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.939 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.939 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.940 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.943 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.943 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.944 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.944 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.944 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.945 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.947 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.948 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.948 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.726 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.418 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.419 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.419 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.419 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.420 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.420 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.421 I llama_model_loader: - type  f32:  194 tensors
0.00.025.421 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.421 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.422 I print_info: file format = GGUF V3 (latest)
0.00.025.422 I print_info: file type   = Q5_K - Medium
0.00.025.424 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.087 I load: special tokens cache size = 25
0.00.039.177 I load: token to piece cache size = 0.2984 MB
0.00.039.180 I print_info: arch             = gptneox
0.00.039.180 I print_info: vocab_only       = 0
0.00.039.180 I print_info: n_ctx_train      = 2048
0.00.039.180 I print_info: n_embd           = 2048
0.00.039.181 I print_info: n_layer          = 24
0.00.039.184 I print_info: n_head           = 16
0.00.039.185 I print_info: n_head_kv        = 16
0.00.039.185 I print_info: n_rot            = 32
0.00.039.185 I print_info: n_swa            = 0
0.00.039.185 I print_info: n_embd_head_k    = 128
0.00.039.185 I print_info: n_embd_head_v    = 128
0.00.039.186 I print_info: n_gqa            = 1
0.00.039.187 I print_info: n_embd_k_gqa     = 2048
0.00.039.187 I print_info: n_embd_v_gqa     = 2048
0.00.039.188 I print_info: f_norm_eps       = 1.0e-05
0.00.039.191 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.191 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.191 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.191 I print_info: f_logit_scale    = 0.0e+00
0.00.039.192 I print_info: n_ff             = 8192
0.00.039.192 I print_info: n_expert         = 0
0.00.039.192 I print_info: n_expert_used    = 0
0.00.039.192 I print_info: causal attn      = 1
0.00.039.192 I print_info: pooling type     = 0
0.00.039.193 I print_info: rope type        = 2
0.00.039.193 I print_info: rope scaling     = linear
0.00.039.193 I print_info: freq_base_train  = 10000.0
0.00.039.194 I print_info: freq_scale_train = 1
0.00.039.194 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.194 I print_info: rope_finetuned   = unknown
0.00.039.194 I print_info: ssm_d_conv       = 0
0.00.039.194 I print_info: ssm_d_inner      = 0
0.00.039.194 I print_info: ssm_d_state      = 0
0.00.039.194 I print_info: ssm_dt_rank      = 0
0.00.039.196 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.196 I print_info: model type       = 1.4B
0.00.039.197 I print_info: model params     = 1.41 B
0.00.039.197 I print_info: general.name     = 1.4B
0.00.039.197 I print_info: vocab type       = BPE
0.00.039.198 I print_info: n_vocab          = 50304
0.00.039.198 I print_info: n_merges         = 50009
0.00.039.198 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.198 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.198 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.199 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.199 I print_info: LF token         = 187 ''
0.00.039.199 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.199 I print_info: max token length = 1024
0.00.039.200 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.594.278 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.294 I load_tensors: offloading output layer to GPU
0.00.594.295 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.325 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.594.326 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.596.067 I llama_init_from_model: n_seq_max     = 1
0.00.596.070 I llama_init_from_model: n_ctx         = 128
0.00.596.070 I llama_init_from_model: n_ctx_per_seq = 128
0.00.596.071 I llama_init_from_model: n_batch       = 128
0.00.596.071 I llama_init_from_model: n_ubatch      = 128
0.00.596.072 I llama_init_from_model: flash_attn    = 0
0.00.596.074 I llama_init_from_model: freq_base     = 10000.0
0.00.596.074 I llama_init_from_model: freq_scale    = 1
0.00.596.075 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.596.081 I ggml_metal_init: allocating
0.00.596.176 I ggml_metal_init: found device: Apple M4
0.00.596.188 I ggml_metal_init: picking default device: Apple M4
0.00.597.789 I ggml_metal_init: using embedded metal library
0.00.604.058 I ggml_metal_init: GPU name:   Apple M4
0.00.604.062 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.063 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.064 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.064 I ggml_metal_init: simdgroup reduction   = true
0.00.604.065 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.065 I ggml_metal_init: has residency sets    = true
0.00.604.065 I ggml_metal_init: has bfloat            = true
0.00.604.065 I ggml_metal_init: use bfloat            = true
0.00.604.067 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.070 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.875 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.348 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.624.352 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.624.384 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.627.715 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.627.717 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.627.717 I llama_init_from_model: graph nodes  = 967
0.00.627.718 I llama_init_from_model: graph splits = 2
0.00.627.720 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.627.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.834 I 
0.00.657.932 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.958 I perplexity: tokenizing the input ..
0.00.663.954 I perplexity: tokenization took 5.994 ms
0.00.663.959 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.815 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.805.257 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.805.280 I llama_perf_context_print:        load time =     647.79 ms
0.00.805.281 I llama_perf_context_print: prompt eval time =     139.63 ms /   128 tokens (    1.09 ms per token,   916.72 tokens per second)
0.00.805.282 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.282 I llama_perf_context_print:       total time =     147.45 ms /   129 tokens
0.00.805.689 I ggml_metal_free: deallocating

real	0m0.821s
user	0m0.075s
sys	0m0.134s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.929 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.114 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.120 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.121 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.123 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.125 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.126 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.126 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.131 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.966 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.794 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.795 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.796 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.796 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.796 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.797 I llama_model_loader: - type  f32:  194 tensors
0.00.025.797 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.798 I print_info: file format = GGUF V3 (latest)
0.00.025.798 I print_info: file type   = Q6_K
0.00.025.799 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.968 I load: special tokens cache size = 25
0.00.040.159 I load: token to piece cache size = 0.2984 MB
0.00.040.162 I print_info: arch             = gptneox
0.00.040.162 I print_info: vocab_only       = 0
0.00.040.162 I print_info: n_ctx_train      = 2048
0.00.040.162 I print_info: n_embd           = 2048
0.00.040.163 I print_info: n_layer          = 24
0.00.040.166 I print_info: n_head           = 16
0.00.040.166 I print_info: n_head_kv        = 16
0.00.040.167 I print_info: n_rot            = 32
0.00.040.167 I print_info: n_swa            = 0
0.00.040.167 I print_info: n_embd_head_k    = 128
0.00.040.167 I print_info: n_embd_head_v    = 128
0.00.040.168 I print_info: n_gqa            = 1
0.00.040.169 I print_info: n_embd_k_gqa     = 2048
0.00.040.170 I print_info: n_embd_v_gqa     = 2048
0.00.040.170 I print_info: f_norm_eps       = 1.0e-05
0.00.040.170 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.171 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.171 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.171 I print_info: f_logit_scale    = 0.0e+00
0.00.040.173 I print_info: n_ff             = 8192
0.00.040.174 I print_info: n_expert         = 0
0.00.040.174 I print_info: n_expert_used    = 0
0.00.040.174 I print_info: causal attn      = 1
0.00.040.174 I print_info: pooling type     = 0
0.00.040.174 I print_info: rope type        = 2
0.00.040.176 I print_info: rope scaling     = linear
0.00.040.178 I print_info: freq_base_train  = 10000.0
0.00.040.178 I print_info: freq_scale_train = 1
0.00.040.178 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.178 I print_info: rope_finetuned   = unknown
0.00.040.179 I print_info: ssm_d_conv       = 0
0.00.040.179 I print_info: ssm_d_inner      = 0
0.00.040.179 I print_info: ssm_d_state      = 0
0.00.040.179 I print_info: ssm_dt_rank      = 0
0.00.040.179 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.179 I print_info: model type       = 1.4B
0.00.040.180 I print_info: model params     = 1.41 B
0.00.040.180 I print_info: general.name     = 1.4B
0.00.040.180 I print_info: vocab type       = BPE
0.00.040.180 I print_info: n_vocab          = 50304
0.00.040.181 I print_info: n_merges         = 50009
0.00.040.181 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.181 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.181 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.181 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.182 I print_info: LF token         = 187 ''
0.00.040.182 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.182 I print_info: max token length = 1024
0.00.040.186 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.668.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.668.164 I load_tensors: offloading output layer to GPU
0.00.668.165 I load_tensors: offloaded 25/25 layers to GPU
0.00.668.190 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.668.191 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.669.583 I llama_init_from_model: n_seq_max     = 1
0.00.669.585 I llama_init_from_model: n_ctx         = 2048
0.00.669.586 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.669.586 I llama_init_from_model: n_batch       = 2048
0.00.669.587 I llama_init_from_model: n_ubatch      = 512
0.00.669.587 I llama_init_from_model: flash_attn    = 0
0.00.669.588 I llama_init_from_model: freq_base     = 10000.0
0.00.669.589 I llama_init_from_model: freq_scale    = 1
0.00.669.590 I ggml_metal_init: allocating
0.00.669.602 I ggml_metal_init: found device: Apple M4
0.00.669.615 I ggml_metal_init: picking default device: Apple M4
0.00.671.041 I ggml_metal_init: using embedded metal library
0.00.677.173 I ggml_metal_init: GPU name:   Apple M4
0.00.677.176 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.677.177 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.677.178 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.677.178 I ggml_metal_init: simdgroup reduction   = true
0.00.677.178 I ggml_metal_init: simdgroup matrix mul. = true
0.00.677.179 I ggml_metal_init: has residency sets    = true
0.00.677.179 I ggml_metal_init: has bfloat            = true
0.00.677.179 I ggml_metal_init: use bfloat            = true
0.00.677.180 I ggml_metal_init: hasUnifiedMemory      = true
0.00.677.181 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.693.494 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.746.103 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.746.118 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.746.147 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.385 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.750.387 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.750.387 I llama_init_from_model: graph nodes  = 967
0.00.750.388 I llama_init_from_model: graph splits = 2
0.00.750.397 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.517 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.518 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.374 I main: llama threadpool init, n_threads = 4
0.00.815.417 I 
0.00.815.439 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.439 I 
0.00.815.593 I sampler seed: 1234
0.00.815.597 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.815.635 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.815.636 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.815.639 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.684.683 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.01.684.683 I llama_perf_context_print:        load time =     805.72 ms
0.01.684.684 I llama_perf_context_print: prompt eval time =      54.15 ms /     7 tokens (    7.74 ms per token,   129.28 tokens per second)
0.01.684.685 I llama_perf_context_print:        eval time =     811.96 ms /    63 runs   (   12.89 ms per token,    77.59 tokens per second)
0.01.684.686 I llama_perf_context_print:       total time =     870.03 ms /    70 tokens
0.01.684.942 I ggml_metal_free: deallocating

real	0m1.702s
user	0m0.108s
sys	0m0.226s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4680 (9ac3457b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.191 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.051 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.058 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.059 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.059 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.059 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.061 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.061 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.061 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.062 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.062 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.062 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.065 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.065 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.821 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.824 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.586 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.587 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.588 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.588 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.588 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.589 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.589 I llama_model_loader: - type  f32:  194 tensors
0.00.024.590 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.591 I print_info: file format = GGUF V3 (latest)
0.00.024.593 I print_info: file type   = Q6_K
0.00.024.594 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.386 I load: special tokens cache size = 25
0.00.038.668 I load: token to piece cache size = 0.2984 MB
0.00.038.671 I print_info: arch             = gptneox
0.00.038.671 I print_info: vocab_only       = 0
0.00.038.671 I print_info: n_ctx_train      = 2048
0.00.038.672 I print_info: n_embd           = 2048
0.00.038.672 I print_info: n_layer          = 24
0.00.038.676 I print_info: n_head           = 16
0.00.038.677 I print_info: n_head_kv        = 16
0.00.038.677 I print_info: n_rot            = 32
0.00.038.677 I print_info: n_swa            = 0
0.00.038.677 I print_info: n_embd_head_k    = 128
0.00.038.678 I print_info: n_embd_head_v    = 128
0.00.038.679 I print_info: n_gqa            = 1
0.00.038.679 I print_info: n_embd_k_gqa     = 2048
0.00.038.682 I print_info: n_embd_v_gqa     = 2048
0.00.038.682 I print_info: f_norm_eps       = 1.0e-05
0.00.038.683 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.684 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.685 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.685 I print_info: f_logit_scale    = 0.0e+00
0.00.038.685 I print_info: n_ff             = 8192
0.00.038.685 I print_info: n_expert         = 0
0.00.038.687 I print_info: n_expert_used    = 0
0.00.038.687 I print_info: causal attn      = 1
0.00.038.687 I print_info: pooling type     = 0
0.00.038.687 I print_info: rope type        = 2
0.00.038.688 I print_info: rope scaling     = linear
0.00.038.688 I print_info: freq_base_train  = 10000.0
0.00.038.689 I print_info: freq_scale_train = 1
0.00.038.689 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.689 I print_info: rope_finetuned   = unknown
0.00.038.689 I print_info: ssm_d_conv       = 0
0.00.038.689 I print_info: ssm_d_inner      = 0
0.00.038.690 I print_info: ssm_d_state      = 0
0.00.038.690 I print_info: ssm_dt_rank      = 0
0.00.038.690 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.691 I print_info: model type       = 1.4B
0.00.038.692 I print_info: model params     = 1.41 B
0.00.038.692 I print_info: general.name     = 1.4B
0.00.038.692 I print_info: vocab type       = BPE
0.00.038.693 I print_info: n_vocab          = 50304
0.00.038.696 I print_info: n_merges         = 50009
0.00.038.696 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.696 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.697 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.697 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.697 I print_info: LF token         = 187 ''
0.00.038.697 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.697 I print_info: max token length = 1024
0.00.038.698 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.952 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.957 I load_tensors: offloading output layer to GPU
0.00.619.959 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.984 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.619.987 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.621.561 I llama_init_from_model: n_seq_max     = 1
0.00.621.563 I llama_init_from_model: n_ctx         = 128
0.00.621.563 I llama_init_from_model: n_ctx_per_seq = 128
0.00.621.563 I llama_init_from_model: n_batch       = 128
0.00.621.564 I llama_init_from_model: n_ubatch      = 128
0.00.621.564 I llama_init_from_model: flash_attn    = 0
0.00.621.565 I llama_init_from_model: freq_base     = 10000.0
0.00.621.566 I llama_init_from_model: freq_scale    = 1
0.00.621.567 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.621.568 I ggml_metal_init: allocating
0.00.621.586 I ggml_metal_init: found device: Apple M4
0.00.621.596 I ggml_metal_init: picking default device: Apple M4
0.00.622.919 I ggml_metal_init: using embedded metal library
0.00.628.688 I ggml_metal_init: GPU name:   Apple M4
0.00.628.691 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.628.691 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.628.692 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.628.693 I ggml_metal_init: simdgroup reduction   = true
0.00.628.693 I ggml_metal_init: simdgroup matrix mul. = true
0.00.628.693 I ggml_metal_init: has residency sets    = true
0.00.628.693 I ggml_metal_init: has bfloat            = true
0.00.628.694 I ggml_metal_init: use bfloat            = true
0.00.628.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.628.702 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.850 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.648.389 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.648.393 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.648.437 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.651.599 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.651.600 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.651.601 I llama_init_from_model: graph nodes  = 967
0.00.651.601 I llama_init_from_model: graph splits = 2
0.00.651.603 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.651.604 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.533 I 
0.00.686.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.617 I perplexity: tokenizing the input ..
0.00.692.074 I perplexity: tokenization took 5.455 ms
0.00.692.077 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.831.175 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.832.528 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.832.552 I llama_perf_context_print:        load time =     677.33 ms
0.00.832.553 I llama_perf_context_print: prompt eval time =     138.86 ms /   128 tokens (    1.08 ms per token,   921.77 tokens per second)
0.00.832.554 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.832.554 I llama_perf_context_print:       total time =     146.02 ms /   129 tokens
0.00.832.933 I ggml_metal_free: deallocating

real	0m0.848s
user	0m0.075s
sys	0m0.142s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4680 (9ac3457b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x109007ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1090085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x109008ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x109009150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x109009700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x109009cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10900a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10900a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10900adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10900b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10900b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10900bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10900c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10900cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10900d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10900dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10900e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10900ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10900f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10900fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x109010310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x109010a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x109011150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1090119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x109012110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1090123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1090129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x109013650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x109013b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x109013e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1090142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1090145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x109014e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x109015380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x109015640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x109015ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x109015f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x109016420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1090168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x109016d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x109017200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1090176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x109017b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x109017fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1090182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1090188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x109018ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1090197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x109019df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10901a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10901aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10901b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10901b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10901bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10901c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10901c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10901cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10901d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10901d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10901de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10901e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10901e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10901ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10901eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10901f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10901f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10901fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x109020150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1090205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x109020a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x109020f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1090213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x109021870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x109021dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x109022310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x109022860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x109022db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x109023300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x109023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x109023da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1090242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x109024840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x109024d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x109104230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1091046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x109104b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x109104f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1091053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x109105860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x109105cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x109106140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1091065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x109106a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x109106e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x109107300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x109107770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x109107be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x109108050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1091084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x109108930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x109108da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x109109210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x109109680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x109109af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x109109f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10910a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10910a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10910acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10910b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10910b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10910ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10910be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10910c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10910c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10910cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10910d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10910d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10910d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10910dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10910e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10910e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10910ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10910ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10910fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10910fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x109110130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1091105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x109110a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x109110e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1091112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x109111760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x109111bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x109112040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1091124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x109112920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x109112d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x109113200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x109113670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x109113ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x109113f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1091143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x109114830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x109114ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x109115110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x109115580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1091159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x109115e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1091162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x109116740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x109116bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x109117020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x109117490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x109117900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x109117d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1091181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x109118650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x109118ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x109118f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1091193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x109119810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x109119c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10911a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10911a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10911a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10911ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10911b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10911b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10911bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10911c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10911c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10911c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10911cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10911d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10911d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10911daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10911df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10911e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10911e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10911ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10911f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10911f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10911f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10911fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x109120290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x109120700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x109120b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x109120fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x109121450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1091218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x109121d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1091221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x109122610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x109122a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x109122ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x109123360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1091237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x109123c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1091240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x109124520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x109124990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x109124e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x109125270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1091256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x109125c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x109126090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x109126500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x109126970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x109126de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x109127300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x109127810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x109128380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x109128640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x109128c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1091291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x109129780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x109129d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10912a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10912a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10912ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10912b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10912ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10912bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10912c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10912cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10912d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10912d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10912dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10912e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10912e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10912edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10912f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10912f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10912ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1091304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x109130a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x109131040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x109131600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x109131bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x109132180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x109132740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x109132d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1091332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x109133880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x109133e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x109134400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1091349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x109134f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x109135540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x109135b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1091360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x109136680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x109136c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x109137200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1091377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x109137d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x109138340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x109138900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x109138ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x109139480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x109139a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10913a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10913a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10913ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10913b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10913b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10913bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10913c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10913c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10913cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10913d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10913d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10913dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10913e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10913e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10913eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10913f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10913f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10913fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10913ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x109140440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x109140940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x109140e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x109141340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x109141d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x109142470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x109142b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1091432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x109143570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x109143d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x109144020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x109144630 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.731.545 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.731.549 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10e704bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10e705030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10e7054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10e705910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10e705d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10e7061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10e706660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10e706ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10e706f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10e7073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10e707820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10e707ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10e708a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10e7091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10e7099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10e70a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10e70a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10e70af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10e70b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10e70be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10e70c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10e70cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10e70d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10e70da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10e70e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10e70e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10e70e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10e70eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10e70f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10e70f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10e70f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10e70fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10e710290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10e710550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10e7109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10e710e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10e7112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10e711710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10e711b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10e711ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10e712460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10e7128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10e712d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10e7131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10e713620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10e713a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10e713f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10e714370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10e7147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10e714c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10e7150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10e715530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10e7159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10e715e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10e716280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10e7166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10e716c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10e717160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10e7175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10e717a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10e717eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10e718320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10e718790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10e718c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10e719070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10e7194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10e719950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10e719dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10e71a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10e71a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10e71ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10e71af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10e71b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10e71b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10e71bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10e71c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10e71c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10e71ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10e71ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10e71d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10e71d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10e71dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10e71e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10e71e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10e71e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10e71eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10e71f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10e71f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10e71faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10e71ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10e7203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10e720840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10e720cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10e721120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10e721590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10e721a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10e721e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10e7222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10e722750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10e722bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10e723030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10e7234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10e723910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10e723d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10e7241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10e724660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10e724ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10e724f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10e7253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10e725820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10e725c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10e726100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10e726570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10e7269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10e726e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10e7272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10e727730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10e727ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10e728010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10e728480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10e7288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10e728d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10e7291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10e729640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10e729ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10e729f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10e72a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10e72a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10e72ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10e72b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10e72b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10e72b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10e72be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10e72c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10e72c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10e72cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10e72cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10e72d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10e72d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10e72dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10e72e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10e72e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10e72ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10e72ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10e72f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10e72f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10e72fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10e7300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10e730530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10e7309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10e730e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10e731280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10e7316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10e731b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10e731fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10e732440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10e7328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10e732d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10e733190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10e733600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10e733a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10e733ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10e734350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10e7347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10e734c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10e7350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10e735cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10e735f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10e736250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10e7366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10e736b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10e736fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10e737410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10e737880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10e737cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10e738160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10e7385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10e738a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10e738eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10e739320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10e739790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10e739c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10e73a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10e73a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10e73a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10e73adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10e73b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10e73b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10e73bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10e73bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10e73c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10e73c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10e73ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10e73d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10e73d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10e73da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10e73de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10e73e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10e73e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10e73ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10e73f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10e73f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10e73fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10e73ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10e7403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10e740810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10e740c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10e7410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10e741610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10e741b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10e742690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10e742950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10e742f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10e7434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10e743a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10e744050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10e744610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10e744bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10e745190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10e745750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10e745d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10e7462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10e746890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10e746e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10e747410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10e7479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10e747f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10e748550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10e748b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10e7490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10e749690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10e749c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10e74a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10e74a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10e74ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10e74b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10e74b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10e74bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10e74c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10e74ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10e74d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10e74d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10e74db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10e74e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10e74e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10e74ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10e74f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10e74f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10e74fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10e7503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10e750990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10e750f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10e751510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10e751ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10e752090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10e752650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10e752c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10e7531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10e753790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10e753d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10e754310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10e7548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10e754e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10e755450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10e755a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10e755fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10e756590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10e756b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10e757050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10e757550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10e757a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10e757f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10e758450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10e758950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10e758e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10e759350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10e759850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10e759d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10e75a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10e75a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10e75ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10e75b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10e75b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10e75c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10e75c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10e75cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10e75d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10e75d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10e75e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10e75e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10e75e940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1090088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x109008e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10901acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10901b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10901d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x109012690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x109019aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10901a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x109018b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x109018560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10901b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10901a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10900bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10901bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x109014870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x109014b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x109012ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x109012f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x109013220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x109019180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x109019440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x109025050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x109025310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1090255d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x109025890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x109025b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x109025e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1090260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x109026390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x109026650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x109026910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x109026bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x109026e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x109027150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x109027410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1090276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x109027990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x109027c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x109027f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1090281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x109028490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x109028750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x109028a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x109028cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x109029210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x109029750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x109029c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x109029f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10902a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10902a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10902af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10902b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10902b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10902b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10902bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10902c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10902c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10902cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10902d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10902d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10902de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10902e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10902e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10902ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10902f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10902f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10902fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10902fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x109030360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x109030800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x109030ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x109031140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1090315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x109031b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x109032080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1090325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x109032b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x109033070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1090335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x109033b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x109034060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1090345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x109034b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x109035050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1090355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x109035af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x109036040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x109036590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x109036ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x109037030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x109037580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x109037ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x109038020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x109038570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x109038ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x109039010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x109039560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x109039ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10903a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10903a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10903aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10903aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10903b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10903ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10903bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10903c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10903ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10903cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10903d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10903da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10903dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10903e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10903ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10903ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10903f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10903f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10903fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x109040180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x109040620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x109040ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x109040f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x109041400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1090418a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x109041d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1090421e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x109042680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x109042b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x109042fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x109043460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x109043900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x109043da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x109044240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1090446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x109044b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x109045020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1090454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x109045960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x109045e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1090462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x109046740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x109046be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x109047080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x109047520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1090479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x109047e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x109048300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1090487a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x109048c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1090490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x109049580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x109049a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x109049ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10904a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10904a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10904aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10904b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10904b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10904ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10904bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10904c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10904c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10904cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10904d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10904d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10904dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10904df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10904e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10904e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10904ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10904f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10904f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10904fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10904ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x109050480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x109050920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x109050dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x109051260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x109051700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x109051ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x109052040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1090524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x109052980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x109052e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1090532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x109053760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x109053c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1090540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x109054540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1090549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x109054e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x109055320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1090557c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x109055c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1090561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x109056700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x109056c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1090571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x109057460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x109057a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x109058080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x109058690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x109058e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x109059320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1090595e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x109059bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10905a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10905a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10905ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10905b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10905b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10905bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10905c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10905ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10905cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10905d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10905da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10905df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10905e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10905ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10905ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10905f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10905f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10905ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x109060490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1090609e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x109060f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x109061480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1090619d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x109061f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x109062470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1090629c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x109062f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x109063460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1090639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x109063f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x109064450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1090649a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x109064ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x109065440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x109065990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x109065ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x109066430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x109066980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x109066ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x109067420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x109067970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x109067ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x109068410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x109068960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x109068eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x109069400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x109069950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x109069ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10906a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10906a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10906ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10906b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10906b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10906be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10906c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10906c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10906ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10906d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10906d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10906de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10906e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10906e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10906eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10906f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10906f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10906fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x109070020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1090704c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x109070960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x109070e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1090712a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x109071740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x109071be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x109072080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x109072520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1090729c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x109072e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1090733b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x109073ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1090741f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x109074910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x109075030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1090752f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x109075ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x109075da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1090763b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.778s
user	0m0.264s
sys	0m0.317s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4680 (9ac3457b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14410aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14410b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14410b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14410bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14410c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14410c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14410cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14410d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14410d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14410de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14410e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14410e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14410f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14410fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144110310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144110a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144111150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144111870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144111f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144112760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144112e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1441135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144113cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144114560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144114c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144114f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144115550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1441161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144116700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1441169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144116e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144117120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1441179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144117ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1441181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144118650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144118af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144118f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144119430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1441198d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144119d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14411a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14411a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14411ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14411ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14411b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14411ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14411c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14411c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14411cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14411d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14411db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14411e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14411e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14411efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14411f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14411f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14411fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1441201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1441209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144120c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144121100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1441215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144121a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144121ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144122380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144122820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144122cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144123160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144123600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144123aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144123f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1441243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144124930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144124e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1441253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144125920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144125e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1441263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144126910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144126e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1441273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144127900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144127e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1441283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1441288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x144128e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x144129390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1441298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144129e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14412a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14412a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14412ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14412b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14412b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14412be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14412c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14411c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14412c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14412cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14412d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14412da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14412df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14412e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14412ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14412ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144008870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144008b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144008fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144009410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144009880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144009cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14400a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14400a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14400aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14400aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14400b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14400b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14400bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14400c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14400c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14400c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14400cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14400d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14400d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14400dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14400e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14400e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14400e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14400ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14400f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14400f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14400fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144010010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144010480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1440108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144010d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1440111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144011640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144011ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144011f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144012390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144012800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144012c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1440130e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144013550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1440139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144013e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1440142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144014710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144014b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144014ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144015460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1440158d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144015d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1440161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144016620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144016a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144016f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144017370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1440177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144017c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1440180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144018530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1440189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144018e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1440196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144019b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144019fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14401a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14401a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14401ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14401b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14401b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14401ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14401bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14401c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14401c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14401cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14401d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14401d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14401d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14401ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14401e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14401e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14401eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14401efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14401f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14401f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14401fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144020170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1440205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144020a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144020ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144021330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1440217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144022080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1440224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144022960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x144022dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144023240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1440236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144023b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x144023f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144024400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144024870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144024ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144025150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144025d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x144025ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1440266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144026c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x144027210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1440277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144027d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x144028320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1440288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144028e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x144029430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1440299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144029f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14402a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14402aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14402b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14402b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14402bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14402c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14402c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14402cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14402d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14402d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14402de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14402e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14402e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14402ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14402f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14402fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144030040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1440305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144030ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144031150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144031700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144031cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144032260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144032810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144032dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144033370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144033920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144033ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144034480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144034a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144035590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144035b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1440360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1440366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144036c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144037200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1440377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144037d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x144038310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1440388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144038e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144039420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1440399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x144039f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14403a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14403a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14403ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14403b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14403b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14403bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14403c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14403c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14403cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14403d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14403d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14403db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14403e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14403e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14403ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14403f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14403fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1440402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1440409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144040cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1440414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144041760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144041d70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.113.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.113.392 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14410b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14410b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14411d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14411de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14411fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144115200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14411bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14411c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14411cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14411b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14411b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14411e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144114200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14410a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144120470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14412ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1441173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1441176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144115810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144115ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144115d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14412f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14412f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14412f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14412fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14412fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14412ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1441302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144130560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144130820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144130ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144130da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144131060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144131320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1441315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1441318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144131b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144131e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1441320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1441323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144132660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144132920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144132be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144132ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144133160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144133420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1441336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1441339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144133c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144133f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1441341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1441344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144134760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144134a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x144134ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144134fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144135260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x144135520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1441357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144135aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144135d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144136020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1441362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1441365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144136860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144136b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144136fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144137460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144137900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144137da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144138240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1441386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144138b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1441390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144139620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144139b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14413a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14413a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14413ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14413b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14413b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14413bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14413c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14413c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14413cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14413d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14413d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14413db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14413e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14413e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14413eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14413f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14413f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14413fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x144140060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1441405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x144140b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144141050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1441415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144141af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x144142040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144142590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x144142ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144143030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x144143580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144143ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144144020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144144570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144144ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144145010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144145560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144145ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144146000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1441464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144146940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144146de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144147280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144147720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144147bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144148060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144148500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1441489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144148e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1441492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144149780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144149c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14414a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14414a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14414aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14414aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14414b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14414b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14414bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14414c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14414c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14414ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14414cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14414d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14414d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14414dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14414e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14414e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14414eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14414ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14414f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14414f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14414fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1441501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144150680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144150b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144150fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144151460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144151900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144151da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144152240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1441526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144152b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144153020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1441534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144153960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144153e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1441542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144154740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144154be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144155080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144155520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1441557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144155cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1441561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1441566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144156bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1441570b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1441575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144157ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144157fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1441584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1441589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144158eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1441593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1441598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144159db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14415a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14415a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14415acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14415b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14415b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14415bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14415c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14415c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14415cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14415cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14415d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14415d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14415deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14415e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14415ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14415efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14415f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14415fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144160190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1441607a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x144160f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144161430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1441616f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144161d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x144162310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144162b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144162fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144163440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1441638e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144164090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1441645e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144164b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144165080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1441655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144165b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144166070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1441665c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144166b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144167060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1441675b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144167b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144168050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1441685a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144168af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144169040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144169590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144169ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14416a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14416a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14416aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14416b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14416b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14416bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14416c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14416c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14416cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14416d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14416d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14416daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14416dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14416e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14416ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14416efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14416f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14416fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14416ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144170520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144170a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144170fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144171510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144171a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144171fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144172500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144172a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144172fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1441734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144173a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144173f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1441744e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144174a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144174f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1441754d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144175a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144175f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1441764c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144176a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x144176eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x144177350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1441777f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144177c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144178130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1441785d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144178a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144178f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1441793b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144179850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144179cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14417a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14417a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14417aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14417af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14417b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14417bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14417c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14417ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14417d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14417d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14417dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14417deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14417e4c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143706630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143706aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143706f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143707380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1437077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143707c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1437080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143708540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1437089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143708eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143709320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1437099a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14370a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14370ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14370b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14370bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14370c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14370c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14370d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14370d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14370dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14370e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14370ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14370f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14370fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14370ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1437101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143710660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143710ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143710f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1437113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1437118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143711d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143712010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143712480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1437128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143712d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1437131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143713640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143713ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143713f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143714390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143714800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143714c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1437150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143715550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1437159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143715e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1437162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143716710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143716b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143716ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143717460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1437178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143717d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1437181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143718720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143718c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143719090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143719500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143719970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143719de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14371a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14371a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14371ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14371afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14371b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14371b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14371bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14371c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14371c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14371ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14371ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14371d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14371d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14371dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14371e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14371e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14371e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14371edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14371f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14371f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14371fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14371ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1437203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143720860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143720cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143721140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1437215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143721a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143721e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143722300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143722770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143722be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143723050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1437234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143723930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143723da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143724210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143724680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143724af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143724f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1437253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143725980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143725df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143726260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1437266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143726b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143726fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143727420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143727890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143727d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143728170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1437285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143728a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143728ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143729330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1437297a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143729c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14372a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14372a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14372a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14372add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14372b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14372b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14372bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14372bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14372c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14372c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14372cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14372d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14372d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14372da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14372dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14372e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14372e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14372ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14372f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14372f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14372f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14372fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143730220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143730690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143730b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143730f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1437313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143731850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143731cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143732130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1437325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143732a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143732e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1437332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143733760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143733bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143734040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1437344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143734920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143734d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143735200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143735670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143735ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143735f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1437363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143736830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143736ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143737110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143737580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1437379f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143737e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1437382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143738740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143738bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143739020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143739490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143739900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143739d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14373a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14373a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14373aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14373af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14373b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14373b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14373bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14373c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14373c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14373c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14373ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14373d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14373d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14373db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14373e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14373e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14373e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14373ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14373f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14373f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14373faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14373ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143740380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1437407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143740c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1437410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143741540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1437419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143741e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143742290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143742820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143742c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143743100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143743c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143743f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1437441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143744640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143744ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143744f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143745390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143745800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143745c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1437460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143746550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1437469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143746e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1437472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143747710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143747b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143747ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143748460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1437488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143748d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1437491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143749620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143749a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143749f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14374a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14374a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14374ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14374b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14374b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14374b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14374be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14374c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14374c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14374cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14374cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14374d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14374d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14374dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14374e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14374e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14374ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14374eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14374f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14374f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14374fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1437500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143750510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143750980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143750df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143751260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1437516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143751b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143751fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143752420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143752890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143752d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143753170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1437535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143753a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143753ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143754330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1437547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143754c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143755080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1437554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143755960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143755dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143756240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1437566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143756b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143756f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143757400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143757870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1437582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143758a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143759120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143759840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143759b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143759f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14375a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14375ab80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.977s
user	0m0.236s
sys	0m0.192s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.45 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.45 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.90 sec*proc (2 tests)

Total Test time (real) =   1.92 sec
        1.94 real         0.51 user         0.23 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.56 real         0.13 user         0.08 sys
```
