### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.29 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.66 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.65 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.40 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.31 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.32 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.94 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.31 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.31 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.21 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.84 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  180.19 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.92 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.39 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.58 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 223.77 sec*proc (28 tests)

Total Test time (real) = 223.78 sec

real	3m43.909s
user	7m39.260s
sys	0m6.422s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.19 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.38 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.41 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.14 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.55 sec*proc (28 tests)

Total Test time (real) =  51.56 sec

real	0m51.573s
user	1m11.846s
sys	0m5.697s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.134 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.560 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.953 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.963 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.027.964 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.964 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.027.965 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.027.966 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.027.967 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.027.968 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.027.969 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.027.969 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.027.970 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.027.973 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.978 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.978 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.027.979 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.027.979 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.027.980 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.027.981 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.033.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.034.208 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.211 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.034.211 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.034.212 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.034.212 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.034.212 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.034.213 I llama_model_loader: - type  f32:  124 tensors
0.00.034.214 I llama_model_loader: - type  f16:   73 tensors
0.00.034.214 I print_info: file format = GGUF V3 (latest)
0.00.034.215 I print_info: file type   = F16
0.00.034.216 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.038.638 I load: special tokens cache size = 5
0.00.040.641 I load: token to piece cache size = 0.2032 MB
0.00.040.645 I print_info: arch             = bert
0.00.040.645 I print_info: vocab_only       = 0
0.00.040.645 I print_info: n_ctx_train      = 512
0.00.040.646 I print_info: n_embd           = 384
0.00.040.646 I print_info: n_layer          = 12
0.00.040.649 I print_info: n_head           = 12
0.00.040.650 I print_info: n_head_kv        = 12
0.00.040.651 I print_info: n_rot            = 32
0.00.040.651 I print_info: n_swa            = 0
0.00.040.651 I print_info: n_embd_head_k    = 32
0.00.040.651 I print_info: n_embd_head_v    = 32
0.00.040.652 I print_info: n_gqa            = 1
0.00.040.653 I print_info: n_embd_k_gqa     = 384
0.00.040.654 I print_info: n_embd_v_gqa     = 384
0.00.040.655 I print_info: f_norm_eps       = 1.0e-12
0.00.040.656 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.656 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.656 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.656 I print_info: f_logit_scale    = 0.0e+00
0.00.040.657 I print_info: n_ff             = 1536
0.00.040.658 I print_info: n_expert         = 0
0.00.040.658 I print_info: n_expert_used    = 0
0.00.040.658 I print_info: causal attn      = 0
0.00.040.658 I print_info: pooling type     = 2
0.00.040.659 I print_info: rope type        = 2
0.00.040.659 I print_info: rope scaling     = linear
0.00.040.659 I print_info: freq_base_train  = 10000.0
0.00.040.660 I print_info: freq_scale_train = 1
0.00.040.660 I print_info: n_ctx_orig_yarn  = 512
0.00.040.660 I print_info: rope_finetuned   = unknown
0.00.040.660 I print_info: ssm_d_conv       = 0
0.00.040.661 I print_info: ssm_d_inner      = 0
0.00.040.661 I print_info: ssm_d_state      = 0
0.00.040.661 I print_info: ssm_dt_rank      = 0
0.00.040.661 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.661 I print_info: model type       = 33M
0.00.040.662 I print_info: model params     = 33.21 M
0.00.040.663 I print_info: general.name     = Bge Small
0.00.040.665 I print_info: vocab type       = WPM
0.00.040.665 I print_info: n_vocab          = 30522
0.00.040.665 I print_info: n_merges         = 0
0.00.040.666 I print_info: BOS token        = 101 '[CLS]'
0.00.040.666 I print_info: UNK token        = 100 '[UNK]'
0.00.040.668 I print_info: SEP token        = 102 '[SEP]'
0.00.040.668 I print_info: PAD token        = 0 '[PAD]'
0.00.040.669 I print_info: MASK token       = 103 '[MASK]'
0.00.040.669 I print_info: LF token         = 0 '[PAD]'
0.00.040.670 I print_info: max token length = 21
0.00.042.721 I load_tensors: offloading 12 repeating layers to GPU
0.00.042.721 I load_tensors: offloading output layer to GPU
0.00.042.722 I load_tensors: offloaded 13/13 layers to GPU
0.00.042.752 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.042.753 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.043.012 I llama_init_from_model: n_seq_max     = 1
0.00.043.013 I llama_init_from_model: n_ctx         = 512
0.00.043.013 I llama_init_from_model: n_ctx_per_seq = 512
0.00.043.014 I llama_init_from_model: n_batch       = 2048
0.00.043.014 I llama_init_from_model: n_ubatch      = 2048
0.00.043.014 I llama_init_from_model: flash_attn    = 0
0.00.043.015 I llama_init_from_model: freq_base     = 10000.0
0.00.043.015 I llama_init_from_model: freq_scale    = 1
0.00.043.016 I ggml_metal_init: allocating
0.00.043.020 I ggml_metal_init: found device: Apple M4
0.00.043.023 I ggml_metal_init: picking default device: Apple M4
0.00.043.873 I ggml_metal_init: using embedded metal library
0.00.048.178 I ggml_metal_init: GPU name:   Apple M4
0.00.048.181 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.048.181 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.048.182 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.048.182 I ggml_metal_init: simdgroup reduction   = true
0.00.048.182 I ggml_metal_init: simdgroup matrix mul. = true
0.00.048.182 I ggml_metal_init: has bfloat            = true
0.00.048.183 I ggml_metal_init: use bfloat            = true
0.00.048.183 I ggml_metal_init: hasUnifiedMemory      = true
0.00.048.184 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.060.490 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.061.170 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.061.173 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.061.174 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.061.956 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.061.958 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.061.958 I llama_init_from_model: graph nodes  = 429
0.00.061.958 I llama_init_from_model: graph splits = 2
0.00.061.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.061.960 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.068.301 I 
0.00.068.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.068.959 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.073.685 I llama_perf_context_print:        load time =      45.73 ms
0.00.073.686 I llama_perf_context_print: prompt eval time =       4.56 ms /     9 tokens (    0.51 ms per token,  1972.39 tokens per second)
0.00.073.686 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.073.687 I llama_perf_context_print:       total time =       5.39 ms /    10 tokens
0.00.073.829 I ggml_metal_free: deallocating

real	0m0.253s
user	0m0.051s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.440 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.253 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.256 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.258 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.258 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.258 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.259 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.259 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.260 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.260 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.260 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.261 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.263 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.265 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.269 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.269 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.270 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.270 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.270 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.797 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.493 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.494 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.495 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.495 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.496 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.496 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.496 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.497 I llama_model_loader: - type  f32:  124 tensors
0.00.015.497 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.497 I print_info: file format = GGUF V3 (latest)
0.00.015.498 I print_info: file type   = Q8_0
0.00.015.499 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.077 I load: special tokens cache size = 5
0.00.019.456 I load: token to piece cache size = 0.2032 MB
0.00.019.459 I print_info: arch             = bert
0.00.019.460 I print_info: vocab_only       = 0
0.00.019.460 I print_info: n_ctx_train      = 512
0.00.019.460 I print_info: n_embd           = 384
0.00.019.460 I print_info: n_layer          = 12
0.00.019.464 I print_info: n_head           = 12
0.00.019.465 I print_info: n_head_kv        = 12
0.00.019.465 I print_info: n_rot            = 32
0.00.019.465 I print_info: n_swa            = 0
0.00.019.466 I print_info: n_embd_head_k    = 32
0.00.019.466 I print_info: n_embd_head_v    = 32
0.00.019.466 I print_info: n_gqa            = 1
0.00.019.467 I print_info: n_embd_k_gqa     = 384
0.00.019.467 I print_info: n_embd_v_gqa     = 384
0.00.019.468 I print_info: f_norm_eps       = 1.0e-12
0.00.019.468 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.468 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.469 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.469 I print_info: f_logit_scale    = 0.0e+00
0.00.019.469 I print_info: n_ff             = 1536
0.00.019.470 I print_info: n_expert         = 0
0.00.019.470 I print_info: n_expert_used    = 0
0.00.019.470 I print_info: causal attn      = 0
0.00.019.470 I print_info: pooling type     = 2
0.00.019.472 I print_info: rope type        = 2
0.00.019.472 I print_info: rope scaling     = linear
0.00.019.473 I print_info: freq_base_train  = 10000.0
0.00.019.473 I print_info: freq_scale_train = 1
0.00.019.473 I print_info: n_ctx_orig_yarn  = 512
0.00.019.473 I print_info: rope_finetuned   = unknown
0.00.019.474 I print_info: ssm_d_conv       = 0
0.00.019.474 I print_info: ssm_d_inner      = 0
0.00.019.474 I print_info: ssm_d_state      = 0
0.00.019.474 I print_info: ssm_dt_rank      = 0
0.00.019.474 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.474 I print_info: model type       = 33M
0.00.019.475 I print_info: model params     = 33.21 M
0.00.019.475 I print_info: general.name     = Bge Small
0.00.019.475 I print_info: vocab type       = WPM
0.00.019.476 I print_info: n_vocab          = 30522
0.00.019.478 I print_info: n_merges         = 0
0.00.019.478 I print_info: BOS token        = 101 '[CLS]'
0.00.019.478 I print_info: UNK token        = 100 '[UNK]'
0.00.019.478 I print_info: SEP token        = 102 '[SEP]'
0.00.019.479 I print_info: PAD token        = 0 '[PAD]'
0.00.019.479 I print_info: MASK token       = 103 '[MASK]'
0.00.019.479 I print_info: LF token         = 0 '[PAD]'
0.00.019.479 I print_info: max token length = 21
0.00.020.855 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.858 I load_tensors: offloading output layer to GPU
0.00.020.858 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.866 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.867 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.030 I llama_init_from_model: n_seq_max     = 1
0.00.021.031 I llama_init_from_model: n_ctx         = 512
0.00.021.031 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.032 I llama_init_from_model: n_batch       = 2048
0.00.021.032 I llama_init_from_model: n_ubatch      = 2048
0.00.021.032 I llama_init_from_model: flash_attn    = 0
0.00.021.032 I llama_init_from_model: freq_base     = 10000.0
0.00.021.033 I llama_init_from_model: freq_scale    = 1
0.00.021.033 I ggml_metal_init: allocating
0.00.021.036 I ggml_metal_init: found device: Apple M4
0.00.021.038 I ggml_metal_init: picking default device: Apple M4
0.00.021.684 I ggml_metal_init: using embedded metal library
0.00.024.252 I ggml_metal_init: GPU name:   Apple M4
0.00.024.255 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.255 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.255 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.256 I ggml_metal_init: simdgroup reduction   = true
0.00.024.256 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.256 I ggml_metal_init: has bfloat            = true
0.00.024.256 I ggml_metal_init: use bfloat            = true
0.00.024.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.259 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.601 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.100 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.104 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.106 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.744 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.745 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.746 I llama_init_from_model: graph nodes  = 429
0.00.035.746 I llama_init_from_model: graph splits = 2
0.00.035.748 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.748 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.109 I 
0.00.041.139 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.694 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.046.215 I llama_perf_context_print:        load time =      31.66 ms
0.00.046.216 I llama_perf_context_print: prompt eval time =       4.40 ms /     9 tokens (    0.49 ms per token,  2047.32 tokens per second)
0.00.046.217 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.046.217 I llama_perf_context_print:       total time =       5.11 ms /    10 tokens
0.00.046.400 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.032s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.202 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.797 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.036 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.044 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.040.045 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.046 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.040.047 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.040.048 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.040.049 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.040.050 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.040.050 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.040.051 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.040.052 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.040.055 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.040.056 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.040.057 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.040.058 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.058 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.048.057 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.050.221 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.055.032 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.032 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.055.033 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.055.033 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.055.033 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.055.034 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.055.034 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.055.035 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.055.035 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.055.035 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.055.036 I llama_model_loader: - type  f32:   40 tensors
0.00.055.036 I llama_model_loader: - type  f16:   30 tensors
0.00.055.037 I print_info: file format = GGUF V3 (latest)
0.00.055.037 I print_info: file type   = F16
0.00.055.039 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.071.906 W load: empty token at index 5
0.00.076.599 W load: model vocab missing newline token, using special_pad_id instead
0.00.077.932 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.077.965 I load: special tokens cache size = 5
0.00.342.561 I load: token to piece cache size = 1.5060 MB
0.00.342.567 I print_info: arch             = jina-bert-v2
0.00.342.567 I print_info: vocab_only       = 0
0.00.342.567 I print_info: n_ctx_train      = 8192
0.00.342.568 I print_info: n_embd           = 384
0.00.342.568 I print_info: n_layer          = 4
0.00.342.573 I print_info: n_head           = 12
0.00.342.573 I print_info: n_head_kv        = 12
0.00.342.574 I print_info: n_rot            = 32
0.00.342.574 I print_info: n_swa            = 0
0.00.342.574 I print_info: n_embd_head_k    = 32
0.00.342.574 I print_info: n_embd_head_v    = 32
0.00.342.575 I print_info: n_gqa            = 1
0.00.342.575 I print_info: n_embd_k_gqa     = 384
0.00.342.576 I print_info: n_embd_v_gqa     = 384
0.00.342.577 I print_info: f_norm_eps       = 1.0e-12
0.00.342.578 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.342.580 I print_info: f_clamp_kqv      = 0.0e+00
0.00.342.580 I print_info: f_max_alibi_bias = 8.0e+00
0.00.342.580 I print_info: f_logit_scale    = 0.0e+00
0.00.342.581 I print_info: n_ff             = 1536
0.00.342.581 I print_info: n_expert         = 0
0.00.342.584 I print_info: n_expert_used    = 0
0.00.342.584 I print_info: causal attn      = 0
0.00.342.584 I print_info: pooling type     = -1
0.00.342.584 I print_info: rope type        = -1
0.00.342.585 I print_info: rope scaling     = linear
0.00.342.585 I print_info: freq_base_train  = 10000.0
0.00.342.586 I print_info: freq_scale_train = 1
0.00.342.586 I print_info: n_ctx_orig_yarn  = 8192
0.00.342.587 I print_info: rope_finetuned   = unknown
0.00.342.587 I print_info: ssm_d_conv       = 0
0.00.342.587 I print_info: ssm_d_inner      = 0
0.00.342.588 I print_info: ssm_d_state      = 0
0.00.342.588 I print_info: ssm_dt_rank      = 0
0.00.342.588 I print_info: ssm_dt_b_c_rms   = 0
0.00.342.588 I print_info: model type       = 33M
0.00.342.589 I print_info: model params     = 32.90 M
0.00.342.589 I print_info: general.name     = Jina Bert Implementation
0.00.342.591 I print_info: vocab type       = BPE
0.00.342.591 I print_info: n_vocab          = 61056
0.00.342.591 I print_info: n_merges         = 39382
0.00.342.592 I print_info: BOS token        = 0 '<s>'
0.00.342.592 I print_info: EOS token        = 2 '</s>'
0.00.342.592 I print_info: UNK token        = 3 '<unk>'
0.00.342.592 I print_info: SEP token        = 2 '</s>'
0.00.342.592 I print_info: PAD token        = 1 '<pad>'
0.00.342.593 I print_info: MASK token       = 4 '<mask>'
0.00.342.593 I print_info: EOG token        = 2 '</s>'
0.00.342.593 I print_info: max token length = 45
0.00.343.808 I load_tensors: offloading 4 repeating layers to GPU
0.00.343.808 I load_tensors: offloading output layer to GPU
0.00.343.812 I load_tensors: offloaded 5/5 layers to GPU
0.00.343.838 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.343.839 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.344.176 I llama_init_from_model: n_seq_max     = 1
0.00.344.177 I llama_init_from_model: n_ctx         = 8192
0.00.344.177 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.344.177 I llama_init_from_model: n_batch       = 2048
0.00.344.178 I llama_init_from_model: n_ubatch      = 2048
0.00.344.178 I llama_init_from_model: flash_attn    = 0
0.00.344.178 I llama_init_from_model: freq_base     = 10000.0
0.00.344.178 I llama_init_from_model: freq_scale    = 1
0.00.344.179 I ggml_metal_init: allocating
0.00.344.182 I ggml_metal_init: found device: Apple M4
0.00.344.184 I ggml_metal_init: picking default device: Apple M4
0.00.345.191 I ggml_metal_init: using embedded metal library
0.00.348.055 I ggml_metal_init: GPU name:   Apple M4
0.00.348.056 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.348.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.348.057 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.348.057 I ggml_metal_init: simdgroup reduction   = true
0.00.348.057 I ggml_metal_init: simdgroup matrix mul. = true
0.00.348.058 I ggml_metal_init: has bfloat            = true
0.00.348.058 I ggml_metal_init: use bfloat            = true
0.00.348.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.348.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.357.525 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.359.996 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.360.000 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.360.005 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.360.587 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.360.588 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.360.589 I llama_init_from_model: graph nodes  = 154
0.00.360.589 I llama_init_from_model: graph splits = 2
0.00.360.590 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.360.590 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.373.430 I 
0.00.373.448 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.373.697 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.373.698 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.373.700 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.373.701 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.373.704 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.373.705 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.374.235 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.377.690 I llama_perf_context_print:        load time =     347.62 ms
0.00.377.690 I llama_perf_context_print: prompt eval time =       3.45 ms /    62 tokens (    0.06 ms per token, 17991.87 tokens per second)
0.00.377.691 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.377.693 I llama_perf_context_print:       total time =       4.26 ms /    63 tokens
0.00.377.856 I ggml_metal_free: deallocating

real	0m1.106s
user	0m0.347s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.114 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.218 I main: llama backend init
0.00.000.222 I main: load the model and apply lora adapter, if any
0.00.024.870 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.055 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.065 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.068 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.072 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.073 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.073 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.074 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.075 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.076 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.076 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.077 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.078 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.078 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.079 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.082 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.082 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.083 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.859 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.853 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.632 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.633 I llama_model_loader: - type  f32:  194 tensors
0.00.047.634 I llama_model_loader: - type  f16:   98 tensors
0.00.047.635 I print_info: file format = GGUF V3 (latest)
0.00.047.637 I print_info: file type   = all F32 (guessed)
0.00.047.638 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.768 I load: special tokens cache size = 25
0.00.072.666 I load: token to piece cache size = 0.2984 MB
0.00.072.669 I print_info: arch             = gptneox
0.00.072.669 I print_info: vocab_only       = 0
0.00.072.669 I print_info: n_ctx_train      = 2048
0.00.072.669 I print_info: n_embd           = 2048
0.00.072.670 I print_info: n_layer          = 24
0.00.072.673 I print_info: n_head           = 16
0.00.072.675 I print_info: n_head_kv        = 16
0.00.072.675 I print_info: n_rot            = 32
0.00.072.675 I print_info: n_swa            = 0
0.00.072.675 I print_info: n_embd_head_k    = 128
0.00.072.675 I print_info: n_embd_head_v    = 128
0.00.072.676 I print_info: n_gqa            = 1
0.00.072.677 I print_info: n_embd_k_gqa     = 2048
0.00.072.678 I print_info: n_embd_v_gqa     = 2048
0.00.072.678 I print_info: f_norm_eps       = 1.0e-05
0.00.072.679 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.679 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.679 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.679 I print_info: f_logit_scale    = 0.0e+00
0.00.072.681 I print_info: n_ff             = 8192
0.00.072.681 I print_info: n_expert         = 0
0.00.072.682 I print_info: n_expert_used    = 0
0.00.072.682 I print_info: causal attn      = 1
0.00.072.682 I print_info: pooling type     = 0
0.00.072.682 I print_info: rope type        = 2
0.00.072.682 I print_info: rope scaling     = linear
0.00.072.683 I print_info: freq_base_train  = 10000.0
0.00.072.683 I print_info: freq_scale_train = 1
0.00.072.683 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.683 I print_info: rope_finetuned   = unknown
0.00.072.683 I print_info: ssm_d_conv       = 0
0.00.072.683 I print_info: ssm_d_inner      = 0
0.00.072.683 I print_info: ssm_d_state      = 0
0.00.072.684 I print_info: ssm_dt_rank      = 0
0.00.072.684 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.684 I print_info: model type       = 1.4B
0.00.072.684 I print_info: model params     = 1.41 B
0.00.072.684 I print_info: general.name     = 1.4B
0.00.072.685 I print_info: vocab type       = BPE
0.00.072.685 I print_info: n_vocab          = 50304
0.00.072.685 I print_info: n_merges         = 50009
0.00.072.686 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.686 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.687 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.687 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.687 I print_info: LF token         = 128 'Ä'
0.00.072.688 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.688 I print_info: max token length = 1024
0.00.075.044 I load_tensors: offloading 24 repeating layers to GPU
0.00.075.044 I load_tensors: offloading output layer to GPU
0.00.075.045 I load_tensors: offloaded 25/25 layers to GPU
0.00.075.065 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.075.066 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.075.344 I llama_init_from_model: n_seq_max     = 1
0.00.075.345 I llama_init_from_model: n_ctx         = 2048
0.00.075.345 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.075.345 I llama_init_from_model: n_batch       = 2048
0.00.075.345 I llama_init_from_model: n_ubatch      = 512
0.00.075.345 I llama_init_from_model: flash_attn    = 0
0.00.075.346 I llama_init_from_model: freq_base     = 10000.0
0.00.075.346 I llama_init_from_model: freq_scale    = 1
0.00.075.347 I ggml_metal_init: allocating
0.00.075.350 I ggml_metal_init: found device: Apple M4
0.00.075.357 I ggml_metal_init: picking default device: Apple M4
0.00.076.012 I ggml_metal_init: using embedded metal library
0.00.106.000 I ggml_metal_init: GPU name:   Apple M4
0.00.106.005 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.106.005 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.106.006 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.106.006 I ggml_metal_init: simdgroup reduction   = true
0.00.106.006 I ggml_metal_init: simdgroup matrix mul. = true
0.00.106.007 I ggml_metal_init: has bfloat            = true
0.00.106.007 I ggml_metal_init: use bfloat            = true
0.00.106.007 I ggml_metal_init: hasUnifiedMemory      = true
0.00.106.009 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.149.396 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.168.723 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.168.727 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.168.750 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.169.706 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.169.708 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.169.708 I llama_init_from_model: graph nodes  = 967
0.00.169.708 I llama_init_from_model: graph splits = 2
0.00.169.711 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.169.841 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.169.842 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.253.031 I main: llama threadpool init, n_threads = 4
0.00.253.070 I 
0.00.253.091 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.253.092 I 
0.00.253.165 I sampler seed: 1234
0.00.253.169 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.253.195 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.253.196 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.253.196 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.099.602 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.02.099.603 I llama_perf_context_print:        load time =     228.15 ms
0.02.099.604 I llama_perf_context_print: prompt eval time =      43.60 ms /     7 tokens (    6.23 ms per token,   160.56 tokens per second)
0.02.099.605 I llama_perf_context_print:        eval time =    1799.92 ms /    63 runs   (   28.57 ms per token,    35.00 tokens per second)
0.02.099.605 I llama_perf_context_print:       total time =    1846.57 ms /    70 tokens
0.02.099.865 I ggml_metal_free: deallocating

real	0m2.398s
user	0m0.123s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.650 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.738 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.778 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.785 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.787 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.788 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.789 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.789 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.794 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.794 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.795 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.796 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.797 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.797 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.646 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.648 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.649 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.649 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.650 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.650 I llama_model_loader: - type  f32:  194 tensors
0.00.048.651 I llama_model_loader: - type  f16:   98 tensors
0.00.048.651 I print_info: file format = GGUF V3 (latest)
0.00.048.652 I print_info: file type   = all F32 (guessed)
0.00.048.653 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.073.788 I load: special tokens cache size = 25
0.00.080.205 I load: token to piece cache size = 0.2984 MB
0.00.080.208 I print_info: arch             = gptneox
0.00.080.208 I print_info: vocab_only       = 0
0.00.080.209 I print_info: n_ctx_train      = 2048
0.00.080.209 I print_info: n_embd           = 2048
0.00.080.209 I print_info: n_layer          = 24
0.00.080.212 I print_info: n_head           = 16
0.00.080.213 I print_info: n_head_kv        = 16
0.00.080.213 I print_info: n_rot            = 32
0.00.080.213 I print_info: n_swa            = 0
0.00.080.213 I print_info: n_embd_head_k    = 128
0.00.080.214 I print_info: n_embd_head_v    = 128
0.00.080.214 I print_info: n_gqa            = 1
0.00.080.215 I print_info: n_embd_k_gqa     = 2048
0.00.080.216 I print_info: n_embd_v_gqa     = 2048
0.00.080.216 I print_info: f_norm_eps       = 1.0e-05
0.00.080.217 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.080.217 I print_info: f_clamp_kqv      = 0.0e+00
0.00.080.217 I print_info: f_max_alibi_bias = 0.0e+00
0.00.080.217 I print_info: f_logit_scale    = 0.0e+00
0.00.080.218 I print_info: n_ff             = 8192
0.00.080.218 I print_info: n_expert         = 0
0.00.080.218 I print_info: n_expert_used    = 0
0.00.080.218 I print_info: causal attn      = 1
0.00.080.218 I print_info: pooling type     = 0
0.00.080.219 I print_info: rope type        = 2
0.00.080.221 I print_info: rope scaling     = linear
0.00.080.221 I print_info: freq_base_train  = 10000.0
0.00.080.222 I print_info: freq_scale_train = 1
0.00.080.222 I print_info: n_ctx_orig_yarn  = 2048
0.00.080.222 I print_info: rope_finetuned   = unknown
0.00.080.222 I print_info: ssm_d_conv       = 0
0.00.080.222 I print_info: ssm_d_inner      = 0
0.00.080.223 I print_info: ssm_d_state      = 0
0.00.080.223 I print_info: ssm_dt_rank      = 0
0.00.080.223 I print_info: ssm_dt_b_c_rms   = 0
0.00.080.223 I print_info: model type       = 1.4B
0.00.080.223 I print_info: model params     = 1.41 B
0.00.080.224 I print_info: general.name     = 1.4B
0.00.080.224 I print_info: vocab type       = BPE
0.00.080.224 I print_info: n_vocab          = 50304
0.00.080.224 I print_info: n_merges         = 50009
0.00.080.225 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.080.228 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.080.228 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.080.229 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.080.229 I print_info: LF token         = 128 'Ä'
0.00.080.230 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.080.230 I print_info: max token length = 1024
0.00.082.690 I load_tensors: offloading 24 repeating layers to GPU
0.00.082.690 I load_tensors: offloading output layer to GPU
0.00.082.691 I load_tensors: offloaded 25/25 layers to GPU
0.00.082.701 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.082.703 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.083.012 I llama_init_from_model: n_seq_max     = 1
0.00.083.013 I llama_init_from_model: n_ctx         = 128
0.00.083.013 I llama_init_from_model: n_ctx_per_seq = 128
0.00.083.013 I llama_init_from_model: n_batch       = 128
0.00.083.013 I llama_init_from_model: n_ubatch      = 128
0.00.083.014 I llama_init_from_model: flash_attn    = 0
0.00.083.014 I llama_init_from_model: freq_base     = 10000.0
0.00.083.014 I llama_init_from_model: freq_scale    = 1
0.00.083.014 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.083.015 I ggml_metal_init: allocating
0.00.083.018 I ggml_metal_init: found device: Apple M4
0.00.083.020 I ggml_metal_init: picking default device: Apple M4
0.00.083.603 I ggml_metal_init: using embedded metal library
0.00.086.107 I ggml_metal_init: GPU name:   Apple M4
0.00.086.109 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.109 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.109 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.110 I ggml_metal_init: simdgroup reduction   = true
0.00.086.110 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.110 I ggml_metal_init: has bfloat            = true
0.00.086.110 I ggml_metal_init: use bfloat            = true
0.00.086.111 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.111 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.462 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.740 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.096.743 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.096.758 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.097.700 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.097.701 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.097.701 I llama_init_from_model: graph nodes  = 967
0.00.097.702 I llama_init_from_model: graph splits = 2
0.00.097.703 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.097.703 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.987.656 I 
0.00.987.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.987.730 I perplexity: tokenizing the input ..
0.01.000.919 I perplexity: tokenization took 13.187 ms
0.01.000.926 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.123.243 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.125.111 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.125.170 I llama_perf_context_print:        load time =     968.91 ms
0.01.125.171 I llama_perf_context_print: prompt eval time =     121.46 ms /   128 tokens (    0.95 ms per token,  1053.85 tokens per second)
0.01.125.173 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.125.173 I llama_perf_context_print:       total time =     137.51 ms /   129 tokens
0.01.125.835 I ggml_metal_free: deallocating

real	0m1.317s
user	0m0.119s
sys	0m0.195s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.791 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.237 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.243 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.244 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.245 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.245 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.245 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.246 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.247 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.247 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.248 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.248 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.249 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.251 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.251 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.184 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.128 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.130 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.130 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.131 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.131 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.131 I llama_model_loader: - type  f32:  194 tensors
0.00.035.132 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.132 I print_info: file format = GGUF V3 (latest)
0.00.035.133 I print_info: file type   = Q8_0
0.00.035.134 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.037 I load: special tokens cache size = 25
0.00.061.206 I load: token to piece cache size = 0.2984 MB
0.00.061.210 I print_info: arch             = gptneox
0.00.061.210 I print_info: vocab_only       = 0
0.00.061.211 I print_info: n_ctx_train      = 2048
0.00.061.211 I print_info: n_embd           = 2048
0.00.061.211 I print_info: n_layer          = 24
0.00.061.217 I print_info: n_head           = 16
0.00.061.218 I print_info: n_head_kv        = 16
0.00.061.218 I print_info: n_rot            = 32
0.00.061.219 I print_info: n_swa            = 0
0.00.061.219 I print_info: n_embd_head_k    = 128
0.00.061.219 I print_info: n_embd_head_v    = 128
0.00.061.220 I print_info: n_gqa            = 1
0.00.061.220 I print_info: n_embd_k_gqa     = 2048
0.00.061.221 I print_info: n_embd_v_gqa     = 2048
0.00.061.221 I print_info: f_norm_eps       = 1.0e-05
0.00.061.222 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.222 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.222 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.223 I print_info: f_logit_scale    = 0.0e+00
0.00.061.223 I print_info: n_ff             = 8192
0.00.061.224 I print_info: n_expert         = 0
0.00.061.224 I print_info: n_expert_used    = 0
0.00.061.224 I print_info: causal attn      = 1
0.00.061.224 I print_info: pooling type     = 0
0.00.061.224 I print_info: rope type        = 2
0.00.061.225 I print_info: rope scaling     = linear
0.00.061.225 I print_info: freq_base_train  = 10000.0
0.00.061.225 I print_info: freq_scale_train = 1
0.00.061.226 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.226 I print_info: rope_finetuned   = unknown
0.00.061.226 I print_info: ssm_d_conv       = 0
0.00.061.226 I print_info: ssm_d_inner      = 0
0.00.061.226 I print_info: ssm_d_state      = 0
0.00.061.228 I print_info: ssm_dt_rank      = 0
0.00.061.228 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.228 I print_info: model type       = 1.4B
0.00.061.229 I print_info: model params     = 1.41 B
0.00.061.229 I print_info: general.name     = 1.4B
0.00.061.230 I print_info: vocab type       = BPE
0.00.061.230 I print_info: n_vocab          = 50304
0.00.061.230 I print_info: n_merges         = 50009
0.00.061.231 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.231 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.231 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.231 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.232 I print_info: LF token         = 128 'Ä'
0.00.061.232 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.232 I print_info: max token length = 1024
0.00.063.657 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.657 I load_tensors: offloading output layer to GPU
0.00.063.657 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.669 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.670 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.000 I llama_init_from_model: n_seq_max     = 1
0.00.064.000 I llama_init_from_model: n_ctx         = 2048
0.00.064.001 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.001 I llama_init_from_model: n_batch       = 2048
0.00.064.001 I llama_init_from_model: n_ubatch      = 512
0.00.064.001 I llama_init_from_model: flash_attn    = 0
0.00.064.002 I llama_init_from_model: freq_base     = 10000.0
0.00.064.002 I llama_init_from_model: freq_scale    = 1
0.00.064.002 I ggml_metal_init: allocating
0.00.064.006 I ggml_metal_init: found device: Apple M4
0.00.064.008 I ggml_metal_init: picking default device: Apple M4
0.00.064.731 I ggml_metal_init: using embedded metal library
0.00.067.307 I ggml_metal_init: GPU name:   Apple M4
0.00.067.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.309 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.309 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.309 I ggml_metal_init: simdgroup reduction   = true
0.00.067.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.310 I ggml_metal_init: has bfloat            = true
0.00.067.310 I ggml_metal_init: use bfloat            = true
0.00.067.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.311 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.628 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.810 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.817 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.842 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.211 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.104.214 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.104.214 I llama_init_from_model: graph nodes  = 967
0.00.104.214 I llama_init_from_model: graph splits = 2
0.00.104.219 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.343 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.344 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.347.045 I main: llama threadpool init, n_threads = 4
0.01.347.078 I 
0.01.347.099 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.347.101 I 
0.01.347.243 I sampler seed: 1234
0.01.347.247 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.347.256 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.347.257 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.347.257 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.434.643 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61365.60 tokens per second)
0.02.434.644 I llama_perf_context_print:        load time =    1337.25 ms
0.02.434.645 I llama_perf_context_print: prompt eval time =      39.91 ms /     7 tokens (    5.70 ms per token,   175.42 tokens per second)
0.02.434.646 I llama_perf_context_print:        eval time =    1044.63 ms /    63 runs   (   16.58 ms per token,    60.31 tokens per second)
0.02.434.646 I llama_perf_context_print:       total time =    1087.60 ms /    70 tokens
0.02.434.884 I ggml_metal_free: deallocating

real	0m2.454s
user	0m0.113s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.118 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.285 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.162 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.170 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.171 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.171 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.172 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.172 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.173 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.173 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.173 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.174 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.174 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.176 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.177 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.594 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.843 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.845 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.845 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.846 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.846 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.846 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.847 I llama_model_loader: - type  f32:  194 tensors
0.00.033.848 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.848 I print_info: file format = GGUF V3 (latest)
0.00.033.849 I print_info: file type   = Q8_0
0.00.033.850 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.417 I load: special tokens cache size = 25
0.00.064.667 I load: token to piece cache size = 0.2984 MB
0.00.064.670 I print_info: arch             = gptneox
0.00.064.671 I print_info: vocab_only       = 0
0.00.064.671 I print_info: n_ctx_train      = 2048
0.00.064.671 I print_info: n_embd           = 2048
0.00.064.671 I print_info: n_layer          = 24
0.00.064.675 I print_info: n_head           = 16
0.00.064.676 I print_info: n_head_kv        = 16
0.00.064.676 I print_info: n_rot            = 32
0.00.064.676 I print_info: n_swa            = 0
0.00.064.676 I print_info: n_embd_head_k    = 128
0.00.064.677 I print_info: n_embd_head_v    = 128
0.00.064.677 I print_info: n_gqa            = 1
0.00.064.678 I print_info: n_embd_k_gqa     = 2048
0.00.064.679 I print_info: n_embd_v_gqa     = 2048
0.00.064.679 I print_info: f_norm_eps       = 1.0e-05
0.00.064.680 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.680 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.680 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.681 I print_info: f_logit_scale    = 0.0e+00
0.00.064.681 I print_info: n_ff             = 8192
0.00.064.681 I print_info: n_expert         = 0
0.00.064.682 I print_info: n_expert_used    = 0
0.00.064.682 I print_info: causal attn      = 1
0.00.064.682 I print_info: pooling type     = 0
0.00.064.682 I print_info: rope type        = 2
0.00.064.682 I print_info: rope scaling     = linear
0.00.064.683 I print_info: freq_base_train  = 10000.0
0.00.064.683 I print_info: freq_scale_train = 1
0.00.064.683 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.683 I print_info: rope_finetuned   = unknown
0.00.064.683 I print_info: ssm_d_conv       = 0
0.00.064.684 I print_info: ssm_d_inner      = 0
0.00.064.684 I print_info: ssm_d_state      = 0
0.00.064.685 I print_info: ssm_dt_rank      = 0
0.00.064.685 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.685 I print_info: model type       = 1.4B
0.00.064.686 I print_info: model params     = 1.41 B
0.00.064.686 I print_info: general.name     = 1.4B
0.00.064.686 I print_info: vocab type       = BPE
0.00.064.687 I print_info: n_vocab          = 50304
0.00.064.687 I print_info: n_merges         = 50009
0.00.064.689 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.689 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.689 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.689 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.690 I print_info: LF token         = 128 'Ä'
0.00.064.690 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.692 I print_info: max token length = 1024
0.00.066.377 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.377 I load_tensors: offloading output layer to GPU
0.00.066.378 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.388 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.389 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.066.692 I llama_init_from_model: n_seq_max     = 1
0.00.066.693 I llama_init_from_model: n_ctx         = 128
0.00.066.693 I llama_init_from_model: n_ctx_per_seq = 128
0.00.066.693 I llama_init_from_model: n_batch       = 128
0.00.066.693 I llama_init_from_model: n_ubatch      = 128
0.00.066.694 I llama_init_from_model: flash_attn    = 0
0.00.066.694 I llama_init_from_model: freq_base     = 10000.0
0.00.066.694 I llama_init_from_model: freq_scale    = 1
0.00.066.695 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.695 I ggml_metal_init: allocating
0.00.066.700 I ggml_metal_init: found device: Apple M4
0.00.066.703 I ggml_metal_init: picking default device: Apple M4
0.00.067.347 I ggml_metal_init: using embedded metal library
0.00.069.930 I ggml_metal_init: GPU name:   Apple M4
0.00.069.932 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.932 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.933 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.933 I ggml_metal_init: simdgroup reduction   = true
0.00.069.933 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.933 I ggml_metal_init: has bfloat            = true
0.00.069.933 I ggml_metal_init: use bfloat            = true
0.00.069.934 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.935 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.462 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.080.705 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.708 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.723 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.081.576 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.081.578 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.081.578 I llama_init_from_model: graph nodes  = 967
0.00.081.578 I llama_init_from_model: graph splits = 2
0.00.081.580 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.925.356 I 
0.00.925.382 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.925.393 I perplexity: tokenizing the input ..
0.00.933.376 I perplexity: tokenization took 7.982 ms
0.00.933.381 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.057.660 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.058.816 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.058.838 I llama_perf_context_print:        load time =     914.07 ms
0.01.058.839 I llama_perf_context_print: prompt eval time =     124.05 ms /   128 tokens (    0.97 ms per token,  1031.82 tokens per second)
0.01.058.840 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.058.840 I llama_perf_context_print:       total time =     133.48 ms /   129 tokens
0.01.059.233 I ggml_metal_free: deallocating

real	0m1.077s
user	0m0.092s
sys	0m0.150s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.015.644 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.291 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.298 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.300 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.301 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.301 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.301 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.302 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.303 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.303 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.305 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.305 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.306 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.306 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.307 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.309 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.309 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.309 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.789 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.666 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.667 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.668 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.668 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.668 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.669 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.670 I llama_model_loader: - type  f32:  194 tensors
0.00.044.670 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.670 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.671 I print_info: file format = GGUF V3 (latest)
0.00.044.672 I print_info: file type   = Q4_0
0.00.044.673 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.073.183 I load: special tokens cache size = 25
0.00.084.023 I load: token to piece cache size = 0.2984 MB
0.00.084.028 I print_info: arch             = gptneox
0.00.084.028 I print_info: vocab_only       = 0
0.00.084.029 I print_info: n_ctx_train      = 2048
0.00.084.029 I print_info: n_embd           = 2048
0.00.084.029 I print_info: n_layer          = 24
0.00.084.036 I print_info: n_head           = 16
0.00.084.037 I print_info: n_head_kv        = 16
0.00.084.037 I print_info: n_rot            = 32
0.00.084.037 I print_info: n_swa            = 0
0.00.084.038 I print_info: n_embd_head_k    = 128
0.00.084.038 I print_info: n_embd_head_v    = 128
0.00.084.039 I print_info: n_gqa            = 1
0.00.084.040 I print_info: n_embd_k_gqa     = 2048
0.00.084.041 I print_info: n_embd_v_gqa     = 2048
0.00.084.042 I print_info: f_norm_eps       = 1.0e-05
0.00.084.043 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.084.047 I print_info: f_clamp_kqv      = 0.0e+00
0.00.084.047 I print_info: f_max_alibi_bias = 0.0e+00
0.00.084.047 I print_info: f_logit_scale    = 0.0e+00
0.00.084.048 I print_info: n_ff             = 8192
0.00.084.048 I print_info: n_expert         = 0
0.00.084.049 I print_info: n_expert_used    = 0
0.00.084.049 I print_info: causal attn      = 1
0.00.084.049 I print_info: pooling type     = 0
0.00.084.049 I print_info: rope type        = 2
0.00.084.050 I print_info: rope scaling     = linear
0.00.084.050 I print_info: freq_base_train  = 10000.0
0.00.084.051 I print_info: freq_scale_train = 1
0.00.084.053 I print_info: n_ctx_orig_yarn  = 2048
0.00.084.054 I print_info: rope_finetuned   = unknown
0.00.084.054 I print_info: ssm_d_conv       = 0
0.00.084.054 I print_info: ssm_d_inner      = 0
0.00.084.054 I print_info: ssm_d_state      = 0
0.00.084.054 I print_info: ssm_dt_rank      = 0
0.00.084.055 I print_info: ssm_dt_b_c_rms   = 0
0.00.084.055 I print_info: model type       = 1.4B
0.00.084.058 I print_info: model params     = 1.41 B
0.00.084.058 I print_info: general.name     = 1.4B
0.00.084.060 I print_info: vocab type       = BPE
0.00.084.060 I print_info: n_vocab          = 50304
0.00.084.060 I print_info: n_merges         = 50009
0.00.084.060 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.084.061 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.084.061 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.084.061 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.084.062 I print_info: LF token         = 128 'Ä'
0.00.084.062 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.084.063 I print_info: max token length = 1024
0.00.086.824 I load_tensors: offloading 24 repeating layers to GPU
0.00.086.824 I load_tensors: offloading output layer to GPU
0.00.086.825 I load_tensors: offloaded 25/25 layers to GPU
0.00.086.836 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.086.838 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.087.320 I llama_init_from_model: n_seq_max     = 1
0.00.087.322 I llama_init_from_model: n_ctx         = 2048
0.00.087.322 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.087.322 I llama_init_from_model: n_batch       = 2048
0.00.087.322 I llama_init_from_model: n_ubatch      = 512
0.00.087.323 I llama_init_from_model: flash_attn    = 0
0.00.087.323 I llama_init_from_model: freq_base     = 10000.0
0.00.087.324 I llama_init_from_model: freq_scale    = 1
0.00.087.324 I ggml_metal_init: allocating
0.00.087.329 I ggml_metal_init: found device: Apple M4
0.00.087.333 I ggml_metal_init: picking default device: Apple M4
0.00.088.374 I ggml_metal_init: using embedded metal library
0.00.092.376 I ggml_metal_init: GPU name:   Apple M4
0.00.092.378 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.379 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.380 I ggml_metal_init: simdgroup reduction   = true
0.00.092.380 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.380 I ggml_metal_init: has bfloat            = true
0.00.092.380 I ggml_metal_init: use bfloat            = true
0.00.092.381 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.382 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.997 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.129.180 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.129.191 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.129.221 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.130.288 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.130.290 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.130.290 I llama_init_from_model: graph nodes  = 967
0.00.130.290 I llama_init_from_model: graph splits = 2
0.00.130.297 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.130.412 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.130.413 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.159 I main: llama threadpool init, n_threads = 4
0.00.719.198 I 
0.00.719.219 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.219 I 
0.00.719.487 I sampler seed: 1234
0.00.719.492 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.719.518 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.719.519 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.719.519 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.397.844 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58101.47 tokens per second)
0.01.397.845 I llama_perf_context_print:        load time =     703.51 ms
0.01.397.846 I llama_perf_context_print: prompt eval time =      42.21 ms /     7 tokens (    6.03 ms per token,   165.83 tokens per second)
0.01.397.846 I llama_perf_context_print:        eval time =     633.15 ms /    63 runs   (   10.05 ms per token,    99.50 tokens per second)
0.01.397.847 I llama_perf_context_print:       total time =     678.69 ms /    70 tokens
0.01.398.072 I ggml_metal_free: deallocating

real	0m1.420s
user	0m0.133s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.095 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.051 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.057 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.058 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.058 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.058 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.059 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.062 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.063 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.065 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.810 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.557 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.558 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.559 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.559 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.559 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.560 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.560 I llama_model_loader: - type  f32:  194 tensors
0.00.025.561 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.561 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.561 I print_info: file format = GGUF V3 (latest)
0.00.025.562 I print_info: file type   = Q4_0
0.00.025.563 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.343 I load: special tokens cache size = 25
0.00.050.262 I load: token to piece cache size = 0.2984 MB
0.00.050.266 I print_info: arch             = gptneox
0.00.050.266 I print_info: vocab_only       = 0
0.00.050.266 I print_info: n_ctx_train      = 2048
0.00.050.266 I print_info: n_embd           = 2048
0.00.050.266 I print_info: n_layer          = 24
0.00.050.270 I print_info: n_head           = 16
0.00.050.270 I print_info: n_head_kv        = 16
0.00.050.271 I print_info: n_rot            = 32
0.00.050.271 I print_info: n_swa            = 0
0.00.050.271 I print_info: n_embd_head_k    = 128
0.00.050.271 I print_info: n_embd_head_v    = 128
0.00.050.272 I print_info: n_gqa            = 1
0.00.050.273 I print_info: n_embd_k_gqa     = 2048
0.00.050.273 I print_info: n_embd_v_gqa     = 2048
0.00.050.275 I print_info: f_norm_eps       = 1.0e-05
0.00.050.275 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.276 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.276 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.276 I print_info: f_logit_scale    = 0.0e+00
0.00.050.277 I print_info: n_ff             = 8192
0.00.050.277 I print_info: n_expert         = 0
0.00.050.277 I print_info: n_expert_used    = 0
0.00.050.277 I print_info: causal attn      = 1
0.00.050.277 I print_info: pooling type     = 0
0.00.050.278 I print_info: rope type        = 2
0.00.050.278 I print_info: rope scaling     = linear
0.00.050.278 I print_info: freq_base_train  = 10000.0
0.00.050.279 I print_info: freq_scale_train = 1
0.00.050.279 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.279 I print_info: rope_finetuned   = unknown
0.00.050.279 I print_info: ssm_d_conv       = 0
0.00.050.279 I print_info: ssm_d_inner      = 0
0.00.050.280 I print_info: ssm_d_state      = 0
0.00.050.280 I print_info: ssm_dt_rank      = 0
0.00.050.280 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.280 I print_info: model type       = 1.4B
0.00.050.280 I print_info: model params     = 1.41 B
0.00.050.281 I print_info: general.name     = 1.4B
0.00.050.281 I print_info: vocab type       = BPE
0.00.050.281 I print_info: n_vocab          = 50304
0.00.050.281 I print_info: n_merges         = 50009
0.00.050.282 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.282 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.282 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.282 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.282 I print_info: LF token         = 128 'Ä'
0.00.050.283 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.283 I print_info: max token length = 1024
0.00.052.246 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.246 I load_tensors: offloading output layer to GPU
0.00.052.246 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.257 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.258 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.542 I llama_init_from_model: n_seq_max     = 1
0.00.052.543 I llama_init_from_model: n_ctx         = 128
0.00.052.543 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.543 I llama_init_from_model: n_batch       = 128
0.00.052.543 I llama_init_from_model: n_ubatch      = 128
0.00.052.543 I llama_init_from_model: flash_attn    = 0
0.00.052.544 I llama_init_from_model: freq_base     = 10000.0
0.00.052.544 I llama_init_from_model: freq_scale    = 1
0.00.052.544 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.545 I ggml_metal_init: allocating
0.00.052.548 I ggml_metal_init: found device: Apple M4
0.00.052.550 I ggml_metal_init: picking default device: Apple M4
0.00.053.091 I ggml_metal_init: using embedded metal library
0.00.055.447 I ggml_metal_init: GPU name:   Apple M4
0.00.055.448 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.449 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.449 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.449 I ggml_metal_init: simdgroup reduction   = true
0.00.055.449 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.449 I ggml_metal_init: has bfloat            = true
0.00.055.450 I ggml_metal_init: use bfloat            = true
0.00.055.450 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.451 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.148 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.353 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.356 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.370 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.321 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.322 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.322 I llama_init_from_model: graph nodes  = 967
0.00.067.322 I llama_init_from_model: graph splits = 2
0.00.067.324 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.324 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.559.756 I 
0.00.559.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.559.813 I perplexity: tokenizing the input ..
0.00.568.193 I perplexity: tokenization took 8.377 ms
0.00.568.197 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.691.063 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.692.285 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.692.316 I llama_perf_context_print:        load time =     549.65 ms
0.00.692.317 I llama_perf_context_print: prompt eval time =     122.64 ms /   128 tokens (    0.96 ms per token,  1043.72 tokens per second)
0.00.692.318 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.692.319 I llama_perf_context_print:       total time =     132.56 ms /   129 tokens
0.00.692.856 I ggml_metal_free: deallocating

real	0m0.708s
user	0m0.077s
sys	0m0.088s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.794 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.202 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.209 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.209 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.212 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.212 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.212 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.213 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.215 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.216 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.083 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.021 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.022 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.022 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.022 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.023 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.023 I llama_model_loader: - type  f32:  194 tensors
0.00.034.023 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.024 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.024 I print_info: file format = GGUF V3 (latest)
0.00.034.024 I print_info: file type   = Q4_1
0.00.034.025 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.053.188 I load: special tokens cache size = 25
0.00.059.205 I load: token to piece cache size = 0.2984 MB
0.00.059.207 I print_info: arch             = gptneox
0.00.059.207 I print_info: vocab_only       = 0
0.00.059.208 I print_info: n_ctx_train      = 2048
0.00.059.208 I print_info: n_embd           = 2048
0.00.059.208 I print_info: n_layer          = 24
0.00.059.211 I print_info: n_head           = 16
0.00.059.212 I print_info: n_head_kv        = 16
0.00.059.212 I print_info: n_rot            = 32
0.00.059.214 I print_info: n_swa            = 0
0.00.059.214 I print_info: n_embd_head_k    = 128
0.00.059.214 I print_info: n_embd_head_v    = 128
0.00.059.215 I print_info: n_gqa            = 1
0.00.059.216 I print_info: n_embd_k_gqa     = 2048
0.00.059.217 I print_info: n_embd_v_gqa     = 2048
0.00.059.217 I print_info: f_norm_eps       = 1.0e-05
0.00.059.218 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.218 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.218 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.218 I print_info: f_logit_scale    = 0.0e+00
0.00.059.219 I print_info: n_ff             = 8192
0.00.059.219 I print_info: n_expert         = 0
0.00.059.219 I print_info: n_expert_used    = 0
0.00.059.225 I print_info: causal attn      = 1
0.00.059.226 I print_info: pooling type     = 0
0.00.059.226 I print_info: rope type        = 2
0.00.059.226 I print_info: rope scaling     = linear
0.00.059.227 I print_info: freq_base_train  = 10000.0
0.00.059.227 I print_info: freq_scale_train = 1
0.00.059.227 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.228 I print_info: rope_finetuned   = unknown
0.00.059.229 I print_info: ssm_d_conv       = 0
0.00.059.229 I print_info: ssm_d_inner      = 0
0.00.059.229 I print_info: ssm_d_state      = 0
0.00.059.229 I print_info: ssm_dt_rank      = 0
0.00.059.229 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.229 I print_info: model type       = 1.4B
0.00.059.230 I print_info: model params     = 1.41 B
0.00.059.230 I print_info: general.name     = 1.4B
0.00.059.230 I print_info: vocab type       = BPE
0.00.059.230 I print_info: n_vocab          = 50304
0.00.059.230 I print_info: n_merges         = 50009
0.00.059.231 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.231 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.231 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.231 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.231 I print_info: LF token         = 128 'Ä'
0.00.059.232 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.232 I print_info: max token length = 1024
0.00.061.201 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.202 I load_tensors: offloading output layer to GPU
0.00.061.202 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.212 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.061.213 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.061.503 I llama_init_from_model: n_seq_max     = 1
0.00.061.503 I llama_init_from_model: n_ctx         = 2048
0.00.061.504 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.061.504 I llama_init_from_model: n_batch       = 2048
0.00.061.504 I llama_init_from_model: n_ubatch      = 512
0.00.061.504 I llama_init_from_model: flash_attn    = 0
0.00.061.505 I llama_init_from_model: freq_base     = 10000.0
0.00.061.505 I llama_init_from_model: freq_scale    = 1
0.00.061.505 I ggml_metal_init: allocating
0.00.061.508 I ggml_metal_init: found device: Apple M4
0.00.061.510 I ggml_metal_init: picking default device: Apple M4
0.00.062.097 I ggml_metal_init: using embedded metal library
0.00.064.444 I ggml_metal_init: GPU name:   Apple M4
0.00.064.446 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.446 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.447 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.447 I ggml_metal_init: simdgroup reduction   = true
0.00.064.447 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.447 I ggml_metal_init: has bfloat            = true
0.00.064.447 I ggml_metal_init: use bfloat            = true
0.00.064.448 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.448 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.241 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.380 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.390 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.411 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.095.479 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.095.480 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.095.481 I llama_init_from_model: graph nodes  = 967
0.00.095.481 I llama_init_from_model: graph splits = 2
0.00.095.484 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.623 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.624 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.819.244 I main: llama threadpool init, n_threads = 4
0.00.819.282 I 
0.00.819.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.819.302 I 
0.00.819.538 I sampler seed: 1234
0.00.819.542 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.819.562 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.819.563 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.819.563 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.542.251 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63111.11 tokens per second)
0.01.542.252 I llama_perf_context_print:        load time =     810.45 ms
0.01.542.253 I llama_perf_context_print: prompt eval time =      39.52 ms /     7 tokens (    5.65 ms per token,   177.11 tokens per second)
0.01.542.254 I llama_perf_context_print:        eval time =     680.27 ms /    63 runs   (   10.80 ms per token,    92.61 tokens per second)
0.01.542.254 I llama_perf_context_print:       total time =     723.01 ms /    70 tokens
0.01.542.471 I ggml_metal_free: deallocating

real	0m1.558s
user	0m0.109s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.970 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.185 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.191 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.193 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.193 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.194 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.194 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.194 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.196 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.196 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.197 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.197 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.198 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.199 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.199 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.200 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.978 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.004 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.771 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.771 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.772 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.772 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.773 I llama_model_loader: - type  f32:  194 tensors
0.00.024.773 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.773 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.774 I print_info: file format = GGUF V3 (latest)
0.00.024.774 I print_info: file type   = Q4_1
0.00.024.775 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.573 I load: special tokens cache size = 25
0.00.049.339 I load: token to piece cache size = 0.2984 MB
0.00.049.344 I print_info: arch             = gptneox
0.00.049.344 I print_info: vocab_only       = 0
0.00.049.344 I print_info: n_ctx_train      = 2048
0.00.049.345 I print_info: n_embd           = 2048
0.00.049.345 I print_info: n_layer          = 24
0.00.049.348 I print_info: n_head           = 16
0.00.049.349 I print_info: n_head_kv        = 16
0.00.049.349 I print_info: n_rot            = 32
0.00.049.349 I print_info: n_swa            = 0
0.00.049.349 I print_info: n_embd_head_k    = 128
0.00.049.352 I print_info: n_embd_head_v    = 128
0.00.049.352 I print_info: n_gqa            = 1
0.00.049.353 I print_info: n_embd_k_gqa     = 2048
0.00.049.354 I print_info: n_embd_v_gqa     = 2048
0.00.049.354 I print_info: f_norm_eps       = 1.0e-05
0.00.049.355 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.355 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.355 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.355 I print_info: f_logit_scale    = 0.0e+00
0.00.049.356 I print_info: n_ff             = 8192
0.00.049.356 I print_info: n_expert         = 0
0.00.049.356 I print_info: n_expert_used    = 0
0.00.049.356 I print_info: causal attn      = 1
0.00.049.357 I print_info: pooling type     = 0
0.00.049.357 I print_info: rope type        = 2
0.00.049.357 I print_info: rope scaling     = linear
0.00.049.357 I print_info: freq_base_train  = 10000.0
0.00.049.358 I print_info: freq_scale_train = 1
0.00.049.358 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.358 I print_info: rope_finetuned   = unknown
0.00.049.358 I print_info: ssm_d_conv       = 0
0.00.049.358 I print_info: ssm_d_inner      = 0
0.00.049.359 I print_info: ssm_d_state      = 0
0.00.049.359 I print_info: ssm_dt_rank      = 0
0.00.049.359 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.359 I print_info: model type       = 1.4B
0.00.049.359 I print_info: model params     = 1.41 B
0.00.049.360 I print_info: general.name     = 1.4B
0.00.049.360 I print_info: vocab type       = BPE
0.00.049.360 I print_info: n_vocab          = 50304
0.00.049.361 I print_info: n_merges         = 50009
0.00.049.361 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.361 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.365 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.365 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.366 I print_info: LF token         = 128 'Ä'
0.00.049.367 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.367 I print_info: max token length = 1024
0.00.051.389 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.390 I load_tensors: offloading output layer to GPU
0.00.051.390 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.400 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.401 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.798 I llama_init_from_model: n_seq_max     = 1
0.00.051.799 I llama_init_from_model: n_ctx         = 128
0.00.051.799 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.799 I llama_init_from_model: n_batch       = 128
0.00.051.799 I llama_init_from_model: n_ubatch      = 128
0.00.051.799 I llama_init_from_model: flash_attn    = 0
0.00.051.800 I llama_init_from_model: freq_base     = 10000.0
0.00.051.800 I llama_init_from_model: freq_scale    = 1
0.00.051.800 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.801 I ggml_metal_init: allocating
0.00.051.804 I ggml_metal_init: found device: Apple M4
0.00.051.805 I ggml_metal_init: picking default device: Apple M4
0.00.052.368 I ggml_metal_init: using embedded metal library
0.00.054.734 I ggml_metal_init: GPU name:   Apple M4
0.00.054.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.736 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.736 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.736 I ggml_metal_init: simdgroup reduction   = true
0.00.054.736 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.736 I ggml_metal_init: has bfloat            = true
0.00.054.737 I ggml_metal_init: use bfloat            = true
0.00.054.737 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.738 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.478 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.783 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.785 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.800 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.758 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.759 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.760 I llama_init_from_model: graph nodes  = 967
0.00.066.760 I llama_init_from_model: graph splits = 2
0.00.066.761 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.762 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.280 I 
0.00.639.322 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.373 I perplexity: tokenizing the input ..
0.00.647.493 I perplexity: tokenization took 8.119 ms
0.00.647.497 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.769.983 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.771.139 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.771.171 I llama_perf_context_print:        load time =     630.31 ms
0.00.771.171 I llama_perf_context_print: prompt eval time =     122.26 ms /   128 tokens (    0.96 ms per token,  1046.93 tokens per second)
0.00.771.172 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.175 I llama_perf_context_print:       total time =     131.89 ms /   129 tokens
0.00.771.700 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.077s
sys	0m0.108s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.065 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.016.925 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.446 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.035.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.453 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.454 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.454 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.456 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.456 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.458 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.458 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.459 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.459 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.459 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.460 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.465 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.465 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.465 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.305 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.848 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.592 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.593 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.594 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.594 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.595 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.595 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.048.596 I llama_model_loader: - type  f32:  194 tensors
0.00.048.596 I llama_model_loader: - type q5_0:   97 tensors
0.00.048.597 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.597 I print_info: file format = GGUF V3 (latest)
0.00.048.598 I print_info: file type   = Q5_0
0.00.048.599 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.087.378 I load: special tokens cache size = 25
0.00.096.640 I load: token to piece cache size = 0.2984 MB
0.00.096.645 I print_info: arch             = gptneox
0.00.096.645 I print_info: vocab_only       = 0
0.00.096.645 I print_info: n_ctx_train      = 2048
0.00.096.645 I print_info: n_embd           = 2048
0.00.096.646 I print_info: n_layer          = 24
0.00.096.650 I print_info: n_head           = 16
0.00.096.651 I print_info: n_head_kv        = 16
0.00.096.651 I print_info: n_rot            = 32
0.00.096.651 I print_info: n_swa            = 0
0.00.096.654 I print_info: n_embd_head_k    = 128
0.00.096.654 I print_info: n_embd_head_v    = 128
0.00.096.655 I print_info: n_gqa            = 1
0.00.096.656 I print_info: n_embd_k_gqa     = 2048
0.00.096.656 I print_info: n_embd_v_gqa     = 2048
0.00.096.657 I print_info: f_norm_eps       = 1.0e-05
0.00.096.661 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.096.663 I print_info: f_clamp_kqv      = 0.0e+00
0.00.096.663 I print_info: f_max_alibi_bias = 0.0e+00
0.00.096.663 I print_info: f_logit_scale    = 0.0e+00
0.00.096.664 I print_info: n_ff             = 8192
0.00.096.664 I print_info: n_expert         = 0
0.00.096.665 I print_info: n_expert_used    = 0
0.00.096.665 I print_info: causal attn      = 1
0.00.096.665 I print_info: pooling type     = 0
0.00.096.665 I print_info: rope type        = 2
0.00.096.665 I print_info: rope scaling     = linear
0.00.096.666 I print_info: freq_base_train  = 10000.0
0.00.096.666 I print_info: freq_scale_train = 1
0.00.096.667 I print_info: n_ctx_orig_yarn  = 2048
0.00.096.667 I print_info: rope_finetuned   = unknown
0.00.096.667 I print_info: ssm_d_conv       = 0
0.00.096.667 I print_info: ssm_d_inner      = 0
0.00.096.667 I print_info: ssm_d_state      = 0
0.00.096.667 I print_info: ssm_dt_rank      = 0
0.00.096.668 I print_info: ssm_dt_b_c_rms   = 0
0.00.096.668 I print_info: model type       = 1.4B
0.00.096.668 I print_info: model params     = 1.41 B
0.00.096.668 I print_info: general.name     = 1.4B
0.00.096.669 I print_info: vocab type       = BPE
0.00.096.669 I print_info: n_vocab          = 50304
0.00.096.670 I print_info: n_merges         = 50009
0.00.096.670 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.096.670 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.096.670 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.096.672 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.096.673 I print_info: LF token         = 128 'Ä'
0.00.096.674 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.096.674 I print_info: max token length = 1024
0.00.099.345 I load_tensors: offloading 24 repeating layers to GPU
0.00.099.346 I load_tensors: offloading output layer to GPU
0.00.099.346 I load_tensors: offloaded 25/25 layers to GPU
0.00.099.358 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.099.359 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.099.869 I llama_init_from_model: n_seq_max     = 1
0.00.099.870 I llama_init_from_model: n_ctx         = 2048
0.00.099.870 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.099.871 I llama_init_from_model: n_batch       = 2048
0.00.099.871 I llama_init_from_model: n_ubatch      = 512
0.00.099.871 I llama_init_from_model: flash_attn    = 0
0.00.099.872 I llama_init_from_model: freq_base     = 10000.0
0.00.099.872 I llama_init_from_model: freq_scale    = 1
0.00.099.873 I ggml_metal_init: allocating
0.00.099.876 I ggml_metal_init: found device: Apple M4
0.00.099.879 I ggml_metal_init: picking default device: Apple M4
0.00.100.680 I ggml_metal_init: using embedded metal library
0.00.103.886 I ggml_metal_init: GPU name:   Apple M4
0.00.103.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.103.889 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.103.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.103.889 I ggml_metal_init: simdgroup reduction   = true
0.00.103.889 I ggml_metal_init: simdgroup matrix mul. = true
0.00.103.890 I ggml_metal_init: has bfloat            = true
0.00.103.890 I ggml_metal_init: use bfloat            = true
0.00.103.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.103.891 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.670 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.135.722 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.135.727 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.135.745 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.136.861 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.136.862 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.136.863 I llama_init_from_model: graph nodes  = 967
0.00.136.863 I llama_init_from_model: graph splits = 2
0.00.136.868 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.136.990 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.136.991 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.317.531 I main: llama threadpool init, n_threads = 4
0.01.317.589 I 
0.01.317.624 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.317.626 I 
0.01.318.017 I sampler seed: 1234
0.01.318.025 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.318.072 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.318.076 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.318.077 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.02.113.199 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.02.113.199 I llama_perf_context_print:        load time =    1300.60 ms
0.02.113.200 I llama_perf_context_print: prompt eval time =      54.42 ms /     7 tokens (    7.77 ms per token,   128.64 tokens per second)
0.02.113.201 I llama_perf_context_print:        eval time =     737.67 ms /    63 runs   (   11.71 ms per token,    85.40 tokens per second)
0.02.113.201 I llama_perf_context_print:       total time =     795.67 ms /    70 tokens
0.02.113.403 I ggml_metal_free: deallocating

real	0m2.160s
user	0m0.155s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.880 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.255 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.260 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.262 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.262 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.263 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.265 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.265 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.266 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.266 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.267 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.267 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.267 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.271 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.271 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.272 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.942 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.951 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.639 I llama_model_loader: - type  f32:  194 tensors
0.00.025.639 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.639 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.640 I print_info: file format = GGUF V3 (latest)
0.00.025.640 I print_info: file type   = Q5_0
0.00.025.641 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.097 I load: special tokens cache size = 25
0.00.051.106 I load: token to piece cache size = 0.2984 MB
0.00.051.110 I print_info: arch             = gptneox
0.00.051.110 I print_info: vocab_only       = 0
0.00.051.110 I print_info: n_ctx_train      = 2048
0.00.051.110 I print_info: n_embd           = 2048
0.00.051.110 I print_info: n_layer          = 24
0.00.051.114 I print_info: n_head           = 16
0.00.051.114 I print_info: n_head_kv        = 16
0.00.051.114 I print_info: n_rot            = 32
0.00.051.115 I print_info: n_swa            = 0
0.00.051.115 I print_info: n_embd_head_k    = 128
0.00.051.115 I print_info: n_embd_head_v    = 128
0.00.051.116 I print_info: n_gqa            = 1
0.00.051.117 I print_info: n_embd_k_gqa     = 2048
0.00.051.117 I print_info: n_embd_v_gqa     = 2048
0.00.051.118 I print_info: f_norm_eps       = 1.0e-05
0.00.051.118 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.119 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.119 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.119 I print_info: f_logit_scale    = 0.0e+00
0.00.051.120 I print_info: n_ff             = 8192
0.00.051.122 I print_info: n_expert         = 0
0.00.051.122 I print_info: n_expert_used    = 0
0.00.051.123 I print_info: causal attn      = 1
0.00.051.123 I print_info: pooling type     = 0
0.00.051.123 I print_info: rope type        = 2
0.00.051.123 I print_info: rope scaling     = linear
0.00.051.124 I print_info: freq_base_train  = 10000.0
0.00.051.124 I print_info: freq_scale_train = 1
0.00.051.124 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.124 I print_info: rope_finetuned   = unknown
0.00.051.125 I print_info: ssm_d_conv       = 0
0.00.051.125 I print_info: ssm_d_inner      = 0
0.00.051.125 I print_info: ssm_d_state      = 0
0.00.051.125 I print_info: ssm_dt_rank      = 0
0.00.051.125 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.125 I print_info: model type       = 1.4B
0.00.051.135 I print_info: model params     = 1.41 B
0.00.051.136 I print_info: general.name     = 1.4B
0.00.051.137 I print_info: vocab type       = BPE
0.00.051.137 I print_info: n_vocab          = 50304
0.00.051.137 I print_info: n_merges         = 50009
0.00.051.138 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.139 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.139 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.139 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.140 I print_info: LF token         = 128 'Ä'
0.00.051.140 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.140 I print_info: max token length = 1024
0.00.053.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.160 I load_tensors: offloading output layer to GPU
0.00.053.160 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.170 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.172 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.453 I llama_init_from_model: n_seq_max     = 1
0.00.053.454 I llama_init_from_model: n_ctx         = 128
0.00.053.454 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.454 I llama_init_from_model: n_batch       = 128
0.00.053.454 I llama_init_from_model: n_ubatch      = 128
0.00.053.454 I llama_init_from_model: flash_attn    = 0
0.00.053.455 I llama_init_from_model: freq_base     = 10000.0
0.00.053.455 I llama_init_from_model: freq_scale    = 1
0.00.053.455 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.456 I ggml_metal_init: allocating
0.00.053.459 I ggml_metal_init: found device: Apple M4
0.00.053.461 I ggml_metal_init: picking default device: Apple M4
0.00.054.043 I ggml_metal_init: using embedded metal library
0.00.056.369 I ggml_metal_init: GPU name:   Apple M4
0.00.056.370 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.371 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.371 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.371 I ggml_metal_init: simdgroup reduction   = true
0.00.056.372 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.372 I ggml_metal_init: has bfloat            = true
0.00.056.372 I ggml_metal_init: use bfloat            = true
0.00.056.372 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.373 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.058 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.313 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.316 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.330 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.228 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.229 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.229 I llama_init_from_model: graph nodes  = 967
0.00.068.229 I llama_init_from_model: graph splits = 2
0.00.068.231 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.231 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.499 I 
0.00.668.524 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.535 I perplexity: tokenizing the input ..
0.00.676.313 I perplexity: tokenization took 7.776 ms
0.00.676.318 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.677 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.812.950 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.812.975 I llama_perf_context_print:        load time =     658.61 ms
0.00.812.977 I llama_perf_context_print: prompt eval time =     135.13 ms /   128 tokens (    1.06 ms per token,   947.20 tokens per second)
0.00.812.978 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.979 I llama_perf_context_print:       total time =     144.48 ms /   129 tokens
0.00.813.480 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.077s
sys	0m0.103s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.866 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.610 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.615 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.617 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.617 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.618 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.618 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.619 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.622 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.622 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.622 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.623 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.623 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.623 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.625 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.625 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.626 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.152 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.153 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.154 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.154 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.154 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.155 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.155 I llama_model_loader: - type  f32:  194 tensors
0.00.025.155 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.156 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.156 I print_info: file format = GGUF V3 (latest)
0.00.025.157 I print_info: file type   = Q5_1
0.00.025.158 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.779 I load: special tokens cache size = 25
0.00.050.844 I load: token to piece cache size = 0.2984 MB
0.00.050.847 I print_info: arch             = gptneox
0.00.050.847 I print_info: vocab_only       = 0
0.00.050.848 I print_info: n_ctx_train      = 2048
0.00.050.848 I print_info: n_embd           = 2048
0.00.050.848 I print_info: n_layer          = 24
0.00.050.851 I print_info: n_head           = 16
0.00.050.851 I print_info: n_head_kv        = 16
0.00.050.853 I print_info: n_rot            = 32
0.00.050.853 I print_info: n_swa            = 0
0.00.050.854 I print_info: n_embd_head_k    = 128
0.00.050.854 I print_info: n_embd_head_v    = 128
0.00.050.854 I print_info: n_gqa            = 1
0.00.050.855 I print_info: n_embd_k_gqa     = 2048
0.00.050.856 I print_info: n_embd_v_gqa     = 2048
0.00.050.856 I print_info: f_norm_eps       = 1.0e-05
0.00.050.857 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.857 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.857 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.857 I print_info: f_logit_scale    = 0.0e+00
0.00.050.858 I print_info: n_ff             = 8192
0.00.050.858 I print_info: n_expert         = 0
0.00.050.858 I print_info: n_expert_used    = 0
0.00.050.858 I print_info: causal attn      = 1
0.00.050.858 I print_info: pooling type     = 0
0.00.050.859 I print_info: rope type        = 2
0.00.050.859 I print_info: rope scaling     = linear
0.00.050.859 I print_info: freq_base_train  = 10000.0
0.00.050.860 I print_info: freq_scale_train = 1
0.00.050.860 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.860 I print_info: rope_finetuned   = unknown
0.00.050.860 I print_info: ssm_d_conv       = 0
0.00.050.860 I print_info: ssm_d_inner      = 0
0.00.050.861 I print_info: ssm_d_state      = 0
0.00.050.861 I print_info: ssm_dt_rank      = 0
0.00.050.862 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.862 I print_info: model type       = 1.4B
0.00.050.862 I print_info: model params     = 1.41 B
0.00.050.863 I print_info: general.name     = 1.4B
0.00.050.863 I print_info: vocab type       = BPE
0.00.050.863 I print_info: n_vocab          = 50304
0.00.050.864 I print_info: n_merges         = 50009
0.00.050.864 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.864 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.865 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.866 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.867 I print_info: LF token         = 128 'Ä'
0.00.050.867 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.867 I print_info: max token length = 1024
0.00.052.870 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.871 I load_tensors: offloading output layer to GPU
0.00.052.871 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.882 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.883 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.212 I llama_init_from_model: n_seq_max     = 1
0.00.053.213 I llama_init_from_model: n_ctx         = 2048
0.00.053.213 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.213 I llama_init_from_model: n_batch       = 2048
0.00.053.214 I llama_init_from_model: n_ubatch      = 512
0.00.053.214 I llama_init_from_model: flash_attn    = 0
0.00.053.214 I llama_init_from_model: freq_base     = 10000.0
0.00.053.214 I llama_init_from_model: freq_scale    = 1
0.00.053.215 I ggml_metal_init: allocating
0.00.053.218 I ggml_metal_init: found device: Apple M4
0.00.053.220 I ggml_metal_init: picking default device: Apple M4
0.00.053.827 I ggml_metal_init: using embedded metal library
0.00.056.248 I ggml_metal_init: GPU name:   Apple M4
0.00.056.249 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.250 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.250 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.250 I ggml_metal_init: simdgroup reduction   = true
0.00.056.251 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.251 I ggml_metal_init: has bfloat            = true
0.00.056.251 I ggml_metal_init: use bfloat            = true
0.00.056.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.252 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.245 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.051 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.065 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.084 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.095 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.097 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.097 I llama_init_from_model: graph nodes  = 967
0.00.086.097 I llama_init_from_model: graph splits = 2
0.00.086.100 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.222 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.223 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.094 I main: llama threadpool init, n_threads = 4
0.00.774.148 I 
0.00.774.194 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.196 I 
0.00.774.441 I sampler seed: 1234
0.00.774.447 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.486 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.501 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.501 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.612.748 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.01.612.748 I llama_perf_context_print:        load time =     765.22 ms
0.01.612.749 I llama_perf_context_print: prompt eval time =      42.23 ms /     7 tokens (    6.03 ms per token,   165.74 tokens per second)
0.01.612.750 I llama_perf_context_print:        eval time =     792.93 ms /    63 runs   (   12.59 ms per token,    79.45 tokens per second)
0.01.612.751 I llama_perf_context_print:       total time =     838.66 ms /    70 tokens
0.01.613.017 I ggml_metal_free: deallocating

real	0m1.631s
user	0m0.111s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.181 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.333 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.342 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.343 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.343 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.344 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.344 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.345 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.345 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.345 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.346 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.347 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.347 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.348 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.349 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.349 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.143 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.162 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.921 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.923 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.923 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.923 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.924 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.924 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.925 I llama_model_loader: - type  f32:  194 tensors
0.00.024.925 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.925 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.926 I print_info: file format = GGUF V3 (latest)
0.00.024.926 I print_info: file type   = Q5_1
0.00.024.927 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.693 I load: special tokens cache size = 25
0.00.050.707 I load: token to piece cache size = 0.2984 MB
0.00.050.712 I print_info: arch             = gptneox
0.00.050.712 I print_info: vocab_only       = 0
0.00.050.712 I print_info: n_ctx_train      = 2048
0.00.050.713 I print_info: n_embd           = 2048
0.00.050.713 I print_info: n_layer          = 24
0.00.050.718 I print_info: n_head           = 16
0.00.050.718 I print_info: n_head_kv        = 16
0.00.050.719 I print_info: n_rot            = 32
0.00.050.720 I print_info: n_swa            = 0
0.00.050.720 I print_info: n_embd_head_k    = 128
0.00.050.721 I print_info: n_embd_head_v    = 128
0.00.050.721 I print_info: n_gqa            = 1
0.00.050.722 I print_info: n_embd_k_gqa     = 2048
0.00.050.722 I print_info: n_embd_v_gqa     = 2048
0.00.050.728 I print_info: f_norm_eps       = 1.0e-05
0.00.050.728 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.728 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.728 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.729 I print_info: f_logit_scale    = 0.0e+00
0.00.050.729 I print_info: n_ff             = 8192
0.00.050.730 I print_info: n_expert         = 0
0.00.050.730 I print_info: n_expert_used    = 0
0.00.050.730 I print_info: causal attn      = 1
0.00.050.730 I print_info: pooling type     = 0
0.00.050.730 I print_info: rope type        = 2
0.00.050.735 I print_info: rope scaling     = linear
0.00.050.736 I print_info: freq_base_train  = 10000.0
0.00.050.736 I print_info: freq_scale_train = 1
0.00.050.736 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.737 I print_info: rope_finetuned   = unknown
0.00.050.737 I print_info: ssm_d_conv       = 0
0.00.050.737 I print_info: ssm_d_inner      = 0
0.00.050.737 I print_info: ssm_d_state      = 0
0.00.050.737 I print_info: ssm_dt_rank      = 0
0.00.050.737 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.740 I print_info: model type       = 1.4B
0.00.050.741 I print_info: model params     = 1.41 B
0.00.050.741 I print_info: general.name     = 1.4B
0.00.050.741 I print_info: vocab type       = BPE
0.00.050.742 I print_info: n_vocab          = 50304
0.00.050.742 I print_info: n_merges         = 50009
0.00.050.742 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.742 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.742 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.743 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.744 I print_info: LF token         = 128 'Ä'
0.00.050.744 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.744 I print_info: max token length = 1024
0.00.052.690 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.690 I load_tensors: offloading output layer to GPU
0.00.052.690 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.701 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.703 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.097 I llama_init_from_model: n_seq_max     = 1
0.00.053.098 I llama_init_from_model: n_ctx         = 128
0.00.053.098 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.098 I llama_init_from_model: n_batch       = 128
0.00.053.099 I llama_init_from_model: n_ubatch      = 128
0.00.053.099 I llama_init_from_model: flash_attn    = 0
0.00.053.099 I llama_init_from_model: freq_base     = 10000.0
0.00.053.099 I llama_init_from_model: freq_scale    = 1
0.00.053.100 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.100 I ggml_metal_init: allocating
0.00.053.103 I ggml_metal_init: found device: Apple M4
0.00.053.105 I ggml_metal_init: picking default device: Apple M4
0.00.053.683 I ggml_metal_init: using embedded metal library
0.00.056.161 I ggml_metal_init: GPU name:   Apple M4
0.00.056.162 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.163 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.163 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.163 I ggml_metal_init: simdgroup reduction   = true
0.00.056.163 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.163 I ggml_metal_init: has bfloat            = true
0.00.056.164 I ggml_metal_init: use bfloat            = true
0.00.056.164 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.165 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.030 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.311 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.315 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.329 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.281 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.282 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.283 I llama_init_from_model: graph nodes  = 967
0.00.068.283 I llama_init_from_model: graph splits = 2
0.00.068.284 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.284 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.792 I 
0.00.738.843 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.871 I perplexity: tokenizing the input ..
0.00.746.875 I perplexity: tokenization took 8.002 ms
0.00.746.878 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.881.142 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.882.660 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.882.684 I llama_perf_context_print:        load time =     729.61 ms
0.00.882.684 I llama_perf_context_print: prompt eval time =     134.02 ms /   128 tokens (    1.05 ms per token,   955.09 tokens per second)
0.00.882.685 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.882.686 I llama_perf_context_print:       total time =     143.89 ms /   129 tokens
0.00.883.072 I ggml_metal_free: deallocating

real	0m0.901s
user	0m0.078s
sys	0m0.111s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.882 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.531 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.536 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.537 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.538 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.538 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.539 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.539 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.540 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.540 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.541 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.541 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.541 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.542 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.542 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.544 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.544 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.544 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.366 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.130 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.131 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.132 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.132 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.132 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.133 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.133 I llama_model_loader: - type  f32:  194 tensors
0.00.025.133 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.134 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.134 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.134 I print_info: file format = GGUF V3 (latest)
0.00.025.135 I print_info: file type   = Q2_K - Medium
0.00.025.140 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.059 I load: special tokens cache size = 25
0.00.050.031 I load: token to piece cache size = 0.2984 MB
0.00.050.034 I print_info: arch             = gptneox
0.00.050.034 I print_info: vocab_only       = 0
0.00.050.034 I print_info: n_ctx_train      = 2048
0.00.050.035 I print_info: n_embd           = 2048
0.00.050.035 I print_info: n_layer          = 24
0.00.050.038 I print_info: n_head           = 16
0.00.050.040 I print_info: n_head_kv        = 16
0.00.050.040 I print_info: n_rot            = 32
0.00.050.040 I print_info: n_swa            = 0
0.00.050.041 I print_info: n_embd_head_k    = 128
0.00.050.041 I print_info: n_embd_head_v    = 128
0.00.050.042 I print_info: n_gqa            = 1
0.00.050.043 I print_info: n_embd_k_gqa     = 2048
0.00.050.043 I print_info: n_embd_v_gqa     = 2048
0.00.050.044 I print_info: f_norm_eps       = 1.0e-05
0.00.050.044 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.044 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.045 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.046 I print_info: f_logit_scale    = 0.0e+00
0.00.050.047 I print_info: n_ff             = 8192
0.00.050.047 I print_info: n_expert         = 0
0.00.050.047 I print_info: n_expert_used    = 0
0.00.050.048 I print_info: causal attn      = 1
0.00.050.048 I print_info: pooling type     = 0
0.00.050.048 I print_info: rope type        = 2
0.00.050.048 I print_info: rope scaling     = linear
0.00.050.049 I print_info: freq_base_train  = 10000.0
0.00.050.049 I print_info: freq_scale_train = 1
0.00.050.049 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.049 I print_info: rope_finetuned   = unknown
0.00.050.049 I print_info: ssm_d_conv       = 0
0.00.050.050 I print_info: ssm_d_inner      = 0
0.00.050.050 I print_info: ssm_d_state      = 0
0.00.050.050 I print_info: ssm_dt_rank      = 0
0.00.050.050 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.050 I print_info: model type       = 1.4B
0.00.050.051 I print_info: model params     = 1.41 B
0.00.050.051 I print_info: general.name     = 1.4B
0.00.050.051 I print_info: vocab type       = BPE
0.00.050.052 I print_info: n_vocab          = 50304
0.00.050.052 I print_info: n_merges         = 50009
0.00.050.052 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.056 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.057 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.057 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.058 I print_info: LF token         = 128 'Ä'
0.00.050.058 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.058 I print_info: max token length = 1024
0.00.051.930 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.931 I load_tensors: offloading output layer to GPU
0.00.051.931 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.941 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.943 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.225 I llama_init_from_model: n_seq_max     = 1
0.00.052.226 I llama_init_from_model: n_ctx         = 2048
0.00.052.226 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.226 I llama_init_from_model: n_batch       = 2048
0.00.052.226 I llama_init_from_model: n_ubatch      = 512
0.00.052.227 I llama_init_from_model: flash_attn    = 0
0.00.052.227 I llama_init_from_model: freq_base     = 10000.0
0.00.052.227 I llama_init_from_model: freq_scale    = 1
0.00.052.228 I ggml_metal_init: allocating
0.00.052.231 I ggml_metal_init: found device: Apple M4
0.00.052.233 I ggml_metal_init: picking default device: Apple M4
0.00.052.811 I ggml_metal_init: using embedded metal library
0.00.055.125 I ggml_metal_init: GPU name:   Apple M4
0.00.055.127 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.127 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.128 I ggml_metal_init: simdgroup reduction   = true
0.00.055.128 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.128 I ggml_metal_init: has bfloat            = true
0.00.055.128 I ggml_metal_init: use bfloat            = true
0.00.055.128 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.129 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.835 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.008 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.014 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.033 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.090 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.092 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.092 I llama_init_from_model: graph nodes  = 967
0.00.085.093 I llama_init_from_model: graph splits = 2
0.00.085.096 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.226 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.473.094 I main: llama threadpool init, n_threads = 4
0.00.473.139 I 
0.00.473.169 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.473.169 I 
0.00.473.408 I sampler seed: 1234
0.00.473.414 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.473.435 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.473.435 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.473.435 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.153.522 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 64899.45 tokens per second)
0.01.153.523 I llama_perf_context_print:        load time =     463.21 ms
0.01.153.524 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.29 tokens per second)
0.01.153.524 I llama_perf_context_print:        eval time =     641.39 ms /    63 runs   (   10.18 ms per token,    98.22 tokens per second)
0.01.153.525 I llama_perf_context_print:       total time =     680.43 ms /    70 tokens
0.01.153.746 I ggml_metal_free: deallocating

real	0m1.172s
user	0m0.110s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.953 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.871 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.025.878 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.884 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.885 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.885 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.885 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.886 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.886 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.887 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.887 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.887 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.888 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.888 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.888 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.890 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.890 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.890 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.626 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.565 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.566 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.567 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.567 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.567 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.568 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.034.568 I llama_model_loader: - type  f32:  194 tensors
0.00.034.569 I llama_model_loader: - type q2_K:   49 tensors
0.00.034.569 I llama_model_loader: - type q3_K:   48 tensors
0.00.034.569 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.570 I print_info: file format = GGUF V3 (latest)
0.00.034.570 I print_info: file type   = Q2_K - Medium
0.00.034.572 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.054.503 I load: special tokens cache size = 25
0.00.060.610 I load: token to piece cache size = 0.2984 MB
0.00.060.614 I print_info: arch             = gptneox
0.00.060.615 I print_info: vocab_only       = 0
0.00.060.615 I print_info: n_ctx_train      = 2048
0.00.060.615 I print_info: n_embd           = 2048
0.00.060.615 I print_info: n_layer          = 24
0.00.060.619 I print_info: n_head           = 16
0.00.060.620 I print_info: n_head_kv        = 16
0.00.060.620 I print_info: n_rot            = 32
0.00.060.620 I print_info: n_swa            = 0
0.00.060.620 I print_info: n_embd_head_k    = 128
0.00.060.620 I print_info: n_embd_head_v    = 128
0.00.060.621 I print_info: n_gqa            = 1
0.00.060.622 I print_info: n_embd_k_gqa     = 2048
0.00.060.622 I print_info: n_embd_v_gqa     = 2048
0.00.060.623 I print_info: f_norm_eps       = 1.0e-05
0.00.060.623 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.623 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.623 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.624 I print_info: f_logit_scale    = 0.0e+00
0.00.060.624 I print_info: n_ff             = 8192
0.00.060.624 I print_info: n_expert         = 0
0.00.060.625 I print_info: n_expert_used    = 0
0.00.060.625 I print_info: causal attn      = 1
0.00.060.625 I print_info: pooling type     = 0
0.00.060.625 I print_info: rope type        = 2
0.00.060.625 I print_info: rope scaling     = linear
0.00.060.625 I print_info: freq_base_train  = 10000.0
0.00.060.626 I print_info: freq_scale_train = 1
0.00.060.626 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.626 I print_info: rope_finetuned   = unknown
0.00.060.626 I print_info: ssm_d_conv       = 0
0.00.060.626 I print_info: ssm_d_inner      = 0
0.00.060.626 I print_info: ssm_d_state      = 0
0.00.060.627 I print_info: ssm_dt_rank      = 0
0.00.060.627 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.627 I print_info: model type       = 1.4B
0.00.060.627 I print_info: model params     = 1.41 B
0.00.060.628 I print_info: general.name     = 1.4B
0.00.060.628 I print_info: vocab type       = BPE
0.00.060.628 I print_info: n_vocab          = 50304
0.00.060.628 I print_info: n_merges         = 50009
0.00.060.629 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.629 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.629 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.629 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.630 I print_info: LF token         = 128 'Ä'
0.00.060.630 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.630 I print_info: max token length = 1024
0.00.062.237 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.237 I load_tensors: offloading output layer to GPU
0.00.062.238 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.248 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.062.249 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.062.587 I llama_init_from_model: n_seq_max     = 1
0.00.062.588 I llama_init_from_model: n_ctx         = 128
0.00.062.588 I llama_init_from_model: n_ctx_per_seq = 128
0.00.062.588 I llama_init_from_model: n_batch       = 128
0.00.062.588 I llama_init_from_model: n_ubatch      = 128
0.00.062.588 I llama_init_from_model: flash_attn    = 0
0.00.062.589 I llama_init_from_model: freq_base     = 10000.0
0.00.062.589 I llama_init_from_model: freq_scale    = 1
0.00.062.589 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.590 I ggml_metal_init: allocating
0.00.062.594 I ggml_metal_init: found device: Apple M4
0.00.062.595 I ggml_metal_init: picking default device: Apple M4
0.00.063.202 I ggml_metal_init: using embedded metal library
0.00.065.974 I ggml_metal_init: GPU name:   Apple M4
0.00.065.976 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.977 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.977 I ggml_metal_init: simdgroup reduction   = true
0.00.065.978 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.978 I ggml_metal_init: has bfloat            = true
0.00.065.978 I ggml_metal_init: use bfloat            = true
0.00.065.978 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.979 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.234 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.076.530 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.533 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.550 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.077.411 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.077.413 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.077.413 I llama_init_from_model: graph nodes  = 967
0.00.077.413 I llama_init_from_model: graph splits = 2
0.00.077.415 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.077.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.185 I 
0.00.604.210 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.220 I perplexity: tokenizing the input ..
0.00.615.118 I perplexity: tokenization took 10.896 ms
0.00.615.123 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.757.436 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.758.823 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.758.850 I llama_perf_context_print:        load time =     587.23 ms
0.00.758.851 I llama_perf_context_print: prompt eval time =     141.99 ms /   128 tokens (    1.11 ms per token,   901.47 tokens per second)
0.00.758.852 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.758.852 I llama_perf_context_print:       total time =     154.66 ms /   129 tokens
0.00.759.213 I ggml_metal_free: deallocating

real	0m0.792s
user	0m0.082s
sys	0m0.075s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.405 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.049 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.054 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.056 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.057 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.057 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.057 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.060 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.061 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.061 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.061 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.062 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.062 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.064 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.064 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.064 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.880 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.610 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.611 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.611 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.611 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.612 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.612 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.613 I llama_model_loader: - type  f32:  194 tensors
0.00.024.613 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.613 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.613 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.614 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.614 I print_info: file format = GGUF V3 (latest)
0.00.024.615 I print_info: file type   = Q3_K - Medium
0.00.024.615 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.224 I load: special tokens cache size = 25
0.00.050.152 I load: token to piece cache size = 0.2984 MB
0.00.050.155 I print_info: arch             = gptneox
0.00.050.155 I print_info: vocab_only       = 0
0.00.050.155 I print_info: n_ctx_train      = 2048
0.00.050.155 I print_info: n_embd           = 2048
0.00.050.155 I print_info: n_layer          = 24
0.00.050.158 I print_info: n_head           = 16
0.00.050.159 I print_info: n_head_kv        = 16
0.00.050.159 I print_info: n_rot            = 32
0.00.050.160 I print_info: n_swa            = 0
0.00.050.160 I print_info: n_embd_head_k    = 128
0.00.050.162 I print_info: n_embd_head_v    = 128
0.00.050.163 I print_info: n_gqa            = 1
0.00.050.164 I print_info: n_embd_k_gqa     = 2048
0.00.050.166 I print_info: n_embd_v_gqa     = 2048
0.00.050.166 I print_info: f_norm_eps       = 1.0e-05
0.00.050.167 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.167 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.167 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.167 I print_info: f_logit_scale    = 0.0e+00
0.00.050.168 I print_info: n_ff             = 8192
0.00.050.168 I print_info: n_expert         = 0
0.00.050.168 I print_info: n_expert_used    = 0
0.00.050.169 I print_info: causal attn      = 1
0.00.050.169 I print_info: pooling type     = 0
0.00.050.169 I print_info: rope type        = 2
0.00.050.170 I print_info: rope scaling     = linear
0.00.050.171 I print_info: freq_base_train  = 10000.0
0.00.050.171 I print_info: freq_scale_train = 1
0.00.050.171 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.172 I print_info: rope_finetuned   = unknown
0.00.050.172 I print_info: ssm_d_conv       = 0
0.00.050.172 I print_info: ssm_d_inner      = 0
0.00.050.172 I print_info: ssm_d_state      = 0
0.00.050.172 I print_info: ssm_dt_rank      = 0
0.00.050.172 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.172 I print_info: model type       = 1.4B
0.00.050.173 I print_info: model params     = 1.41 B
0.00.050.177 I print_info: general.name     = 1.4B
0.00.050.177 I print_info: vocab type       = BPE
0.00.050.178 I print_info: n_vocab          = 50304
0.00.050.178 I print_info: n_merges         = 50009
0.00.050.178 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.178 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.178 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.178 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.179 I print_info: LF token         = 128 'Ä'
0.00.050.179 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.180 I print_info: max token length = 1024
0.00.052.134 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.134 I load_tensors: offloading output layer to GPU
0.00.052.135 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.145 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.146 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.434 I llama_init_from_model: n_seq_max     = 1
0.00.052.434 I llama_init_from_model: n_ctx         = 2048
0.00.052.434 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.435 I llama_init_from_model: n_batch       = 2048
0.00.052.435 I llama_init_from_model: n_ubatch      = 512
0.00.052.435 I llama_init_from_model: flash_attn    = 0
0.00.052.435 I llama_init_from_model: freq_base     = 10000.0
0.00.052.436 I llama_init_from_model: freq_scale    = 1
0.00.052.436 I ggml_metal_init: allocating
0.00.052.439 I ggml_metal_init: found device: Apple M4
0.00.052.441 I ggml_metal_init: picking default device: Apple M4
0.00.053.047 I ggml_metal_init: using embedded metal library
0.00.055.468 I ggml_metal_init: GPU name:   Apple M4
0.00.055.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.470 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.471 I ggml_metal_init: simdgroup reduction   = true
0.00.055.471 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.471 I ggml_metal_init: has bfloat            = true
0.00.055.471 I ggml_metal_init: use bfloat            = true
0.00.055.472 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.472 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.446 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.802 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.810 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.831 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.807 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.808 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.809 I llama_init_from_model: graph nodes  = 967
0.00.086.809 I llama_init_from_model: graph splits = 2
0.00.086.811 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.943 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.943 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.530.760 I main: llama threadpool init, n_threads = 4
0.00.530.806 I 
0.00.530.832 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.530.832 I 
0.00.531.071 I sampler seed: 1234
0.00.531.075 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.531.116 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.531.116 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.531.116 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.281.828 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.281.829 I llama_perf_context_print:        load time =     521.35 ms
0.01.281.830 I llama_perf_context_print: prompt eval time =      44.41 ms /     7 tokens (    6.34 ms per token,   157.63 tokens per second)
0.01.281.830 I llama_perf_context_print:        eval time =     703.26 ms /    63 runs   (   11.16 ms per token,    89.58 tokens per second)
0.01.281.831 I llama_perf_context_print:       total time =     751.07 ms /    70 tokens
0.01.282.087 I ggml_metal_free: deallocating

real	0m1.299s
user	0m0.111s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.050 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.659 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.666 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.668 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.670 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.670 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.671 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.671 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.672 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.672 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.672 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.673 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.675 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.675 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.675 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.344 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.318 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.320 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.320 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.320 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.321 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.321 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.322 I llama_model_loader: - type  f32:  194 tensors
0.00.026.322 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.322 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.323 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.323 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.323 I print_info: file format = GGUF V3 (latest)
0.00.026.324 I print_info: file type   = Q3_K - Medium
0.00.026.328 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.839 I load: special tokens cache size = 25
0.00.052.014 I load: token to piece cache size = 0.2984 MB
0.00.052.019 I print_info: arch             = gptneox
0.00.052.019 I print_info: vocab_only       = 0
0.00.052.019 I print_info: n_ctx_train      = 2048
0.00.052.019 I print_info: n_embd           = 2048
0.00.052.020 I print_info: n_layer          = 24
0.00.052.024 I print_info: n_head           = 16
0.00.052.025 I print_info: n_head_kv        = 16
0.00.052.025 I print_info: n_rot            = 32
0.00.052.025 I print_info: n_swa            = 0
0.00.052.026 I print_info: n_embd_head_k    = 128
0.00.052.026 I print_info: n_embd_head_v    = 128
0.00.052.026 I print_info: n_gqa            = 1
0.00.052.027 I print_info: n_embd_k_gqa     = 2048
0.00.052.028 I print_info: n_embd_v_gqa     = 2048
0.00.052.028 I print_info: f_norm_eps       = 1.0e-05
0.00.052.029 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.029 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.032 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.032 I print_info: f_logit_scale    = 0.0e+00
0.00.052.032 I print_info: n_ff             = 8192
0.00.052.033 I print_info: n_expert         = 0
0.00.052.033 I print_info: n_expert_used    = 0
0.00.052.033 I print_info: causal attn      = 1
0.00.052.060 I print_info: pooling type     = 0
0.00.052.062 I print_info: rope type        = 2
0.00.052.062 I print_info: rope scaling     = linear
0.00.052.063 I print_info: freq_base_train  = 10000.0
0.00.052.063 I print_info: freq_scale_train = 1
0.00.052.063 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.064 I print_info: rope_finetuned   = unknown
0.00.052.064 I print_info: ssm_d_conv       = 0
0.00.052.064 I print_info: ssm_d_inner      = 0
0.00.052.064 I print_info: ssm_d_state      = 0
0.00.052.064 I print_info: ssm_dt_rank      = 0
0.00.052.064 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.065 I print_info: model type       = 1.4B
0.00.052.065 I print_info: model params     = 1.41 B
0.00.052.065 I print_info: general.name     = 1.4B
0.00.052.067 I print_info: vocab type       = BPE
0.00.052.067 I print_info: n_vocab          = 50304
0.00.052.067 I print_info: n_merges         = 50009
0.00.052.068 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.069 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.069 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.069 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.069 I print_info: LF token         = 128 'Ä'
0.00.052.072 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.072 I print_info: max token length = 1024
0.00.053.964 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.964 I load_tensors: offloading output layer to GPU
0.00.053.965 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.975 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.977 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.054.265 I llama_init_from_model: n_seq_max     = 1
0.00.054.266 I llama_init_from_model: n_ctx         = 128
0.00.054.266 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.266 I llama_init_from_model: n_batch       = 128
0.00.054.266 I llama_init_from_model: n_ubatch      = 128
0.00.054.267 I llama_init_from_model: flash_attn    = 0
0.00.054.267 I llama_init_from_model: freq_base     = 10000.0
0.00.054.267 I llama_init_from_model: freq_scale    = 1
0.00.054.268 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.268 I ggml_metal_init: allocating
0.00.054.271 I ggml_metal_init: found device: Apple M4
0.00.054.273 I ggml_metal_init: picking default device: Apple M4
0.00.054.879 I ggml_metal_init: using embedded metal library
0.00.057.331 I ggml_metal_init: GPU name:   Apple M4
0.00.057.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.334 I ggml_metal_init: simdgroup reduction   = true
0.00.057.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.334 I ggml_metal_init: has bfloat            = true
0.00.057.334 I ggml_metal_init: use bfloat            = true
0.00.057.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.419 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.715 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.717 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.732 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.681 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.682 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.682 I llama_init_from_model: graph nodes  = 967
0.00.069.682 I llama_init_from_model: graph splits = 2
0.00.069.684 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.684 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.476 I 
0.00.796.533 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.560 I perplexity: tokenizing the input ..
0.00.804.499 I perplexity: tokenization took 7.936 ms
0.00.804.502 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.936.983 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.938.144 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.938.166 I llama_perf_context_print:        load time =     787.42 ms
0.00.938.167 I llama_perf_context_print: prompt eval time =     132.25 ms /   128 tokens (    1.03 ms per token,   967.87 tokens per second)
0.00.938.168 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.938.168 I llama_perf_context_print:       total time =     141.70 ms /   129 tokens
0.00.938.647 I ggml_metal_free: deallocating

real	0m0.952s
user	0m0.079s
sys	0m0.096s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.884 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.577 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.582 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.584 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.585 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.585 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.585 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.586 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.588 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.412 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.193 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.194 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.195 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.195 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.196 I llama_model_loader: - type  f32:  194 tensors
0.00.024.196 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.197 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.197 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.197 I print_info: file format = GGUF V3 (latest)
0.00.024.198 I print_info: file type   = Q4_K - Medium
0.00.024.199 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.902 I load: special tokens cache size = 25
0.00.049.901 I load: token to piece cache size = 0.2984 MB
0.00.049.904 I print_info: arch             = gptneox
0.00.049.904 I print_info: vocab_only       = 0
0.00.049.905 I print_info: n_ctx_train      = 2048
0.00.049.905 I print_info: n_embd           = 2048
0.00.049.905 I print_info: n_layer          = 24
0.00.049.908 I print_info: n_head           = 16
0.00.049.910 I print_info: n_head_kv        = 16
0.00.049.910 I print_info: n_rot            = 32
0.00.049.910 I print_info: n_swa            = 0
0.00.049.911 I print_info: n_embd_head_k    = 128
0.00.049.911 I print_info: n_embd_head_v    = 128
0.00.049.913 I print_info: n_gqa            = 1
0.00.049.914 I print_info: n_embd_k_gqa     = 2048
0.00.049.914 I print_info: n_embd_v_gqa     = 2048
0.00.049.915 I print_info: f_norm_eps       = 1.0e-05
0.00.049.915 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.916 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.916 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.916 I print_info: f_logit_scale    = 0.0e+00
0.00.049.917 I print_info: n_ff             = 8192
0.00.049.917 I print_info: n_expert         = 0
0.00.049.917 I print_info: n_expert_used    = 0
0.00.049.917 I print_info: causal attn      = 1
0.00.049.918 I print_info: pooling type     = 0
0.00.049.918 I print_info: rope type        = 2
0.00.049.919 I print_info: rope scaling     = linear
0.00.049.920 I print_info: freq_base_train  = 10000.0
0.00.049.920 I print_info: freq_scale_train = 1
0.00.049.920 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.920 I print_info: rope_finetuned   = unknown
0.00.049.920 I print_info: ssm_d_conv       = 0
0.00.049.921 I print_info: ssm_d_inner      = 0
0.00.049.921 I print_info: ssm_d_state      = 0
0.00.049.921 I print_info: ssm_dt_rank      = 0
0.00.049.921 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.921 I print_info: model type       = 1.4B
0.00.049.922 I print_info: model params     = 1.41 B
0.00.049.922 I print_info: general.name     = 1.4B
0.00.049.922 I print_info: vocab type       = BPE
0.00.049.923 I print_info: n_vocab          = 50304
0.00.049.923 I print_info: n_merges         = 50009
0.00.049.923 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.923 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.924 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.924 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.924 I print_info: LF token         = 128 'Ä'
0.00.049.924 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.924 I print_info: max token length = 1024
0.00.051.924 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.924 I load_tensors: offloading output layer to GPU
0.00.051.924 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.935 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.937 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.218 I llama_init_from_model: n_seq_max     = 1
0.00.052.218 I llama_init_from_model: n_ctx         = 2048
0.00.052.218 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.219 I llama_init_from_model: n_batch       = 2048
0.00.052.219 I llama_init_from_model: n_ubatch      = 512
0.00.052.219 I llama_init_from_model: flash_attn    = 0
0.00.052.219 I llama_init_from_model: freq_base     = 10000.0
0.00.052.220 I llama_init_from_model: freq_scale    = 1
0.00.052.220 I ggml_metal_init: allocating
0.00.052.223 I ggml_metal_init: found device: Apple M4
0.00.052.225 I ggml_metal_init: picking default device: Apple M4
0.00.052.812 I ggml_metal_init: using embedded metal library
0.00.055.200 I ggml_metal_init: GPU name:   Apple M4
0.00.055.201 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.202 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.202 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.202 I ggml_metal_init: simdgroup reduction   = true
0.00.055.203 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.203 I ggml_metal_init: has bfloat            = true
0.00.055.203 I ggml_metal_init: use bfloat            = true
0.00.055.203 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.204 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.152 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.238 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.246 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.269 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.339 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.340 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.340 I llama_init_from_model: graph nodes  = 967
0.00.085.341 I llama_init_from_model: graph splits = 2
0.00.085.344 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.460 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.461 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.903 I main: llama threadpool init, n_threads = 4
0.00.601.950 I 
0.00.601.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.601.986 I 
0.00.602.228 I sampler seed: 1234
0.00.602.232 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.602.266 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.602.276 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.602.276 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.364.409 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.364.410 I llama_perf_context_print:        load time =     593.01 ms
0.01.364.410 I llama_perf_context_print: prompt eval time =      47.13 ms /     7 tokens (    6.73 ms per token,   148.53 tokens per second)
0.01.364.411 I llama_perf_context_print:        eval time =     711.88 ms /    63 runs   (   11.30 ms per token,    88.50 tokens per second)
0.01.364.411 I llama_perf_context_print:       total time =     762.51 ms /    70 tokens
0.01.364.672 I ggml_metal_free: deallocating

real	0m1.381s
user	0m0.110s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.921 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.839 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.845 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.846 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.846 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.848 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.849 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.853 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.853 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.854 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.645 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.685 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.458 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.458 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.459 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.459 I llama_model_loader: - type  f32:  194 tensors
0.00.027.459 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.460 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.460 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.460 I print_info: file format = GGUF V3 (latest)
0.00.027.461 I print_info: file type   = Q4_K - Medium
0.00.027.462 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.768 I load: special tokens cache size = 25
0.00.052.921 I load: token to piece cache size = 0.2984 MB
0.00.052.924 I print_info: arch             = gptneox
0.00.052.924 I print_info: vocab_only       = 0
0.00.052.924 I print_info: n_ctx_train      = 2048
0.00.052.925 I print_info: n_embd           = 2048
0.00.052.925 I print_info: n_layer          = 24
0.00.052.927 I print_info: n_head           = 16
0.00.052.928 I print_info: n_head_kv        = 16
0.00.052.931 I print_info: n_rot            = 32
0.00.052.931 I print_info: n_swa            = 0
0.00.052.932 I print_info: n_embd_head_k    = 128
0.00.052.932 I print_info: n_embd_head_v    = 128
0.00.052.933 I print_info: n_gqa            = 1
0.00.052.934 I print_info: n_embd_k_gqa     = 2048
0.00.052.934 I print_info: n_embd_v_gqa     = 2048
0.00.052.935 I print_info: f_norm_eps       = 1.0e-05
0.00.052.935 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.935 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.936 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.936 I print_info: f_logit_scale    = 0.0e+00
0.00.052.940 I print_info: n_ff             = 8192
0.00.052.941 I print_info: n_expert         = 0
0.00.052.941 I print_info: n_expert_used    = 0
0.00.052.941 I print_info: causal attn      = 1
0.00.052.942 I print_info: pooling type     = 0
0.00.052.942 I print_info: rope type        = 2
0.00.052.943 I print_info: rope scaling     = linear
0.00.052.943 I print_info: freq_base_train  = 10000.0
0.00.052.943 I print_info: freq_scale_train = 1
0.00.052.944 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.944 I print_info: rope_finetuned   = unknown
0.00.052.944 I print_info: ssm_d_conv       = 0
0.00.052.944 I print_info: ssm_d_inner      = 0
0.00.052.944 I print_info: ssm_d_state      = 0
0.00.052.945 I print_info: ssm_dt_rank      = 0
0.00.052.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.948 I print_info: model type       = 1.4B
0.00.052.948 I print_info: model params     = 1.41 B
0.00.052.948 I print_info: general.name     = 1.4B
0.00.052.949 I print_info: vocab type       = BPE
0.00.052.949 I print_info: n_vocab          = 50304
0.00.052.949 I print_info: n_merges         = 50009
0.00.052.949 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.949 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.949 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.950 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.950 I print_info: LF token         = 128 'Ä'
0.00.052.950 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.950 I print_info: max token length = 1024
0.00.054.890 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.890 I load_tensors: offloading output layer to GPU
0.00.054.890 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.901 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.902 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.055.188 I llama_init_from_model: n_seq_max     = 1
0.00.055.189 I llama_init_from_model: n_ctx         = 128
0.00.055.189 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.190 I llama_init_from_model: n_batch       = 128
0.00.055.190 I llama_init_from_model: n_ubatch      = 128
0.00.055.190 I llama_init_from_model: flash_attn    = 0
0.00.055.190 I llama_init_from_model: freq_base     = 10000.0
0.00.055.191 I llama_init_from_model: freq_scale    = 1
0.00.055.191 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.191 I ggml_metal_init: allocating
0.00.055.194 I ggml_metal_init: found device: Apple M4
0.00.055.196 I ggml_metal_init: picking default device: Apple M4
0.00.055.778 I ggml_metal_init: using embedded metal library
0.00.058.154 I ggml_metal_init: GPU name:   Apple M4
0.00.058.155 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.156 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.156 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.156 I ggml_metal_init: simdgroup reduction   = true
0.00.058.156 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.157 I ggml_metal_init: has bfloat            = true
0.00.058.157 I ggml_metal_init: use bfloat            = true
0.00.058.157 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.158 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.955 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.248 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.251 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.264 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.070.139 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.070.140 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.070.140 I llama_init_from_model: graph nodes  = 967
0.00.070.140 I llama_init_from_model: graph splits = 2
0.00.070.141 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.142 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.213 I 
0.00.629.251 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.264 I perplexity: tokenizing the input ..
0.00.637.331 I perplexity: tokenization took 8.065 ms
0.00.637.334 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.771.494 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.772.678 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.772.704 I llama_perf_context_print:        load time =     620.28 ms
0.00.772.706 I llama_perf_context_print: prompt eval time =     133.94 ms /   128 tokens (    1.05 ms per token,   955.68 tokens per second)
0.00.772.707 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.772.707 I llama_perf_context_print:       total time =     143.49 ms /   129 tokens
0.00.773.169 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.078s
sys	0m0.098s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.405 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.695 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.699 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.701 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.702 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.702 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.702 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.703 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.703 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.704 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.704 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.705 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.705 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.705 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.706 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.707 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.708 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.708 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.425 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.479 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.211 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.212 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.212 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.213 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.213 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.213 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.214 I llama_model_loader: - type  f32:  194 tensors
0.00.026.214 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.214 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.215 I print_info: file format = GGUF V3 (latest)
0.00.026.215 I print_info: file type   = Q5_K - Medium
0.00.026.216 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.085 I load: special tokens cache size = 25
0.00.051.093 I load: token to piece cache size = 0.2984 MB
0.00.051.103 I print_info: arch             = gptneox
0.00.051.105 I print_info: vocab_only       = 0
0.00.051.105 I print_info: n_ctx_train      = 2048
0.00.051.105 I print_info: n_embd           = 2048
0.00.051.105 I print_info: n_layer          = 24
0.00.051.111 I print_info: n_head           = 16
0.00.051.111 I print_info: n_head_kv        = 16
0.00.051.112 I print_info: n_rot            = 32
0.00.051.112 I print_info: n_swa            = 0
0.00.051.112 I print_info: n_embd_head_k    = 128
0.00.051.112 I print_info: n_embd_head_v    = 128
0.00.051.113 I print_info: n_gqa            = 1
0.00.051.114 I print_info: n_embd_k_gqa     = 2048
0.00.051.115 I print_info: n_embd_v_gqa     = 2048
0.00.051.115 I print_info: f_norm_eps       = 1.0e-05
0.00.051.116 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.116 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.116 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.116 I print_info: f_logit_scale    = 0.0e+00
0.00.051.117 I print_info: n_ff             = 8192
0.00.051.118 I print_info: n_expert         = 0
0.00.051.118 I print_info: n_expert_used    = 0
0.00.051.120 I print_info: causal attn      = 1
0.00.051.120 I print_info: pooling type     = 0
0.00.051.120 I print_info: rope type        = 2
0.00.051.120 I print_info: rope scaling     = linear
0.00.051.120 I print_info: freq_base_train  = 10000.0
0.00.051.121 I print_info: freq_scale_train = 1
0.00.051.121 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.121 I print_info: rope_finetuned   = unknown
0.00.051.121 I print_info: ssm_d_conv       = 0
0.00.051.121 I print_info: ssm_d_inner      = 0
0.00.051.122 I print_info: ssm_d_state      = 0
0.00.051.122 I print_info: ssm_dt_rank      = 0
0.00.051.122 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.122 I print_info: model type       = 1.4B
0.00.051.123 I print_info: model params     = 1.41 B
0.00.051.123 I print_info: general.name     = 1.4B
0.00.051.124 I print_info: vocab type       = BPE
0.00.051.124 I print_info: n_vocab          = 50304
0.00.051.126 I print_info: n_merges         = 50009
0.00.051.126 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.126 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.126 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.126 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.127 I print_info: LF token         = 128 'Ä'
0.00.051.128 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.128 I print_info: max token length = 1024
0.00.053.121 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.121 I load_tensors: offloading output layer to GPU
0.00.053.121 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.132 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.133 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.452 I llama_init_from_model: n_seq_max     = 1
0.00.053.453 I llama_init_from_model: n_ctx         = 2048
0.00.053.453 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.453 I llama_init_from_model: n_batch       = 2048
0.00.053.453 I llama_init_from_model: n_ubatch      = 512
0.00.053.453 I llama_init_from_model: flash_attn    = 0
0.00.053.454 I llama_init_from_model: freq_base     = 10000.0
0.00.053.454 I llama_init_from_model: freq_scale    = 1
0.00.053.454 I ggml_metal_init: allocating
0.00.053.457 I ggml_metal_init: found device: Apple M4
0.00.053.459 I ggml_metal_init: picking default device: Apple M4
0.00.054.053 I ggml_metal_init: using embedded metal library
0.00.056.386 I ggml_metal_init: GPU name:   Apple M4
0.00.056.388 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.388 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.389 I ggml_metal_init: simdgroup reduction   = true
0.00.056.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.389 I ggml_metal_init: has bfloat            = true
0.00.056.389 I ggml_metal_init: use bfloat            = true
0.00.056.389 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.390 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.097 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.401 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.408 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.428 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.429 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.430 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.430 I llama_init_from_model: graph nodes  = 967
0.00.087.430 I llama_init_from_model: graph splits = 2
0.00.087.433 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.564 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.564 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.098 I main: llama threadpool init, n_threads = 4
0.00.687.132 I 
0.00.687.154 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.154 I 
0.00.687.378 I sampler seed: 1234
0.00.687.382 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.687.412 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.687.413 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.687.413 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.537.016 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.537.017 I llama_perf_context_print:        load time =     677.69 ms
0.01.537.017 I llama_perf_context_print: prompt eval time =      51.49 ms /     7 tokens (    7.36 ms per token,   135.94 tokens per second)
0.01.537.018 I llama_perf_context_print:        eval time =     795.03 ms /    63 runs   (   12.62 ms per token,    79.24 tokens per second)
0.01.537.018 I llama_perf_context_print:       total time =     849.92 ms /    70 tokens
0.01.537.248 I ggml_metal_free: deallocating

real	0m1.554s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.684 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.062 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.067 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.068 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.069 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.069 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.070 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.070 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.071 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.071 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.072 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.072 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.072 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.073 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.073 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.074 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.075 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.075 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.853 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.844 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.592 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.593 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.594 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.594 I llama_model_loader: - type  f32:  194 tensors
0.00.027.595 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.595 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.595 I print_info: file format = GGUF V3 (latest)
0.00.027.596 I print_info: file type   = Q5_K - Medium
0.00.027.597 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.046.324 I load: special tokens cache size = 25
0.00.052.538 I load: token to piece cache size = 0.2984 MB
0.00.052.541 I print_info: arch             = gptneox
0.00.052.541 I print_info: vocab_only       = 0
0.00.052.541 I print_info: n_ctx_train      = 2048
0.00.052.541 I print_info: n_embd           = 2048
0.00.052.542 I print_info: n_layer          = 24
0.00.052.544 I print_info: n_head           = 16
0.00.052.545 I print_info: n_head_kv        = 16
0.00.052.545 I print_info: n_rot            = 32
0.00.052.545 I print_info: n_swa            = 0
0.00.052.546 I print_info: n_embd_head_k    = 128
0.00.052.546 I print_info: n_embd_head_v    = 128
0.00.052.546 I print_info: n_gqa            = 1
0.00.052.547 I print_info: n_embd_k_gqa     = 2048
0.00.052.548 I print_info: n_embd_v_gqa     = 2048
0.00.052.549 I print_info: f_norm_eps       = 1.0e-05
0.00.052.549 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.549 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.550 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.550 I print_info: f_logit_scale    = 0.0e+00
0.00.052.552 I print_info: n_ff             = 8192
0.00.052.552 I print_info: n_expert         = 0
0.00.052.552 I print_info: n_expert_used    = 0
0.00.052.552 I print_info: causal attn      = 1
0.00.052.552 I print_info: pooling type     = 0
0.00.052.552 I print_info: rope type        = 2
0.00.052.553 I print_info: rope scaling     = linear
0.00.052.553 I print_info: freq_base_train  = 10000.0
0.00.052.554 I print_info: freq_scale_train = 1
0.00.052.554 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.555 I print_info: rope_finetuned   = unknown
0.00.052.555 I print_info: ssm_d_conv       = 0
0.00.052.555 I print_info: ssm_d_inner      = 0
0.00.052.556 I print_info: ssm_d_state      = 0
0.00.052.556 I print_info: ssm_dt_rank      = 0
0.00.052.556 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.556 I print_info: model type       = 1.4B
0.00.052.558 I print_info: model params     = 1.41 B
0.00.052.558 I print_info: general.name     = 1.4B
0.00.052.559 I print_info: vocab type       = BPE
0.00.052.559 I print_info: n_vocab          = 50304
0.00.052.559 I print_info: n_merges         = 50009
0.00.052.559 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.561 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.561 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.561 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.562 I print_info: LF token         = 128 'Ä'
0.00.052.562 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.563 I print_info: max token length = 1024
0.00.054.497 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.497 I load_tensors: offloading output layer to GPU
0.00.054.497 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.508 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.509 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.794 I llama_init_from_model: n_seq_max     = 1
0.00.054.795 I llama_init_from_model: n_ctx         = 128
0.00.054.795 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.795 I llama_init_from_model: n_batch       = 128
0.00.054.795 I llama_init_from_model: n_ubatch      = 128
0.00.054.795 I llama_init_from_model: flash_attn    = 0
0.00.054.796 I llama_init_from_model: freq_base     = 10000.0
0.00.054.796 I llama_init_from_model: freq_scale    = 1
0.00.054.796 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.797 I ggml_metal_init: allocating
0.00.054.800 I ggml_metal_init: found device: Apple M4
0.00.054.802 I ggml_metal_init: picking default device: Apple M4
0.00.055.364 I ggml_metal_init: using embedded metal library
0.00.057.718 I ggml_metal_init: GPU name:   Apple M4
0.00.057.719 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.720 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.720 I ggml_metal_init: simdgroup reduction   = true
0.00.057.721 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.721 I ggml_metal_init: has bfloat            = true
0.00.057.721 I ggml_metal_init: use bfloat            = true
0.00.057.721 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.298 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.505 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.507 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.521 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.442 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.443 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.443 I llama_init_from_model: graph nodes  = 967
0.00.069.444 I llama_init_from_model: graph splits = 2
0.00.069.445 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.445 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.212 I 
0.00.646.266 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.326 I perplexity: tokenizing the input ..
0.00.654.643 I perplexity: tokenization took 8.316 ms
0.00.654.647 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.412 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.796.600 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.796.624 I llama_perf_context_print:        load time =     634.52 ms
0.00.796.625 I llama_perf_context_print: prompt eval time =     140.54 ms /   128 tokens (    1.10 ms per token,   910.78 tokens per second)
0.00.796.626 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.631 I llama_perf_context_print:       total time =     150.42 ms /   129 tokens
0.00.797.109 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.078s
sys	0m0.123s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.704 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.682 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.686 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.688 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.688 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.689 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.689 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.689 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.690 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.691 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.691 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.691 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.692 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.692 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.693 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.694 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.695 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.695 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.513 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.588 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.424 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.425 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.425 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.426 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.426 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.426 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.427 I llama_model_loader: - type  f32:  194 tensors
0.00.024.427 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.427 I print_info: file format = GGUF V3 (latest)
0.00.024.428 I print_info: file type   = Q6_K
0.00.024.429 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.356 I load: special tokens cache size = 25
0.00.049.287 I load: token to piece cache size = 0.2984 MB
0.00.049.290 I print_info: arch             = gptneox
0.00.049.290 I print_info: vocab_only       = 0
0.00.049.291 I print_info: n_ctx_train      = 2048
0.00.049.291 I print_info: n_embd           = 2048
0.00.049.291 I print_info: n_layer          = 24
0.00.049.294 I print_info: n_head           = 16
0.00.049.297 I print_info: n_head_kv        = 16
0.00.049.297 I print_info: n_rot            = 32
0.00.049.298 I print_info: n_swa            = 0
0.00.049.298 I print_info: n_embd_head_k    = 128
0.00.049.298 I print_info: n_embd_head_v    = 128
0.00.049.299 I print_info: n_gqa            = 1
0.00.049.304 I print_info: n_embd_k_gqa     = 2048
0.00.049.305 I print_info: n_embd_v_gqa     = 2048
0.00.049.306 I print_info: f_norm_eps       = 1.0e-05
0.00.049.306 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.308 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.308 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.308 I print_info: f_logit_scale    = 0.0e+00
0.00.049.309 I print_info: n_ff             = 8192
0.00.049.309 I print_info: n_expert         = 0
0.00.049.309 I print_info: n_expert_used    = 0
0.00.049.309 I print_info: causal attn      = 1
0.00.049.310 I print_info: pooling type     = 0
0.00.049.310 I print_info: rope type        = 2
0.00.049.310 I print_info: rope scaling     = linear
0.00.049.311 I print_info: freq_base_train  = 10000.0
0.00.049.312 I print_info: freq_scale_train = 1
0.00.049.312 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.312 I print_info: rope_finetuned   = unknown
0.00.049.312 I print_info: ssm_d_conv       = 0
0.00.049.313 I print_info: ssm_d_inner      = 0
0.00.049.313 I print_info: ssm_d_state      = 0
0.00.049.313 I print_info: ssm_dt_rank      = 0
0.00.049.313 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.313 I print_info: model type       = 1.4B
0.00.049.314 I print_info: model params     = 1.41 B
0.00.049.314 I print_info: general.name     = 1.4B
0.00.049.316 I print_info: vocab type       = BPE
0.00.049.316 I print_info: n_vocab          = 50304
0.00.049.316 I print_info: n_merges         = 50009
0.00.049.316 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.316 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.316 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.316 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.317 I print_info: LF token         = 128 'Ä'
0.00.049.317 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.317 I print_info: max token length = 1024
0.00.051.370 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.370 I load_tensors: offloading output layer to GPU
0.00.051.371 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.381 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.383 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.671 I llama_init_from_model: n_seq_max     = 1
0.00.051.672 I llama_init_from_model: n_ctx         = 2048
0.00.051.672 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.672 I llama_init_from_model: n_batch       = 2048
0.00.051.672 I llama_init_from_model: n_ubatch      = 512
0.00.051.672 I llama_init_from_model: flash_attn    = 0
0.00.051.673 I llama_init_from_model: freq_base     = 10000.0
0.00.051.673 I llama_init_from_model: freq_scale    = 1
0.00.051.673 I ggml_metal_init: allocating
0.00.051.676 I ggml_metal_init: found device: Apple M4
0.00.051.678 I ggml_metal_init: picking default device: Apple M4
0.00.052.278 I ggml_metal_init: using embedded metal library
0.00.054.591 I ggml_metal_init: GPU name:   Apple M4
0.00.054.593 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.593 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.594 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.594 I ggml_metal_init: simdgroup reduction   = true
0.00.054.594 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.594 I ggml_metal_init: has bfloat            = true
0.00.054.594 I ggml_metal_init: use bfloat            = true
0.00.054.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.368 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.857 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.868 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.891 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.897 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.899 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.899 I llama_init_from_model: graph nodes  = 967
0.00.085.899 I llama_init_from_model: graph splits = 2
0.00.085.902 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.031 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.032 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.135 I main: llama threadpool init, n_threads = 4
0.00.750.171 I 
0.00.750.198 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.198 I 
0.00.750.446 I sampler seed: 1234
0.00.750.450 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.496 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.498 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.498 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.623.123 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.623.125 I llama_perf_context_print:        load time =     741.43 ms
0.01.623.125 I llama_perf_context_print: prompt eval time =      54.38 ms /     7 tokens (    7.77 ms per token,   128.71 tokens per second)
0.01.623.126 I llama_perf_context_print:        eval time =     815.23 ms /    63 runs   (   12.94 ms per token,    77.28 tokens per second)
0.01.623.126 I llama_perf_context_print:       total time =     872.99 ms /    70 tokens
0.01.623.337 I ggml_metal_free: deallocating

real	0m1.641s
user	0m0.110s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4465 (9a483999) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.879 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.390 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.396 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.399 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.399 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.400 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.400 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.400 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.401 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.401 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.402 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.402 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.402 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.403 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.403 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.405 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.405 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.405 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.215 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.213 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.973 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.974 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.975 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.975 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.975 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.976 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.976 I llama_model_loader: - type  f32:  194 tensors
0.00.023.977 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.977 I print_info: file format = GGUF V3 (latest)
0.00.023.978 I print_info: file type   = Q6_K
0.00.023.979 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.703 I load: special tokens cache size = 25
0.00.048.459 I load: token to piece cache size = 0.2984 MB
0.00.048.462 I print_info: arch             = gptneox
0.00.048.462 I print_info: vocab_only       = 0
0.00.048.462 I print_info: n_ctx_train      = 2048
0.00.048.463 I print_info: n_embd           = 2048
0.00.048.463 I print_info: n_layer          = 24
0.00.048.466 I print_info: n_head           = 16
0.00.048.466 I print_info: n_head_kv        = 16
0.00.048.468 I print_info: n_rot            = 32
0.00.048.469 I print_info: n_swa            = 0
0.00.048.469 I print_info: n_embd_head_k    = 128
0.00.048.469 I print_info: n_embd_head_v    = 128
0.00.048.470 I print_info: n_gqa            = 1
0.00.048.470 I print_info: n_embd_k_gqa     = 2048
0.00.048.476 I print_info: n_embd_v_gqa     = 2048
0.00.048.476 I print_info: f_norm_eps       = 1.0e-05
0.00.048.477 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.477 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.477 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.479 I print_info: f_logit_scale    = 0.0e+00
0.00.048.480 I print_info: n_ff             = 8192
0.00.048.480 I print_info: n_expert         = 0
0.00.048.480 I print_info: n_expert_used    = 0
0.00.048.480 I print_info: causal attn      = 1
0.00.048.480 I print_info: pooling type     = 0
0.00.048.480 I print_info: rope type        = 2
0.00.048.481 I print_info: rope scaling     = linear
0.00.048.481 I print_info: freq_base_train  = 10000.0
0.00.048.481 I print_info: freq_scale_train = 1
0.00.048.482 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.485 I print_info: rope_finetuned   = unknown
0.00.048.485 I print_info: ssm_d_conv       = 0
0.00.048.485 I print_info: ssm_d_inner      = 0
0.00.048.485 I print_info: ssm_d_state      = 0
0.00.048.486 I print_info: ssm_dt_rank      = 0
0.00.048.486 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.486 I print_info: model type       = 1.4B
0.00.048.486 I print_info: model params     = 1.41 B
0.00.048.487 I print_info: general.name     = 1.4B
0.00.048.487 I print_info: vocab type       = BPE
0.00.048.487 I print_info: n_vocab          = 50304
0.00.048.487 I print_info: n_merges         = 50009
0.00.048.487 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.488 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.488 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.488 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.488 I print_info: LF token         = 128 'Ä'
0.00.048.489 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.489 I print_info: max token length = 1024
0.00.050.516 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.516 I load_tensors: offloading output layer to GPU
0.00.050.517 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.527 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.528 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.809 I llama_init_from_model: n_seq_max     = 1
0.00.050.810 I llama_init_from_model: n_ctx         = 128
0.00.050.810 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.810 I llama_init_from_model: n_batch       = 128
0.00.050.811 I llama_init_from_model: n_ubatch      = 128
0.00.050.811 I llama_init_from_model: flash_attn    = 0
0.00.050.811 I llama_init_from_model: freq_base     = 10000.0
0.00.050.811 I llama_init_from_model: freq_scale    = 1
0.00.050.812 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.812 I ggml_metal_init: allocating
0.00.050.815 I ggml_metal_init: found device: Apple M4
0.00.050.817 I ggml_metal_init: picking default device: Apple M4
0.00.051.391 I ggml_metal_init: using embedded metal library
0.00.053.732 I ggml_metal_init: GPU name:   Apple M4
0.00.053.733 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.734 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.734 I ggml_metal_init: simdgroup reduction   = true
0.00.053.734 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.735 I ggml_metal_init: has bfloat            = true
0.00.053.735 I ggml_metal_init: use bfloat            = true
0.00.053.735 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.736 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.288 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.575 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.589 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.470 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.471 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.472 I llama_init_from_model: graph nodes  = 967
0.00.065.472 I llama_init_from_model: graph splits = 2
0.00.065.473 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.331.137 I 
0.00.331.172 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.331.183 I perplexity: tokenizing the input ..
0.00.338.703 I perplexity: tokenization took 7.518 ms
0.00.338.707 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.478.934 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.480.110 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.480.136 I llama_perf_context_print:        load time =     322.25 ms
0.00.480.137 I llama_perf_context_print: prompt eval time =     139.99 ms /   128 tokens (    1.09 ms per token,   914.34 tokens per second)
0.00.480.138 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.480.138 I llama_perf_context_print:       total time =     149.00 ms /   129 tokens
0.00.480.602 I ggml_metal_free: deallocating

real	0m0.495s
user	0m0.077s
sys	0m0.073s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4465 (9a483999)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13730a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13730b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13730b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13730bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13730c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13730c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13730ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13730d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13730d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13730dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13730e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13730e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13730f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13730f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1373101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137310910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137311030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137311750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137311e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137312640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137312d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137313480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137313ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137314440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137314b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137314e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137315430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1373160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1373165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1373168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137316d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137317000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137317890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137317dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137318090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137318530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1373189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137318e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137319310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1373197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137319c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13731a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13731a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13731aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13731acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13731b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13731b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13731c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13731c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13731ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13731d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13731da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13731e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13731e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13731ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13731f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13731f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13731fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137320090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137320880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137320b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137320fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137321480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137321920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137321dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137322260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137322700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137322ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137323040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1373234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137323980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137323e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1373242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137324810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137324d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1373252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137325800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137325d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1373262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1373267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137326d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137327290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1373277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137327d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137328280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1373287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137328d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137329270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1373297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137329d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13732a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13732a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13732ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13732b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13732b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13732bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13732c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13731bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13732c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13732ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13732d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13732d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13732de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13732e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13732e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13732ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13732f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13732f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13732fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137330380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1373308d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137330e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137331370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137331810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137331cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137332150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1373325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137332a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137332f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1373333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137333870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137333d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1373341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137334650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137334af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137334f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137335430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1373358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137335d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137336210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1373366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137336b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137336ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137337490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137337930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137337dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137338270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137338710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137338bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137339050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1373394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137339990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137339e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13733a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13733a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13733ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13733b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13733b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13733b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13733be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13733c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13733c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13733cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13733d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13733d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13733da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13733def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13733e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13733e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13733ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13733f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13733f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13733fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13733ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1373403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137340890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137340d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1373411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137341670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137341b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137341fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137342450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1373428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137342d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137343230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1373436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137343b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137344010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1373444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137344950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137344df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137345290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137345730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137345bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137346070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137346510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1373469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137346e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1373472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137347790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137347c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1373480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137348570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137348ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137349010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137349560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137349ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137349d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13734a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13734a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13734afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13734b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13734bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13734bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13734c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13734cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13734d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13734d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13734dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13734e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13734e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13734ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13734f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13734f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13734fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137350320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137350870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137350dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137351310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137351860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137351db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137352300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137352850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137352da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1373532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137353840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137353d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1373542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137354830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137354d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1373552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137355820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137355d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1373562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137356810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137356d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1373572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137357800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137357d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1373582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1373587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137358d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137359290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1373597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137359d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13735a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13735a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13735ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13735b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13735b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13735bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13735c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13735c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13735cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13735d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13735d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13735dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13735e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13735e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13735ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13735f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13735f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13735fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137360220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137360770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137360cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137361210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1373616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137361b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137361ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137362490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137362930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137362dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137363270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137363710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137363bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137364050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1373644f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137364990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137364e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1373652d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137365770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137365cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1373663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137366b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137367220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137367940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137367c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1373683f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1373686b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137368cc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.137.237 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.137.241 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137368970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13734a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13734a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13734ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13731dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13731d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13731fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13734c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1373150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13731bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13731c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13731cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13731afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13731d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1373140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137320350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13732c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137367ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1373172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137317580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13734cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13734b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1373156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1373159b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137315c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137369120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1373693e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1373696a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137369960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137369c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137369ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13736a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13736a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13736a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13736a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13736aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13736af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13736b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13736b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13736b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13736ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13736bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13736bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13736c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13736c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13736c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13736cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13736cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13736d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13736d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13736d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13736d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13736db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13736de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13736e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13736e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13736e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13736e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13736ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13736eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13736f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13736f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13736f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13736f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13736fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13736ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1373701e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1373704a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137370760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137370a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137370ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137370fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137371260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137371520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1373717e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137371aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137371d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137372020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1373722e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1373725a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137372860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137372b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137372de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1373730a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137373360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137373620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1373738e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137373ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137373e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137374120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1373743e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1373746a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137374960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137374c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137374ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1373751a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137375460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137375720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1373759e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137375ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137375f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137376220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1373764e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1373767a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137376a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137376d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137376fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1373772a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137377560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137377820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137377ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137377da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137378060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137378320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1373785e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1373788a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137378b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137378e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1373790e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1373793a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137379660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137379920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137379be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137379ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13737a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13737a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13737a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13737a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13737ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13737af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13737b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13737b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13737b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13737ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13737bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13737bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13737c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13737c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13737c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13737caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13737cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13737d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13737d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13737d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13737d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13737db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13737dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13737e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13737e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13737e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13737e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13737eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13737ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13737f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13737f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13737f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13737f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13737fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13737fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1373801a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137380460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137380720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1373809e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137380ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137380f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137381220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1373814e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1373817a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137381a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137381d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137381fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1373822a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137382560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137382820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137382ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137382da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137383060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137383320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1373835e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1373838a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137383b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137383e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1373840e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1373843a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137384660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137384920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137384be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137384ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137385160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137385420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1373856e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1373859a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137385c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137385f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1373861e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1373864a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137386760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137386a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137386ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137386fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137387260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137387520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1373877e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137387aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137387d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137388020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1373882e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1373885a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137388860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137388b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1373892d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137389590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137389850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137389cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13738a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13738a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13738aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13738ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13738b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13738b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13738bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13738c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13738c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13738c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13738cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13738d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13738d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13738dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13738df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13738e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13738e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13738eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13738f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13738f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13738f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13738fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1373902d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137390740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137390bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137391020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137391490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137391900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137391d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1373921e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137392650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137392ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137392f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1373933a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137393810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137393c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1373940f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137394560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1373949d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137394e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1373952b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137395720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137395b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137396000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137396470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1373968e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137396d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1373971c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137397630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137397aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137397f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137398380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1373987f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137398c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1373990d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137399540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1373999b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137399e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13739a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13739a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13739ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13739afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13739b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13739b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13739bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13739c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13739c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13739ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13739cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13739d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13739e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13739e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13739eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13739f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13739f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13739fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1373a0240 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13739d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13739fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13739f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1373a06a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1373a0960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1373a0c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1373a0ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1373a11a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1373a1460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1373a1720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1373a19e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1373a1ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1373a2270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1373a2840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1373a2e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1373a3130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1373a33f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1373a36b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1373a3970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1373a3c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1373a3ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1373a41b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1373a4470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1373a4730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1373a49f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1373a4cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1373a4f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1373a5230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1373a54f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1373a57b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1373a5a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1373a5d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1373a5ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1373a62b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1373a6570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1373a6830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1373a6af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1373a6db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1373a7070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1373a7330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1373a75f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1373a78b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1373a7b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1373a7e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1373a80f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1373a83b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1373a8670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1373a8930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1373a8bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1373a8eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1373a9170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1373a9430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1373a96f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1373a99b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1373a9c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1373a9f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1373aa1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1373aa4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1373aa770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1373aaa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1373aacf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1373aafb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1373ab270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1373ab530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1373ab7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1373abab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1373abd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1373ac030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1373ac2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1373ac5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1373ac870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1373acb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1373acdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1373ad0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1373ad370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1373ad630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1373ad8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1373adbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1373ade70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1373ae130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1373ae3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1373ae6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1373ae970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1373aec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1373aeef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1373af1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1373af470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1373af730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1373af9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1373afcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1373aff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1373b0230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1373b04f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1373b07b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1373b0a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1373b0d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1373b0ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1373b12b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1373b1570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1373b1830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1373b1af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1373b1db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1373b2070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1373b2330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1373b25f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1373b28b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1373b2b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1373b2e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1373b30f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1373b33b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1373b3670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1373b3930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1373b3bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1373b3eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1373b4170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1373b4430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1373b46f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1373b49b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1373b4c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1373b4f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1373b51f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1373b54b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1373b5770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1373b5a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1373b5cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1373b5fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1373b6270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1373b6530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1373b67f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1373b6ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1373b6d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1373b7030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1373b72f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1373b75b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1373b7870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1373b7b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1373b7df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1373b80b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1373b8370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1373b8630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1373b88f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1373b8bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1373b8e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1373b9130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1373b93f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1373b96b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1373b9970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1373b9c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1373b9ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1373ba1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1373ba470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1373ba730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1373ba9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1373bacb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1373baf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1373bb230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1373bb4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1373bb7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1373bba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1373bbd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1373bbff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1373bc2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1373bc570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1373bc830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1373bcaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1373bcdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1373bd070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1373bd330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1373bd5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1373bd8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1373bdb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1373bde30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1373be0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1373be3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1373be670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1373be930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1373bebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1373beeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1373bf170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1373bf430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1373bf6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1373bf9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1373bfc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1373bff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1373c01f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1373c04b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1373c0770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1373c0a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1373c0cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1373c0fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1373c1270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1373c1530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1373c17f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1373c1ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1373c1d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1373c2030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1373c22f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1373c25b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1373c2870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1373c2b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1373c2df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1373c30b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1373c3370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1373c3630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1373c38f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1373c3bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1373c3e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1373c4130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1373c43f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1373c46b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1373c4c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1373c4f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1373c5200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1373c54c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1373c5780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1373c5a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1373c5d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1373c5fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1373c6280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1373c6540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1373c6800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1373c6ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1373c6d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1373c7040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1373c7300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1373c75c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1373c7880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1373c7b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1373c7e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1373c80c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1373c8380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1373c8640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1373c8900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1373c8bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1373c8e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1373c9140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1373c9400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1373c96c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1373c9980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1373c9c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1373c9f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1373ca1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1373ca480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1373ca740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1373caa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1373cacc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1373caf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1373cb240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1373cb500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1373cb7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1373cba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1373cbd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1373cc000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1373cc2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1373cc580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1373cc840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1373ccb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1373ccdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1373cd080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1373cd340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1373cd600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1373cd8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1373cdb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1373cde40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1373ce100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1373ce3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1373ce680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1373ce940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1373cec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1373ceec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1373cf180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1373cf440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1373cf840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1373cfb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1373cfdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1373d0230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1373d06a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1373d0b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1373d0f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1373d13f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1373d1860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1373d1cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1373d2140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1373d2cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1373d33d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1373d3af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1373d4210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1373d44d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1373d4790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1373d4cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1373d5130 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.779s
user	0m0.293s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4465 (9a483999)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13de0b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13de0bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13de0c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13de0c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13de0cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13de0d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13de0d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13de0de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13de0e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13de0e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13de0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13de0f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13de0fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13de105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13de10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13de11500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13de11c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13de12340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13de12a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13de13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13de13950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13de14070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13de14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13de15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13de15750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13de15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13de16020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13de16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13de171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13de17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13de17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13de17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13de18480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13de189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13de18c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13de19120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13de195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13de19a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13de19f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13de1a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13de1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13de1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13de1b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13de1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13de1b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13de1bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13de1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13de1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13de1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13de1da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13de1e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13de1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13de1ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13de1f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13de1fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13de1ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13de203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13de20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13de20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13de21470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13de21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13de21bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13de22070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13de22510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13de229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13de22e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13de232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13de23790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13de23c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13de240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13de24570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13de24a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13de24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13de25400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13de25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13de25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13de263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13de26940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13de26e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13de273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13de27930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13de27e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13de283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13de28920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13de28e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13de293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13de29910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13de29e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13de2a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13de2a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13de2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13de2b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13de2b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13de2be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13de2c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13de2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13de2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13de1cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13de2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13de2da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13de2dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13de2e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13de2ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13de2ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13de2f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13de2fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13de2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13de304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13de30a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13de30f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13de314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13de31a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13de31f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13de32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13de328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13de32d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13de331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13de33680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13de33b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13de33fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13de34460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13de34900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13de34da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13de35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13de356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13de35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13de36020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13de364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13de36960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13de36e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13de372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13de37740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13de37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13de38080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13de38520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13de389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13de38e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13de39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13de397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13de39c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13de3a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13de3a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13de3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13de3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13de3b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13de3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13de3bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13de3c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13de3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13de3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13de3cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13de3d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13de3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13de3dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13de3e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13de3e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13de3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13de3ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13de3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13de3f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13de3fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13de40200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13de406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13de40b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13de40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13de41480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13de41920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13de41dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13de42260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13de42700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13de42ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13de43040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13de434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13de43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13de43e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13de442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13de44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13de44c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13de450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13de45540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13de459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13de45e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13de46320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13de467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13de46c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13de47100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13de475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13de47a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13de47ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13de48380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13de48820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13de48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13de49160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13de496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13de49c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13de4a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13de4a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13de4a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13de4af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13de4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13de4bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13de4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13de4c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13de4cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13de4d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13de4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13de4def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13de4e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13de4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13de4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13de4f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13de4f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13de4ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13de50470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13de509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13de50f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13de51460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13de519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13de51f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13de52450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13de529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13de52ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13de53440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13de53990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13de53ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13de54430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13de54980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13de54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13de55420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13de55970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13de55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13de56410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13de56960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13de56eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13de57400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13de57950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13de57ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13de583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13de58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13de58e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13de593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13de59930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13de59e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13de5a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13de5a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13de5ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13de5b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13de5b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13de5be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13de5c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13de5c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13de5ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13de5d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13de5d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13de5de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13de5e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13de5e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13de5ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13de5f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13de5f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13de5fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13de60370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13de608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13de60e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13de61360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13de618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13de61e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13de622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13de62740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13de62be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13de63080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13de63520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13de639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13de63e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13de64300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13de647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13de64c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13de650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13de65580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13de65a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13de65ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13de66360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13de668b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13de66fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13de676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13de67e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13de68530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13de687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13de68fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13de692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13de698b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.088.373 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.377 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13de69560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13de4cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13de4ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13de4b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13de1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13de1e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13de20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13de4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13de15cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13de1c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13de1d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13de1d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13de1bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13de1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13de14cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13de20f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13de2d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13de68ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13de17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13de18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13de4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13de4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13de162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13de165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13de16860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13de69d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13de69fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13de6a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13de6a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13de6a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13de6aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13de6ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13de6b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13de6b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13de6b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13de6b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13de6bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13de6be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13de6c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13de6c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13de6c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13de6c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13de6cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13de6ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13de6d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13de6d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13de6d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13de6d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13de6dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13de6df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13de6e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13de6e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13de6e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13de6ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13de6ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13de6ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13de6f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13de6f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13de6f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13de6fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13de6fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13de70010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13de702d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13de70590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13de70850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13de70b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13de70dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13de71090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13de71350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13de71610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13de718d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13de71b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13de71e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13de72110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13de723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13de72690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13de72950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13de72c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13de72ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13de73190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13de73450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13de73710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13de739d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13de73c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13de73f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13de74210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13de744d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13de74790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13de74a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13de74d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13de74fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13de75290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13de75550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13de75810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13de75ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13de75d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13de76050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13de76310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13de765d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13de76890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13de76b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13de76e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13de770d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13de77390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13de77650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13de77910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13de77bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13de77e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13de78150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13de78410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13de786d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13de78990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13de78c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13de78f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13de791d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13de79490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13de79750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13de79a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13de79cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13de79f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13de7a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13de7a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13de7a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13de7aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13de7ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13de7b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13de7b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13de7b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13de7b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13de7bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13de7bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13de7c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13de7c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13de7c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13de7c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13de7cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13de7ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13de7d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13de7d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13de7d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13de7d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13de7dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13de7ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13de7e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13de7e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13de7e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13de7e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13de7ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13de7ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13de7f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13de7f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13de7f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13de7fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13de7fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13de7ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13de80290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13de80550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13de80810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13de80ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13de80d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13de81050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13de81310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13de815d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13de81890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13de81b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13de81e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13de820d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13de82390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13de82650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13de82910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13de82bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13de82e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13de83150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13de83410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13de836d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13de83990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13de83c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13de83f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13de841d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13de84490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13de84750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13de84a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13de84cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13de84f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13de85250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13de85510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13de857d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13de85a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13de85d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13de86010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13de862d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13de86590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13de86850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13de86b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13de86dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13de87090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13de87350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13de87610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13de878d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13de87b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13de87e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13de88110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13de883d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13de88690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13de88950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13de88c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13de88ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13de89190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13de89450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13de89710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13de89ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13de89fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13de8a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13de8a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13de8ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13de8afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13de8b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13de8b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13de8bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13de8c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13de8c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13de8ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13de8cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13de8d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13de8d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13de8dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13de8e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13de8e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13de8e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13de8edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13de8f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13de8f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13de8fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13de8ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13de90400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13de90870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13de90ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13de91150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13de915c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13de91a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13de91ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13de92310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13de92780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13de92bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13de93060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13de934d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13de93940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13de93db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13de94220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13de94690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13de94b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13de94f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13de953e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13de95850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13de95cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13de96130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13de965a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13de96a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13de96e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13de972f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13de97760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13de97bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13de98040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13de984b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13de98920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13de98d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13de99200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13de99670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13de99ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13de99f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13de9a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13de9a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13de9aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13de9b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13de9b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13de9b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13de9be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13de9c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13de9c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13de9cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13de9d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13de9d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13de9d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13de9e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13de9ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13de9f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13de9f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13de9fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13dea0380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13dea0640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13dea0c50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f0044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f0056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f0063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f007810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f008330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f008ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f0092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f009a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f00a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f00a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f00af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f00b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f00be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f00c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f00cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f00d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f00dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f00dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f00e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f00e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f00e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f00edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f00f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f00f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f00fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f00fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f0102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f010760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f010bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f011040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f0114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f011920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f011d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f012200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f012670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f012ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f012f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f0133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f013830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f013ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f014110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f014580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f0149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f014e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f0152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f015740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f015bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f016020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f016590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f016a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f016f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f017370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f0177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f017c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f0180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f018530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f0189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f018e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f0196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f019b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f019fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f01a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f01a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f01ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f01b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f01b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f01ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f01bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f01c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f01c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f01cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f01d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f01d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f01d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f01ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f01e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f01e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f01eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f01efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f01f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f01f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f01fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f020170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f0205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f020a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f020ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f021330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f0217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f022080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f0224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f022960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f022dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f023240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f023ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f023d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f024200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f024670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f024ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f024f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f0253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f025830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f025ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f026110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f026580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f0269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f026e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f0272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f027740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f027bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f028020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f028490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f028900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f028d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f0291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f029650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f029ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f029f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f02a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f02a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f02ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f02b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f02b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f02b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f02be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f02c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f02c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f02cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f02d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f02d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f02d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f02dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f02e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f02e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f02eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f02ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f02f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f02f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f02fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f0300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f030540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f0309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f030e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f031290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f031700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f031b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f031fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f032450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f0328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f032d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f0331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f033610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f033a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f033ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f034360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f0347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f034c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f0350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f035520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f035990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f035e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f036270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f0366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f036b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f036fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f037430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f0378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f037d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f038180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f0385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f038a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f038ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f039340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f0397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f039c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f03a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f03a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f03a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f03ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f03b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f03b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f03bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f03bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f03c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f03c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f03ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f03d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f03d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f03da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f03deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f03e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f03e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f03ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f03f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f03f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f03f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f03fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f040230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f0406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f040b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f040f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f041b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f041dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f042080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f0424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f042960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f042dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f043240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f0436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f043b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f044400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f044870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f044ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f045150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f0455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f045a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f045ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f046310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f046780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f046bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f047060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f0474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f047940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f047db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f048220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f048690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f048b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f048f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f0493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f049850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f049cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f04a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f04a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f04aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f04ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f04b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f04b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f04bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f04c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f04c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f04c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f04cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f04d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f04d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f04dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f04df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f04e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f04e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f04eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f04f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f04f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f04f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f04fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f0502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f050740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f050bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f051020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f051490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f051900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f051d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f0521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f052650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f052ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f052f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f0533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f053810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f053c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f0540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f054560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f0549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f054e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f0552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f055720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f056190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f0568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f056fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f0576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f0579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f057e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f058420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f058a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.903s
user	0m0.242s
sys	0m0.134s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.52 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.08 sec*proc (2 tests)

Total Test time (real) =   1.09 sec
        1.11 real         0.70 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.51 real         0.14 user         0.04 sys
```
