Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.694s
user	0m0.701s
sys	0m1.023s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  7%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  7%] Built target build_info
[  7%] Built target xxhash
[  7%] Built target sha256
[  7%] Built target sha1
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 13%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 16%] Linking C shared library libggml-metal.dylib
[ 16%] Built target ggml-metal
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 23%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 23%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Linking C executable ../bin/test-c
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 29%] Built target llava
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Linking CXX static library libcommon.a
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llama-simple-chat
[ 32%] Built target test-c
[ 32%] Built target llama-simple
[ 32%] Built target llava_static
[ 32%] Built target llava_shared
[ 32%] Built target common
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 36%] Linking CXX executable ../bin/test-tokenizer-0
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-sampling
[ 44%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Built target test-sampling
[ 47%] Built target test-log
[ 48%] Built target test-llama-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 49%] Built target test-arg-parser
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Linking CXX executable ../bin/test-chat-template
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Linking CXX executable ../bin/test-model-load-cancel
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-rope
[ 59%] Linking CXX executable ../../bin/llama-batched-bench
[ 59%] Built target test-chat-template
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Built target test-model-load-cancel
[ 60%] Built target test-backend-ops
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Built target test-autorelease
[ 61%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 61%] Linking CXX executable ../../bin/llama-batched
[ 61%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 61%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 62%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 62%] Built target test-rope
[ 62%] Built target llama-batched-bench
[ 62%] Built target test-quantize-perf
[ 63%] Linking CXX executable ../../bin/llama-embedding
[ 63%] Built target test-barrier
[ 64%] Linking CXX executable ../../bin/llama-eval-callback
[ 64%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 65%] Linking CXX executable ../../bin/llama-gguf-split
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Built target llama-batched
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-infill
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-embedding
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-eval-callback
[ 70%] Built target llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-bench
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-lookahead
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Built target llama-infill
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-bench
[ 75%] Linking CXX executable ../../bin/llama-lookup-stats
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 76%] Built target llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-cli
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 76%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 78%] Built target llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-perplexity
[ 79%] Built target llama-lookup-stats
[ 79%] Built target llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-cli
[ 80%] Linking CXX executable ../../bin/llama-retrieval
[ 80%] Generating loading.html.hpp
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-create
[ 82%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Built target llama-parallel
[ 85%] Built target llama-quantize
[ 85%] Built target llama-retrieval
[ 85%] Built target llama-passkey
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Linking CXX executable ../../bin/llama-run
[ 85%] Built target llama-perplexity
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-save-load-state
[ 89%] Built target llama-run
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Built target llama-tts
[ 95%] Built target llama-gen-docs
[ 95%] Built target llama-tokenize
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.877s
user	0m5.783s
sys	0m8.869s

main: quantize time =  2195.85 ms
main:    total time =  2195.85 ms

main: quantize time =  1321.41 ms
main:    total time =  1321.41 ms

main: quantize time =  1293.41 ms
main:    total time =  1293.41 ms

main: quantize time =  1503.49 ms
main:    total time =  1503.49 ms

main: quantize time =  2987.07 ms
main:    total time =  2987.07 ms

main: quantize time =  5070.59 ms
main:    total time =  5070.59 ms

main: quantize time =  5625.21 ms
main:    total time =  5625.21 ms

main: quantize time =  7025.08 ms
main:    total time =  7025.08 ms

main: quantize time =  5812.31 ms
main:    total time =  5812.31 ms

main: quantize time =  4515.20 ms
main:    total time =  4515.20 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.097 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.203 I main: llama backend init
0.00.000.208 I main: load the model and apply lora adapter, if any
0.00.064.377 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.075.344 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.075.358 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.075.367 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.075.368 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.075.369 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.075.370 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.075.370 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.075.372 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.075.373 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.075.374 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.075.375 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.075.375 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.075.376 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.075.377 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.075.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.075.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.075.383 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.082.237 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.084.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.091.491 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.091.498 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.091.499 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.091.499 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.091.500 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.091.502 I llama_model_loader: - type  f32:  194 tensors
0.00.091.502 I llama_model_loader: - type  f16:   98 tensors
0.00.125.936 I llm_load_vocab: special tokens cache size = 25
0.00.133.813 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.133.817 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.133.817 I llm_load_print_meta: arch             = gptneox
0.00.133.817 I llm_load_print_meta: vocab type       = BPE
0.00.133.818 I llm_load_print_meta: n_vocab          = 50304
0.00.133.818 I llm_load_print_meta: n_merges         = 50009
0.00.133.818 I llm_load_print_meta: vocab_only       = 0
0.00.133.818 I llm_load_print_meta: n_ctx_train      = 2048
0.00.133.820 I llm_load_print_meta: n_embd           = 2048
0.00.133.820 I llm_load_print_meta: n_layer          = 24
0.00.133.824 I llm_load_print_meta: n_head           = 16
0.00.133.825 I llm_load_print_meta: n_head_kv        = 16
0.00.133.825 I llm_load_print_meta: n_rot            = 32
0.00.133.825 I llm_load_print_meta: n_swa            = 0
0.00.133.826 I llm_load_print_meta: n_embd_head_k    = 128
0.00.133.826 I llm_load_print_meta: n_embd_head_v    = 128
0.00.133.827 I llm_load_print_meta: n_gqa            = 1
0.00.133.827 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.133.828 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.133.829 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.133.829 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.133.829 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.133.829 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.133.830 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.133.831 I llm_load_print_meta: n_ff             = 8192
0.00.133.831 I llm_load_print_meta: n_expert         = 0
0.00.133.831 I llm_load_print_meta: n_expert_used    = 0
0.00.133.832 I llm_load_print_meta: causal attn      = 1
0.00.133.832 I llm_load_print_meta: pooling type     = 0
0.00.133.832 I llm_load_print_meta: rope type        = 2
0.00.133.834 I llm_load_print_meta: rope scaling     = linear
0.00.133.834 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.133.835 I llm_load_print_meta: freq_scale_train = 1
0.00.133.835 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.133.836 I llm_load_print_meta: rope_finetuned   = unknown
0.00.133.837 I llm_load_print_meta: ssm_d_conv       = 0
0.00.133.837 I llm_load_print_meta: ssm_d_inner      = 0
0.00.133.838 I llm_load_print_meta: ssm_d_state      = 0
0.00.133.838 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.133.838 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.133.838 I llm_load_print_meta: model type       = 1.4B
0.00.133.839 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.133.839 I llm_load_print_meta: model params     = 1.41 B
0.00.133.840 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.133.840 I llm_load_print_meta: general.name     = 1.4B
0.00.133.840 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.133.840 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.133.841 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.133.841 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.133.842 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.133.843 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.133.843 I llm_load_print_meta: max token length = 1024
0.00.135.894 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.135.895 I llm_load_tensors: offloading output layer to GPU
0.00.135.895 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.135.914 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.135.915 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.136.899 I llama_new_context_with_model: n_seq_max     = 1
0.00.136.900 I llama_new_context_with_model: n_ctx         = 2048
0.00.136.901 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.136.901 I llama_new_context_with_model: n_batch       = 2048
0.00.136.901 I llama_new_context_with_model: n_ubatch      = 512
0.00.136.901 I llama_new_context_with_model: flash_attn    = 0
0.00.136.902 I llama_new_context_with_model: freq_base     = 10000.0
0.00.136.902 I llama_new_context_with_model: freq_scale    = 1
0.00.136.902 I ggml_metal_init: allocating
0.00.136.906 I ggml_metal_init: found device: Apple M4
0.00.136.908 I ggml_metal_init: picking default device: Apple M4
0.00.137.620 I ggml_metal_init: using embedded metal library
0.00.147.822 I ggml_metal_init: GPU name:   Apple M4
0.00.147.826 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.147.826 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.147.826 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.147.827 I ggml_metal_init: simdgroup reduction   = true
0.00.147.827 I ggml_metal_init: simdgroup matrix mul. = true
0.00.147.827 I ggml_metal_init: has bfloat            = true
0.00.147.827 I ggml_metal_init: use bfloat            = true
0.00.147.828 I ggml_metal_init: hasUnifiedMemory      = true
0.00.147.829 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.172.164 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.191.737 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.191.743 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.191.762 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.192.729 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.192.730 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.192.730 I llama_new_context_with_model: graph nodes  = 967
0.00.192.731 I llama_new_context_with_model: graph splits = 2
0.00.192.755 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.192.901 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.192.902 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.268.719 I main: llama threadpool init, n_threads = 4
0.00.268.751 I 
0.00.268.786 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.268.787 I 
0.00.268.857 I sampler seed: 1234
0.00.268.862 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.268.897 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.268.899 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.268.899 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.125.873 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52282.77 tokens per second)
0.02.125.874 I llama_perf_context_print:        load time =     204.33 ms
0.02.125.874 I llama_perf_context_print: prompt eval time =      43.90 ms /     7 tokens (    6.27 ms per token,   159.44 tokens per second)
0.02.125.875 I llama_perf_context_print:        eval time =    1810.03 ms /    63 runs   (   28.73 ms per token,    34.81 tokens per second)
0.02.125.876 I llama_perf_context_print:       total time =    1857.16 ms /    70 tokens
0.02.126.057 I ggml_metal_free: deallocating

real	0m2.463s
user	0m0.144s
sys	0m0.090s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.807 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.257 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.263 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.264 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.265 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.267 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.268 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.269 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.269 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.269 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.270 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.270 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.270 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.273 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.273 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.273 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.067 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.200 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.328 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.329 I llama_model_loader: - type  f32:  194 tensors
0.00.036.329 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.902 I llm_load_vocab: special tokens cache size = 25
0.00.067.312 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.316 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.316 I llm_load_print_meta: arch             = gptneox
0.00.067.317 I llm_load_print_meta: vocab type       = BPE
0.00.067.317 I llm_load_print_meta: n_vocab          = 50304
0.00.067.317 I llm_load_print_meta: n_merges         = 50009
0.00.067.317 I llm_load_print_meta: vocab_only       = 0
0.00.067.319 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.319 I llm_load_print_meta: n_embd           = 2048
0.00.067.319 I llm_load_print_meta: n_layer          = 24
0.00.067.324 I llm_load_print_meta: n_head           = 16
0.00.067.325 I llm_load_print_meta: n_head_kv        = 16
0.00.067.325 I llm_load_print_meta: n_rot            = 32
0.00.067.325 I llm_load_print_meta: n_swa            = 0
0.00.067.325 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.325 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.326 I llm_load_print_meta: n_gqa            = 1
0.00.067.327 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.327 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.328 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.328 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.329 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.329 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.329 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.330 I llm_load_print_meta: n_ff             = 8192
0.00.067.330 I llm_load_print_meta: n_expert         = 0
0.00.067.330 I llm_load_print_meta: n_expert_used    = 0
0.00.067.330 I llm_load_print_meta: causal attn      = 1
0.00.067.330 I llm_load_print_meta: pooling type     = 0
0.00.067.330 I llm_load_print_meta: rope type        = 2
0.00.067.330 I llm_load_print_meta: rope scaling     = linear
0.00.067.331 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.331 I llm_load_print_meta: freq_scale_train = 1
0.00.067.331 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.332 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.333 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.333 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.333 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.333 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.333 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.334 I llm_load_print_meta: model type       = 1.4B
0.00.067.334 I llm_load_print_meta: model ftype      = Q8_0
0.00.067.334 I llm_load_print_meta: model params     = 1.41 B
0.00.067.336 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.067.337 I llm_load_print_meta: general.name     = 1.4B
0.00.067.337 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.337 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.337 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.337 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.337 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.067.338 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.338 I llm_load_print_meta: max token length = 1024
0.00.069.828 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.828 I llm_load_tensors: offloading output layer to GPU
0.00.069.829 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.839 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.840 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.831 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.831 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.832 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.832 I llama_new_context_with_model: n_batch       = 2048
0.00.070.832 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.832 I llama_new_context_with_model: flash_attn    = 0
0.00.070.833 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.833 I llama_new_context_with_model: freq_scale    = 1
0.00.070.833 I ggml_metal_init: allocating
0.00.070.837 I ggml_metal_init: found device: Apple M4
0.00.070.839 I ggml_metal_init: picking default device: Apple M4
0.00.071.619 I ggml_metal_init: using embedded metal library
0.00.074.401 I ggml_metal_init: GPU name:   Apple M4
0.00.074.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.403 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.404 I ggml_metal_init: simdgroup reduction   = true
0.00.074.404 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.404 I ggml_metal_init: has bfloat            = true
0.00.074.404 I ggml_metal_init: use bfloat            = true
0.00.074.404 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.405 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.440 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.112.492 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.500 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.526 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.718 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.113.722 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.113.722 I llama_new_context_with_model: graph nodes  = 967
0.00.113.722 I llama_new_context_with_model: graph splits = 2
0.00.113.742 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.113.884 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.113.884 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.278.208 I main: llama threadpool init, n_threads = 4
0.01.278.248 I 
0.01.278.288 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.278.291 I 
0.01.278.546 I sampler seed: 1234
0.01.278.550 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.278.566 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.278.567 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.278.567 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.367.050 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62280.70 tokens per second)
0.02.367.051 I llama_perf_context_print:        load time =    1268.40 ms
0.02.367.052 I llama_perf_context_print: prompt eval time =      42.95 ms /     7 tokens (    6.14 ms per token,   162.99 tokens per second)
0.02.367.052 I llama_perf_context_print:        eval time =    1042.76 ms /    63 runs   (   16.55 ms per token,    60.42 tokens per second)
0.02.367.056 I llama_perf_context_print:       total time =    1088.85 ms /    70 tokens
0.02.367.227 I ggml_metal_free: deallocating

real	0m2.386s
user	0m0.119s
sys	0m0.231s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.011.591 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.664 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.669 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.671 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.671 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.672 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.672 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.674 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.675 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.676 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.676 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.676 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.677 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.677 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.680 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.681 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.681 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.474 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.546 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.418 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.419 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.420 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.420 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.420 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.421 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.029.421 I llama_model_loader: - type  f32:  194 tensors
0.00.029.422 I llama_model_loader: - type q4_0:   97 tensors
0.00.029.422 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.749 I llm_load_vocab: special tokens cache size = 25
0.00.056.767 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.770 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.771 I llm_load_print_meta: arch             = gptneox
0.00.056.771 I llm_load_print_meta: vocab type       = BPE
0.00.056.771 I llm_load_print_meta: n_vocab          = 50304
0.00.056.771 I llm_load_print_meta: n_merges         = 50009
0.00.056.772 I llm_load_print_meta: vocab_only       = 0
0.00.056.772 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.772 I llm_load_print_meta: n_embd           = 2048
0.00.056.772 I llm_load_print_meta: n_layer          = 24
0.00.056.777 I llm_load_print_meta: n_head           = 16
0.00.056.779 I llm_load_print_meta: n_head_kv        = 16
0.00.056.779 I llm_load_print_meta: n_rot            = 32
0.00.056.779 I llm_load_print_meta: n_swa            = 0
0.00.056.779 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.779 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.780 I llm_load_print_meta: n_gqa            = 1
0.00.056.781 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.782 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.783 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.783 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.783 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.783 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.783 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.784 I llm_load_print_meta: n_ff             = 8192
0.00.056.784 I llm_load_print_meta: n_expert         = 0
0.00.056.785 I llm_load_print_meta: n_expert_used    = 0
0.00.056.787 I llm_load_print_meta: causal attn      = 1
0.00.056.788 I llm_load_print_meta: pooling type     = 0
0.00.056.789 I llm_load_print_meta: rope type        = 2
0.00.056.789 I llm_load_print_meta: rope scaling     = linear
0.00.056.789 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.790 I llm_load_print_meta: freq_scale_train = 1
0.00.056.790 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.790 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.790 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.790 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.790 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.791 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.795 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.796 I llm_load_print_meta: model type       = 1.4B
0.00.056.796 I llm_load_print_meta: model ftype      = Q4_0
0.00.056.796 I llm_load_print_meta: model params     = 1.41 B
0.00.056.797 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.056.798 I llm_load_print_meta: general.name     = 1.4B
0.00.056.798 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.798 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.798 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.798 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.798 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.799 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.799 I llm_load_print_meta: max token length = 1024
0.00.059.057 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.058 I llm_load_tensors: offloading output layer to GPU
0.00.059.058 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.069 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.059.070 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.060.062 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.063 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.064 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.064 I llama_new_context_with_model: n_batch       = 2048
0.00.060.064 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.064 I llama_new_context_with_model: flash_attn    = 0
0.00.060.065 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.065 I llama_new_context_with_model: freq_scale    = 1
0.00.060.065 I ggml_metal_init: allocating
0.00.060.068 I ggml_metal_init: found device: Apple M4
0.00.060.070 I ggml_metal_init: picking default device: Apple M4
0.00.060.840 I ggml_metal_init: using embedded metal library
0.00.063.380 I ggml_metal_init: GPU name:   Apple M4
0.00.063.382 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.382 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.382 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.383 I ggml_metal_init: simdgroup reduction   = true
0.00.063.383 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.383 I ggml_metal_init: has bfloat            = true
0.00.063.385 I ggml_metal_init: use bfloat            = true
0.00.063.385 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.386 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.074 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.098.042 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.051 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.076 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.187 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.190 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.190 I llama_new_context_with_model: graph nodes  = 967
0.00.099.191 I llama_new_context_with_model: graph splits = 2
0.00.099.209 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.354 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.725.456 I main: llama threadpool init, n_threads = 4
0.00.725.491 I 
0.00.725.524 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.725.526 I 
0.00.725.759 I sampler seed: 1234
0.00.725.763 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.725.808 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.725.809 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.725.809 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.406.244 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50070.52 tokens per second)
0.01.406.245 I llama_perf_context_print:        load time =     713.86 ms
0.01.406.246 I llama_perf_context_print: prompt eval time =      45.79 ms /     7 tokens (    6.54 ms per token,   152.88 tokens per second)
0.01.406.247 I llama_perf_context_print:        eval time =     631.88 ms /    63 runs   (   10.03 ms per token,    99.70 tokens per second)
0.01.406.247 I llama_perf_context_print:       total time =     680.79 ms /    70 tokens
0.01.406.465 I ggml_metal_free: deallocating

real	0m1.426s
user	0m0.111s
sys	0m0.158s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.012.127 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.081 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.085 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.086 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.086 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.087 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.087 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.087 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.088 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.088 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.089 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.089 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.089 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.090 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.094 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.094 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.094 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.091 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.145 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.251 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.252 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.252 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.253 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.253 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.253 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.254 I llama_model_loader: - type  f32:  194 tensors
0.00.036.254 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.254 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.828 I llm_load_vocab: special tokens cache size = 25
0.00.070.318 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.321 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.321 I llm_load_print_meta: arch             = gptneox
0.00.070.322 I llm_load_print_meta: vocab type       = BPE
0.00.070.322 I llm_load_print_meta: n_vocab          = 50304
0.00.070.322 I llm_load_print_meta: n_merges         = 50009
0.00.070.322 I llm_load_print_meta: vocab_only       = 0
0.00.070.322 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.323 I llm_load_print_meta: n_embd           = 2048
0.00.070.323 I llm_load_print_meta: n_layer          = 24
0.00.070.326 I llm_load_print_meta: n_head           = 16
0.00.070.327 I llm_load_print_meta: n_head_kv        = 16
0.00.070.330 I llm_load_print_meta: n_rot            = 32
0.00.070.330 I llm_load_print_meta: n_swa            = 0
0.00.070.330 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.330 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.331 I llm_load_print_meta: n_gqa            = 1
0.00.070.332 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.333 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.334 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.334 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.334 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.336 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.336 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.337 I llm_load_print_meta: n_ff             = 8192
0.00.070.337 I llm_load_print_meta: n_expert         = 0
0.00.070.337 I llm_load_print_meta: n_expert_used    = 0
0.00.070.339 I llm_load_print_meta: causal attn      = 1
0.00.070.339 I llm_load_print_meta: pooling type     = 0
0.00.070.339 I llm_load_print_meta: rope type        = 2
0.00.070.339 I llm_load_print_meta: rope scaling     = linear
0.00.070.340 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.340 I llm_load_print_meta: freq_scale_train = 1
0.00.070.340 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.341 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.342 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.342 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.342 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.342 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.343 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.344 I llm_load_print_meta: model type       = 1.4B
0.00.070.345 I llm_load_print_meta: model ftype      = Q4_1
0.00.070.345 I llm_load_print_meta: model params     = 1.41 B
0.00.070.346 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.070.346 I llm_load_print_meta: general.name     = 1.4B
0.00.070.351 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.352 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.352 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.352 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.354 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.070.354 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.354 I llm_load_print_meta: max token length = 1024
0.00.072.894 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.895 I llm_load_tensors: offloading output layer to GPU
0.00.072.896 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.907 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.072.908 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.074.146 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.148 I llama_new_context_with_model: n_ctx         = 2048
0.00.074.148 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.074.148 I llama_new_context_with_model: n_batch       = 2048
0.00.074.148 I llama_new_context_with_model: n_ubatch      = 512
0.00.074.149 I llama_new_context_with_model: flash_attn    = 0
0.00.074.149 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.150 I llama_new_context_with_model: freq_scale    = 1
0.00.074.150 I ggml_metal_init: allocating
0.00.074.159 I ggml_metal_init: found device: Apple M4
0.00.074.162 I ggml_metal_init: picking default device: Apple M4
0.00.074.925 I ggml_metal_init: using embedded metal library
0.00.078.197 I ggml_metal_init: GPU name:   Apple M4
0.00.078.199 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.200 I ggml_metal_init: simdgroup reduction   = true
0.00.078.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.201 I ggml_metal_init: has bfloat            = true
0.00.078.202 I ggml_metal_init: use bfloat            = true
0.00.078.203 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.696 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.112.981 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.987 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.015 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.060 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.114.062 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.114.062 I llama_new_context_with_model: graph nodes  = 967
0.00.114.063 I llama_new_context_with_model: graph splits = 2
0.00.114.079 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.114.220 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.114.221 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.866.821 I main: llama threadpool init, n_threads = 4
0.00.866.862 I 
0.00.866.892 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.866.893 I 
0.00.867.123 I sampler seed: 1234
0.00.867.127 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.867.142 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.867.143 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.867.143 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.600.345 I llama_perf_sampler_print:    sampling time =       1.06 ms /    71 runs   (    0.01 ms per token, 66981.13 tokens per second)
0.01.600.345 I llama_perf_context_print:        load time =     854.69 ms
0.01.600.346 I llama_perf_context_print: prompt eval time =      45.14 ms /     7 tokens (    6.45 ms per token,   155.07 tokens per second)
0.01.600.347 I llama_perf_context_print:        eval time =     685.27 ms /    63 runs   (   10.88 ms per token,    91.93 tokens per second)
0.01.600.347 I llama_perf_context_print:       total time =     733.53 ms /    70 tokens
0.01.600.498 I ggml_metal_free: deallocating

real	0m1.621s
user	0m0.123s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.191 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.696 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.702 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.702 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.702 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.707 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.707 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.710 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.710 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.711 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.711 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.711 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.712 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.719 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.719 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.003 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.004 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.004 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.004 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.005 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.005 I llama_model_loader: - type  f32:  194 tensors
0.00.024.005 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.006 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.453 I llm_load_vocab: special tokens cache size = 25
0.00.050.496 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.499 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.499 I llm_load_print_meta: arch             = gptneox
0.00.050.500 I llm_load_print_meta: vocab type       = BPE
0.00.050.500 I llm_load_print_meta: n_vocab          = 50304
0.00.050.500 I llm_load_print_meta: n_merges         = 50009
0.00.050.500 I llm_load_print_meta: vocab_only       = 0
0.00.050.500 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.501 I llm_load_print_meta: n_embd           = 2048
0.00.050.501 I llm_load_print_meta: n_layer          = 24
0.00.050.503 I llm_load_print_meta: n_head           = 16
0.00.050.504 I llm_load_print_meta: n_head_kv        = 16
0.00.050.504 I llm_load_print_meta: n_rot            = 32
0.00.050.505 I llm_load_print_meta: n_swa            = 0
0.00.050.505 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.505 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.506 I llm_load_print_meta: n_gqa            = 1
0.00.050.507 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.508 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.508 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.509 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.509 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.509 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.509 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.510 I llm_load_print_meta: n_ff             = 8192
0.00.050.510 I llm_load_print_meta: n_expert         = 0
0.00.050.510 I llm_load_print_meta: n_expert_used    = 0
0.00.050.512 I llm_load_print_meta: causal attn      = 1
0.00.050.514 I llm_load_print_meta: pooling type     = 0
0.00.050.514 I llm_load_print_meta: rope type        = 2
0.00.050.514 I llm_load_print_meta: rope scaling     = linear
0.00.050.514 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.515 I llm_load_print_meta: freq_scale_train = 1
0.00.050.515 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.515 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.515 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.515 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.515 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.516 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.516 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.516 I llm_load_print_meta: model type       = 1.4B
0.00.050.516 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.517 I llm_load_print_meta: model params     = 1.41 B
0.00.050.517 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.517 I llm_load_print_meta: general.name     = 1.4B
0.00.050.518 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.518 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.522 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.522 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.527 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.527 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.528 I llm_load_print_meta: max token length = 1024
0.00.052.465 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.465 I llm_load_tensors: offloading output layer to GPU
0.00.052.466 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.476 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.477 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.344 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.345 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.345 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.345 I llama_new_context_with_model: n_batch       = 2048
0.00.053.345 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.346 I llama_new_context_with_model: flash_attn    = 0
0.00.053.346 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.346 I llama_new_context_with_model: freq_scale    = 1
0.00.053.347 I ggml_metal_init: allocating
0.00.053.351 I ggml_metal_init: found device: Apple M4
0.00.053.353 I ggml_metal_init: picking default device: Apple M4
0.00.053.957 I ggml_metal_init: using embedded metal library
0.00.056.296 I ggml_metal_init: GPU name:   Apple M4
0.00.056.298 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.298 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.298 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.299 I ggml_metal_init: simdgroup reduction   = true
0.00.056.299 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.299 I ggml_metal_init: has bfloat            = true
0.00.056.299 I ggml_metal_init: use bfloat            = true
0.00.056.299 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.301 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.937 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.136 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.142 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.164 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.194 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.196 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.196 I llama_new_context_with_model: graph nodes  = 967
0.00.086.196 I llama_new_context_with_model: graph splits = 2
0.00.086.213 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.366 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.904 I main: llama threadpool init, n_threads = 4
0.00.772.940 I 
0.00.772.966 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.967 I 
0.00.773.210 I sampler seed: 1234
0.00.773.214 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.229 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.231 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.231 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.566.106 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.566.107 I llama_perf_context_print:        load time =     763.71 ms
0.01.566.108 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.58 tokens per second)
0.01.566.108 I llama_perf_context_print:        eval time =     742.70 ms /    63 runs   (   11.79 ms per token,    84.83 tokens per second)
0.01.566.109 I llama_perf_context_print:       total time =     793.21 ms /    70 tokens
0.01.566.317 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.109s
sys	0m0.170s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.786 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.573 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.582 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.583 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.406 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.237 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.239 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.239 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.240 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.240 I llama_model_loader: - type  f32:  194 tensors
0.00.024.240 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.241 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.601 I llm_load_vocab: special tokens cache size = 25
0.00.050.764 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.767 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.767 I llm_load_print_meta: arch             = gptneox
0.00.050.768 I llm_load_print_meta: vocab type       = BPE
0.00.050.768 I llm_load_print_meta: n_vocab          = 50304
0.00.050.768 I llm_load_print_meta: n_merges         = 50009
0.00.050.768 I llm_load_print_meta: vocab_only       = 0
0.00.050.768 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.768 I llm_load_print_meta: n_embd           = 2048
0.00.050.769 I llm_load_print_meta: n_layer          = 24
0.00.050.771 I llm_load_print_meta: n_head           = 16
0.00.050.772 I llm_load_print_meta: n_head_kv        = 16
0.00.050.772 I llm_load_print_meta: n_rot            = 32
0.00.050.772 I llm_load_print_meta: n_swa            = 0
0.00.050.773 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.773 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.774 I llm_load_print_meta: n_gqa            = 1
0.00.050.774 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.775 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.776 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.776 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.776 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.777 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.777 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.777 I llm_load_print_meta: n_ff             = 8192
0.00.050.778 I llm_load_print_meta: n_expert         = 0
0.00.050.778 I llm_load_print_meta: n_expert_used    = 0
0.00.050.778 I llm_load_print_meta: causal attn      = 1
0.00.050.778 I llm_load_print_meta: pooling type     = 0
0.00.050.779 I llm_load_print_meta: rope type        = 2
0.00.050.779 I llm_load_print_meta: rope scaling     = linear
0.00.050.782 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.782 I llm_load_print_meta: freq_scale_train = 1
0.00.050.782 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.782 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.783 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.783 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.783 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.783 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.783 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.783 I llm_load_print_meta: model type       = 1.4B
0.00.050.784 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.784 I llm_load_print_meta: model params     = 1.41 B
0.00.050.785 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.785 I llm_load_print_meta: general.name     = 1.4B
0.00.050.785 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.785 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.786 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.786 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.790 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.790 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.790 I llm_load_print_meta: max token length = 1024
0.00.052.822 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.822 I llm_load_tensors: offloading output layer to GPU
0.00.052.822 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.833 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.834 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.725 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.725 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.725 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.726 I llama_new_context_with_model: n_batch       = 2048
0.00.053.726 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.726 I llama_new_context_with_model: flash_attn    = 0
0.00.053.726 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.727 I llama_new_context_with_model: freq_scale    = 1
0.00.053.727 I ggml_metal_init: allocating
0.00.053.730 I ggml_metal_init: found device: Apple M4
0.00.053.732 I ggml_metal_init: picking default device: Apple M4
0.00.054.317 I ggml_metal_init: using embedded metal library
0.00.056.629 I ggml_metal_init: GPU name:   Apple M4
0.00.056.631 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.631 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.632 I ggml_metal_init: simdgroup reduction   = true
0.00.056.632 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.632 I ggml_metal_init: has bfloat            = true
0.00.056.632 I ggml_metal_init: use bfloat            = true
0.00.056.633 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.633 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.325 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.117 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.126 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.151 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.102 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.103 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.104 I llama_new_context_with_model: graph nodes  = 967
0.00.086.104 I llama_new_context_with_model: graph splits = 2
0.00.086.119 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.261 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.972 I main: llama threadpool init, n_threads = 4
0.00.697.008 I 
0.00.697.037 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.037 I 
0.00.697.259 I sampler seed: 1234
0.00.697.264 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.697.279 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.697.279 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.697.279 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.535.400 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.535.400 I llama_perf_context_print:        load time =     688.18 ms
0.01.535.401 I llama_perf_context_print: prompt eval time =      42.25 ms /     7 tokens (    6.04 ms per token,   165.68 tokens per second)
0.01.535.401 I llama_perf_context_print:        eval time =     792.84 ms /    63 runs   (   12.58 ms per token,    79.46 tokens per second)
0.01.535.402 I llama_perf_context_print:       total time =     838.43 ms /    70 tokens
0.01.535.565 I ggml_metal_free: deallocating

real	0m1.552s
user	0m0.110s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.032 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.632 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.638 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.639 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.640 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.640 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.640 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.641 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.642 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.642 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.643 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.643 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.643 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.644 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.644 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.646 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.646 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.646 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.475 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.501 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.299 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.300 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.300 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.301 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.301 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.301 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.302 I llama_model_loader: - type  f32:  194 tensors
0.00.024.302 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.302 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.303 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.727 I llm_load_vocab: special tokens cache size = 25
0.00.050.786 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.790 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.790 I llm_load_print_meta: arch             = gptneox
0.00.050.791 I llm_load_print_meta: vocab type       = BPE
0.00.050.791 I llm_load_print_meta: n_vocab          = 50304
0.00.050.791 I llm_load_print_meta: n_merges         = 50009
0.00.050.796 I llm_load_print_meta: vocab_only       = 0
0.00.050.796 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.797 I llm_load_print_meta: n_embd           = 2048
0.00.050.797 I llm_load_print_meta: n_layer          = 24
0.00.050.799 I llm_load_print_meta: n_head           = 16
0.00.050.800 I llm_load_print_meta: n_head_kv        = 16
0.00.050.800 I llm_load_print_meta: n_rot            = 32
0.00.050.800 I llm_load_print_meta: n_swa            = 0
0.00.050.801 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.801 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.802 I llm_load_print_meta: n_gqa            = 1
0.00.050.803 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.804 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.804 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.805 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.805 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.805 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.805 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.807 I llm_load_print_meta: n_ff             = 8192
0.00.050.808 I llm_load_print_meta: n_expert         = 0
0.00.050.808 I llm_load_print_meta: n_expert_used    = 0
0.00.050.808 I llm_load_print_meta: causal attn      = 1
0.00.050.808 I llm_load_print_meta: pooling type     = 0
0.00.050.808 I llm_load_print_meta: rope type        = 2
0.00.050.808 I llm_load_print_meta: rope scaling     = linear
0.00.050.809 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.809 I llm_load_print_meta: freq_scale_train = 1
0.00.050.809 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.810 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.810 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.810 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.810 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.810 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.810 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.813 I llm_load_print_meta: model type       = 1.4B
0.00.050.813 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.814 I llm_load_print_meta: model params     = 1.41 B
0.00.050.814 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.814 I llm_load_print_meta: general.name     = 1.4B
0.00.050.815 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.815 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.815 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.815 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.815 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.816 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.816 I llm_load_print_meta: max token length = 1024
0.00.052.757 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.758 I llm_load_tensors: offloading output layer to GPU
0.00.052.758 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.768 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.769 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.741 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.742 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.742 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.742 I llama_new_context_with_model: n_batch       = 2048
0.00.053.743 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.743 I llama_new_context_with_model: flash_attn    = 0
0.00.053.743 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.743 I llama_new_context_with_model: freq_scale    = 1
0.00.053.744 I ggml_metal_init: allocating
0.00.053.747 I ggml_metal_init: found device: Apple M4
0.00.053.749 I ggml_metal_init: picking default device: Apple M4
0.00.054.336 I ggml_metal_init: using embedded metal library
0.00.056.644 I ggml_metal_init: GPU name:   Apple M4
0.00.056.645 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.646 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.646 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.646 I ggml_metal_init: simdgroup reduction   = true
0.00.056.647 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.647 I ggml_metal_init: has bfloat            = true
0.00.056.647 I ggml_metal_init: use bfloat            = true
0.00.056.647 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.648 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.328 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.773 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.780 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.798 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.849 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.850 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.850 I llama_new_context_with_model: graph nodes  = 967
0.00.086.851 I llama_new_context_with_model: graph splits = 2
0.00.086.866 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.014 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.014 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.451.850 I main: llama threadpool init, n_threads = 4
0.00.451.885 I 
0.00.451.916 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.451.917 I 
0.00.452.134 I sampler seed: 1234
0.00.452.139 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.452.190 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.452.192 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.452.192 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.135.483 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63963.96 tokens per second)
0.01.135.483 I llama_perf_context_print:        load time =     441.81 ms
0.01.135.484 I llama_perf_context_print: prompt eval time =      39.71 ms /     7 tokens (    5.67 ms per token,   176.27 tokens per second)
0.01.135.484 I llama_perf_context_print:        eval time =     640.69 ms /    63 runs   (   10.17 ms per token,    98.33 tokens per second)
0.01.135.485 I llama_perf_context_print:       total time =     683.64 ms /    70 tokens
0.01.135.670 I ggml_metal_free: deallocating

real	0m1.154s
user	0m0.109s
sys	0m0.114s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.603 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.070 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.076 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.077 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.082 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.082 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.084 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.085 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.085 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.086 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.086 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.086 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.090 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.092 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.093 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.093 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.986 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.033 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.971 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.972 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.973 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.973 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.973 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.974 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.974 I llama_model_loader: - type  f32:  194 tensors
0.00.024.975 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.975 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.975 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.975 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.106 I llm_load_vocab: special tokens cache size = 25
0.00.052.141 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.144 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.144 I llm_load_print_meta: arch             = gptneox
0.00.052.145 I llm_load_print_meta: vocab type       = BPE
0.00.052.145 I llm_load_print_meta: n_vocab          = 50304
0.00.052.145 I llm_load_print_meta: n_merges         = 50009
0.00.052.145 I llm_load_print_meta: vocab_only       = 0
0.00.052.145 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.146 I llm_load_print_meta: n_embd           = 2048
0.00.052.146 I llm_load_print_meta: n_layer          = 24
0.00.052.148 I llm_load_print_meta: n_head           = 16
0.00.052.149 I llm_load_print_meta: n_head_kv        = 16
0.00.052.149 I llm_load_print_meta: n_rot            = 32
0.00.052.150 I llm_load_print_meta: n_swa            = 0
0.00.052.150 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.150 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.151 I llm_load_print_meta: n_gqa            = 1
0.00.052.153 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.153 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.154 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.154 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.154 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.154 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.155 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.155 I llm_load_print_meta: n_ff             = 8192
0.00.052.157 I llm_load_print_meta: n_expert         = 0
0.00.052.158 I llm_load_print_meta: n_expert_used    = 0
0.00.052.159 I llm_load_print_meta: causal attn      = 1
0.00.052.159 I llm_load_print_meta: pooling type     = 0
0.00.052.159 I llm_load_print_meta: rope type        = 2
0.00.052.159 I llm_load_print_meta: rope scaling     = linear
0.00.052.159 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.160 I llm_load_print_meta: freq_scale_train = 1
0.00.052.160 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.160 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.160 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.160 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.161 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.161 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.161 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.161 I llm_load_print_meta: model type       = 1.4B
0.00.052.163 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.163 I llm_load_print_meta: model params     = 1.41 B
0.00.052.164 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.164 I llm_load_print_meta: general.name     = 1.4B
0.00.052.166 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.166 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.166 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.166 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.167 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.167 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.167 I llm_load_print_meta: max token length = 1024
0.00.054.173 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.174 I llm_load_tensors: offloading output layer to GPU
0.00.054.175 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.185 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.186 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.094 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.095 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.095 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.095 I llama_new_context_with_model: n_batch       = 2048
0.00.055.095 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.095 I llama_new_context_with_model: flash_attn    = 0
0.00.055.096 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.096 I llama_new_context_with_model: freq_scale    = 1
0.00.055.096 I ggml_metal_init: allocating
0.00.055.100 I ggml_metal_init: found device: Apple M4
0.00.055.102 I ggml_metal_init: picking default device: Apple M4
0.00.055.721 I ggml_metal_init: using embedded metal library
0.00.058.942 I ggml_metal_init: GPU name:   Apple M4
0.00.058.945 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.945 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.945 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.946 I ggml_metal_init: simdgroup reduction   = true
0.00.058.946 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.946 I ggml_metal_init: has bfloat            = true
0.00.058.946 I ggml_metal_init: use bfloat            = true
0.00.058.947 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.947 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.260 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.177 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.188 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.215 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.231 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.232 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.232 I llama_new_context_with_model: graph nodes  = 967
0.00.090.233 I llama_new_context_with_model: graph splits = 2
0.00.090.250 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.433 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.504.591 I main: llama threadpool init, n_threads = 4
0.00.504.645 I 
0.00.504.673 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.504.673 I 
0.00.504.929 I sampler seed: 1234
0.00.504.964 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.504.996 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.504.998 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.504.998 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.246.527 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47651.01 tokens per second)
0.01.246.527 I llama_perf_context_print:        load time =     494.98 ms
0.01.246.528 I llama_perf_context_print: prompt eval time =      40.36 ms /     7 tokens (    5.77 ms per token,   173.45 tokens per second)
0.01.246.529 I llama_perf_context_print:        eval time =     698.63 ms /    63 runs   (   11.09 ms per token,    90.18 tokens per second)
0.01.246.530 I llama_perf_context_print:       total time =     741.94 ms /    70 tokens
0.01.246.725 I ggml_metal_free: deallocating

real	0m1.264s
user	0m0.110s
sys	0m0.106s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.991 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.337 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.342 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.348 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.349 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.349 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.350 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.350 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.351 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.351 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.351 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.352 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.352 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.354 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.189 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.226 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.034 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.035 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.035 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.035 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.036 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.036 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.037 I llama_model_loader: - type  f32:  194 tensors
0.00.027.037 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.037 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.037 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.512 I llm_load_vocab: special tokens cache size = 25
0.00.053.581 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.583 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.584 I llm_load_print_meta: arch             = gptneox
0.00.053.584 I llm_load_print_meta: vocab type       = BPE
0.00.053.584 I llm_load_print_meta: n_vocab          = 50304
0.00.053.585 I llm_load_print_meta: n_merges         = 50009
0.00.053.585 I llm_load_print_meta: vocab_only       = 0
0.00.053.585 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.585 I llm_load_print_meta: n_embd           = 2048
0.00.053.586 I llm_load_print_meta: n_layer          = 24
0.00.053.589 I llm_load_print_meta: n_head           = 16
0.00.053.589 I llm_load_print_meta: n_head_kv        = 16
0.00.053.590 I llm_load_print_meta: n_rot            = 32
0.00.053.590 I llm_load_print_meta: n_swa            = 0
0.00.053.590 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.590 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.591 I llm_load_print_meta: n_gqa            = 1
0.00.053.592 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.592 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.593 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.593 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.595 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.595 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.595 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.596 I llm_load_print_meta: n_ff             = 8192
0.00.053.596 I llm_load_print_meta: n_expert         = 0
0.00.053.596 I llm_load_print_meta: n_expert_used    = 0
0.00.053.596 I llm_load_print_meta: causal attn      = 1
0.00.053.596 I llm_load_print_meta: pooling type     = 0
0.00.053.598 I llm_load_print_meta: rope type        = 2
0.00.053.598 I llm_load_print_meta: rope scaling     = linear
0.00.053.599 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.599 I llm_load_print_meta: freq_scale_train = 1
0.00.053.599 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.599 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.599 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.600 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.600 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.600 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.600 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.600 I llm_load_print_meta: model type       = 1.4B
0.00.053.601 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.601 I llm_load_print_meta: model params     = 1.41 B
0.00.053.602 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.602 I llm_load_print_meta: general.name     = 1.4B
0.00.053.602 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.602 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.602 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.603 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.603 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.603 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.603 I llm_load_print_meta: max token length = 1024
0.00.055.642 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.643 I llm_load_tensors: offloading output layer to GPU
0.00.055.643 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.653 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.654 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.633 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.634 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.634 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.634 I llama_new_context_with_model: n_batch       = 2048
0.00.056.634 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.634 I llama_new_context_with_model: flash_attn    = 0
0.00.056.635 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.635 I llama_new_context_with_model: freq_scale    = 1
0.00.056.635 I ggml_metal_init: allocating
0.00.056.638 I ggml_metal_init: found device: Apple M4
0.00.056.640 I ggml_metal_init: picking default device: Apple M4
0.00.057.221 I ggml_metal_init: using embedded metal library
0.00.059.532 I ggml_metal_init: GPU name:   Apple M4
0.00.059.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.534 I ggml_metal_init: simdgroup reduction   = true
0.00.059.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.535 I ggml_metal_init: has bfloat            = true
0.00.059.535 I ggml_metal_init: use bfloat            = true
0.00.059.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.266 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.061 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.067 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.086 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.058 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.059 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.059 I llama_new_context_with_model: graph nodes  = 967
0.00.090.060 I llama_new_context_with_model: graph splits = 2
0.00.090.075 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.216 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.216 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.621.727 I main: llama threadpool init, n_threads = 4
0.00.621.770 I 
0.00.621.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.621.805 I 
0.00.622.034 I sampler seed: 1234
0.00.622.041 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.622.056 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.622.057 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.622.057 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.387.082 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.387.083 I llama_perf_context_print:        load time =     609.73 ms
0.01.387.084 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.37 ms per token,   135.78 tokens per second)
0.01.387.085 I llama_perf_context_print:        eval time =     710.47 ms /    63 runs   (   11.28 ms per token,    88.67 tokens per second)
0.01.387.085 I llama_perf_context_print:       total time =     765.36 ms /    70 tokens
0.01.387.290 I ggml_metal_free: deallocating

real	0m1.404s
user	0m0.110s
sys	0m0.141s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.635 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.463 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.463 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.464 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.464 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.465 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.465 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.466 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.467 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.468 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.468 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.468 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.470 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.470 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.470 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.402 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.375 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.375 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.376 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.376 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.376 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.377 I llama_model_loader: - type  f32:  194 tensors
0.00.024.377 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.377 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.477 I llm_load_vocab: special tokens cache size = 25
0.00.051.361 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.364 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.364 I llm_load_print_meta: arch             = gptneox
0.00.051.364 I llm_load_print_meta: vocab type       = BPE
0.00.051.365 I llm_load_print_meta: n_vocab          = 50304
0.00.051.365 I llm_load_print_meta: n_merges         = 50009
0.00.051.365 I llm_load_print_meta: vocab_only       = 0
0.00.051.365 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.365 I llm_load_print_meta: n_embd           = 2048
0.00.051.365 I llm_load_print_meta: n_layer          = 24
0.00.051.368 I llm_load_print_meta: n_head           = 16
0.00.051.369 I llm_load_print_meta: n_head_kv        = 16
0.00.051.369 I llm_load_print_meta: n_rot            = 32
0.00.051.369 I llm_load_print_meta: n_swa            = 0
0.00.051.369 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.370 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.370 I llm_load_print_meta: n_gqa            = 1
0.00.051.371 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.372 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.373 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.375 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.375 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.375 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.375 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.376 I llm_load_print_meta: n_ff             = 8192
0.00.051.376 I llm_load_print_meta: n_expert         = 0
0.00.051.376 I llm_load_print_meta: n_expert_used    = 0
0.00.051.377 I llm_load_print_meta: causal attn      = 1
0.00.051.377 I llm_load_print_meta: pooling type     = 0
0.00.051.377 I llm_load_print_meta: rope type        = 2
0.00.051.377 I llm_load_print_meta: rope scaling     = linear
0.00.051.378 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.378 I llm_load_print_meta: freq_scale_train = 1
0.00.051.378 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.378 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.378 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.379 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.379 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.380 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.380 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.380 I llm_load_print_meta: model type       = 1.4B
0.00.051.381 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.381 I llm_load_print_meta: model params     = 1.41 B
0.00.051.382 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.382 I llm_load_print_meta: general.name     = 1.4B
0.00.051.382 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.382 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.383 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.383 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.383 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.387 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.387 I llm_load_print_meta: max token length = 1024
0.00.053.404 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.405 I llm_load_tensors: offloading output layer to GPU
0.00.053.405 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.415 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.416 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.330 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.331 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.331 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.332 I llama_new_context_with_model: n_batch       = 2048
0.00.054.332 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.332 I llama_new_context_with_model: flash_attn    = 0
0.00.054.333 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.333 I llama_new_context_with_model: freq_scale    = 1
0.00.054.333 I ggml_metal_init: allocating
0.00.054.341 I ggml_metal_init: found device: Apple M4
0.00.054.344 I ggml_metal_init: picking default device: Apple M4
0.00.054.925 I ggml_metal_init: using embedded metal library
0.00.057.239 I ggml_metal_init: GPU name:   Apple M4
0.00.057.240 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.241 I ggml_metal_init: simdgroup reduction   = true
0.00.057.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.241 I ggml_metal_init: has bfloat            = true
0.00.057.242 I ggml_metal_init: use bfloat            = true
0.00.057.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.127 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.816 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.822 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.844 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.838 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.839 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.839 I llama_new_context_with_model: graph nodes  = 967
0.00.087.840 I llama_new_context_with_model: graph splits = 2
0.00.087.854 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.983 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.984 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.739 I main: llama threadpool init, n_threads = 4
0.00.707.779 I 
0.00.707.827 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.828 I 
0.00.708.048 I sampler seed: 1234
0.00.708.052 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.708.085 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.708.088 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.708.088 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.557.160 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.01.557.161 I llama_perf_context_print:        load time =     699.10 ms
0.01.557.162 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.36 ms per token,   135.80 tokens per second)
0.01.557.162 I llama_perf_context_print:        eval time =     794.59 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.557.163 I llama_perf_context_print:       total time =     849.43 ms /    70 tokens
0.01.557.329 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.109s
sys	0m0.165s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.627 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.124 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.130 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.130 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.132 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.132 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.133 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.133 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.133 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.134 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.136 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.137 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.137 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.102 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.895 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.897 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.897 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.897 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.897 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.898 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.898 I llama_model_loader: - type  f32:  194 tensors
0.00.024.899 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.179 I llm_load_vocab: special tokens cache size = 25
0.00.052.229 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.232 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.232 I llm_load_print_meta: arch             = gptneox
0.00.052.233 I llm_load_print_meta: vocab type       = BPE
0.00.052.233 I llm_load_print_meta: n_vocab          = 50304
0.00.052.233 I llm_load_print_meta: n_merges         = 50009
0.00.052.234 I llm_load_print_meta: vocab_only       = 0
0.00.052.234 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.234 I llm_load_print_meta: n_embd           = 2048
0.00.052.234 I llm_load_print_meta: n_layer          = 24
0.00.052.237 I llm_load_print_meta: n_head           = 16
0.00.052.238 I llm_load_print_meta: n_head_kv        = 16
0.00.052.238 I llm_load_print_meta: n_rot            = 32
0.00.052.238 I llm_load_print_meta: n_swa            = 0
0.00.052.239 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.239 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.239 I llm_load_print_meta: n_gqa            = 1
0.00.052.242 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.243 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.243 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.244 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.244 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.244 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.246 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.246 I llm_load_print_meta: n_ff             = 8192
0.00.052.247 I llm_load_print_meta: n_expert         = 0
0.00.052.247 I llm_load_print_meta: n_expert_used    = 0
0.00.052.247 I llm_load_print_meta: causal attn      = 1
0.00.052.249 I llm_load_print_meta: pooling type     = 0
0.00.052.249 I llm_load_print_meta: rope type        = 2
0.00.052.249 I llm_load_print_meta: rope scaling     = linear
0.00.052.250 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.250 I llm_load_print_meta: freq_scale_train = 1
0.00.052.251 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.251 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.252 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.253 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.253 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.253 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.253 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.253 I llm_load_print_meta: model type       = 1.4B
0.00.052.254 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.255 I llm_load_print_meta: model params     = 1.41 B
0.00.052.256 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.256 I llm_load_print_meta: general.name     = 1.4B
0.00.052.259 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.259 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.260 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.260 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.260 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.260 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.261 I llm_load_print_meta: max token length = 1024
0.00.054.345 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.346 I llm_load_tensors: offloading output layer to GPU
0.00.054.346 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.357 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.358 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.243 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.244 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.244 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.244 I llama_new_context_with_model: n_batch       = 2048
0.00.055.244 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.244 I llama_new_context_with_model: flash_attn    = 0
0.00.055.245 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.245 I llama_new_context_with_model: freq_scale    = 1
0.00.055.245 I ggml_metal_init: allocating
0.00.055.251 I ggml_metal_init: found device: Apple M4
0.00.055.253 I ggml_metal_init: picking default device: Apple M4
0.00.055.844 I ggml_metal_init: using embedded metal library
0.00.058.183 I ggml_metal_init: GPU name:   Apple M4
0.00.058.185 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.185 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.186 I ggml_metal_init: simdgroup reduction   = true
0.00.058.187 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.187 I ggml_metal_init: has bfloat            = true
0.00.058.187 I ggml_metal_init: use bfloat            = true
0.00.058.188 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.188 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.619 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.467 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.477 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.499 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.504 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.506 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.506 I llama_new_context_with_model: graph nodes  = 967
0.00.088.506 I llama_new_context_with_model: graph splits = 2
0.00.088.522 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.666 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.354 I main: llama threadpool init, n_threads = 4
0.00.767.391 I 
0.00.767.419 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.419 I 
0.00.767.660 I sampler seed: 1234
0.00.767.665 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.712 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.716 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.716 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.647.731 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.647.732 I llama_perf_context_print:        load time =     757.72 ms
0.01.647.733 I llama_perf_context_print: prompt eval time =      54.59 ms /     7 tokens (    7.80 ms per token,   128.23 tokens per second)
0.01.647.733 I llama_perf_context_print:        eval time =     822.47 ms /    63 runs   (   13.06 ms per token,    76.60 tokens per second)
0.01.647.734 I llama_perf_context_print:       total time =     880.38 ms /    70 tokens
0.01.647.907 I ggml_metal_free: deallocating

real	0m1.668s
user	0m0.111s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.835 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.972 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.127 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.132 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.133 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.134 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.137 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.137 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.138 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.139 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.139 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.140 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.140 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.140 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.141 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.141 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.146 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.146 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.147 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.704 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.650 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.652 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.653 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.654 I llama_model_loader: - type  f32:  194 tensors
0.00.053.655 I llama_model_loader: - type  f16:   98 tensors
0.00.082.487 I llm_load_vocab: special tokens cache size = 25
0.00.090.459 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.462 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.462 I llm_load_print_meta: arch             = gptneox
0.00.090.462 I llm_load_print_meta: vocab type       = BPE
0.00.090.463 I llm_load_print_meta: n_vocab          = 50304
0.00.090.463 I llm_load_print_meta: n_merges         = 50009
0.00.090.463 I llm_load_print_meta: vocab_only       = 0
0.00.090.463 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.463 I llm_load_print_meta: n_embd           = 2048
0.00.090.463 I llm_load_print_meta: n_layer          = 24
0.00.090.466 I llm_load_print_meta: n_head           = 16
0.00.090.467 I llm_load_print_meta: n_head_kv        = 16
0.00.090.467 I llm_load_print_meta: n_rot            = 32
0.00.090.467 I llm_load_print_meta: n_swa            = 0
0.00.090.467 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.468 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.471 I llm_load_print_meta: n_gqa            = 1
0.00.090.471 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.474 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.474 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.475 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.475 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.475 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.475 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.476 I llm_load_print_meta: n_ff             = 8192
0.00.090.476 I llm_load_print_meta: n_expert         = 0
0.00.090.476 I llm_load_print_meta: n_expert_used    = 0
0.00.090.476 I llm_load_print_meta: causal attn      = 1
0.00.090.477 I llm_load_print_meta: pooling type     = 0
0.00.090.477 I llm_load_print_meta: rope type        = 2
0.00.090.478 I llm_load_print_meta: rope scaling     = linear
0.00.090.479 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.479 I llm_load_print_meta: freq_scale_train = 1
0.00.090.479 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.479 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.479 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.479 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.481 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.481 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.481 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.481 I llm_load_print_meta: model type       = 1.4B
0.00.090.482 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.482 I llm_load_print_meta: model params     = 1.41 B
0.00.090.483 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.483 I llm_load_print_meta: general.name     = 1.4B
0.00.090.483 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.483 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.483 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.484 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.485 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.485 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.485 I llm_load_print_meta: max token length = 1024
0.00.092.961 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.961 I llm_load_tensors: offloading output layer to GPU
0.00.092.962 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.972 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.973 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.889 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.890 I llama_new_context_with_model: n_ctx         = 128
0.00.093.890 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.890 I llama_new_context_with_model: n_batch       = 128
0.00.093.890 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.890 I llama_new_context_with_model: flash_attn    = 0
0.00.093.891 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.891 I llama_new_context_with_model: freq_scale    = 1
0.00.093.891 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.892 I ggml_metal_init: allocating
0.00.093.896 I ggml_metal_init: found device: Apple M4
0.00.093.900 I ggml_metal_init: picking default device: Apple M4
0.00.094.493 I ggml_metal_init: using embedded metal library
0.00.096.947 I ggml_metal_init: GPU name:   Apple M4
0.00.096.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.948 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.949 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.949 I ggml_metal_init: simdgroup reduction   = true
0.00.096.949 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.949 I ggml_metal_init: has bfloat            = true
0.00.096.950 I ggml_metal_init: use bfloat            = true
0.00.096.950 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.951 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.272 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.107.521 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.524 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.540 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.459 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.460 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.460 I llama_new_context_with_model: graph nodes  = 967
0.00.108.460 I llama_new_context_with_model: graph splits = 2
0.00.108.472 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.043.654 I 
0.01.043.706 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.043.744 I perplexity: tokenizing the input ..
0.01.057.242 I perplexity: tokenization took 13.493 ms
0.01.057.248 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.176.811 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.178.115 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.178.132 I llama_perf_context_print:        load time =    1020.67 ms
0.01.178.134 I llama_perf_context_print: prompt eval time =     119.00 ms /   128 tokens (    0.93 ms per token,  1075.59 tokens per second)
0.01.178.135 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.178.135 I llama_perf_context_print:       total time =     134.48 ms /   129 tokens
0.01.178.486 I ggml_metal_free: deallocating

real	0m1.363s
user	0m0.120s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.020 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.450 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.458 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.462 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.462 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.462 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.463 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.463 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.463 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.464 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.467 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.467 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.467 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.330 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.425 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.366 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.367 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.367 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.368 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.368 I llama_model_loader: - type  f32:  194 tensors
0.00.025.369 I llama_model_loader: - type q8_0:   98 tensors
0.00.047.380 I llm_load_vocab: special tokens cache size = 25
0.00.053.412 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.416 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.417 I llm_load_print_meta: arch             = gptneox
0.00.053.417 I llm_load_print_meta: vocab type       = BPE
0.00.053.417 I llm_load_print_meta: n_vocab          = 50304
0.00.053.417 I llm_load_print_meta: n_merges         = 50009
0.00.053.420 I llm_load_print_meta: vocab_only       = 0
0.00.053.420 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.420 I llm_load_print_meta: n_embd           = 2048
0.00.053.420 I llm_load_print_meta: n_layer          = 24
0.00.053.424 I llm_load_print_meta: n_head           = 16
0.00.053.425 I llm_load_print_meta: n_head_kv        = 16
0.00.053.426 I llm_load_print_meta: n_rot            = 32
0.00.053.426 I llm_load_print_meta: n_swa            = 0
0.00.053.426 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.426 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.427 I llm_load_print_meta: n_gqa            = 1
0.00.053.428 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.428 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.429 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.429 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.430 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.430 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.431 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.432 I llm_load_print_meta: n_ff             = 8192
0.00.053.432 I llm_load_print_meta: n_expert         = 0
0.00.053.432 I llm_load_print_meta: n_expert_used    = 0
0.00.053.432 I llm_load_print_meta: causal attn      = 1
0.00.053.432 I llm_load_print_meta: pooling type     = 0
0.00.053.432 I llm_load_print_meta: rope type        = 2
0.00.053.433 I llm_load_print_meta: rope scaling     = linear
0.00.053.433 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.433 I llm_load_print_meta: freq_scale_train = 1
0.00.053.435 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.436 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.436 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.436 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.437 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.437 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.437 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.437 I llm_load_print_meta: model type       = 1.4B
0.00.053.441 I llm_load_print_meta: model ftype      = Q8_0
0.00.053.443 I llm_load_print_meta: model params     = 1.41 B
0.00.053.443 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.053.443 I llm_load_print_meta: general.name     = 1.4B
0.00.053.443 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.443 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.445 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.445 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.445 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.445 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.446 I llm_load_print_meta: max token length = 1024
0.00.055.540 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.541 I llm_load_tensors: offloading output layer to GPU
0.00.055.542 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.552 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.055.553 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.056.485 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.486 I llama_new_context_with_model: n_ctx         = 128
0.00.056.486 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.486 I llama_new_context_with_model: n_batch       = 128
0.00.056.487 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.487 I llama_new_context_with_model: flash_attn    = 0
0.00.056.487 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.487 I llama_new_context_with_model: freq_scale    = 1
0.00.056.488 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.488 I ggml_metal_init: allocating
0.00.056.493 I ggml_metal_init: found device: Apple M4
0.00.056.495 I ggml_metal_init: picking default device: Apple M4
0.00.057.100 I ggml_metal_init: using embedded metal library
0.00.059.532 I ggml_metal_init: GPU name:   Apple M4
0.00.059.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.535 I ggml_metal_init: simdgroup reduction   = true
0.00.059.535 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.535 I ggml_metal_init: has bfloat            = true
0.00.059.535 I ggml_metal_init: use bfloat            = true
0.00.059.536 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.538 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.304 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.070.639 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.646 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.668 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.502 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.503 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.503 I llama_new_context_with_model: graph nodes  = 967
0.00.071.504 I llama_new_context_with_model: graph splits = 2
0.00.071.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.517 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.914.357 I 
0.00.914.387 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.914.405 I perplexity: tokenizing the input ..
0.00.922.297 I perplexity: tokenization took 7.89 ms
0.00.922.306 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.047.160 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.048.395 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.048.419 I llama_perf_context_print:        load time =     904.33 ms
0.01.048.422 I llama_perf_context_print: prompt eval time =     124.62 ms /   128 tokens (    0.97 ms per token,  1027.08 tokens per second)
0.01.048.423 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.048.424 I llama_perf_context_print:       total time =     134.06 ms /   129 tokens
0.01.048.824 I ggml_metal_free: deallocating

real	0m1.064s
user	0m0.081s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.078 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.005 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.009 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.011 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.011 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.011 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.012 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.012 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.013 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.013 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.014 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.014 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.014 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.015 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.017 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.019 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.019 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.828 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.682 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.683 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.685 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.685 I llama_model_loader: - type  f32:  194 tensors
0.00.024.686 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.686 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.949 I llm_load_vocab: special tokens cache size = 25
0.00.050.843 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.846 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.846 I llm_load_print_meta: arch             = gptneox
0.00.050.846 I llm_load_print_meta: vocab type       = BPE
0.00.050.847 I llm_load_print_meta: n_vocab          = 50304
0.00.050.847 I llm_load_print_meta: n_merges         = 50009
0.00.050.847 I llm_load_print_meta: vocab_only       = 0
0.00.050.847 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.847 I llm_load_print_meta: n_embd           = 2048
0.00.050.847 I llm_load_print_meta: n_layer          = 24
0.00.050.850 I llm_load_print_meta: n_head           = 16
0.00.050.851 I llm_load_print_meta: n_head_kv        = 16
0.00.050.851 I llm_load_print_meta: n_rot            = 32
0.00.050.851 I llm_load_print_meta: n_swa            = 0
0.00.050.852 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.852 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.853 I llm_load_print_meta: n_gqa            = 1
0.00.050.853 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.854 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.855 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.855 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.855 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.855 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.855 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.856 I llm_load_print_meta: n_ff             = 8192
0.00.050.856 I llm_load_print_meta: n_expert         = 0
0.00.050.856 I llm_load_print_meta: n_expert_used    = 0
0.00.050.857 I llm_load_print_meta: causal attn      = 1
0.00.050.857 I llm_load_print_meta: pooling type     = 0
0.00.050.857 I llm_load_print_meta: rope type        = 2
0.00.050.857 I llm_load_print_meta: rope scaling     = linear
0.00.050.857 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.858 I llm_load_print_meta: freq_scale_train = 1
0.00.050.858 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.858 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.858 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.858 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.858 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.858 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.859 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.859 I llm_load_print_meta: model type       = 1.4B
0.00.050.860 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.860 I llm_load_print_meta: model params     = 1.41 B
0.00.050.861 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.863 I llm_load_print_meta: general.name     = 1.4B
0.00.050.863 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.863 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.863 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.863 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.864 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.864 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.864 I llm_load_print_meta: max token length = 1024
0.00.052.829 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.829 I llm_load_tensors: offloading output layer to GPU
0.00.052.830 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.840 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.841 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.779 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.780 I llama_new_context_with_model: n_ctx         = 128
0.00.053.780 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.780 I llama_new_context_with_model: n_batch       = 128
0.00.053.780 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.780 I llama_new_context_with_model: flash_attn    = 0
0.00.053.781 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.781 I llama_new_context_with_model: freq_scale    = 1
0.00.053.781 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.782 I ggml_metal_init: allocating
0.00.053.785 I ggml_metal_init: found device: Apple M4
0.00.053.787 I ggml_metal_init: picking default device: Apple M4
0.00.054.354 I ggml_metal_init: using embedded metal library
0.00.056.652 I ggml_metal_init: GPU name:   Apple M4
0.00.056.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.654 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.654 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.655 I ggml_metal_init: simdgroup reduction   = true
0.00.056.655 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.655 I ggml_metal_init: has bfloat            = true
0.00.056.655 I ggml_metal_init: use bfloat            = true
0.00.056.655 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.656 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.677 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.983 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.985 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.999 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.928 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.929 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.930 I llama_new_context_with_model: graph nodes  = 967
0.00.068.930 I llama_new_context_with_model: graph splits = 2
0.00.068.942 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.943 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.342 I 
0.00.614.387 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.398 I perplexity: tokenizing the input ..
0.00.622.054 I perplexity: tokenization took 7.654 ms
0.00.622.057 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.744.171 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.745.450 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.745.463 I llama_perf_context_print:        load time =     604.26 ms
0.00.745.464 I llama_perf_context_print: prompt eval time =     121.89 ms /   128 tokens (    0.95 ms per token,  1050.13 tokens per second)
0.00.745.466 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.745.467 I llama_perf_context_print:       total time =     131.12 ms /   129 tokens
0.00.745.918 I ggml_metal_free: deallocating

real	0m0.760s
user	0m0.077s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.615 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.201 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.211 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.211 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.212 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.212 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.212 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.214 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.214 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.214 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.216 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.217 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.217 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.219 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.219 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.219 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.986 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.021 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.857 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.858 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.858 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.858 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.858 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.859 I llama_model_loader: - type  f32:  194 tensors
0.00.022.859 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.860 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.112 I llm_load_vocab: special tokens cache size = 25
0.00.049.138 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.140 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.141 I llm_load_print_meta: arch             = gptneox
0.00.049.141 I llm_load_print_meta: vocab type       = BPE
0.00.049.141 I llm_load_print_meta: n_vocab          = 50304
0.00.049.142 I llm_load_print_meta: n_merges         = 50009
0.00.049.142 I llm_load_print_meta: vocab_only       = 0
0.00.049.142 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.142 I llm_load_print_meta: n_embd           = 2048
0.00.049.142 I llm_load_print_meta: n_layer          = 24
0.00.049.145 I llm_load_print_meta: n_head           = 16
0.00.049.145 I llm_load_print_meta: n_head_kv        = 16
0.00.049.146 I llm_load_print_meta: n_rot            = 32
0.00.049.146 I llm_load_print_meta: n_swa            = 0
0.00.049.146 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.146 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.147 I llm_load_print_meta: n_gqa            = 1
0.00.049.148 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.148 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.149 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.149 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.149 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.150 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.150 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.151 I llm_load_print_meta: n_ff             = 8192
0.00.049.151 I llm_load_print_meta: n_expert         = 0
0.00.049.152 I llm_load_print_meta: n_expert_used    = 0
0.00.049.152 I llm_load_print_meta: causal attn      = 1
0.00.049.152 I llm_load_print_meta: pooling type     = 0
0.00.049.152 I llm_load_print_meta: rope type        = 2
0.00.049.152 I llm_load_print_meta: rope scaling     = linear
0.00.049.154 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.154 I llm_load_print_meta: freq_scale_train = 1
0.00.049.155 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.155 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.155 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.155 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.155 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.155 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.156 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.156 I llm_load_print_meta: model type       = 1.4B
0.00.049.156 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.157 I llm_load_print_meta: model params     = 1.41 B
0.00.049.161 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.161 I llm_load_print_meta: general.name     = 1.4B
0.00.049.161 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.162 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.162 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.162 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.162 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.163 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.163 I llm_load_print_meta: max token length = 1024
0.00.051.156 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.156 I llm_load_tensors: offloading output layer to GPU
0.00.051.157 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.167 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.168 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.084 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.085 I llama_new_context_with_model: n_ctx         = 128
0.00.052.085 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.086 I llama_new_context_with_model: n_batch       = 128
0.00.052.086 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.086 I llama_new_context_with_model: flash_attn    = 0
0.00.052.086 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.087 I llama_new_context_with_model: freq_scale    = 1
0.00.052.087 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.088 I ggml_metal_init: allocating
0.00.052.094 I ggml_metal_init: found device: Apple M4
0.00.052.097 I ggml_metal_init: picking default device: Apple M4
0.00.052.666 I ggml_metal_init: using embedded metal library
0.00.055.003 I ggml_metal_init: GPU name:   Apple M4
0.00.055.004 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.005 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.005 I ggml_metal_init: simdgroup reduction   = true
0.00.055.005 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.005 I ggml_metal_init: has bfloat            = true
0.00.055.005 I ggml_metal_init: use bfloat            = true
0.00.055.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.634 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.867 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.871 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.885 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.775 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.776 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.776 I llama_new_context_with_model: graph nodes  = 967
0.00.066.776 I llama_new_context_with_model: graph splits = 2
0.00.066.784 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.785 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.715 I 
0.00.665.758 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.771 I perplexity: tokenizing the input ..
0.00.673.785 I perplexity: tokenization took 8.012 ms
0.00.673.788 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.444 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.797.614 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.797.632 I llama_perf_context_print:        load time =     657.09 ms
0.00.797.633 I llama_perf_context_print: prompt eval time =     122.43 ms /   128 tokens (    0.96 ms per token,  1045.50 tokens per second)
0.00.797.634 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.634 I llama_perf_context_print:       total time =     131.92 ms /   129 tokens
0.00.798.168 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.078s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.277 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.944 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.950 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.951 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.952 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.953 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.953 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.954 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.954 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.954 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.955 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.956 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.956 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.957 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.721 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.762 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.441 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.442 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.442 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.442 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.443 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.443 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.443 I llama_model_loader: - type  f32:  194 tensors
0.00.023.444 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.444 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.701 I llm_load_vocab: special tokens cache size = 25
0.00.050.991 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.994 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.994 I llm_load_print_meta: arch             = gptneox
0.00.050.995 I llm_load_print_meta: vocab type       = BPE
0.00.050.995 I llm_load_print_meta: n_vocab          = 50304
0.00.050.995 I llm_load_print_meta: n_merges         = 50009
0.00.050.995 I llm_load_print_meta: vocab_only       = 0
0.00.050.995 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.995 I llm_load_print_meta: n_embd           = 2048
0.00.050.996 I llm_load_print_meta: n_layer          = 24
0.00.050.998 I llm_load_print_meta: n_head           = 16
0.00.050.999 I llm_load_print_meta: n_head_kv        = 16
0.00.050.999 I llm_load_print_meta: n_rot            = 32
0.00.050.999 I llm_load_print_meta: n_swa            = 0
0.00.050.999 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.000 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.000 I llm_load_print_meta: n_gqa            = 1
0.00.051.001 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.002 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.003 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.003 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.003 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.003 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.003 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.004 I llm_load_print_meta: n_ff             = 8192
0.00.051.004 I llm_load_print_meta: n_expert         = 0
0.00.051.004 I llm_load_print_meta: n_expert_used    = 0
0.00.051.005 I llm_load_print_meta: causal attn      = 1
0.00.051.005 I llm_load_print_meta: pooling type     = 0
0.00.051.005 I llm_load_print_meta: rope type        = 2
0.00.051.005 I llm_load_print_meta: rope scaling     = linear
0.00.051.008 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.008 I llm_load_print_meta: freq_scale_train = 1
0.00.051.008 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.009 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.009 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.009 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.009 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.009 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.009 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.009 I llm_load_print_meta: model type       = 1.4B
0.00.051.010 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.010 I llm_load_print_meta: model params     = 1.41 B
0.00.051.011 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.011 I llm_load_print_meta: general.name     = 1.4B
0.00.051.011 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.012 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.012 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.012 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.012 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.013 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.013 I llm_load_print_meta: max token length = 1024
0.00.052.992 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.993 I llm_load_tensors: offloading output layer to GPU
0.00.052.993 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.003 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.004 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.909 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.910 I llama_new_context_with_model: n_ctx         = 128
0.00.053.910 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.910 I llama_new_context_with_model: n_batch       = 128
0.00.053.910 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.910 I llama_new_context_with_model: flash_attn    = 0
0.00.053.911 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.911 I llama_new_context_with_model: freq_scale    = 1
0.00.053.911 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.912 I ggml_metal_init: allocating
0.00.053.915 I ggml_metal_init: found device: Apple M4
0.00.053.917 I ggml_metal_init: picking default device: Apple M4
0.00.054.469 I ggml_metal_init: using embedded metal library
0.00.056.777 I ggml_metal_init: GPU name:   Apple M4
0.00.056.779 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.779 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.780 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.780 I ggml_metal_init: simdgroup reduction   = true
0.00.056.780 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.780 I ggml_metal_init: has bfloat            = true
0.00.056.780 I ggml_metal_init: use bfloat            = true
0.00.056.781 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.345 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.636 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.638 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.654 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.564 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.564 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.565 I llama_new_context_with_model: graph nodes  = 967
0.00.068.565 I llama_new_context_with_model: graph splits = 2
0.00.068.577 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.578 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.040 I 
0.00.717.078 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.090 I perplexity: tokenizing the input ..
0.00.725.403 I perplexity: tokenization took 8.311 ms
0.00.725.406 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.859.745 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.860.911 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.860.926 I llama_perf_context_print:        load time =     707.76 ms
0.00.860.927 I llama_perf_context_print: prompt eval time =     134.11 ms /   128 tokens (    1.05 ms per token,   954.44 tokens per second)
0.00.860.928 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.860.928 I llama_perf_context_print:       total time =     143.89 ms /   129 tokens
0.00.861.426 I ggml_metal_free: deallocating

real	0m0.878s
user	0m0.079s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.403 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.843 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.847 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.849 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.854 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.854 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.855 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.856 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.857 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.857 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.858 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.859 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.860 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.860 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.539 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.290 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.292 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.292 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.292 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.293 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.293 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.293 I llama_model_loader: - type  f32:  194 tensors
0.00.025.294 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.294 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.497 I llm_load_vocab: special tokens cache size = 25
0.00.051.459 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.462 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.462 I llm_load_print_meta: arch             = gptneox
0.00.051.463 I llm_load_print_meta: vocab type       = BPE
0.00.051.463 I llm_load_print_meta: n_vocab          = 50304
0.00.051.463 I llm_load_print_meta: n_merges         = 50009
0.00.051.463 I llm_load_print_meta: vocab_only       = 0
0.00.051.463 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.464 I llm_load_print_meta: n_embd           = 2048
0.00.051.464 I llm_load_print_meta: n_layer          = 24
0.00.051.466 I llm_load_print_meta: n_head           = 16
0.00.051.467 I llm_load_print_meta: n_head_kv        = 16
0.00.051.467 I llm_load_print_meta: n_rot            = 32
0.00.051.467 I llm_load_print_meta: n_swa            = 0
0.00.051.468 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.468 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.469 I llm_load_print_meta: n_gqa            = 1
0.00.051.469 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.470 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.471 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.471 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.471 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.471 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.472 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.472 I llm_load_print_meta: n_ff             = 8192
0.00.051.472 I llm_load_print_meta: n_expert         = 0
0.00.051.473 I llm_load_print_meta: n_expert_used    = 0
0.00.051.475 I llm_load_print_meta: causal attn      = 1
0.00.051.476 I llm_load_print_meta: pooling type     = 0
0.00.051.476 I llm_load_print_meta: rope type        = 2
0.00.051.476 I llm_load_print_meta: rope scaling     = linear
0.00.051.476 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.477 I llm_load_print_meta: freq_scale_train = 1
0.00.051.477 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.477 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.477 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.477 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.477 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.477 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.478 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.478 I llm_load_print_meta: model type       = 1.4B
0.00.051.478 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.479 I llm_load_print_meta: model params     = 1.41 B
0.00.051.479 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.479 I llm_load_print_meta: general.name     = 1.4B
0.00.051.480 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.480 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.484 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.484 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.484 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.485 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.485 I llm_load_print_meta: max token length = 1024
0.00.053.517 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.518 I llm_load_tensors: offloading output layer to GPU
0.00.053.518 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.529 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.530 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.429 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.430 I llama_new_context_with_model: n_ctx         = 128
0.00.054.431 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.431 I llama_new_context_with_model: n_batch       = 128
0.00.054.431 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.431 I llama_new_context_with_model: flash_attn    = 0
0.00.054.432 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.432 I llama_new_context_with_model: freq_scale    = 1
0.00.054.432 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.433 I ggml_metal_init: allocating
0.00.054.436 I ggml_metal_init: found device: Apple M4
0.00.054.438 I ggml_metal_init: picking default device: Apple M4
0.00.054.985 I ggml_metal_init: using embedded metal library
0.00.057.338 I ggml_metal_init: GPU name:   Apple M4
0.00.057.339 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.340 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.340 I ggml_metal_init: simdgroup reduction   = true
0.00.057.340 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.340 I ggml_metal_init: has bfloat            = true
0.00.057.340 I ggml_metal_init: use bfloat            = true
0.00.057.341 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.341 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.946 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.339 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.341 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.358 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.202 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.203 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.203 I llama_new_context_with_model: graph nodes  = 967
0.00.069.203 I llama_new_context_with_model: graph splits = 2
0.00.069.216 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.216 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.229 I 
0.00.644.265 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.644.276 I perplexity: tokenizing the input ..
0.00.652.055 I perplexity: tokenization took 7.777 ms
0.00.652.059 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.337 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.788.602 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.788.622 I llama_perf_context_print:        load time =     634.82 ms
0.00.788.623 I llama_perf_context_print: prompt eval time =     135.03 ms /   128 tokens (    1.05 ms per token,   947.96 tokens per second)
0.00.788.624 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.624 I llama_perf_context_print:       total time =     144.39 ms /   129 tokens
0.00.789.034 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.077s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.886 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.449 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.453 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.455 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.456 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.456 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.457 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.457 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.458 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.458 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.458 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.459 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.459 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.459 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.460 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.461 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.461 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.462 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.294 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.172 I llama_model_loader: - type  f32:  194 tensors
0.00.025.173 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.173 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.173 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.095 I llm_load_vocab: special tokens cache size = 25
0.00.052.077 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.080 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.080 I llm_load_print_meta: arch             = gptneox
0.00.052.080 I llm_load_print_meta: vocab type       = BPE
0.00.052.081 I llm_load_print_meta: n_vocab          = 50304
0.00.052.081 I llm_load_print_meta: n_merges         = 50009
0.00.052.081 I llm_load_print_meta: vocab_only       = 0
0.00.052.081 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.081 I llm_load_print_meta: n_embd           = 2048
0.00.052.082 I llm_load_print_meta: n_layer          = 24
0.00.052.084 I llm_load_print_meta: n_head           = 16
0.00.052.085 I llm_load_print_meta: n_head_kv        = 16
0.00.052.085 I llm_load_print_meta: n_rot            = 32
0.00.052.086 I llm_load_print_meta: n_swa            = 0
0.00.052.086 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.086 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.087 I llm_load_print_meta: n_gqa            = 1
0.00.052.088 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.088 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.089 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.089 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.089 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.092 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.092 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.092 I llm_load_print_meta: n_ff             = 8192
0.00.052.093 I llm_load_print_meta: n_expert         = 0
0.00.052.093 I llm_load_print_meta: n_expert_used    = 0
0.00.052.093 I llm_load_print_meta: causal attn      = 1
0.00.052.093 I llm_load_print_meta: pooling type     = 0
0.00.052.094 I llm_load_print_meta: rope type        = 2
0.00.052.101 I llm_load_print_meta: rope scaling     = linear
0.00.052.102 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.103 I llm_load_print_meta: freq_scale_train = 1
0.00.052.103 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.103 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.104 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.104 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.104 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.104 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.104 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.104 I llm_load_print_meta: model type       = 1.4B
0.00.052.105 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.105 I llm_load_print_meta: model params     = 1.41 B
0.00.052.105 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.106 I llm_load_print_meta: general.name     = 1.4B
0.00.052.106 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.106 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.106 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.106 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.106 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.107 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.107 I llm_load_print_meta: max token length = 1024
0.00.053.900 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.900 I llm_load_tensors: offloading output layer to GPU
0.00.053.901 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.906 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.906 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.798 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.798 I llama_new_context_with_model: n_ctx         = 128
0.00.054.799 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.799 I llama_new_context_with_model: n_batch       = 128
0.00.054.799 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.799 I llama_new_context_with_model: flash_attn    = 0
0.00.054.799 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.800 I llama_new_context_with_model: freq_scale    = 1
0.00.054.800 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.800 I ggml_metal_init: allocating
0.00.054.806 I ggml_metal_init: found device: Apple M4
0.00.054.808 I ggml_metal_init: picking default device: Apple M4
0.00.055.344 I ggml_metal_init: using embedded metal library
0.00.057.692 I ggml_metal_init: GPU name:   Apple M4
0.00.057.694 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.694 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.694 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.694 I ggml_metal_init: simdgroup reduction   = true
0.00.057.695 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.695 I ggml_metal_init: has bfloat            = true
0.00.057.695 I ggml_metal_init: use bfloat            = true
0.00.057.695 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.696 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.186 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.561 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.563 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.578 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.400 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.401 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.401 I llama_new_context_with_model: graph nodes  = 967
0.00.069.401 I llama_new_context_with_model: graph splits = 2
0.00.069.408 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.414.538 I 
0.00.414.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.414.590 I perplexity: tokenizing the input ..
0.00.422.344 I perplexity: tokenization took 7.753 ms
0.00.422.348 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.554.497 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.555.682 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.555.704 I llama_perf_context_print:        load time =     403.65 ms
0.00.555.704 I llama_perf_context_print: prompt eval time =     131.92 ms /   128 tokens (    1.03 ms per token,   970.26 tokens per second)
0.00.555.705 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.555.706 I llama_perf_context_print:       total time =     141.17 ms /   129 tokens
0.00.556.177 I ggml_metal_free: deallocating

real	0m0.572s
user	0m0.078s
sys	0m0.075s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.157 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.687 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.693 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.694 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.694 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.695 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.697 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.698 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.698 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.699 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.702 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.702 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.484 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.538 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.266 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.267 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.267 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.268 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.268 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.269 I llama_model_loader: - type  f32:  194 tensors
0.00.025.269 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.270 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.270 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.270 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.409 I llm_load_vocab: special tokens cache size = 25
0.00.051.364 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.367 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.368 I llm_load_print_meta: arch             = gptneox
0.00.051.368 I llm_load_print_meta: vocab type       = BPE
0.00.051.368 I llm_load_print_meta: n_vocab          = 50304
0.00.051.369 I llm_load_print_meta: n_merges         = 50009
0.00.051.369 I llm_load_print_meta: vocab_only       = 0
0.00.051.369 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.369 I llm_load_print_meta: n_embd           = 2048
0.00.051.369 I llm_load_print_meta: n_layer          = 24
0.00.051.372 I llm_load_print_meta: n_head           = 16
0.00.051.373 I llm_load_print_meta: n_head_kv        = 16
0.00.051.373 I llm_load_print_meta: n_rot            = 32
0.00.051.373 I llm_load_print_meta: n_swa            = 0
0.00.051.373 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.373 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.374 I llm_load_print_meta: n_gqa            = 1
0.00.051.375 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.376 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.376 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.377 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.377 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.377 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.377 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.378 I llm_load_print_meta: n_ff             = 8192
0.00.051.378 I llm_load_print_meta: n_expert         = 0
0.00.051.378 I llm_load_print_meta: n_expert_used    = 0
0.00.051.378 I llm_load_print_meta: causal attn      = 1
0.00.051.378 I llm_load_print_meta: pooling type     = 0
0.00.051.379 I llm_load_print_meta: rope type        = 2
0.00.051.380 I llm_load_print_meta: rope scaling     = linear
0.00.051.380 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.381 I llm_load_print_meta: freq_scale_train = 1
0.00.051.381 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.381 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.381 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.381 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.381 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.381 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.382 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.382 I llm_load_print_meta: model type       = 1.4B
0.00.051.382 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.382 I llm_load_print_meta: model params     = 1.41 B
0.00.051.383 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.383 I llm_load_print_meta: general.name     = 1.4B
0.00.051.383 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.384 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.384 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.384 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.384 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.385 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.385 I llm_load_print_meta: max token length = 1024
0.00.052.968 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.968 I llm_load_tensors: offloading output layer to GPU
0.00.052.969 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.979 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.980 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.817 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.818 I llama_new_context_with_model: n_ctx         = 128
0.00.053.818 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.818 I llama_new_context_with_model: n_batch       = 128
0.00.053.818 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.818 I llama_new_context_with_model: flash_attn    = 0
0.00.053.819 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.819 I llama_new_context_with_model: freq_scale    = 1
0.00.053.819 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.820 I ggml_metal_init: allocating
0.00.053.823 I ggml_metal_init: found device: Apple M4
0.00.053.825 I ggml_metal_init: picking default device: Apple M4
0.00.054.375 I ggml_metal_init: using embedded metal library
0.00.056.670 I ggml_metal_init: GPU name:   Apple M4
0.00.056.672 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.672 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.672 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.673 I ggml_metal_init: simdgroup reduction   = true
0.00.056.673 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.673 I ggml_metal_init: has bfloat            = true
0.00.056.673 I ggml_metal_init: use bfloat            = true
0.00.056.674 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.674 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.271 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.548 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.551 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.566 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.477 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.478 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.479 I llama_new_context_with_model: graph nodes  = 967
0.00.068.479 I llama_new_context_with_model: graph splits = 2
0.00.068.491 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.491 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.030 I 
0.00.496.075 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.086 I perplexity: tokenizing the input ..
0.00.504.253 I perplexity: tokenization took 8.165 ms
0.00.504.256 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.636.482 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.637.629 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.637.648 I llama_perf_context_print:        load time =     486.87 ms
0.00.637.649 I llama_perf_context_print: prompt eval time =     132.00 ms /   128 tokens (    1.03 ms per token,   969.70 tokens per second)
0.00.637.650 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.637.650 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.638.162 I ggml_metal_free: deallocating

real	0m0.651s
user	0m0.078s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.783 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.710 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.715 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.717 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.717 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.720 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.720 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.720 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.721 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.721 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.723 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.723 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.723 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.574 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.426 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.427 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.427 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.428 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.428 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.428 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.429 I llama_model_loader: - type  f32:  194 tensors
0.00.023.429 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.429 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.430 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.577 I llm_load_vocab: special tokens cache size = 25
0.00.050.519 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.522 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.523 I llm_load_print_meta: arch             = gptneox
0.00.050.523 I llm_load_print_meta: vocab type       = BPE
0.00.050.523 I llm_load_print_meta: n_vocab          = 50304
0.00.050.523 I llm_load_print_meta: n_merges         = 50009
0.00.050.523 I llm_load_print_meta: vocab_only       = 0
0.00.050.524 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.524 I llm_load_print_meta: n_embd           = 2048
0.00.050.524 I llm_load_print_meta: n_layer          = 24
0.00.050.527 I llm_load_print_meta: n_head           = 16
0.00.050.528 I llm_load_print_meta: n_head_kv        = 16
0.00.050.528 I llm_load_print_meta: n_rot            = 32
0.00.050.528 I llm_load_print_meta: n_swa            = 0
0.00.050.528 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.528 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.529 I llm_load_print_meta: n_gqa            = 1
0.00.050.530 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.531 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.531 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.532 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.532 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.533 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.533 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.534 I llm_load_print_meta: n_ff             = 8192
0.00.050.534 I llm_load_print_meta: n_expert         = 0
0.00.050.534 I llm_load_print_meta: n_expert_used    = 0
0.00.050.534 I llm_load_print_meta: causal attn      = 1
0.00.050.534 I llm_load_print_meta: pooling type     = 0
0.00.050.534 I llm_load_print_meta: rope type        = 2
0.00.050.535 I llm_load_print_meta: rope scaling     = linear
0.00.050.537 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.538 I llm_load_print_meta: freq_scale_train = 1
0.00.050.538 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.538 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.538 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.538 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.538 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.539 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.539 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.539 I llm_load_print_meta: model type       = 1.4B
0.00.050.539 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.540 I llm_load_print_meta: model params     = 1.41 B
0.00.050.545 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.545 I llm_load_print_meta: general.name     = 1.4B
0.00.050.547 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.547 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.547 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.547 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.547 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.547 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.547 I llm_load_print_meta: max token length = 1024
0.00.052.610 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.611 I llm_load_tensors: offloading output layer to GPU
0.00.052.611 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.621 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.622 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.537 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.538 I llama_new_context_with_model: n_ctx         = 128
0.00.053.538 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.538 I llama_new_context_with_model: n_batch       = 128
0.00.053.538 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.539 I llama_new_context_with_model: flash_attn    = 0
0.00.053.539 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.539 I llama_new_context_with_model: freq_scale    = 1
0.00.053.540 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.540 I ggml_metal_init: allocating
0.00.053.544 I ggml_metal_init: found device: Apple M4
0.00.053.546 I ggml_metal_init: picking default device: Apple M4
0.00.054.131 I ggml_metal_init: using embedded metal library
0.00.056.480 I ggml_metal_init: GPU name:   Apple M4
0.00.056.481 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.482 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.482 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.482 I ggml_metal_init: simdgroup reduction   = true
0.00.056.483 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.483 I ggml_metal_init: has bfloat            = true
0.00.056.483 I ggml_metal_init: use bfloat            = true
0.00.056.483 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.484 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.352 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.612 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.614 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.627 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.544 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.546 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.546 I llama_new_context_with_model: graph nodes  = 967
0.00.068.546 I llama_new_context_with_model: graph splits = 2
0.00.068.559 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.560 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.578.534 I 
0.00.578.562 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.578.573 I perplexity: tokenizing the input ..
0.00.586.579 I perplexity: tokenization took 8.004 ms
0.00.586.582 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.721.514 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.722.769 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.722.786 I llama_perf_context_print:        load time =     569.75 ms
0.00.722.787 I llama_perf_context_print: prompt eval time =     134.70 ms /   128 tokens (    1.05 ms per token,   950.25 tokens per second)
0.00.722.788 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.722.788 I llama_perf_context_print:       total time =     144.25 ms /   129 tokens
0.00.723.203 I ggml_metal_free: deallocating

real	0m0.737s
user	0m0.079s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.496 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.221 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.225 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.227 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.228 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.228 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.228 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.229 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.230 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.230 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.233 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.233 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.234 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.237 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.237 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.237 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.010 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.044 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.764 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.765 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.765 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.766 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.766 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.766 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.767 I llama_model_loader: - type  f32:  194 tensors
0.00.023.768 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.768 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.941 I llm_load_vocab: special tokens cache size = 25
0.00.049.849 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.852 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.852 I llm_load_print_meta: arch             = gptneox
0.00.049.852 I llm_load_print_meta: vocab type       = BPE
0.00.049.853 I llm_load_print_meta: n_vocab          = 50304
0.00.049.853 I llm_load_print_meta: n_merges         = 50009
0.00.049.853 I llm_load_print_meta: vocab_only       = 0
0.00.049.853 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.853 I llm_load_print_meta: n_embd           = 2048
0.00.049.854 I llm_load_print_meta: n_layer          = 24
0.00.049.856 I llm_load_print_meta: n_head           = 16
0.00.049.857 I llm_load_print_meta: n_head_kv        = 16
0.00.049.857 I llm_load_print_meta: n_rot            = 32
0.00.049.857 I llm_load_print_meta: n_swa            = 0
0.00.049.857 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.857 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.860 I llm_load_print_meta: n_gqa            = 1
0.00.049.861 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.862 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.863 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.864 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.864 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.864 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.865 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.865 I llm_load_print_meta: n_ff             = 8192
0.00.049.865 I llm_load_print_meta: n_expert         = 0
0.00.049.866 I llm_load_print_meta: n_expert_used    = 0
0.00.049.866 I llm_load_print_meta: causal attn      = 1
0.00.049.866 I llm_load_print_meta: pooling type     = 0
0.00.049.866 I llm_load_print_meta: rope type        = 2
0.00.049.866 I llm_load_print_meta: rope scaling     = linear
0.00.049.867 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.867 I llm_load_print_meta: freq_scale_train = 1
0.00.049.867 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.867 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.868 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.868 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.868 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.868 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.868 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.868 I llm_load_print_meta: model type       = 1.4B
0.00.049.869 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.869 I llm_load_print_meta: model params     = 1.41 B
0.00.049.870 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.870 I llm_load_print_meta: general.name     = 1.4B
0.00.049.871 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.872 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.872 I llm_load_print_meta: max token length = 1024
0.00.051.836 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.837 I llm_load_tensors: offloading output layer to GPU
0.00.051.837 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.847 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.848 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.759 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.759 I llama_new_context_with_model: n_ctx         = 128
0.00.052.760 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.760 I llama_new_context_with_model: n_batch       = 128
0.00.052.760 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.760 I llama_new_context_with_model: flash_attn    = 0
0.00.052.761 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.761 I llama_new_context_with_model: freq_scale    = 1
0.00.052.761 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.762 I ggml_metal_init: allocating
0.00.052.765 I ggml_metal_init: found device: Apple M4
0.00.052.767 I ggml_metal_init: picking default device: Apple M4
0.00.053.463 I ggml_metal_init: using embedded metal library
0.00.055.730 I ggml_metal_init: GPU name:   Apple M4
0.00.055.731 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.732 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.732 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.732 I ggml_metal_init: simdgroup reduction   = true
0.00.055.732 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.732 I ggml_metal_init: has bfloat            = true
0.00.055.733 I ggml_metal_init: use bfloat            = true
0.00.055.733 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.734 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.335 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.579 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.581 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.596 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.520 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.521 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.521 I llama_new_context_with_model: graph nodes  = 967
0.00.067.521 I llama_new_context_with_model: graph splits = 2
0.00.067.529 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.432 I 
0.00.674.478 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.491 I perplexity: tokenizing the input ..
0.00.682.158 I perplexity: tokenization took 7.665 ms
0.00.682.163 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.822.243 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.823.535 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.823.552 I llama_perf_context_print:        load time =     664.93 ms
0.00.823.553 I llama_perf_context_print: prompt eval time =     139.83 ms /   128 tokens (    1.09 ms per token,   915.40 tokens per second)
0.00.823.554 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.557 I llama_perf_context_print:       total time =     149.12 ms /   129 tokens
0.00.823.911 I ggml_metal_free: deallocating

real	0m0.840s
user	0m0.077s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.284 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.285 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.292 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.293 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.293 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.299 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.299 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.300 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.300 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.300 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.301 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.303 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.275 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.230 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.232 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.232 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.232 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.233 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.233 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.234 I llama_model_loader: - type  f32:  194 tensors
0.00.024.234 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.139 I llm_load_vocab: special tokens cache size = 25
0.00.052.342 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.347 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.347 I llm_load_print_meta: arch             = gptneox
0.00.052.348 I llm_load_print_meta: vocab type       = BPE
0.00.052.348 I llm_load_print_meta: n_vocab          = 50304
0.00.052.348 I llm_load_print_meta: n_merges         = 50009
0.00.052.350 I llm_load_print_meta: vocab_only       = 0
0.00.052.350 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.350 I llm_load_print_meta: n_embd           = 2048
0.00.052.350 I llm_load_print_meta: n_layer          = 24
0.00.052.354 I llm_load_print_meta: n_head           = 16
0.00.052.354 I llm_load_print_meta: n_head_kv        = 16
0.00.052.355 I llm_load_print_meta: n_rot            = 32
0.00.052.355 I llm_load_print_meta: n_swa            = 0
0.00.052.355 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.355 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.357 I llm_load_print_meta: n_gqa            = 1
0.00.052.358 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.358 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.359 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.363 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.363 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.363 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.363 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.364 I llm_load_print_meta: n_ff             = 8192
0.00.052.364 I llm_load_print_meta: n_expert         = 0
0.00.052.364 I llm_load_print_meta: n_expert_used    = 0
0.00.052.364 I llm_load_print_meta: causal attn      = 1
0.00.052.364 I llm_load_print_meta: pooling type     = 0
0.00.052.364 I llm_load_print_meta: rope type        = 2
0.00.052.367 I llm_load_print_meta: rope scaling     = linear
0.00.052.368 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.368 I llm_load_print_meta: freq_scale_train = 1
0.00.052.368 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.368 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.368 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.369 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.369 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.369 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.369 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.369 I llm_load_print_meta: model type       = 1.4B
0.00.052.369 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.370 I llm_load_print_meta: model params     = 1.41 B
0.00.052.370 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.370 I llm_load_print_meta: general.name     = 1.4B
0.00.052.371 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.371 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.371 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.371 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.373 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.373 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.373 I llm_load_print_meta: max token length = 1024
0.00.054.421 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.423 I llm_load_tensors: offloading output layer to GPU
0.00.054.423 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.433 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.435 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.382 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.383 I llama_new_context_with_model: n_ctx         = 128
0.00.055.383 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.383 I llama_new_context_with_model: n_batch       = 128
0.00.055.383 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.383 I llama_new_context_with_model: flash_attn    = 0
0.00.055.384 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.384 I llama_new_context_with_model: freq_scale    = 1
0.00.055.384 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.385 I ggml_metal_init: allocating
0.00.055.390 I ggml_metal_init: found device: Apple M4
0.00.055.392 I ggml_metal_init: picking default device: Apple M4
0.00.056.015 I ggml_metal_init: using embedded metal library
0.00.058.489 I ggml_metal_init: GPU name:   Apple M4
0.00.058.491 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.492 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.492 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.492 I ggml_metal_init: simdgroup reduction   = true
0.00.058.492 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.493 I ggml_metal_init: has bfloat            = true
0.00.058.493 I ggml_metal_init: use bfloat            = true
0.00.058.493 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.494 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.962 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.284 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.286 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.304 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.334 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.335 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.335 I llama_new_context_with_model: graph nodes  = 967
0.00.070.336 I llama_new_context_with_model: graph splits = 2
0.00.070.348 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.349 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.303.933 I 
0.00.303.961 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.303.973 I perplexity: tokenizing the input ..
0.00.312.050 I perplexity: tokenization took 8.075 ms
0.00.312.057 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.451.375 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.452.747 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.452.759 I llama_perf_context_print:        load time =     294.64 ms
0.00.452.760 I llama_perf_context_print: prompt eval time =     139.07 ms /   128 tokens (    1.09 ms per token,   920.39 tokens per second)
0.00.452.761 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.452.761 I llama_perf_context_print:       total time =     148.83 ms /   129 tokens
0.00.453.062 I ggml_metal_free: deallocating

real	0m0.468s
user	0m0.081s
sys	0m0.054s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.290 I build: 4367 (91a3530d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.771 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.249 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.256 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.264 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.264 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.265 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.266 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.266 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.267 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.268 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.268 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.269 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.269 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.270 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.273 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.273 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.273 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.052 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.581 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.584 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.585 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.585 I llama_model_loader: - type  f32:  194 tensors
0.00.048.586 I llama_model_loader: - type  f16:   98 tensors
0.00.075.258 I llm_load_vocab: special tokens cache size = 25
0.00.081.728 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.081.730 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.081.730 I llm_load_print_meta: arch             = gptneox
0.00.081.731 I llm_load_print_meta: vocab type       = BPE
0.00.081.731 I llm_load_print_meta: n_vocab          = 50304
0.00.081.731 I llm_load_print_meta: n_merges         = 50009
0.00.081.731 I llm_load_print_meta: vocab_only       = 0
0.00.081.731 I llm_load_print_meta: n_ctx_train      = 2048
0.00.081.731 I llm_load_print_meta: n_embd           = 2048
0.00.081.732 I llm_load_print_meta: n_layer          = 24
0.00.081.735 I llm_load_print_meta: n_head           = 16
0.00.081.735 I llm_load_print_meta: n_head_kv        = 16
0.00.081.736 I llm_load_print_meta: n_rot            = 32
0.00.081.736 I llm_load_print_meta: n_swa            = 0
0.00.081.736 I llm_load_print_meta: n_embd_head_k    = 128
0.00.081.736 I llm_load_print_meta: n_embd_head_v    = 128
0.00.081.737 I llm_load_print_meta: n_gqa            = 1
0.00.081.738 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.081.739 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.081.739 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.081.742 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.081.742 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.081.742 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.081.742 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.081.743 I llm_load_print_meta: n_ff             = 8192
0.00.081.743 I llm_load_print_meta: n_expert         = 0
0.00.081.743 I llm_load_print_meta: n_expert_used    = 0
0.00.081.743 I llm_load_print_meta: causal attn      = 1
0.00.081.744 I llm_load_print_meta: pooling type     = 0
0.00.081.744 I llm_load_print_meta: rope type        = 2
0.00.081.744 I llm_load_print_meta: rope scaling     = linear
0.00.081.744 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.081.745 I llm_load_print_meta: freq_scale_train = 1
0.00.081.745 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.081.747 I llm_load_print_meta: rope_finetuned   = unknown
0.00.081.747 I llm_load_print_meta: ssm_d_conv       = 0
0.00.081.747 I llm_load_print_meta: ssm_d_inner      = 0
0.00.081.747 I llm_load_print_meta: ssm_d_state      = 0
0.00.081.747 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.081.747 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.081.747 I llm_load_print_meta: model type       = 1.4B
0.00.081.748 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.081.748 I llm_load_print_meta: model params     = 1.41 B
0.00.081.748 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.081.749 I llm_load_print_meta: general.name     = 1.4B
0.00.081.749 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.081.749 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.081.749 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.081.752 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.081.753 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.081.753 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.081.753 I llm_load_print_meta: max token length = 1024
0.00.084.197 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.084.198 I llm_load_tensors: offloading output layer to GPU
0.00.084.198 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.084.208 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.084.209 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.085.156 I llama_new_context_with_model: n_seq_max     = 1
0.00.085.157 I llama_new_context_with_model: n_ctx         = 128
0.00.085.157 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.085.157 I llama_new_context_with_model: n_batch       = 128
0.00.085.157 I llama_new_context_with_model: n_ubatch      = 128
0.00.085.157 I llama_new_context_with_model: flash_attn    = 0
0.00.085.158 I llama_new_context_with_model: freq_base     = 10000.0
0.00.085.158 I llama_new_context_with_model: freq_scale    = 1
0.00.085.158 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.085.159 I ggml_metal_init: allocating
0.00.085.168 I ggml_metal_init: found device: Apple M4
0.00.085.171 I ggml_metal_init: picking default device: Apple M4
0.00.085.773 I ggml_metal_init: using embedded metal library
0.00.088.264 I ggml_metal_init: GPU name:   Apple M4
0.00.088.266 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.266 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.267 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.267 I ggml_metal_init: simdgroup reduction   = true
0.00.088.267 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.267 I ggml_metal_init: has bfloat            = true
0.00.088.268 I ggml_metal_init: use bfloat            = true
0.00.088.268 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.123 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.098.385 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.098.387 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.098.400 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.226 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.099.227 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.099.227 I llama_new_context_with_model: graph nodes  = 967
0.00.099.227 I llama_new_context_with_model: graph splits = 2
0.00.099.239 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.099.240 I 
0.00.099.272 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.099.273 I compute_imatrix: tokenizing the input ..
0.00.106.131 I compute_imatrix: tokenization took 6.857 ms
0.00.106.132 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.666.781 I compute_imatrix: 1.56 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.669.087 I llama_perf_context_print:        load time =    1646.01 ms
0.01.669.088 I llama_perf_context_print: prompt eval time =    1560.01 ms /   128 tokens (   12.19 ms per token,    82.05 tokens per second)
0.01.669.089 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.669.089 I llama_perf_context_print:       total time =    1648.31 ms /   129 tokens
0.01.669.581 I ggml_metal_free: deallocating

real	0m1.854s
user	0m0.166s
sys	0m0.248s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4367 (91a3530d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125607590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125607ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125608250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125608800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125608db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125609360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125609910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125609ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12560a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12560a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12560ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12560b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12560be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12560c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12560ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12560d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12560dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12560e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12560ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12560f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12560f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1256100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125610800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1256110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1256117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125611a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125612090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125612d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125613240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125613500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1256139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125613c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1256144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125614a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125614cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125615190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125615630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125615ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125615f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125616410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1256168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125616d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1256171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125617950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125617f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125618570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125618e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1256194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125619ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12561a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12561a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12561ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12561b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12561bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12561bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12561c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12561c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12561ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12561d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12561d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12561dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12561e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12561e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12561ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12561eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12561f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12561f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12561fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125620140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1256205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125620a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125620f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125621470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1256219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125621f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125622460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1256229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125622f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125623450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1256239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125623ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125624440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125624990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125624ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125625430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125625980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125626420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125626970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125626ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125627410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125627960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125627eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125628400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125628950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125628ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125618b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125629310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125629ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12562a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12562a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12562aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12562b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12562b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12562baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12562bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12562c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12562ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12562cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12562d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12562da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12562dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12562e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12562e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12562edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12562f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12562f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12562fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125630030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1256304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125630970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125630e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1256312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125631750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125631bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125632090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125632530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1256329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125632e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125633310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1256337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125633c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1256340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125634590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125634a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125634ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125635370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125635810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125635cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125636150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1256365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125636a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125636f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1256373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125637870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125637d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1256381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125638650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125638af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125638f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125639430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1256398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125639d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12563a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12563a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12563ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12563aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12563b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12563b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12563bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12563c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12563c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12563cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12563d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12563d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12563d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12563de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12563e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12563e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12563ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12563f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12563f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12563f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12563fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125640330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1256407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125640c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125641110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1256415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125641a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125641ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125642830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125642cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125643170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125643610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125643ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125643f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1256443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125644890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125644d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1256451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125645720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125645c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1256461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125646710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1256469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125646fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1256475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125647c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1256483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125648890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125648b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125649160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125649770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125649f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12564a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12564a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12564ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12564b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12564ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12564bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12564c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12564ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12564cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12564d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12564da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12564df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12564e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12564ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12564ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12564f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12564fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12564ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1256504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1256509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125650f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125651490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1256519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125651f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125652480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1256529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125652f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125653470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1256539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125653f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125654460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1256549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125654f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125655450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1256559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125655ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125656440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125656990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125656ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125657430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125657980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125657ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125658420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125658970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125658ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125659410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125659960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125659eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12565a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12565a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12565aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12565b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12565b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12565be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12565c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12565c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12565ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12565d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12565d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12565de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12565e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12565e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12565ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12565f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12565f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12565fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12565fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125660810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125660cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125661150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1256615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125661a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125661f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1256623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125662920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125663040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125663760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125663e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1256645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125664860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125665050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125665310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125665920 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.144.891 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.895 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x115f04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x115f04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x115f05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x115f058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x115f05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x115f06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x115f065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x115f06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x115f06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x115f07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x115f077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x115f07ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x115f089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x115f09170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x115f09980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x115f0a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x115f0a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x115f0aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x115f0b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x115f0bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x115f0c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x115f0cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x115f0d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x115f0d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x115f0e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x115f0e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x115f0e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x115f0eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x115f0ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x115f0f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x115f0f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x115f0fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x115f101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x115f10470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x115f108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x115f10d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x115f111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x115f11630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x115f11aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x115f11f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x115f12380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x115f127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x115f12c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x115f130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x115f13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x115f139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x115f13e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x115f14290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x115f14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x115f14b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x115f14fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x115f15450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x115f158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x115f15d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x115f161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x115f16610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x115f16b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x115f17080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x115f174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x115f17960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x115f17dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x115f18240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x115f186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x115f18b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x115f18f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x115f19400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x115f19870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x115f19ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x115f1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x115f1a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x115f1aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x115f1aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x115f1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x115f1b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x115f1bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x115f1c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x115f1c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x115f1c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x115f1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x115f1d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x115f1d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x115f1db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x115f1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x115f1e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x115f1e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x115f1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x115f1f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x115f1f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x115f1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x115f1fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x115f202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x115f20760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x115f20bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x115f21040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x115f214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x115f21920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x115f21d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x115f22200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x115f22670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x115f22ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x115f22f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x115f233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x115f23830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x115f23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x115f24110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x115f24580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x115f249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x115f24e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x115f252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x115f25740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x115f25bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x115f26020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x115f26490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x115f26900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x115f26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x115f271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x115f27650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x115f27ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x115f27f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x115f283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x115f28810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x115f28c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x115f290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x115f29560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x115f299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x115f29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x115f2a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x115f2a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x115f2ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x115f2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115f2b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115f2b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x115f2bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x115f2c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115f2c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115f2caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x115f2cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x115f2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x115f2d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115f2dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115f2e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115f2e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115f2e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x115f2ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x115f2f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x115f2f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x115f2fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x115f2ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x115f30450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x115f308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x115f30d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x115f311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x115f31610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x115f31a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x115f31ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x115f32360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x115f327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x115f32c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x115f330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x115f33520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x115f33990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x115f33e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x115f34270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x115f346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x115f34b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x115f34fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x115f35430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x115f358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x115f35d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x115f36180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x115f365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x115f36a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x115f36ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x115f37340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x115f377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x115f37c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x115f38090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x115f38500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x115f38970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x115f38de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x115f39250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x115f396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x115f39b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x115f39fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x115f3a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x115f3a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x115f3acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x115f3b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x115f3b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x115f3ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x115f3beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x115f3c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x115f3c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x115f3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x115f3d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x115f3d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x115f3d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x115f3ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x115f3e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115f3e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x115f3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x115f3ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x115f3f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115f3f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x115f3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x115f40140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x115f405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x115f40b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x115f40fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x115f41420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x115f41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x115f42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x115f424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115f42960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x115f42dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x115f43240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x115f436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x115f43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x115f43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x115f44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x115f44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x115f44ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x115f45150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115f455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x115f45a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x115f45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x115f46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x115f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x115f46bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x115f47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x115f474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x115f47940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x115f47db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x115f48220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x115f48690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x115f48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x115f48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x115f493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x115f49850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x115f49cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x115f4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x115f4a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x115f4aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x115f4ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x115f4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x115f4b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x115f4bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x115f4c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x115f4c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x115f4c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x115f4cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x115f4d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x115f4d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x115f4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x115f4df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x115f4e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x115f4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x115f4eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x115f4f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x115f4f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x115f4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x115f4fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x115f502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x115f50740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x115f50bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x115f51020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x115f51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x115f51900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x115f51d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x115f521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x115f52650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115f52ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x115f52f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115f533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115f53810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115f53c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x115f540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115f54560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x115f549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115f54e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x115f552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x115f55720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115f55b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115f56600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x115f56d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x115f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x115f57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x115f57e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x115f58290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x115f58890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x115f58ea0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x135608480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135608bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135609030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1356094a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x135609910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135609d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13560a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13560a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135606d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1356071c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135607630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13560ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13560b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13560c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13560c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13560cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13560d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13560ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13560e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13560ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13560f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13560fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x135610160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x135610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x135610fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x135611260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x135611520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x135611990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x135611e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x135612270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1356126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x135612c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x135613080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x135613340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1356137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135613c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135614090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135614500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135614970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135614de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135615250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1356156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135615b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135615fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135616410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x135616880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135616cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135617160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1356175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135617a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135617eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135618320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x135618790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135618c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135619070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1356194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135619a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135619f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13561a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13561a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13561aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13561b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13561b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13561b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13561be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13561c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13561c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13561cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13561d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13561d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13561d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13561dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13561e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13561e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13561eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13561ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13561f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13561f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13561fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1356200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x135620560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1356209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x135620e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1356212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x135621720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x135621b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x135622000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x135622470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1356228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x135622d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1356231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135623630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x135623aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135623f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x135624380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1356247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x135624c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1356250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135625540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1356259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135625e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135626290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135626700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x135626fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135627450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1356278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135627d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1356281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x135628610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135628a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135628ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135629360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1356297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135629c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13562a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13562a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13562a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13562ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13562b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13562b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13562bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13562bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13562c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13562c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13562cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13562d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13562d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13562da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13562ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13562e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13562e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13562ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13562f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13562f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13562f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13562fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x135630250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1356306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x135630b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x135630fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x135631410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x135631880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x135631cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x135632160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1356325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x135632a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x135632eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x135633320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135633790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135633c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x135634070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1356344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135634950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x135634dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135635230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1356356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x135635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135635f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1356363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135636860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135636cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135637140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1356375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x135637a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135637e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x135638300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135638770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135638be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135639050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1356394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135639930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x135639da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13563a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13563a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13563aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13563af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13563b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13563b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13563bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13563c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13563c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13563ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13563ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13563d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13563d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13563dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13563e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13563e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13563e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13563ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13563f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13563f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13563fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13563ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1356403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x135640820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x135640c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x135641100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x135641570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1356419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x135641e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1356422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x135642730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x135642ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x135643010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135643480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x135643a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135643e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1356442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135644e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135645100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1356453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x135645830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x135645ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135646110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135646580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1356469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135646e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1356472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135647740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135647bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135648020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135648490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135648900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135648d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1356491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135649650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135649ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135649f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13564a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13564a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13564ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13564b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13564b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13564b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13564be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13564c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13564c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13564cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13564d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13564d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13564d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13564dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13564e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13564e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13564edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13564f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13564f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13564fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13564ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x135650420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x135650890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x135650d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x135651170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1356515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x135651a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x135651ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x135652330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1356527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x135652c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x135653080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1356534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135653960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135653dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135654240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1356546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135654b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135654f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135655400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135655870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135655ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135656150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1356565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135656a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135656ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135657310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135657780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135657bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135658060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1356584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x135658940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135658db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135659820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135659f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13565a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13565ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13565b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13565b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13565bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13565c0c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.801s
user	0m0.295s
sys	0m0.297s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4367 (91a3530d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14600a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14600a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14600ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14600b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14600b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14600bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14600c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14600cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14600d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14600d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14600dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14600dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14600ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14600f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14600fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1460101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1460108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146010ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146011710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146011ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146012600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146012d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146013440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146013ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146014400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1460146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146014cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146015940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146015e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146016140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1460165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1460168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146017130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146017670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146017930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146017dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146018270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146018710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146018bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146019050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1460194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146019990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146019e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14601a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14601a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14601aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14601b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14601bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14601c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14601c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14601cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14601d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14601d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14601df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14601e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14601ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14601f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14601f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14601f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146020120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1460203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146020880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146020d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1460211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146021660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146021b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146021fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146022440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1460228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146022d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146023220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1460236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146023b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1460240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146024600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146024b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1460250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1460255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146025b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146026090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1460265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146026b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146027080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1460275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146027b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146028070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1460285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146028b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146029060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1460295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146029b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14602a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14602a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14602aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14602b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14602b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14602bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14601b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14602bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14602c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14602cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14602d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14602d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14602dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14602e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14602e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14602ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14602f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14602f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14602fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146030170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1460306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146030c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1460310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146031550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1460319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146031e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146032330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1460327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146032c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146033110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1460335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146033a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146033ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146034390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146034830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146034cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146035170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146035610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146035ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146035f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1460363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146036890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146036d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1460371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146037670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146037b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146037fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146038450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1460388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146038d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146039230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1460396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146039b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14603a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14603a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14603a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14603adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14603b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14603b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14603bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14603c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14603c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14603c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14603ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14603d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14603d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14603dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14603e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14603e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14603ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14603eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14603f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14603f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14603fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146040130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1460405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146040a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146040f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1460413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146041850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146041cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146042190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146042630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146042ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146042f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1460438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146043d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1460441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146044690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146044b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146044fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146045470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146045910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146045db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146046250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1460466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146046b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146047030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1460474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146047970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146047e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146048360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1460488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146048e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146049350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146049610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146049c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14604a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14604a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14604b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14604b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14604b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14604bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14604c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14604cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14604d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14604d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14604d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14604e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14604e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14604ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14604f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14604f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14604fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146050110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146050660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146050bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146051100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146051650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146051ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1460520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146052640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146052b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1460530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146053630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146053b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1460540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146054620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146054b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1460550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146055610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146055b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1460560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146056600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146056b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1460570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1460575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146057b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146058090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1460585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146058b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146059080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1460595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146059b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14605a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14605a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14605ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14605b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14605b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14605bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14605c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14605c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14605caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14605d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14605d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14605dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14605e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14605e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14605ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14605f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14605f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14605fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146060010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146060560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146060ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146060f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1460613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146061890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146061d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1460621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146062670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146062b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146062fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146063450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1460638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146063d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146064230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1460646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146064b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146065010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146065560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146065c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1460663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146066ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1460671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1460674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146067c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146067f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146068560 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.095.174 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134f04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134f05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134f056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134f05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134f05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134f06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134f06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134f06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134f07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134f075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134f07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134f08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134f08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134f093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134f09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134f0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134f0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134f0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134f0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134f0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134f0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134f0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134f0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134f0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134f0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134f0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134f0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134f0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134f0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134f0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134f0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134f10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134f106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134f10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134f10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134f11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134f118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134f11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134f12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134f12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134f12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134f12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134f13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134f137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134f13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134f140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134f14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134f14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134f14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134f15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134f156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134f15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134f15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134f16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134f16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134f16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134f17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134f17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134f17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134f18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134f184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134f18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134f18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134f19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134f19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134f19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134f19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134f1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134f1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134f1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134f1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134f1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134f1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134f1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134f1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134f1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134f1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134f1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134f1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134f1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134f1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134f1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134f1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134f1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134f1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134f1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134f1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134f1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134f20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134f20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134f209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134f20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134f212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134f21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134f21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134f22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134f22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134f228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134f22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134f231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134f23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134f23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134f23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134f24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134f24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134f24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134f250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134f25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134f259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134f25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134f262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134f26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134f26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134f26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134f27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134f278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134f27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134f281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134f28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134f28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134f28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134f29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134f297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134f29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134f2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134f2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134f2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134f2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134f2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134f2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134f2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134f2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134f2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134f2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134f2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134f2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134f2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134f2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134f2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134f2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134f2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134f2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134f2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134f2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134f2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134f2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134f30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134f306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134f30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134f30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134f31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134f31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134f31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134f32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134f325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134f32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134f32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134f33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134f337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134f33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134f34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134f344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134f34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134f34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134f35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134f356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134f35b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134f35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134f36400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134f36870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134f36ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134f37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134f375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134f37a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134f37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134f38310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134f38780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134f38bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134f39060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134f394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134f39940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134f39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134f3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134f3a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134f3ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134f3af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134f3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134f3b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134f3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134f3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134f3c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134f3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134f3ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134f3d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134f3d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134f3dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134f3e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134f3e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134f3e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134f3ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134f3f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134f3f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134f3fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134f3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134f403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134f40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134f40dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134f41230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134f416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134f421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134f424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134f42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134f42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134f43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134f434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134f43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134f43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134f44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134f44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134f44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134f44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134f453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134f45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134f45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134f46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134f46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134f46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134f46e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134f472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134f47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134f47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134f48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134f484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134f48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134f48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134f491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134f49660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134f49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134f49f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134f4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134f4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134f4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134f4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134f4b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134f4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134f4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134f4c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134f4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134f4cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134f4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134f4d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134f4d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134f4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134f4e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134f4e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134f4eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134f4ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134f4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134f4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134f4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134f500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134f50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134f509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134f50e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134f512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134f51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134f51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134f51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134f52460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134f528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134f52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134f531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134f53620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134f53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134f53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134f54370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134f547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134f54c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134f550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134f55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134f559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134f55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134f56880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134f56fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134f576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134f57de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134f580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134f58510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134f58b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134f59120 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146024c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1460250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146025550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1460259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146025e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1460262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146026710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146026b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146026ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146027460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1460278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146027eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1460287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146028f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146029700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146029df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14602a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14602abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14602b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14602bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14602c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14602ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14602d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14602d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14602def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14602e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14602e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14602ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14602f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14602f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14602f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14602fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146030270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146030530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1460309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146030e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146031280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1460316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146031b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146031fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146032440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1460328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146032d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146033190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146033600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146033a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146033ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146034350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1460347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146034c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1460350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146035510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146035980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146035df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146036260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1460366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146036b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146036fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146037420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146037890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146037d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146038170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1460385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146038a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146038ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146039330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1460397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146039c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14603a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14603a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14603a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14603add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14603b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14603b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14603bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14603bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14603c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14603c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14603cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14603d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14603d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14603da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14603dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14603e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14603e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14603ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14603f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14603f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14603f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14603fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146040220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146040690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146040b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146040f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1460413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146041850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146041cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146042130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1460425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146042a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146042e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1460432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146043760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146043bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146044040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1460444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146044920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146044d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146045200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146045670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146045ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146045f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1460463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146046830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146046ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146047110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146047580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1460479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146047e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1460482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146048740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146048bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146049020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146049490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146049900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146049d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14604a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14604a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14604aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14604af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14604b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14604b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14604bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14604c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14604c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14604c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14604ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14604d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14604d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14604db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14604e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14604e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14604e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14604ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14604f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14604f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14604faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14604ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146050380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1460507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146050c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1460510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146051540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1460519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146051e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146052290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146052700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146052b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146052fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146053450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1460538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146053d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1460541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146054610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146054a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146054ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146055360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1460557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146055c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1460560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146056520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146056990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146056e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146057270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1460576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146057b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146057fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146058430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1460588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146058d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146059180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1460595f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146059a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146059ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14605a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14605a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14605ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14605b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14605b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14605b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14605bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14605c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14605c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14605cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14605cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14605d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14605d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14605dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14605e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14605e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14605ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14605eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14605f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14605f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14605fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146060070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1460604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146060950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146060dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146061230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1460619b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146061e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146062290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146062700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146062b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146062fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146063450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1460638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146063d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1460641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146064610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146064a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146064ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146065360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1460657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146065c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1460660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146066520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146066990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146066e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146067270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1460676e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146067b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146067fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146068430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14600b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14600ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146009800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14600a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146017810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146017c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1460180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146018560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1460189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146018e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1460192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146019720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146019b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14601a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14601a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14601a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14601ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14601b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14601b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14601baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14601bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14601c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14601c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14601cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14601d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14601d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14601d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14601de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14601e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14601e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14601eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14601efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14601f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14601f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14601fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1460201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146020610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146020a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146020ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146021360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1460217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146021c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1460220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146022520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146022990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146022e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146023270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1460236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146023dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1460244c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1460162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146016990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146016e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14600d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14600d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14600de50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.945s
user	0m0.243s
sys	0m0.152s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
