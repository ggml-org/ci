Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:44 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.637s
user	0m0.701s
sys	0m0.985s
++ nproc
+ make -j10
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target sha256
[  6%] Built target sha1
[  6%] Built target build_info
[  6%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/amx/mmq.cpp.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/cpu-feats-x86.cpp.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 12%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 13%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/amx/amx.cpp.o
[ 13%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 23%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Linking CXX executable ../../bin/llama-simple-chat
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-run
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Built target llava
[ 33%] Linking CXX static library libcommon.a
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Built target llama-simple-chat
[ 34%] Built target llama-run
[ 34%] Built target test-c
[ 34%] Linking CXX shared library libllava_shared.dylib
[ 34%] Built target llama-quantize-stats
[ 34%] Built target llama-simple
[ 34%] Built target common
[ 34%] Built target llava_static
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Built target llava_shared
[ 35%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 39%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Linking CXX executable ../bin/test-chat-template
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Built target test-chat-template
[ 51%] Built target test-grammar-parser
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Built target test-grammar-integration
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 55%] Linking CXX executable ../bin/test-autorelease
[ 55%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 55%] Built target test-llama-grammar
[ 56%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-barrier
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 64%] Built target test-autorelease
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-backend-ops
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-json-schema-to-grammar
[ 64%] Built target test-rope
[ 64%] Built target llama-batched-bench
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gbnf-validator
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-gguf-split
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Built target llama-infill
[ 74%] Built target llama-lookahead
[ 74%] Built target llama-bench
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Built target llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Generating loading.html.hpp
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-stats
[ 84%] Generating index.html.hpp
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Built target llama-cli
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-quantize
[ 85%] Built target llama-parallel
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Built target llama-retrieval
[ 86%] Built target llama-perplexity
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 89%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-save-load-state
[ 96%] Built target llama-speculative
[ 96%] Built target llama-speculative-simple
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Built target llama-tokenize
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Built target llama-minicpmv-cli
[ 97%] Built target llama-export-lora
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 98%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.455s
user	0m5.114s
sys	0m8.598s

main: quantize time =  4798.67 ms
main:    total time =  4798.67 ms

main: quantize time =  2203.33 ms
main:    total time =  2203.33 ms

main: quantize time =  1896.84 ms
main:    total time =  1896.84 ms

main: quantize time =  1924.78 ms
main:    total time =  1924.78 ms

main: quantize time =  2588.33 ms
main:    total time =  2588.33 ms

main: quantize time =  5271.56 ms
main:    total time =  5271.56 ms

main: quantize time =  5738.24 ms
main:    total time =  5738.24 ms

main: quantize time =  6827.76 ms
main:    total time =  6827.76 ms

main: quantize time =  5806.81 ms
main:    total time =  5806.81 ms

main: quantize time =  4564.30 ms
main:    total time =  4564.30 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.118 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.257 I main: llama backend init
0.00.000.268 I main: load the model and apply lora adapter, if any
0.00.039.794 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.050.977 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.050.993 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.050.996 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.051.001 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.051.001 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.051.002 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.051.002 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.051.018 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.051.019 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.051.019 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.051.020 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.051.020 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.051.021 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.051.021 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.051.027 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.051.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.051.028 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.060.035 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.062.391 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.070.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.070.468 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.070.469 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.070.470 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.070.470 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.070.471 I llama_model_loader: - type  f32:  194 tensors
0.00.070.472 I llama_model_loader: - type  f16:   98 tensors
0.00.101.614 I llm_load_vocab: special tokens cache size = 25
0.00.108.345 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.108.348 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.108.348 I llm_load_print_meta: arch             = gptneox
0.00.108.349 I llm_load_print_meta: vocab type       = BPE
0.00.108.349 I llm_load_print_meta: n_vocab          = 50304
0.00.108.349 I llm_load_print_meta: n_merges         = 50009
0.00.108.349 I llm_load_print_meta: vocab_only       = 0
0.00.108.349 I llm_load_print_meta: n_ctx_train      = 2048
0.00.108.349 I llm_load_print_meta: n_embd           = 2048
0.00.108.350 I llm_load_print_meta: n_layer          = 24
0.00.108.352 I llm_load_print_meta: n_head           = 16
0.00.108.353 I llm_load_print_meta: n_head_kv        = 16
0.00.108.353 I llm_load_print_meta: n_rot            = 32
0.00.108.353 I llm_load_print_meta: n_swa            = 0
0.00.108.354 I llm_load_print_meta: n_embd_head_k    = 128
0.00.108.354 I llm_load_print_meta: n_embd_head_v    = 128
0.00.108.354 I llm_load_print_meta: n_gqa            = 1
0.00.108.355 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.108.357 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.108.357 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.108.358 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.108.358 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.108.358 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.108.358 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.108.359 I llm_load_print_meta: n_ff             = 8192
0.00.108.359 I llm_load_print_meta: n_expert         = 0
0.00.108.359 I llm_load_print_meta: n_expert_used    = 0
0.00.108.359 I llm_load_print_meta: causal attn      = 1
0.00.108.359 I llm_load_print_meta: pooling type     = 0
0.00.108.360 I llm_load_print_meta: rope type        = 2
0.00.108.360 I llm_load_print_meta: rope scaling     = linear
0.00.108.360 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.108.360 I llm_load_print_meta: freq_scale_train = 1
0.00.108.361 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.108.361 I llm_load_print_meta: rope_finetuned   = unknown
0.00.108.361 I llm_load_print_meta: ssm_d_conv       = 0
0.00.108.361 I llm_load_print_meta: ssm_d_inner      = 0
0.00.108.361 I llm_load_print_meta: ssm_d_state      = 0
0.00.108.363 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.108.363 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.108.375 I llm_load_print_meta: model type       = 1.4B
0.00.108.375 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.108.376 I llm_load_print_meta: model params     = 1.41 B
0.00.108.376 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.108.376 I llm_load_print_meta: general.name     = 1.4B
0.00.108.377 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.108.377 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.108.377 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.108.377 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.108.377 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.108.378 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.108.378 I llm_load_print_meta: max token length = 1024
0.00.110.923 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.110.924 I llm_load_tensors: offloading output layer to GPU
0.00.110.924 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.110.942 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.110.943 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.111.884 I llama_new_context_with_model: n_seq_max     = 1
0.00.111.885 I llama_new_context_with_model: n_ctx         = 2048
0.00.111.885 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.111.886 I llama_new_context_with_model: n_batch       = 2048
0.00.111.886 I llama_new_context_with_model: n_ubatch      = 512
0.00.111.886 I llama_new_context_with_model: flash_attn    = 0
0.00.111.886 I llama_new_context_with_model: freq_base     = 10000.0
0.00.111.887 I llama_new_context_with_model: freq_scale    = 1
0.00.111.887 I ggml_metal_init: allocating
0.00.111.890 I ggml_metal_init: found device: Apple M4
0.00.111.892 I ggml_metal_init: picking default device: Apple M4
0.00.112.527 I ggml_metal_init: using embedded metal library
0.00.122.424 I ggml_metal_init: GPU name:   Apple M4
0.00.122.426 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.122.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.122.426 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.122.427 I ggml_metal_init: simdgroup reduction   = true
0.00.122.427 I ggml_metal_init: simdgroup matrix mul. = true
0.00.122.427 I ggml_metal_init: has bfloat            = true
0.00.122.427 I ggml_metal_init: use bfloat            = true
0.00.122.428 I ggml_metal_init: hasUnifiedMemory      = true
0.00.122.428 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.164.733 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.164.739 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.164.758 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.165.708 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.165.710 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.165.710 I llama_new_context_with_model: graph nodes  = 967
0.00.165.710 I llama_new_context_with_model: graph splits = 2
0.00.165.732 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.242.213 I main: llama threadpool init, n_threads = 4
0.00.242.247 I 
0.00.242.281 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.242.282 I 
0.00.242.367 I sampler seed: 1234
0.00.242.372 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.242.395 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.242.397 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.242.397 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.107.910 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.02.107.911 I llama_perf_context_print:        load time =     202.41 ms
0.02.107.911 I llama_perf_context_print: prompt eval time =      54.60 ms /     7 tokens (    7.80 ms per token,   128.20 tokens per second)
0.02.107.913 I llama_perf_context_print:        eval time =    1807.92 ms /    63 runs   (   28.70 ms per token,    34.85 tokens per second)
0.02.107.914 I llama_perf_context_print:       total time =    1865.70 ms /    70 tokens
0.02.108.107 I ggml_metal_free: deallocating

real	0m2.397s
user	0m0.145s
sys	0m0.100s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.614 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.455 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.461 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.463 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.469 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.469 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.470 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.483 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.484 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.484 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.485 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.488 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.434 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.498 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.493 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.495 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.495 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.495 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.496 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.496 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.497 I llama_model_loader: - type  f32:  194 tensors
0.00.037.497 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.454 I llm_load_vocab: special tokens cache size = 25
0.00.069.359 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.364 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.364 I llm_load_print_meta: arch             = gptneox
0.00.069.364 I llm_load_print_meta: vocab type       = BPE
0.00.069.365 I llm_load_print_meta: n_vocab          = 50304
0.00.069.365 I llm_load_print_meta: n_merges         = 50009
0.00.069.367 I llm_load_print_meta: vocab_only       = 0
0.00.069.367 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.367 I llm_load_print_meta: n_embd           = 2048
0.00.069.368 I llm_load_print_meta: n_layer          = 24
0.00.069.372 I llm_load_print_meta: n_head           = 16
0.00.069.373 I llm_load_print_meta: n_head_kv        = 16
0.00.069.373 I llm_load_print_meta: n_rot            = 32
0.00.069.374 I llm_load_print_meta: n_swa            = 0
0.00.069.374 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.374 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.375 I llm_load_print_meta: n_gqa            = 1
0.00.069.375 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.376 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.377 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.377 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.377 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.377 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.378 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.378 I llm_load_print_meta: n_ff             = 8192
0.00.069.379 I llm_load_print_meta: n_expert         = 0
0.00.069.379 I llm_load_print_meta: n_expert_used    = 0
0.00.069.379 I llm_load_print_meta: causal attn      = 1
0.00.069.379 I llm_load_print_meta: pooling type     = 0
0.00.069.381 I llm_load_print_meta: rope type        = 2
0.00.069.381 I llm_load_print_meta: rope scaling     = linear
0.00.069.382 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.382 I llm_load_print_meta: freq_scale_train = 1
0.00.069.382 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.382 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.382 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.382 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.382 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.383 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.383 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.397 I llm_load_print_meta: model type       = 1.4B
0.00.069.397 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.397 I llm_load_print_meta: model params     = 1.41 B
0.00.069.398 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.398 I llm_load_print_meta: general.name     = 1.4B
0.00.069.398 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.399 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.399 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.399 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.399 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.069.399 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.400 I llm_load_print_meta: max token length = 1024
0.00.071.974 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.974 I llm_load_tensors: offloading output layer to GPU
0.00.071.974 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.986 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.987 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.073.028 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.029 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.029 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.030 I llama_new_context_with_model: n_batch       = 2048
0.00.073.030 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.030 I llama_new_context_with_model: flash_attn    = 0
0.00.073.030 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.031 I llama_new_context_with_model: freq_scale    = 1
0.00.073.031 I ggml_metal_init: allocating
0.00.073.037 I ggml_metal_init: found device: Apple M4
0.00.073.039 I ggml_metal_init: picking default device: Apple M4
0.00.073.760 I ggml_metal_init: using embedded metal library
0.00.076.720 I ggml_metal_init: GPU name:   Apple M4
0.00.076.722 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.723 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.723 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.723 I ggml_metal_init: simdgroup reduction   = true
0.00.076.723 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.724 I ggml_metal_init: has bfloat            = true
0.00.076.724 I ggml_metal_init: use bfloat            = true
0.00.076.724 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.725 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.115.567 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.115.577 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.115.605 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.116.791 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.116.796 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.116.796 I llama_new_context_with_model: graph nodes  = 967
0.00.116.797 I llama_new_context_with_model: graph splits = 2
0.00.116.813 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.284.281 I main: llama threadpool init, n_threads = 4
0.01.284.356 I 
0.01.284.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.284.425 I 
0.01.284.945 I sampler seed: 1234
0.01.284.954 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.285.024 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.285.029 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.285.029 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.388.416 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.02.388.417 I llama_perf_context_print:        load time =    1274.65 ms
0.02.388.419 I llama_perf_context_print: prompt eval time =      50.56 ms /     7 tokens (    7.22 ms per token,   138.44 tokens per second)
0.02.388.421 I llama_perf_context_print:        eval time =    1049.97 ms /    63 runs   (   16.67 ms per token,    60.00 tokens per second)
0.02.388.422 I llama_perf_context_print:       total time =    1104.15 ms /    70 tokens
0.02.388.601 I ggml_metal_free: deallocating

real	0m2.406s
user	0m0.127s
sys	0m0.247s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.016.329 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.307 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.312 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.315 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.316 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.316 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.318 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.327 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.327 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.327 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.328 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.328 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.328 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.329 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.334 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.334 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.334 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.419 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.811 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.910 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.911 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.912 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.912 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.912 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.913 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.913 I llama_model_loader: - type  f32:  194 tensors
0.00.046.914 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.914 I llama_model_loader: - type q6_K:    1 tensors
0.00.079.936 I llm_load_vocab: special tokens cache size = 25
0.00.089.732 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.736 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.737 I llm_load_print_meta: arch             = gptneox
0.00.089.737 I llm_load_print_meta: vocab type       = BPE
0.00.089.738 I llm_load_print_meta: n_vocab          = 50304
0.00.089.738 I llm_load_print_meta: n_merges         = 50009
0.00.089.738 I llm_load_print_meta: vocab_only       = 0
0.00.089.741 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.742 I llm_load_print_meta: n_embd           = 2048
0.00.089.742 I llm_load_print_meta: n_layer          = 24
0.00.089.747 I llm_load_print_meta: n_head           = 16
0.00.089.748 I llm_load_print_meta: n_head_kv        = 16
0.00.089.748 I llm_load_print_meta: n_rot            = 32
0.00.089.748 I llm_load_print_meta: n_swa            = 0
0.00.089.748 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.748 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.749 I llm_load_print_meta: n_gqa            = 1
0.00.089.750 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.751 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.752 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.752 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.752 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.753 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.753 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.754 I llm_load_print_meta: n_ff             = 8192
0.00.089.754 I llm_load_print_meta: n_expert         = 0
0.00.089.754 I llm_load_print_meta: n_expert_used    = 0
0.00.089.756 I llm_load_print_meta: causal attn      = 1
0.00.089.758 I llm_load_print_meta: pooling type     = 0
0.00.089.758 I llm_load_print_meta: rope type        = 2
0.00.089.758 I llm_load_print_meta: rope scaling     = linear
0.00.089.759 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.759 I llm_load_print_meta: freq_scale_train = 1
0.00.089.760 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.760 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.760 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.760 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.760 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.760 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.760 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.774 I llm_load_print_meta: model type       = 1.4B
0.00.089.775 I llm_load_print_meta: model ftype      = Q4_0
0.00.089.775 I llm_load_print_meta: model params     = 1.41 B
0.00.089.776 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.089.778 I llm_load_print_meta: general.name     = 1.4B
0.00.089.778 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.779 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.779 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.779 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.779 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.780 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.780 I llm_load_print_meta: max token length = 1024
0.00.092.619 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.619 I llm_load_tensors: offloading output layer to GPU
0.00.092.619 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.632 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.092.634 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.094.068 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.070 I llama_new_context_with_model: n_ctx         = 2048
0.00.094.070 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.094.070 I llama_new_context_with_model: n_batch       = 2048
0.00.094.070 I llama_new_context_with_model: n_ubatch      = 512
0.00.094.071 I llama_new_context_with_model: flash_attn    = 0
0.00.094.071 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.072 I llama_new_context_with_model: freq_scale    = 1
0.00.094.072 I ggml_metal_init: allocating
0.00.094.080 I ggml_metal_init: found device: Apple M4
0.00.094.083 I ggml_metal_init: picking default device: Apple M4
0.00.094.992 I ggml_metal_init: using embedded metal library
0.00.098.535 I ggml_metal_init: GPU name:   Apple M4
0.00.098.537 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.537 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.538 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.538 I ggml_metal_init: simdgroup reduction   = true
0.00.098.538 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.538 I ggml_metal_init: has bfloat            = true
0.00.098.540 I ggml_metal_init: use bfloat            = true
0.00.098.541 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.542 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.133.183 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.133.190 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.133.215 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.134.281 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.134.282 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.134.283 I llama_new_context_with_model: graph nodes  = 967
0.00.134.283 I llama_new_context_with_model: graph splits = 2
0.00.134.298 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.844.148 I main: llama threadpool init, n_threads = 4
0.00.844.226 I 
0.00.844.295 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.844.297 I 
0.00.844.890 I sampler seed: 1234
0.00.844.895 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.844.961 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.844.963 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.844.963 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.530.198 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.530.199 I llama_perf_context_print:        load time =     827.81 ms
0.01.530.200 I llama_perf_context_print: prompt eval time =      47.91 ms /     7 tokens (    6.84 ms per token,   146.09 tokens per second)
0.01.530.201 I llama_perf_context_print:        eval time =     634.36 ms /    63 runs   (   10.07 ms per token,    99.31 tokens per second)
0.01.530.201 I llama_perf_context_print:       total time =     686.05 ms /    70 tokens
0.01.530.400 I ggml_metal_free: deallocating

real	0m1.558s
user	0m0.138s
sys	0m0.188s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.016.226 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.302 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.029.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.309 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.310 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.323 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.323 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.323 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.324 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.324 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.324 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.325 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.327 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.862 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.374 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.700 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.702 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.702 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.703 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.703 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.703 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.041.704 I llama_model_loader: - type  f32:  194 tensors
0.00.041.705 I llama_model_loader: - type q4_1:   97 tensors
0.00.041.705 I llama_model_loader: - type q6_K:    1 tensors
0.00.079.996 I llm_load_vocab: special tokens cache size = 25
0.00.089.254 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.258 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.258 I llm_load_print_meta: arch             = gptneox
0.00.089.259 I llm_load_print_meta: vocab type       = BPE
0.00.089.259 I llm_load_print_meta: n_vocab          = 50304
0.00.089.259 I llm_load_print_meta: n_merges         = 50009
0.00.089.260 I llm_load_print_meta: vocab_only       = 0
0.00.089.260 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.260 I llm_load_print_meta: n_embd           = 2048
0.00.089.260 I llm_load_print_meta: n_layer          = 24
0.00.089.264 I llm_load_print_meta: n_head           = 16
0.00.089.265 I llm_load_print_meta: n_head_kv        = 16
0.00.089.265 I llm_load_print_meta: n_rot            = 32
0.00.089.265 I llm_load_print_meta: n_swa            = 0
0.00.089.265 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.265 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.266 I llm_load_print_meta: n_gqa            = 1
0.00.089.267 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.268 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.269 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.269 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.269 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.270 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.270 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.271 I llm_load_print_meta: n_ff             = 8192
0.00.089.271 I llm_load_print_meta: n_expert         = 0
0.00.089.271 I llm_load_print_meta: n_expert_used    = 0
0.00.089.273 I llm_load_print_meta: causal attn      = 1
0.00.089.276 I llm_load_print_meta: pooling type     = 0
0.00.089.276 I llm_load_print_meta: rope type        = 2
0.00.089.276 I llm_load_print_meta: rope scaling     = linear
0.00.089.277 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.277 I llm_load_print_meta: freq_scale_train = 1
0.00.089.277 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.277 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.278 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.278 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.278 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.278 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.278 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.284 I llm_load_print_meta: model type       = 1.4B
0.00.089.284 I llm_load_print_meta: model ftype      = Q4_1
0.00.089.285 I llm_load_print_meta: model params     = 1.41 B
0.00.089.287 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.089.287 I llm_load_print_meta: general.name     = 1.4B
0.00.089.287 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.288 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.288 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.288 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.291 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.291 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.291 I llm_load_print_meta: max token length = 1024
0.00.091.678 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.678 I llm_load_tensors: offloading output layer to GPU
0.00.091.678 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.684 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.091.685 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.093.070 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.071 I llama_new_context_with_model: n_ctx         = 2048
0.00.093.072 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.093.072 I llama_new_context_with_model: n_batch       = 2048
0.00.093.072 I llama_new_context_with_model: n_ubatch      = 512
0.00.093.072 I llama_new_context_with_model: flash_attn    = 0
0.00.093.073 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.073 I llama_new_context_with_model: freq_scale    = 1
0.00.093.074 I ggml_metal_init: allocating
0.00.093.083 I ggml_metal_init: found device: Apple M4
0.00.093.088 I ggml_metal_init: picking default device: Apple M4
0.00.093.828 I ggml_metal_init: using embedded metal library
0.00.097.102 I ggml_metal_init: GPU name:   Apple M4
0.00.097.104 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.104 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.105 I ggml_metal_init: simdgroup reduction   = true
0.00.097.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.105 I ggml_metal_init: has bfloat            = true
0.00.097.107 I ggml_metal_init: use bfloat            = true
0.00.097.108 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.128.524 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.128.533 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.128.555 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.129.488 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.129.489 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.129.489 I llama_new_context_with_model: graph nodes  = 967
0.00.129.490 I llama_new_context_with_model: graph splits = 2
0.00.129.504 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.367 I main: llama threadpool init, n_threads = 4
0.00.714.449 I 
0.00.714.514 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.714.516 I 
0.00.715.082 I sampler seed: 1234
0.00.715.089 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.715.161 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.715.167 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.715.167 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.452.107 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.01.452.108 I llama_perf_context_print:        load time =     698.13 ms
0.01.452.108 I llama_perf_context_print: prompt eval time =      49.51 ms /     7 tokens (    7.07 ms per token,   141.39 tokens per second)
0.01.452.110 I llama_perf_context_print:        eval time =     684.64 ms /    63 runs   (   10.87 ms per token,    92.02 tokens per second)
0.01.452.111 I llama_perf_context_print:       total time =     737.75 ms /    70 tokens
0.01.452.320 I ggml_metal_free: deallocating

real	0m1.493s
user	0m0.150s
sys	0m0.178s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.971 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.126 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.023.130 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.142 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.142 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.143 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.143 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.143 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.144 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.144 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.147 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.081 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.146 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.073 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.073 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.073 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.074 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.074 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.032.075 I llama_model_loader: - type  f32:  194 tensors
0.00.032.075 I llama_model_loader: - type q5_0:   97 tensors
0.00.032.075 I llama_model_loader: - type q6_K:    1 tensors
0.00.053.342 I llm_load_vocab: special tokens cache size = 25
0.00.059.458 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.460 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.461 I llm_load_print_meta: arch             = gptneox
0.00.059.461 I llm_load_print_meta: vocab type       = BPE
0.00.059.461 I llm_load_print_meta: n_vocab          = 50304
0.00.059.461 I llm_load_print_meta: n_merges         = 50009
0.00.059.462 I llm_load_print_meta: vocab_only       = 0
0.00.059.462 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.462 I llm_load_print_meta: n_embd           = 2048
0.00.059.462 I llm_load_print_meta: n_layer          = 24
0.00.059.465 I llm_load_print_meta: n_head           = 16
0.00.059.466 I llm_load_print_meta: n_head_kv        = 16
0.00.059.466 I llm_load_print_meta: n_rot            = 32
0.00.059.467 I llm_load_print_meta: n_swa            = 0
0.00.059.467 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.467 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.468 I llm_load_print_meta: n_gqa            = 1
0.00.059.468 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.469 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.470 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.470 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.471 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.471 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.471 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.472 I llm_load_print_meta: n_ff             = 8192
0.00.059.472 I llm_load_print_meta: n_expert         = 0
0.00.059.472 I llm_load_print_meta: n_expert_used    = 0
0.00.059.474 I llm_load_print_meta: causal attn      = 1
0.00.059.476 I llm_load_print_meta: pooling type     = 0
0.00.059.476 I llm_load_print_meta: rope type        = 2
0.00.059.476 I llm_load_print_meta: rope scaling     = linear
0.00.059.476 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.477 I llm_load_print_meta: freq_scale_train = 1
0.00.059.477 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.477 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.477 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.477 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.477 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.478 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.478 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.490 I llm_load_print_meta: model type       = 1.4B
0.00.059.490 I llm_load_print_meta: model ftype      = Q5_0
0.00.059.491 I llm_load_print_meta: model params     = 1.41 B
0.00.059.491 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.059.491 I llm_load_print_meta: general.name     = 1.4B
0.00.059.491 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.492 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.492 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.492 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.492 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.492 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.492 I llm_load_print_meta: max token length = 1024
0.00.061.476 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.477 I llm_load_tensors: offloading output layer to GPU
0.00.061.477 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.487 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.061.488 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.062.438 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.439 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.439 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.439 I llama_new_context_with_model: n_batch       = 2048
0.00.062.439 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.439 I llama_new_context_with_model: flash_attn    = 0
0.00.062.440 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.440 I llama_new_context_with_model: freq_scale    = 1
0.00.062.441 I ggml_metal_init: allocating
0.00.062.448 I ggml_metal_init: found device: Apple M4
0.00.062.450 I ggml_metal_init: picking default device: Apple M4
0.00.063.014 I ggml_metal_init: using embedded metal library
0.00.065.344 I ggml_metal_init: GPU name:   Apple M4
0.00.065.345 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.346 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.346 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.346 I ggml_metal_init: simdgroup reduction   = true
0.00.065.346 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.347 I ggml_metal_init: has bfloat            = true
0.00.065.347 I ggml_metal_init: use bfloat            = true
0.00.065.347 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.349 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.915 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.921 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.941 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.838 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.839 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.840 I llama_new_context_with_model: graph nodes  = 967
0.00.095.840 I llama_new_context_with_model: graph splits = 2
0.00.095.849 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.905.266 I main: llama threadpool init, n_threads = 4
0.00.905.303 I 
0.00.905.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.905.331 I 
0.00.905.574 I sampler seed: 1234
0.00.905.578 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.905.589 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.905.590 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.905.590 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.697.366 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.697.366 I llama_perf_context_print:        load time =     896.29 ms
0.01.697.367 I llama_perf_context_print: prompt eval time =      43.29 ms /     7 tokens (    6.18 ms per token,   161.69 tokens per second)
0.01.697.368 I llama_perf_context_print:        eval time =     745.54 ms /    63 runs   (   11.83 ms per token,    84.50 tokens per second)
0.01.697.368 I llama_perf_context_print:       total time =     792.10 ms /    70 tokens
0.01.697.548 I ggml_metal_free: deallocating

real	0m1.715s
user	0m0.110s
sys	0m0.164s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.016.506 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.698 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.033.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.709 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.710 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.712 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.712 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.725 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.726 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.726 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.727 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.727 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.727 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.728 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.729 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.730 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.730 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.275 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.836 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.642 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.644 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.645 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.645 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.646 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.046.646 I llama_model_loader: - type  f32:  194 tensors
0.00.046.647 I llama_model_loader: - type q5_1:   97 tensors
0.00.046.647 I llama_model_loader: - type q6_K:    1 tensors
0.00.086.172 I llm_load_vocab: special tokens cache size = 25
0.00.095.028 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.031 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.032 I llm_load_print_meta: arch             = gptneox
0.00.095.032 I llm_load_print_meta: vocab type       = BPE
0.00.095.032 I llm_load_print_meta: n_vocab          = 50304
0.00.095.033 I llm_load_print_meta: n_merges         = 50009
0.00.095.033 I llm_load_print_meta: vocab_only       = 0
0.00.095.033 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.033 I llm_load_print_meta: n_embd           = 2048
0.00.095.033 I llm_load_print_meta: n_layer          = 24
0.00.095.037 I llm_load_print_meta: n_head           = 16
0.00.095.038 I llm_load_print_meta: n_head_kv        = 16
0.00.095.038 I llm_load_print_meta: n_rot            = 32
0.00.095.038 I llm_load_print_meta: n_swa            = 0
0.00.095.039 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.039 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.039 I llm_load_print_meta: n_gqa            = 1
0.00.095.040 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.042 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.043 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.043 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.044 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.044 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.044 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.045 I llm_load_print_meta: n_ff             = 8192
0.00.095.045 I llm_load_print_meta: n_expert         = 0
0.00.095.045 I llm_load_print_meta: n_expert_used    = 0
0.00.095.045 I llm_load_print_meta: causal attn      = 1
0.00.095.046 I llm_load_print_meta: pooling type     = 0
0.00.095.046 I llm_load_print_meta: rope type        = 2
0.00.095.046 I llm_load_print_meta: rope scaling     = linear
0.00.095.047 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.047 I llm_load_print_meta: freq_scale_train = 1
0.00.095.047 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.047 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.048 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.048 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.048 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.050 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.050 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.063 I llm_load_print_meta: model type       = 1.4B
0.00.095.063 I llm_load_print_meta: model ftype      = Q5_1
0.00.095.064 I llm_load_print_meta: model params     = 1.41 B
0.00.095.064 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.095.064 I llm_load_print_meta: general.name     = 1.4B
0.00.095.065 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.066 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.066 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.067 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.067 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.095.067 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.068 I llm_load_print_meta: max token length = 1024
0.00.097.623 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.624 I llm_load_tensors: offloading output layer to GPU
0.00.097.624 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.635 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.097.637 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.098.788 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.789 I llama_new_context_with_model: n_ctx         = 2048
0.00.098.790 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.098.790 I llama_new_context_with_model: n_batch       = 2048
0.00.098.790 I llama_new_context_with_model: n_ubatch      = 512
0.00.098.790 I llama_new_context_with_model: flash_attn    = 0
0.00.098.791 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.791 I llama_new_context_with_model: freq_scale    = 1
0.00.098.792 I ggml_metal_init: allocating
0.00.098.794 I ggml_metal_init: found device: Apple M4
0.00.098.797 I ggml_metal_init: picking default device: Apple M4
0.00.099.524 I ggml_metal_init: using embedded metal library
0.00.102.648 I ggml_metal_init: GPU name:   Apple M4
0.00.102.650 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.102.650 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.102.650 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.102.651 I ggml_metal_init: simdgroup reduction   = true
0.00.102.651 I ggml_metal_init: simdgroup matrix mul. = true
0.00.102.651 I ggml_metal_init: has bfloat            = true
0.00.102.651 I ggml_metal_init: use bfloat            = true
0.00.102.652 I ggml_metal_init: hasUnifiedMemory      = true
0.00.102.652 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.134.694 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.134.701 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.134.722 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.135.699 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.135.700 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.135.700 I llama_new_context_with_model: graph nodes  = 967
0.00.135.701 I llama_new_context_with_model: graph splits = 2
0.00.135.716 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.003.705 I main: llama threadpool init, n_threads = 4
0.01.003.791 I 
0.01.003.851 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.003.853 I 
0.01.004.439 I sampler seed: 1234
0.01.004.449 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.004.484 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.004.486 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.004.486 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.863.994 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.01.863.994 I llama_perf_context_print:        load time =     987.19 ms
0.01.863.996 I llama_perf_context_print: prompt eval time =      53.72 ms /     7 tokens (    7.67 ms per token,   130.30 tokens per second)
0.01.863.998 I llama_perf_context_print:        eval time =     802.72 ms /    63 runs   (   12.74 ms per token,    78.48 tokens per second)
0.01.863.998 I llama_perf_context_print:       total time =     860.30 ms /    70 tokens
0.01.864.205 I ggml_metal_free: deallocating

real	0m1.905s
user	0m0.150s
sys	0m0.207s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.428 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.056 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.062 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.062 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.063 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.063 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.064 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.070 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.070 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.071 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.071 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.071 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.072 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.072 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.074 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.074 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.074 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.952 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.755 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.756 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.757 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.757 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.758 I llama_model_loader: - type  f32:  194 tensors
0.00.023.758 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.759 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.759 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.184 I llm_load_vocab: special tokens cache size = 25
0.00.050.121 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.123 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.123 I llm_load_print_meta: arch             = gptneox
0.00.050.124 I llm_load_print_meta: vocab type       = BPE
0.00.050.124 I llm_load_print_meta: n_vocab          = 50304
0.00.050.124 I llm_load_print_meta: n_merges         = 50009
0.00.050.124 I llm_load_print_meta: vocab_only       = 0
0.00.050.125 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.125 I llm_load_print_meta: n_embd           = 2048
0.00.050.125 I llm_load_print_meta: n_layer          = 24
0.00.050.127 I llm_load_print_meta: n_head           = 16
0.00.050.129 I llm_load_print_meta: n_head_kv        = 16
0.00.050.129 I llm_load_print_meta: n_rot            = 32
0.00.050.129 I llm_load_print_meta: n_swa            = 0
0.00.050.129 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.129 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.131 I llm_load_print_meta: n_gqa            = 1
0.00.050.132 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.133 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.133 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.133 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.134 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.135 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.135 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.135 I llm_load_print_meta: n_ff             = 8192
0.00.050.136 I llm_load_print_meta: n_expert         = 0
0.00.050.136 I llm_load_print_meta: n_expert_used    = 0
0.00.050.136 I llm_load_print_meta: causal attn      = 1
0.00.050.138 I llm_load_print_meta: pooling type     = 0
0.00.050.138 I llm_load_print_meta: rope type        = 2
0.00.050.138 I llm_load_print_meta: rope scaling     = linear
0.00.050.138 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.139 I llm_load_print_meta: freq_scale_train = 1
0.00.050.139 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.139 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.139 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.139 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.139 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.139 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.140 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.151 I llm_load_print_meta: model type       = 1.4B
0.00.050.151 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.151 I llm_load_print_meta: model params     = 1.41 B
0.00.050.152 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.152 I llm_load_print_meta: general.name     = 1.4B
0.00.050.152 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.152 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.153 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.153 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.153 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.153 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.153 I llm_load_print_meta: max token length = 1024
0.00.051.700 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.700 I llm_load_tensors: offloading output layer to GPU
0.00.051.701 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.711 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.712 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.517 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.518 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.518 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.519 I llama_new_context_with_model: n_batch       = 2048
0.00.052.519 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.519 I llama_new_context_with_model: flash_attn    = 0
0.00.052.520 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.520 I llama_new_context_with_model: freq_scale    = 1
0.00.052.520 I ggml_metal_init: allocating
0.00.052.524 I ggml_metal_init: found device: Apple M4
0.00.052.526 I ggml_metal_init: picking default device: Apple M4
0.00.053.090 I ggml_metal_init: using embedded metal library
0.00.055.517 I ggml_metal_init: GPU name:   Apple M4
0.00.055.519 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.519 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.520 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.520 I ggml_metal_init: simdgroup reduction   = true
0.00.055.520 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.520 I ggml_metal_init: has bfloat            = true
0.00.055.520 I ggml_metal_init: use bfloat            = true
0.00.055.521 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.521 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.132 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.137 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.154 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.136 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.137 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.138 I llama_new_context_with_model: graph nodes  = 967
0.00.085.138 I llama_new_context_with_model: graph splits = 2
0.00.085.153 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.746 I main: llama threadpool init, n_threads = 4
0.00.519.828 I 
0.00.519.863 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.519.863 I 
0.00.520.102 I sampler seed: 1234
0.00.520.107 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.520.134 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.520.135 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.520.135 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.200.808 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.200.809 I llama_perf_context_print:        load time =     511.31 ms
0.01.200.809 I llama_perf_context_print: prompt eval time =      35.85 ms /     7 tokens (    5.12 ms per token,   195.25 tokens per second)
0.01.200.812 I llama_perf_context_print:        eval time =     641.86 ms /    63 runs   (   10.19 ms per token,    98.15 tokens per second)
0.01.200.813 I llama_perf_context_print:       total time =     681.07 ms /    70 tokens
0.01.201.000 I ggml_metal_free: deallocating

real	0m1.217s
user	0m0.110s
sys	0m0.121s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.013.345 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.378 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.031.383 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.385 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.386 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.386 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.387 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.399 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.399 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.400 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.400 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.401 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.401 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.404 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.404 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.404 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.448 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.854 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.846 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.849 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.042.850 I llama_model_loader: - type  f32:  194 tensors
0.00.042.850 I llama_model_loader: - type q3_K:   25 tensors
0.00.042.851 I llama_model_loader: - type q4_K:   71 tensors
0.00.042.851 I llama_model_loader: - type q5_K:    1 tensors
0.00.042.851 I llama_model_loader: - type q6_K:    1 tensors
0.00.077.592 I llm_load_vocab: special tokens cache size = 25
0.00.087.976 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.980 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.980 I llm_load_print_meta: arch             = gptneox
0.00.087.980 I llm_load_print_meta: vocab type       = BPE
0.00.087.981 I llm_load_print_meta: n_vocab          = 50304
0.00.087.981 I llm_load_print_meta: n_merges         = 50009
0.00.087.981 I llm_load_print_meta: vocab_only       = 0
0.00.087.981 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.982 I llm_load_print_meta: n_embd           = 2048
0.00.087.982 I llm_load_print_meta: n_layer          = 24
0.00.087.986 I llm_load_print_meta: n_head           = 16
0.00.087.987 I llm_load_print_meta: n_head_kv        = 16
0.00.087.987 I llm_load_print_meta: n_rot            = 32
0.00.087.987 I llm_load_print_meta: n_swa            = 0
0.00.087.988 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.988 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.988 I llm_load_print_meta: n_gqa            = 1
0.00.087.989 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.996 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.997 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.998 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.998 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.998 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.999 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.000 I llm_load_print_meta: n_ff             = 8192
0.00.088.003 I llm_load_print_meta: n_expert         = 0
0.00.088.005 I llm_load_print_meta: n_expert_used    = 0
0.00.088.005 I llm_load_print_meta: causal attn      = 1
0.00.088.005 I llm_load_print_meta: pooling type     = 0
0.00.088.006 I llm_load_print_meta: rope type        = 2
0.00.088.006 I llm_load_print_meta: rope scaling     = linear
0.00.088.006 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.007 I llm_load_print_meta: freq_scale_train = 1
0.00.088.007 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.007 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.007 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.008 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.008 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.008 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.010 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.022 I llm_load_print_meta: model type       = 1.4B
0.00.088.023 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.088.023 I llm_load_print_meta: model params     = 1.41 B
0.00.088.026 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.088.026 I llm_load_print_meta: general.name     = 1.4B
0.00.088.026 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.027 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.027 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.027 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.027 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.088.028 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.028 I llm_load_print_meta: max token length = 1024
0.00.090.698 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.698 I llm_load_tensors: offloading output layer to GPU
0.00.090.699 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.710 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.090.712 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.092.045 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.046 I llama_new_context_with_model: n_ctx         = 2048
0.00.092.046 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.092.047 I llama_new_context_with_model: n_batch       = 2048
0.00.092.047 I llama_new_context_with_model: n_ubatch      = 512
0.00.092.047 I llama_new_context_with_model: flash_attn    = 0
0.00.092.048 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.048 I llama_new_context_with_model: freq_scale    = 1
0.00.092.049 I ggml_metal_init: allocating
0.00.092.058 I ggml_metal_init: found device: Apple M4
0.00.092.061 I ggml_metal_init: picking default device: Apple M4
0.00.092.907 I ggml_metal_init: using embedded metal library
0.00.096.507 I ggml_metal_init: GPU name:   Apple M4
0.00.096.510 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.510 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.511 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.511 I ggml_metal_init: simdgroup reduction   = true
0.00.096.511 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.512 I ggml_metal_init: has bfloat            = true
0.00.096.512 I ggml_metal_init: use bfloat            = true
0.00.096.512 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.513 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.130.099 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.130.107 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.130.127 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.131.211 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.131.213 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.131.213 I llama_new_context_with_model: graph nodes  = 967
0.00.131.213 I llama_new_context_with_model: graph splits = 2
0.00.131.225 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.436 I main: llama threadpool init, n_threads = 4
0.00.679.520 I 
0.00.679.590 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.679.590 I 
0.00.680.167 I sampler seed: 1234
0.00.680.178 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.680.208 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.680.212 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.680.212 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.435.035 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.435.036 I llama_perf_context_print:        load time =     666.08 ms
0.01.435.037 I llama_perf_context_print: prompt eval time =      49.75 ms /     7 tokens (    7.11 ms per token,   140.70 tokens per second)
0.01.435.037 I llama_perf_context_print:        eval time =     701.98 ms /    63 runs   (   11.14 ms per token,    89.75 tokens per second)
0.01.435.039 I llama_perf_context_print:       total time =     755.60 ms /    70 tokens
0.01.435.220 I ggml_metal_free: deallocating

real	0m1.472s
user	0m0.146s
sys	0m0.167s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.637 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.057 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.062 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.064 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.065 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.065 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.065 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.066 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.072 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.073 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.073 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.074 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.074 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.074 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.075 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.077 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.079 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.080 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.889 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.872 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.613 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.614 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.615 I llama_model_loader: - type  f32:  194 tensors
0.00.023.615 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.615 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.616 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.117 I llm_load_vocab: special tokens cache size = 25
0.00.050.186 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.189 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.189 I llm_load_print_meta: arch             = gptneox
0.00.050.190 I llm_load_print_meta: vocab type       = BPE
0.00.050.190 I llm_load_print_meta: n_vocab          = 50304
0.00.050.190 I llm_load_print_meta: n_merges         = 50009
0.00.050.190 I llm_load_print_meta: vocab_only       = 0
0.00.050.190 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.191 I llm_load_print_meta: n_embd           = 2048
0.00.050.191 I llm_load_print_meta: n_layer          = 24
0.00.050.193 I llm_load_print_meta: n_head           = 16
0.00.050.194 I llm_load_print_meta: n_head_kv        = 16
0.00.050.194 I llm_load_print_meta: n_rot            = 32
0.00.050.194 I llm_load_print_meta: n_swa            = 0
0.00.050.195 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.195 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.195 I llm_load_print_meta: n_gqa            = 1
0.00.050.196 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.197 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.198 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.198 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.198 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.198 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.199 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.199 I llm_load_print_meta: n_ff             = 8192
0.00.050.199 I llm_load_print_meta: n_expert         = 0
0.00.050.201 I llm_load_print_meta: n_expert_used    = 0
0.00.050.203 I llm_load_print_meta: causal attn      = 1
0.00.050.203 I llm_load_print_meta: pooling type     = 0
0.00.050.203 I llm_load_print_meta: rope type        = 2
0.00.050.203 I llm_load_print_meta: rope scaling     = linear
0.00.050.204 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.204 I llm_load_print_meta: freq_scale_train = 1
0.00.050.204 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.204 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.205 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.205 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.205 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.205 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.205 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.217 I llm_load_print_meta: model type       = 1.4B
0.00.050.217 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.217 I llm_load_print_meta: model params     = 1.41 B
0.00.050.218 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.218 I llm_load_print_meta: general.name     = 1.4B
0.00.050.218 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.218 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.219 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.219 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.219 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.219 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.219 I llm_load_print_meta: max token length = 1024
0.00.052.180 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.180 I llm_load_tensors: offloading output layer to GPU
0.00.052.180 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.191 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.192 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.072 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.073 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.073 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.073 I llama_new_context_with_model: n_batch       = 2048
0.00.053.073 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.073 I llama_new_context_with_model: flash_attn    = 0
0.00.053.074 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.074 I llama_new_context_with_model: freq_scale    = 1
0.00.053.074 I ggml_metal_init: allocating
0.00.053.081 I ggml_metal_init: found device: Apple M4
0.00.053.083 I ggml_metal_init: picking default device: Apple M4
0.00.053.627 I ggml_metal_init: using embedded metal library
0.00.055.932 I ggml_metal_init: GPU name:   Apple M4
0.00.055.933 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.934 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.936 I ggml_metal_init: simdgroup reduction   = true
0.00.055.936 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.936 I ggml_metal_init: has bfloat            = true
0.00.055.936 I ggml_metal_init: use bfloat            = true
0.00.055.937 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.706 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.717 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.737 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.697 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.698 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.699 I llama_new_context_with_model: graph nodes  = 967
0.00.086.699 I llama_new_context_with_model: graph splits = 2
0.00.086.713 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.568 I main: llama threadpool init, n_threads = 4
0.00.635.608 I 
0.00.635.651 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.635.652 I 
0.00.635.893 I sampler seed: 1234
0.00.635.897 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.635.938 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.635.939 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.635.939 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.395.821 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.395.821 I llama_perf_context_print:        load time =     626.93 ms
0.01.395.822 I llama_perf_context_print: prompt eval time =      47.03 ms /     7 tokens (    6.72 ms per token,   148.84 tokens per second)
0.01.395.823 I llama_perf_context_print:        eval time =     709.72 ms /    63 runs   (   11.27 ms per token,    88.77 tokens per second)
0.01.395.823 I llama_perf_context_print:       total time =     760.26 ms /    70 tokens
0.01.396.010 I ggml_metal_free: deallocating

real	0m1.412s
user	0m0.109s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.849 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.035 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.040 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.042 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.043 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.043 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.043 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.051 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.051 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.051 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.052 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.052 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.053 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.053 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.057 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.058 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.058 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.964 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.043 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.942 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.943 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.944 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.944 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.944 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.945 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.945 I llama_model_loader: - type  f32:  194 tensors
0.00.025.945 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.946 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.374 I llm_load_vocab: special tokens cache size = 25
0.00.053.381 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.384 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.384 I llm_load_print_meta: arch             = gptneox
0.00.053.385 I llm_load_print_meta: vocab type       = BPE
0.00.053.385 I llm_load_print_meta: n_vocab          = 50304
0.00.053.385 I llm_load_print_meta: n_merges         = 50009
0.00.053.385 I llm_load_print_meta: vocab_only       = 0
0.00.053.386 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.386 I llm_load_print_meta: n_embd           = 2048
0.00.053.386 I llm_load_print_meta: n_layer          = 24
0.00.053.389 I llm_load_print_meta: n_head           = 16
0.00.053.389 I llm_load_print_meta: n_head_kv        = 16
0.00.053.390 I llm_load_print_meta: n_rot            = 32
0.00.053.390 I llm_load_print_meta: n_swa            = 0
0.00.053.390 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.390 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.391 I llm_load_print_meta: n_gqa            = 1
0.00.053.392 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.394 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.394 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.395 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.395 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.395 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.395 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.396 I llm_load_print_meta: n_ff             = 8192
0.00.053.396 I llm_load_print_meta: n_expert         = 0
0.00.053.396 I llm_load_print_meta: n_expert_used    = 0
0.00.053.398 I llm_load_print_meta: causal attn      = 1
0.00.053.400 I llm_load_print_meta: pooling type     = 0
0.00.053.400 I llm_load_print_meta: rope type        = 2
0.00.053.400 I llm_load_print_meta: rope scaling     = linear
0.00.053.400 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.401 I llm_load_print_meta: freq_scale_train = 1
0.00.053.401 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.401 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.401 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.401 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.401 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.402 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.402 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.414 I llm_load_print_meta: model type       = 1.4B
0.00.053.414 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.414 I llm_load_print_meta: model params     = 1.41 B
0.00.053.415 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.415 I llm_load_print_meta: general.name     = 1.4B
0.00.053.415 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.416 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.416 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.416 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.417 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.417 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.417 I llm_load_print_meta: max token length = 1024
0.00.055.436 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.436 I llm_load_tensors: offloading output layer to GPU
0.00.055.436 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.447 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.448 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.357 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.357 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.358 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.358 I llama_new_context_with_model: n_batch       = 2048
0.00.056.358 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.358 I llama_new_context_with_model: flash_attn    = 0
0.00.056.359 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.359 I llama_new_context_with_model: freq_scale    = 1
0.00.056.359 I ggml_metal_init: allocating
0.00.056.363 I ggml_metal_init: found device: Apple M4
0.00.056.365 I ggml_metal_init: picking default device: Apple M4
0.00.056.935 I ggml_metal_init: using embedded metal library
0.00.059.299 I ggml_metal_init: GPU name:   Apple M4
0.00.059.301 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.302 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.302 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.303 I ggml_metal_init: simdgroup reduction   = true
0.00.059.303 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.303 I ggml_metal_init: has bfloat            = true
0.00.059.303 I ggml_metal_init: use bfloat            = true
0.00.059.304 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.308 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.501 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.507 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.525 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.567 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.568 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.568 I llama_new_context_with_model: graph nodes  = 967
0.00.090.568 I llama_new_context_with_model: graph splits = 2
0.00.090.582 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.679 I main: llama threadpool init, n_threads = 4
0.00.713.716 I 
0.00.713.745 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.713.745 I 
0.00.713.982 I sampler seed: 1234
0.00.713.987 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.023 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.042 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.043 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.563.128 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60631.94 tokens per second)
0.01.563.129 I llama_perf_context_print:        load time =     702.83 ms
0.01.563.130 I llama_perf_context_print: prompt eval time =      51.68 ms /     7 tokens (    7.38 ms per token,   135.46 tokens per second)
0.01.563.131 I llama_perf_context_print:        eval time =     794.43 ms /    63 runs   (   12.61 ms per token,    79.30 tokens per second)
0.01.563.134 I llama_perf_context_print:       total time =     849.45 ms /    70 tokens
0.01.563.339 I ggml_metal_free: deallocating

real	0m1.583s
user	0m0.113s
sys	0m0.164s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.568 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.482 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.486 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.488 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.489 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.491 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.491 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.498 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.498 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.498 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.499 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.502 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.403 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.244 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.245 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.245 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.246 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.246 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.246 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.247 I llama_model_loader: - type  f32:  194 tensors
0.00.024.247 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.372 I llm_load_vocab: special tokens cache size = 25
0.00.052.467 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.469 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.470 I llm_load_print_meta: arch             = gptneox
0.00.052.470 I llm_load_print_meta: vocab type       = BPE
0.00.052.470 I llm_load_print_meta: n_vocab          = 50304
0.00.052.471 I llm_load_print_meta: n_merges         = 50009
0.00.052.471 I llm_load_print_meta: vocab_only       = 0
0.00.052.471 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.471 I llm_load_print_meta: n_embd           = 2048
0.00.052.471 I llm_load_print_meta: n_layer          = 24
0.00.052.474 I llm_load_print_meta: n_head           = 16
0.00.052.476 I llm_load_print_meta: n_head_kv        = 16
0.00.052.476 I llm_load_print_meta: n_rot            = 32
0.00.052.476 I llm_load_print_meta: n_swa            = 0
0.00.052.476 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.476 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.477 I llm_load_print_meta: n_gqa            = 1
0.00.052.478 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.478 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.479 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.479 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.479 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.480 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.480 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.480 I llm_load_print_meta: n_ff             = 8192
0.00.052.481 I llm_load_print_meta: n_expert         = 0
0.00.052.481 I llm_load_print_meta: n_expert_used    = 0
0.00.052.481 I llm_load_print_meta: causal attn      = 1
0.00.052.482 I llm_load_print_meta: pooling type     = 0
0.00.052.483 I llm_load_print_meta: rope type        = 2
0.00.052.484 I llm_load_print_meta: rope scaling     = linear
0.00.052.484 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.485 I llm_load_print_meta: freq_scale_train = 1
0.00.052.485 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.485 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.485 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.485 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.485 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.486 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.486 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.497 I llm_load_print_meta: model type       = 1.4B
0.00.052.498 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.498 I llm_load_print_meta: model params     = 1.41 B
0.00.052.498 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.498 I llm_load_print_meta: general.name     = 1.4B
0.00.052.499 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.499 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.500 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.500 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.500 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.500 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.501 I llm_load_print_meta: max token length = 1024
0.00.054.104 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.104 I llm_load_tensors: offloading output layer to GPU
0.00.054.104 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.114 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.115 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.964 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.965 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.965 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.965 I llama_new_context_with_model: n_batch       = 2048
0.00.054.966 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.966 I llama_new_context_with_model: flash_attn    = 0
0.00.054.966 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.967 I llama_new_context_with_model: freq_scale    = 1
0.00.054.967 I ggml_metal_init: allocating
0.00.054.970 I ggml_metal_init: found device: Apple M4
0.00.054.972 I ggml_metal_init: picking default device: Apple M4
0.00.055.531 I ggml_metal_init: using embedded metal library
0.00.057.838 I ggml_metal_init: GPU name:   Apple M4
0.00.057.839 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.839 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.840 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.840 I ggml_metal_init: simdgroup reduction   = true
0.00.057.842 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.842 I ggml_metal_init: has bfloat            = true
0.00.057.842 I ggml_metal_init: use bfloat            = true
0.00.057.842 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.843 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.818 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.823 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.850 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.865 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.867 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.867 I llama_new_context_with_model: graph nodes  = 967
0.00.089.867 I llama_new_context_with_model: graph splits = 2
0.00.089.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.821 I main: llama threadpool init, n_threads = 4
0.00.779.855 I 
0.00.779.904 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.779.906 I 
0.00.780.143 I sampler seed: 1234
0.00.780.147 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.158 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.159 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.159 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.661.881 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.01.661.882 I llama_perf_context_print:        load time =     771.25 ms
0.01.661.883 I llama_perf_context_print: prompt eval time =      54.45 ms /     7 tokens (    7.78 ms per token,   128.55 tokens per second)
0.01.661.883 I llama_perf_context_print:        eval time =     824.30 ms /    63 runs   (   13.08 ms per token,    76.43 tokens per second)
0.01.661.884 I llama_perf_context_print:       total time =     882.06 ms /    70 tokens
0.01.662.074 I ggml_metal_free: deallocating

real	0m1.676s
user	0m0.112s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.570 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.010 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.079 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.094 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.095 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.096 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.097 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.114 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.114 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.115 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.116 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.117 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.117 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.118 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.121 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.121 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.122 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.401 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.703 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.288 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.290 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.291 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.291 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.292 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.292 I llama_model_loader: - type  f32:  194 tensors
0.00.056.293 I llama_model_loader: - type  f16:   98 tensors
0.00.086.377 I llm_load_vocab: special tokens cache size = 25
0.00.093.082 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.093.085 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.093.085 I llm_load_print_meta: arch             = gptneox
0.00.093.086 I llm_load_print_meta: vocab type       = BPE
0.00.093.086 I llm_load_print_meta: n_vocab          = 50304
0.00.093.086 I llm_load_print_meta: n_merges         = 50009
0.00.093.086 I llm_load_print_meta: vocab_only       = 0
0.00.093.086 I llm_load_print_meta: n_ctx_train      = 2048
0.00.093.086 I llm_load_print_meta: n_embd           = 2048
0.00.093.087 I llm_load_print_meta: n_layer          = 24
0.00.093.089 I llm_load_print_meta: n_head           = 16
0.00.093.090 I llm_load_print_meta: n_head_kv        = 16
0.00.093.090 I llm_load_print_meta: n_rot            = 32
0.00.093.090 I llm_load_print_meta: n_swa            = 0
0.00.093.091 I llm_load_print_meta: n_embd_head_k    = 128
0.00.093.091 I llm_load_print_meta: n_embd_head_v    = 128
0.00.093.091 I llm_load_print_meta: n_gqa            = 1
0.00.093.092 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.093.093 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.093.093 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.093.093 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.093.094 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.093.094 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.093.094 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.093.095 I llm_load_print_meta: n_ff             = 8192
0.00.093.095 I llm_load_print_meta: n_expert         = 0
0.00.093.096 I llm_load_print_meta: n_expert_used    = 0
0.00.093.098 I llm_load_print_meta: causal attn      = 1
0.00.093.098 I llm_load_print_meta: pooling type     = 0
0.00.093.098 I llm_load_print_meta: rope type        = 2
0.00.093.098 I llm_load_print_meta: rope scaling     = linear
0.00.093.099 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.093.099 I llm_load_print_meta: freq_scale_train = 1
0.00.093.099 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.093.099 I llm_load_print_meta: rope_finetuned   = unknown
0.00.093.099 I llm_load_print_meta: ssm_d_conv       = 0
0.00.093.100 I llm_load_print_meta: ssm_d_inner      = 0
0.00.093.100 I llm_load_print_meta: ssm_d_state      = 0
0.00.093.100 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.093.100 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.093.112 I llm_load_print_meta: model type       = 1.4B
0.00.093.113 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.093.113 I llm_load_print_meta: model params     = 1.41 B
0.00.093.114 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.093.114 I llm_load_print_meta: general.name     = 1.4B
0.00.093.114 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.093.114 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.093.114 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.093.115 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.093.115 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.093.115 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.093.115 I llm_load_print_meta: max token length = 1024
0.00.095.712 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.713 I llm_load_tensors: offloading output layer to GPU
0.00.095.713 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.723 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.725 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.096.714 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.715 I llama_new_context_with_model: n_ctx         = 128
0.00.096.715 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.096.715 I llama_new_context_with_model: n_batch       = 128
0.00.096.715 I llama_new_context_with_model: n_ubatch      = 128
0.00.096.715 I llama_new_context_with_model: flash_attn    = 0
0.00.096.716 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.716 I llama_new_context_with_model: freq_scale    = 1
0.00.096.717 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.096.717 I ggml_metal_init: allocating
0.00.096.725 I ggml_metal_init: found device: Apple M4
0.00.096.727 I ggml_metal_init: picking default device: Apple M4
0.00.097.343 I ggml_metal_init: using embedded metal library
0.00.099.888 I ggml_metal_init: GPU name:   Apple M4
0.00.099.890 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.891 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.892 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.892 I ggml_metal_init: simdgroup reduction   = true
0.00.099.892 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.892 I ggml_metal_init: has bfloat            = true
0.00.099.892 I ggml_metal_init: use bfloat            = true
0.00.099.893 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.894 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.867 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.869 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.883 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.716 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.717 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.718 I llama_new_context_with_model: graph nodes  = 967
0.00.110.718 I llama_new_context_with_model: graph splits = 2
0.00.110.725 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.036.251 I 
0.01.036.309 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.036.373 I perplexity: tokenizing the input ..
0.01.049.339 I perplexity: tokenization took 12.964 ms
0.01.049.370 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.171.689 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.173.673 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.173.704 I llama_perf_context_print:        load time =    1011.23 ms
0.01.173.706 I llama_perf_context_print: prompt eval time =     121.64 ms /   128 tokens (    0.95 ms per token,  1052.30 tokens per second)
0.01.173.708 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.173.708 I llama_perf_context_print:       total time =     137.46 ms /   129 tokens
0.01.174.618 I ggml_metal_free: deallocating

real	0m1.368s
user	0m0.128s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.296 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.810 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.816 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.818 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.819 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.819 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.820 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.820 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.835 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.835 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.836 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.837 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.837 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.839 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.840 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.840 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.778 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.720 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.722 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.723 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.725 I llama_model_loader: - type  f32:  194 tensors
0.00.032.725 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.650 I llm_load_vocab: special tokens cache size = 25
0.00.064.810 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.813 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.813 I llm_load_print_meta: arch             = gptneox
0.00.064.813 I llm_load_print_meta: vocab type       = BPE
0.00.064.814 I llm_load_print_meta: n_vocab          = 50304
0.00.064.814 I llm_load_print_meta: n_merges         = 50009
0.00.064.814 I llm_load_print_meta: vocab_only       = 0
0.00.064.814 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.814 I llm_load_print_meta: n_embd           = 2048
0.00.064.814 I llm_load_print_meta: n_layer          = 24
0.00.064.821 I llm_load_print_meta: n_head           = 16
0.00.064.822 I llm_load_print_meta: n_head_kv        = 16
0.00.064.822 I llm_load_print_meta: n_rot            = 32
0.00.064.822 I llm_load_print_meta: n_swa            = 0
0.00.064.822 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.822 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.824 I llm_load_print_meta: n_gqa            = 1
0.00.064.825 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.825 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.826 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.826 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.826 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.826 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.827 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.827 I llm_load_print_meta: n_ff             = 8192
0.00.064.827 I llm_load_print_meta: n_expert         = 0
0.00.064.828 I llm_load_print_meta: n_expert_used    = 0
0.00.064.828 I llm_load_print_meta: causal attn      = 1
0.00.064.828 I llm_load_print_meta: pooling type     = 0
0.00.064.828 I llm_load_print_meta: rope type        = 2
0.00.064.828 I llm_load_print_meta: rope scaling     = linear
0.00.064.828 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.829 I llm_load_print_meta: freq_scale_train = 1
0.00.064.830 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.830 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.830 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.831 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.831 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.831 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.831 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.843 I llm_load_print_meta: model type       = 1.4B
0.00.064.844 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.844 I llm_load_print_meta: model params     = 1.41 B
0.00.064.844 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.844 I llm_load_print_meta: general.name     = 1.4B
0.00.064.845 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.845 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.845 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.845 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.846 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.846 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.846 I llm_load_print_meta: max token length = 1024
0.00.067.171 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.171 I llm_load_tensors: offloading output layer to GPU
0.00.067.171 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.182 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.183 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.169 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.170 I llama_new_context_with_model: n_ctx         = 128
0.00.068.170 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.068.170 I llama_new_context_with_model: n_batch       = 128
0.00.068.170 I llama_new_context_with_model: n_ubatch      = 128
0.00.068.170 I llama_new_context_with_model: flash_attn    = 0
0.00.068.171 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.171 I llama_new_context_with_model: freq_scale    = 1
0.00.068.171 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.172 I ggml_metal_init: allocating
0.00.068.177 I ggml_metal_init: found device: Apple M4
0.00.068.179 I ggml_metal_init: picking default device: Apple M4
0.00.068.783 I ggml_metal_init: using embedded metal library
0.00.071.395 I ggml_metal_init: GPU name:   Apple M4
0.00.071.397 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.397 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.398 I ggml_metal_init: simdgroup reduction   = true
0.00.071.398 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.398 I ggml_metal_init: has bfloat            = true
0.00.071.398 I ggml_metal_init: use bfloat            = true
0.00.071.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.875 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.877 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.894 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.818 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.083.819 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.083.820 I llama_new_context_with_model: graph nodes  = 967
0.00.083.820 I llama_new_context_with_model: graph splits = 2
0.00.083.833 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.633 I 
0.00.847.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.847.709 I perplexity: tokenizing the input ..
0.00.856.021 I perplexity: tokenization took 8.312 ms
0.00.856.031 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.980.372 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.981.710 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.981.728 I llama_perf_context_print:        load time =     836.33 ms
0.00.981.729 I llama_perf_context_print: prompt eval time =     124.11 ms /   128 tokens (    0.97 ms per token,  1031.33 tokens per second)
0.00.981.729 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.981.733 I llama_perf_context_print:       total time =     134.09 ms /   129 tokens
0.00.982.280 I ggml_metal_free: deallocating

real	0m1.002s
user	0m0.096s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.730 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.515 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.519 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.520 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.521 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.521 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.529 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.530 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.531 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.390 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.392 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.125 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.126 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.126 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.127 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.127 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.127 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.128 I llama_model_loader: - type  f32:  194 tensors
0.00.024.128 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.128 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.470 I llm_load_vocab: special tokens cache size = 25
0.00.050.273 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.276 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.276 I llm_load_print_meta: arch             = gptneox
0.00.050.276 I llm_load_print_meta: vocab type       = BPE
0.00.050.277 I llm_load_print_meta: n_vocab          = 50304
0.00.050.277 I llm_load_print_meta: n_merges         = 50009
0.00.050.277 I llm_load_print_meta: vocab_only       = 0
0.00.050.277 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.277 I llm_load_print_meta: n_embd           = 2048
0.00.050.278 I llm_load_print_meta: n_layer          = 24
0.00.050.280 I llm_load_print_meta: n_head           = 16
0.00.050.281 I llm_load_print_meta: n_head_kv        = 16
0.00.050.281 I llm_load_print_meta: n_rot            = 32
0.00.050.281 I llm_load_print_meta: n_swa            = 0
0.00.050.282 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.282 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.283 I llm_load_print_meta: n_gqa            = 1
0.00.050.283 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.284 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.284 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.285 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.285 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.286 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.286 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.287 I llm_load_print_meta: n_ff             = 8192
0.00.050.287 I llm_load_print_meta: n_expert         = 0
0.00.050.287 I llm_load_print_meta: n_expert_used    = 0
0.00.050.287 I llm_load_print_meta: causal attn      = 1
0.00.050.288 I llm_load_print_meta: pooling type     = 0
0.00.050.288 I llm_load_print_meta: rope type        = 2
0.00.050.288 I llm_load_print_meta: rope scaling     = linear
0.00.050.288 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.289 I llm_load_print_meta: freq_scale_train = 1
0.00.050.289 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.289 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.289 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.289 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.289 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.290 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.290 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.301 I llm_load_print_meta: model type       = 1.4B
0.00.050.302 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.302 I llm_load_print_meta: model params     = 1.41 B
0.00.050.302 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.303 I llm_load_print_meta: general.name     = 1.4B
0.00.050.303 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.303 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.303 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.303 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.303 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.304 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.304 I llm_load_print_meta: max token length = 1024
0.00.052.243 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.244 I llm_load_tensors: offloading output layer to GPU
0.00.052.244 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.254 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.255 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.157 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.158 I llama_new_context_with_model: n_ctx         = 128
0.00.053.158 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.158 I llama_new_context_with_model: n_batch       = 128
0.00.053.158 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.158 I llama_new_context_with_model: flash_attn    = 0
0.00.053.159 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.159 I llama_new_context_with_model: freq_scale    = 1
0.00.053.159 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.160 I ggml_metal_init: allocating
0.00.053.163 I ggml_metal_init: found device: Apple M4
0.00.053.165 I ggml_metal_init: picking default device: Apple M4
0.00.053.704 I ggml_metal_init: using embedded metal library
0.00.056.004 I ggml_metal_init: GPU name:   Apple M4
0.00.056.005 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.005 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.006 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.006 I ggml_metal_init: simdgroup reduction   = true
0.00.056.006 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.006 I ggml_metal_init: has bfloat            = true
0.00.056.006 I ggml_metal_init: use bfloat            = true
0.00.056.007 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.784 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.786 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.800 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.716 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.717 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.717 I llama_new_context_with_model: graph nodes  = 967
0.00.067.717 I llama_new_context_with_model: graph splits = 2
0.00.067.730 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.021 I 
0.00.629.061 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.629.075 I perplexity: tokenizing the input ..
0.00.637.026 I perplexity: tokenization took 7.948 ms
0.00.637.036 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.759.737 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.761.055 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.761.075 I llama_perf_context_print:        load time =     619.28 ms
0.00.761.076 I llama_perf_context_print: prompt eval time =     122.47 ms /   128 tokens (    0.96 ms per token,  1045.15 tokens per second)
0.00.761.077 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.077 I llama_perf_context_print:       total time =     132.06 ms /   129 tokens
0.00.761.447 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.079s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.799 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.745 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.760 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.760 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.760 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.761 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.761 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.761 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.762 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.763 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.764 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.764 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.614 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.716 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.592 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.592 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.592 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.593 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.593 I llama_model_loader: - type  f32:  194 tensors
0.00.023.594 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.594 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.966 I llm_load_vocab: special tokens cache size = 25
0.00.049.904 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.906 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.906 I llm_load_print_meta: arch             = gptneox
0.00.049.907 I llm_load_print_meta: vocab type       = BPE
0.00.049.907 I llm_load_print_meta: n_vocab          = 50304
0.00.049.907 I llm_load_print_meta: n_merges         = 50009
0.00.049.907 I llm_load_print_meta: vocab_only       = 0
0.00.049.908 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.908 I llm_load_print_meta: n_embd           = 2048
0.00.049.908 I llm_load_print_meta: n_layer          = 24
0.00.049.910 I llm_load_print_meta: n_head           = 16
0.00.049.911 I llm_load_print_meta: n_head_kv        = 16
0.00.049.911 I llm_load_print_meta: n_rot            = 32
0.00.049.912 I llm_load_print_meta: n_swa            = 0
0.00.049.912 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.912 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.913 I llm_load_print_meta: n_gqa            = 1
0.00.049.913 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.914 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.914 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.915 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.915 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.915 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.915 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.916 I llm_load_print_meta: n_ff             = 8192
0.00.049.916 I llm_load_print_meta: n_expert         = 0
0.00.049.916 I llm_load_print_meta: n_expert_used    = 0
0.00.049.916 I llm_load_print_meta: causal attn      = 1
0.00.049.917 I llm_load_print_meta: pooling type     = 0
0.00.049.917 I llm_load_print_meta: rope type        = 2
0.00.049.917 I llm_load_print_meta: rope scaling     = linear
0.00.049.917 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.918 I llm_load_print_meta: freq_scale_train = 1
0.00.049.918 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.918 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.918 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.918 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.918 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.918 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.919 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.930 I llm_load_print_meta: model type       = 1.4B
0.00.049.931 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.931 I llm_load_print_meta: model params     = 1.41 B
0.00.049.932 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.932 I llm_load_print_meta: general.name     = 1.4B
0.00.049.932 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.932 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.932 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.932 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.933 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.933 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.935 I llm_load_print_meta: max token length = 1024
0.00.051.897 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.897 I llm_load_tensors: offloading output layer to GPU
0.00.051.898 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.908 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.909 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.797 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.797 I llama_new_context_with_model: n_ctx         = 128
0.00.052.797 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.798 I llama_new_context_with_model: n_batch       = 128
0.00.052.798 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.798 I llama_new_context_with_model: flash_attn    = 0
0.00.052.798 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.799 I llama_new_context_with_model: freq_scale    = 1
0.00.052.799 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.799 I ggml_metal_init: allocating
0.00.052.805 I ggml_metal_init: found device: Apple M4
0.00.052.807 I ggml_metal_init: picking default device: Apple M4
0.00.053.376 I ggml_metal_init: using embedded metal library
0.00.055.684 I ggml_metal_init: GPU name:   Apple M4
0.00.055.685 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.686 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.686 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.686 I ggml_metal_init: simdgroup reduction   = true
0.00.055.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.687 I ggml_metal_init: has bfloat            = true
0.00.055.687 I ggml_metal_init: use bfloat            = true
0.00.055.687 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.688 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.581 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.587 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.601 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.476 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.477 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.477 I llama_new_context_with_model: graph nodes  = 967
0.00.067.477 I llama_new_context_with_model: graph splits = 2
0.00.067.490 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.360 I 
0.00.648.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.648.402 I perplexity: tokenizing the input ..
0.00.656.307 I perplexity: tokenization took 7.904 ms
0.00.656.322 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.481 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.780.894 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.780.907 I llama_perf_context_print:        load time =     639.56 ms
0.00.780.907 I llama_perf_context_print: prompt eval time =     122.92 ms /   128 tokens (    0.96 ms per token,  1041.29 tokens per second)
0.00.780.910 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.910 I llama_perf_context_print:       total time =     132.55 ms /   129 tokens
0.00.781.315 I ggml_metal_free: deallocating

real	0m0.794s
user	0m0.079s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.844 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.509 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.513 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.519 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.520 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.520 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.520 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.532 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.534 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.359 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.387 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.177 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.177 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.178 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.178 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.179 I llama_model_loader: - type  f32:  194 tensors
0.00.024.179 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.180 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.512 I llm_load_vocab: special tokens cache size = 25
0.00.050.578 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.581 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.581 I llm_load_print_meta: arch             = gptneox
0.00.050.582 I llm_load_print_meta: vocab type       = BPE
0.00.050.582 I llm_load_print_meta: n_vocab          = 50304
0.00.050.582 I llm_load_print_meta: n_merges         = 50009
0.00.050.582 I llm_load_print_meta: vocab_only       = 0
0.00.050.582 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.583 I llm_load_print_meta: n_embd           = 2048
0.00.050.583 I llm_load_print_meta: n_layer          = 24
0.00.050.585 I llm_load_print_meta: n_head           = 16
0.00.050.586 I llm_load_print_meta: n_head_kv        = 16
0.00.050.586 I llm_load_print_meta: n_rot            = 32
0.00.050.586 I llm_load_print_meta: n_swa            = 0
0.00.050.587 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.587 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.588 I llm_load_print_meta: n_gqa            = 1
0.00.050.588 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.589 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.590 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.590 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.590 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.590 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.591 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.591 I llm_load_print_meta: n_ff             = 8192
0.00.050.591 I llm_load_print_meta: n_expert         = 0
0.00.050.591 I llm_load_print_meta: n_expert_used    = 0
0.00.050.592 I llm_load_print_meta: causal attn      = 1
0.00.050.592 I llm_load_print_meta: pooling type     = 0
0.00.050.592 I llm_load_print_meta: rope type        = 2
0.00.050.592 I llm_load_print_meta: rope scaling     = linear
0.00.050.592 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.593 I llm_load_print_meta: freq_scale_train = 1
0.00.050.593 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.593 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.593 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.593 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.594 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.594 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.594 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.601 I llm_load_print_meta: model type       = 1.4B
0.00.050.601 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.601 I llm_load_print_meta: model params     = 1.41 B
0.00.050.602 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.603 I llm_load_print_meta: general.name     = 1.4B
0.00.050.604 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.604 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.604 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.604 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.604 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.605 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.605 I llm_load_print_meta: max token length = 1024
0.00.052.473 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.473 I llm_load_tensors: offloading output layer to GPU
0.00.052.473 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.484 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.485 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.339 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.339 I llama_new_context_with_model: n_ctx         = 128
0.00.053.340 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.340 I llama_new_context_with_model: n_batch       = 128
0.00.053.340 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.340 I llama_new_context_with_model: flash_attn    = 0
0.00.053.340 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.341 I llama_new_context_with_model: freq_scale    = 1
0.00.053.341 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.342 I ggml_metal_init: allocating
0.00.053.344 I ggml_metal_init: found device: Apple M4
0.00.053.346 I ggml_metal_init: picking default device: Apple M4
0.00.053.893 I ggml_metal_init: using embedded metal library
0.00.056.199 I ggml_metal_init: GPU name:   Apple M4
0.00.056.201 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.201 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.202 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.202 I ggml_metal_init: simdgroup reduction   = true
0.00.056.202 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.202 I ggml_metal_init: has bfloat            = true
0.00.056.202 I ggml_metal_init: use bfloat            = true
0.00.056.203 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.205 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.924 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.931 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.944 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.814 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.815 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.816 I llama_new_context_with_model: graph nodes  = 967
0.00.067.816 I llama_new_context_with_model: graph splits = 2
0.00.067.828 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.325 I 
0.00.655.355 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.655.369 I perplexity: tokenizing the input ..
0.00.663.443 I perplexity: tokenization took 8.073 ms
0.00.663.460 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.716 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.800.137 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.800.153 I llama_perf_context_print:        load time =     645.48 ms
0.00.800.154 I llama_perf_context_print: prompt eval time =     134.99 ms /   128 tokens (    1.05 ms per token,   948.23 tokens per second)
0.00.800.155 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.157 I llama_perf_context_print:       total time =     144.83 ms /   129 tokens
0.00.800.608 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.079s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.035 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.863 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.867 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.869 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.871 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.872 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.872 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.872 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.880 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.880 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.880 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.881 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.881 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.881 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.882 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.886 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.887 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.696 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.599 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.602 I llama_model_loader: - type  f32:  194 tensors
0.00.023.602 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.602 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.908 I llm_load_vocab: special tokens cache size = 25
0.00.049.866 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.869 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.869 I llm_load_print_meta: arch             = gptneox
0.00.049.870 I llm_load_print_meta: vocab type       = BPE
0.00.049.870 I llm_load_print_meta: n_vocab          = 50304
0.00.049.870 I llm_load_print_meta: n_merges         = 50009
0.00.049.870 I llm_load_print_meta: vocab_only       = 0
0.00.049.870 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.871 I llm_load_print_meta: n_embd           = 2048
0.00.049.871 I llm_load_print_meta: n_layer          = 24
0.00.049.873 I llm_load_print_meta: n_head           = 16
0.00.049.874 I llm_load_print_meta: n_head_kv        = 16
0.00.049.874 I llm_load_print_meta: n_rot            = 32
0.00.049.875 I llm_load_print_meta: n_swa            = 0
0.00.049.875 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.875 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.875 I llm_load_print_meta: n_gqa            = 1
0.00.049.876 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.879 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.879 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.880 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.881 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.882 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.882 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.882 I llm_load_print_meta: n_ff             = 8192
0.00.049.883 I llm_load_print_meta: n_expert         = 0
0.00.049.883 I llm_load_print_meta: n_expert_used    = 0
0.00.049.883 I llm_load_print_meta: causal attn      = 1
0.00.049.883 I llm_load_print_meta: pooling type     = 0
0.00.049.883 I llm_load_print_meta: rope type        = 2
0.00.049.883 I llm_load_print_meta: rope scaling     = linear
0.00.049.886 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.888 I llm_load_print_meta: freq_scale_train = 1
0.00.049.888 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.888 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.888 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.888 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.889 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.889 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.889 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.895 I llm_load_print_meta: model type       = 1.4B
0.00.049.897 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.897 I llm_load_print_meta: model params     = 1.41 B
0.00.049.898 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.898 I llm_load_print_meta: general.name     = 1.4B
0.00.049.898 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.898 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.898 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.899 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.899 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.899 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.899 I llm_load_print_meta: max token length = 1024
0.00.051.645 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.645 I llm_load_tensors: offloading output layer to GPU
0.00.051.646 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.651 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.651 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.530 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.530 I llama_new_context_with_model: n_ctx         = 128
0.00.052.531 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.531 I llama_new_context_with_model: n_batch       = 128
0.00.052.531 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.531 I llama_new_context_with_model: flash_attn    = 0
0.00.052.532 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.532 I llama_new_context_with_model: freq_scale    = 1
0.00.052.532 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.533 I ggml_metal_init: allocating
0.00.052.539 I ggml_metal_init: found device: Apple M4
0.00.052.541 I ggml_metal_init: picking default device: Apple M4
0.00.053.095 I ggml_metal_init: using embedded metal library
0.00.055.388 I ggml_metal_init: GPU name:   Apple M4
0.00.055.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.391 I ggml_metal_init: simdgroup reduction   = true
0.00.055.391 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.391 I ggml_metal_init: has bfloat            = true
0.00.055.391 I ggml_metal_init: use bfloat            = true
0.00.055.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.092 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.098 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.114 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.985 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.986 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.986 I llama_new_context_with_model: graph nodes  = 967
0.00.066.987 I llama_new_context_with_model: graph splits = 2
0.00.066.994 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.371 I 
0.00.692.402 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.692.415 I perplexity: tokenizing the input ..
0.00.700.449 I perplexity: tokenization took 8.033 ms
0.00.700.460 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.835.654 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.837.066 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.837.079 I llama_perf_context_print:        load time =     683.33 ms
0.00.837.080 I llama_perf_context_print: prompt eval time =     134.94 ms /   128 tokens (    1.05 ms per token,   948.58 tokens per second)
0.00.837.081 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.081 I llama_perf_context_print:       total time =     144.71 ms /   129 tokens
0.00.837.607 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.079s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.787 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.394 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.399 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.401 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.401 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.402 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.402 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.409 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.410 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.211 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.272 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.157 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.158 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.158 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.158 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.158 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.159 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.159 I llama_model_loader: - type  f32:  194 tensors
0.00.024.160 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.160 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.160 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.320 I llm_load_vocab: special tokens cache size = 25
0.00.050.355 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.357 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.358 I llm_load_print_meta: arch             = gptneox
0.00.050.358 I llm_load_print_meta: vocab type       = BPE
0.00.050.358 I llm_load_print_meta: n_vocab          = 50304
0.00.050.358 I llm_load_print_meta: n_merges         = 50009
0.00.050.359 I llm_load_print_meta: vocab_only       = 0
0.00.050.359 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.359 I llm_load_print_meta: n_embd           = 2048
0.00.050.359 I llm_load_print_meta: n_layer          = 24
0.00.050.362 I llm_load_print_meta: n_head           = 16
0.00.050.363 I llm_load_print_meta: n_head_kv        = 16
0.00.050.363 I llm_load_print_meta: n_rot            = 32
0.00.050.363 I llm_load_print_meta: n_swa            = 0
0.00.050.364 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.364 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.365 I llm_load_print_meta: n_gqa            = 1
0.00.050.365 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.366 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.366 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.367 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.369 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.370 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.370 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.370 I llm_load_print_meta: n_ff             = 8192
0.00.050.371 I llm_load_print_meta: n_expert         = 0
0.00.050.371 I llm_load_print_meta: n_expert_used    = 0
0.00.050.372 I llm_load_print_meta: causal attn      = 1
0.00.050.372 I llm_load_print_meta: pooling type     = 0
0.00.050.372 I llm_load_print_meta: rope type        = 2
0.00.050.374 I llm_load_print_meta: rope scaling     = linear
0.00.050.374 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.375 I llm_load_print_meta: freq_scale_train = 1
0.00.050.375 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.375 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.376 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.376 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.376 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.377 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.382 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.389 I llm_load_print_meta: model type       = 1.4B
0.00.050.390 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.390 I llm_load_print_meta: model params     = 1.41 B
0.00.050.391 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.391 I llm_load_print_meta: general.name     = 1.4B
0.00.050.391 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.391 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.391 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.391 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.392 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.392 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.392 I llm_load_print_meta: max token length = 1024
0.00.052.112 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.112 I llm_load_tensors: offloading output layer to GPU
0.00.052.113 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.118 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.118 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.052 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.053 I llama_new_context_with_model: n_ctx         = 128
0.00.053.053 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.053 I llama_new_context_with_model: n_batch       = 128
0.00.053.053 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.054 I llama_new_context_with_model: flash_attn    = 0
0.00.053.054 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.054 I llama_new_context_with_model: freq_scale    = 1
0.00.053.055 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.055 I ggml_metal_init: allocating
0.00.053.060 I ggml_metal_init: found device: Apple M4
0.00.053.062 I ggml_metal_init: picking default device: Apple M4
0.00.053.602 I ggml_metal_init: using embedded metal library
0.00.055.878 I ggml_metal_init: GPU name:   Apple M4
0.00.055.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.880 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.880 I ggml_metal_init: simdgroup reduction   = true
0.00.055.881 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.881 I ggml_metal_init: has bfloat            = true
0.00.055.881 I ggml_metal_init: use bfloat            = true
0.00.055.881 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.882 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.643 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.645 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.658 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.561 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.562 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.562 I llama_new_context_with_model: graph nodes  = 967
0.00.067.563 I llama_new_context_with_model: graph splits = 2
0.00.067.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.452.051 I 
0.00.452.081 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.452.091 I perplexity: tokenizing the input ..
0.00.459.650 I perplexity: tokenization took 7.558 ms
0.00.459.663 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.592.045 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.593.462 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.593.477 I llama_perf_context_print:        load time =     442.26 ms
0.00.593.478 I llama_perf_context_print: prompt eval time =     132.14 ms /   128 tokens (    1.03 ms per token,   968.69 tokens per second)
0.00.593.479 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.593.479 I llama_perf_context_print:       total time =     141.43 ms /   129 tokens
0.00.593.888 I ggml_metal_free: deallocating

real	0m0.610s
user	0m0.079s
sys	0m0.073s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.717 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.618 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.622 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.624 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.625 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.625 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.633 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.633 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.633 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.634 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.634 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.634 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.635 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.638 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.638 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.525 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.572 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.416 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.417 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.417 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.418 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.418 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.418 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.419 I llama_model_loader: - type  f32:  194 tensors
0.00.023.419 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.420 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.420 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.420 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.799 I llm_load_vocab: special tokens cache size = 25
0.00.049.874 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.877 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.878 I llm_load_print_meta: arch             = gptneox
0.00.049.878 I llm_load_print_meta: vocab type       = BPE
0.00.049.878 I llm_load_print_meta: n_vocab          = 50304
0.00.049.878 I llm_load_print_meta: n_merges         = 50009
0.00.049.879 I llm_load_print_meta: vocab_only       = 0
0.00.049.879 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.879 I llm_load_print_meta: n_embd           = 2048
0.00.049.879 I llm_load_print_meta: n_layer          = 24
0.00.049.882 I llm_load_print_meta: n_head           = 16
0.00.049.885 I llm_load_print_meta: n_head_kv        = 16
0.00.049.885 I llm_load_print_meta: n_rot            = 32
0.00.049.885 I llm_load_print_meta: n_swa            = 0
0.00.049.885 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.886 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.886 I llm_load_print_meta: n_gqa            = 1
0.00.049.887 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.888 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.888 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.889 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.889 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.889 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.889 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.890 I llm_load_print_meta: n_ff             = 8192
0.00.049.890 I llm_load_print_meta: n_expert         = 0
0.00.049.890 I llm_load_print_meta: n_expert_used    = 0
0.00.049.890 I llm_load_print_meta: causal attn      = 1
0.00.049.890 I llm_load_print_meta: pooling type     = 0
0.00.049.892 I llm_load_print_meta: rope type        = 2
0.00.049.892 I llm_load_print_meta: rope scaling     = linear
0.00.049.894 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.894 I llm_load_print_meta: freq_scale_train = 1
0.00.049.895 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.895 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.895 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.895 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.896 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.897 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.897 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.908 I llm_load_print_meta: model type       = 1.4B
0.00.049.909 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.909 I llm_load_print_meta: model params     = 1.41 B
0.00.049.911 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.911 I llm_load_print_meta: general.name     = 1.4B
0.00.049.911 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.911 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.911 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.911 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.912 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.912 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.912 I llm_load_print_meta: max token length = 1024
0.00.051.838 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.838 I llm_load_tensors: offloading output layer to GPU
0.00.051.838 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.849 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.850 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.810 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.811 I llama_new_context_with_model: n_ctx         = 128
0.00.052.811 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.811 I llama_new_context_with_model: n_batch       = 128
0.00.052.811 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.812 I llama_new_context_with_model: flash_attn    = 0
0.00.052.812 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.812 I llama_new_context_with_model: freq_scale    = 1
0.00.052.813 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.813 I ggml_metal_init: allocating
0.00.052.816 I ggml_metal_init: found device: Apple M4
0.00.052.818 I ggml_metal_init: picking default device: Apple M4
0.00.053.367 I ggml_metal_init: using embedded metal library
0.00.055.678 I ggml_metal_init: GPU name:   Apple M4
0.00.055.679 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.680 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.682 I ggml_metal_init: simdgroup reduction   = true
0.00.055.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.682 I ggml_metal_init: has bfloat            = true
0.00.055.682 I ggml_metal_init: use bfloat            = true
0.00.055.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.683 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.447 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.449 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.463 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.394 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.395 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.396 I llama_new_context_with_model: graph nodes  = 967
0.00.067.396 I llama_new_context_with_model: graph splits = 2
0.00.067.408 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.488.822 I 
0.00.488.855 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.488.877 I perplexity: tokenizing the input ..
0.00.496.679 I perplexity: tokenization took 7.801 ms
0.00.496.690 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.628.393 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.629.742 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.629.756 I llama_perf_context_print:        load time =     480.10 ms
0.00.629.757 I llama_perf_context_print: prompt eval time =     131.48 ms /   128 tokens (    1.03 ms per token,   973.56 tokens per second)
0.00.629.758 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.629.758 I llama_perf_context_print:       total time =     140.93 ms /   129 tokens
0.00.630.190 I ggml_metal_free: deallocating

real	0m0.642s
user	0m0.079s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.652 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.579 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.584 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.585 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.586 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.586 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.587 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.596 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.599 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.600 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.600 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.426 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.307 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.308 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.309 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.309 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.310 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.310 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.310 I llama_model_loader: - type  f32:  194 tensors
0.00.024.311 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.311 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.311 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.216 I llm_load_vocab: special tokens cache size = 25
0.00.051.302 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.305 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.305 I llm_load_print_meta: arch             = gptneox
0.00.051.306 I llm_load_print_meta: vocab type       = BPE
0.00.051.306 I llm_load_print_meta: n_vocab          = 50304
0.00.051.306 I llm_load_print_meta: n_merges         = 50009
0.00.051.306 I llm_load_print_meta: vocab_only       = 0
0.00.051.307 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.307 I llm_load_print_meta: n_embd           = 2048
0.00.051.307 I llm_load_print_meta: n_layer          = 24
0.00.051.310 I llm_load_print_meta: n_head           = 16
0.00.051.311 I llm_load_print_meta: n_head_kv        = 16
0.00.051.311 I llm_load_print_meta: n_rot            = 32
0.00.051.311 I llm_load_print_meta: n_swa            = 0
0.00.051.311 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.311 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.312 I llm_load_print_meta: n_gqa            = 1
0.00.051.313 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.313 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.314 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.314 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.314 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.315 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.315 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.315 I llm_load_print_meta: n_ff             = 8192
0.00.051.316 I llm_load_print_meta: n_expert         = 0
0.00.051.316 I llm_load_print_meta: n_expert_used    = 0
0.00.051.316 I llm_load_print_meta: causal attn      = 1
0.00.051.316 I llm_load_print_meta: pooling type     = 0
0.00.051.316 I llm_load_print_meta: rope type        = 2
0.00.051.316 I llm_load_print_meta: rope scaling     = linear
0.00.051.317 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.317 I llm_load_print_meta: freq_scale_train = 1
0.00.051.317 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.321 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.321 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.321 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.321 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.321 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.321 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.332 I llm_load_print_meta: model type       = 1.4B
0.00.051.333 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.333 I llm_load_print_meta: model params     = 1.41 B
0.00.051.334 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.334 I llm_load_print_meta: general.name     = 1.4B
0.00.051.334 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.334 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.334 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.334 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.335 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.335 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.335 I llm_load_print_meta: max token length = 1024
0.00.052.851 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.852 I llm_load_tensors: offloading output layer to GPU
0.00.052.852 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.862 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.863 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.707 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.707 I llama_new_context_with_model: n_ctx         = 128
0.00.053.708 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.708 I llama_new_context_with_model: n_batch       = 128
0.00.053.708 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.708 I llama_new_context_with_model: flash_attn    = 0
0.00.053.709 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.709 I llama_new_context_with_model: freq_scale    = 1
0.00.053.709 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.709 I ggml_metal_init: allocating
0.00.053.716 I ggml_metal_init: found device: Apple M4
0.00.053.718 I ggml_metal_init: picking default device: Apple M4
0.00.054.280 I ggml_metal_init: using embedded metal library
0.00.056.640 I ggml_metal_init: GPU name:   Apple M4
0.00.056.641 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.642 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.642 I ggml_metal_init: simdgroup reduction   = true
0.00.056.642 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.642 I ggml_metal_init: has bfloat            = true
0.00.056.643 I ggml_metal_init: use bfloat            = true
0.00.056.643 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.644 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.185 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.187 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.200 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.088 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.089 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.089 I llama_new_context_with_model: graph nodes  = 967
0.00.068.090 I llama_new_context_with_model: graph splits = 2
0.00.068.102 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.857 I 
0.00.589.904 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.589.924 I perplexity: tokenizing the input ..
0.00.598.085 I perplexity: tokenization took 8.159 ms
0.00.598.099 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.732.422 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.733.759 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.733.771 I llama_perf_context_print:        load time =     580.20 ms
0.00.733.772 I llama_perf_context_print: prompt eval time =     134.09 ms /   128 tokens (    1.05 ms per token,   954.55 tokens per second)
0.00.733.775 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.775 I llama_perf_context_print:       total time =     143.92 ms /   129 tokens
0.00.734.104 I ggml_metal_free: deallocating

real	0m0.751s
user	0m0.080s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.537 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.405 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.410 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.412 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.412 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.413 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.413 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.420 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.420 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.421 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.421 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.422 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.422 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.422 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.425 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.425 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.426 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.292 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.277 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.278 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.278 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.279 I llama_model_loader: - type  f32:  194 tensors
0.00.023.280 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.280 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.323 I llm_load_vocab: special tokens cache size = 25
0.00.050.313 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.316 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.316 I llm_load_print_meta: arch             = gptneox
0.00.050.317 I llm_load_print_meta: vocab type       = BPE
0.00.050.317 I llm_load_print_meta: n_vocab          = 50304
0.00.050.317 I llm_load_print_meta: n_merges         = 50009
0.00.050.317 I llm_load_print_meta: vocab_only       = 0
0.00.050.317 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.317 I llm_load_print_meta: n_embd           = 2048
0.00.050.318 I llm_load_print_meta: n_layer          = 24
0.00.050.321 I llm_load_print_meta: n_head           = 16
0.00.050.321 I llm_load_print_meta: n_head_kv        = 16
0.00.050.322 I llm_load_print_meta: n_rot            = 32
0.00.050.322 I llm_load_print_meta: n_swa            = 0
0.00.050.322 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.322 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.324 I llm_load_print_meta: n_gqa            = 1
0.00.050.325 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.325 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.326 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.326 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.326 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.326 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.328 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.329 I llm_load_print_meta: n_ff             = 8192
0.00.050.329 I llm_load_print_meta: n_expert         = 0
0.00.050.329 I llm_load_print_meta: n_expert_used    = 0
0.00.050.329 I llm_load_print_meta: causal attn      = 1
0.00.050.329 I llm_load_print_meta: pooling type     = 0
0.00.050.330 I llm_load_print_meta: rope type        = 2
0.00.050.330 I llm_load_print_meta: rope scaling     = linear
0.00.050.330 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.331 I llm_load_print_meta: freq_scale_train = 1
0.00.050.331 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.331 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.331 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.331 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.331 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.331 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.332 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.344 I llm_load_print_meta: model type       = 1.4B
0.00.050.344 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.345 I llm_load_print_meta: model params     = 1.41 B
0.00.050.345 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.345 I llm_load_print_meta: general.name     = 1.4B
0.00.050.345 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.346 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.346 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.346 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.346 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.346 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.347 I llm_load_print_meta: max token length = 1024
0.00.052.250 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.250 I llm_load_tensors: offloading output layer to GPU
0.00.052.250 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.261 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.262 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.142 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.143 I llama_new_context_with_model: n_ctx         = 128
0.00.053.143 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.143 I llama_new_context_with_model: n_batch       = 128
0.00.053.143 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.143 I llama_new_context_with_model: flash_attn    = 0
0.00.053.144 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.144 I llama_new_context_with_model: freq_scale    = 1
0.00.053.144 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.144 I ggml_metal_init: allocating
0.00.053.147 I ggml_metal_init: found device: Apple M4
0.00.053.149 I ggml_metal_init: picking default device: Apple M4
0.00.053.682 I ggml_metal_init: using embedded metal library
0.00.055.995 I ggml_metal_init: GPU name:   Apple M4
0.00.055.996 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.997 I ggml_metal_init: simdgroup reduction   = true
0.00.055.998 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.998 I ggml_metal_init: has bfloat            = true
0.00.055.998 I ggml_metal_init: use bfloat            = true
0.00.055.998 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.999 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.554 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.557 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.570 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.430 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.431 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.431 I llama_new_context_with_model: graph nodes  = 967
0.00.067.432 I llama_new_context_with_model: graph splits = 2
0.00.067.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.764 I 
0.00.655.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.655.817 I perplexity: tokenizing the input ..
0.00.663.497 I perplexity: tokenization took 7.679 ms
0.00.663.510 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.395 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.804.833 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.804.846 I llama_perf_context_print:        load time =     647.22 ms
0.00.804.847 I llama_perf_context_print: prompt eval time =     139.66 ms /   128 tokens (    1.09 ms per token,   916.51 tokens per second)
0.00.804.848 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.804.848 I llama_perf_context_print:       total time =     149.08 ms /   129 tokens
0.00.805.261 I ggml_metal_free: deallocating

real	0m0.819s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.937 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.708 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.712 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.718 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.719 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.719 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.727 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.728 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.728 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.729 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.730 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.731 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.732 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.733 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.733 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.553 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.618 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.412 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.413 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.413 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.414 I llama_model_loader: - type  f32:  194 tensors
0.00.024.414 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.286 I llm_load_vocab: special tokens cache size = 25
0.00.051.110 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.113 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.114 I llm_load_print_meta: arch             = gptneox
0.00.051.114 I llm_load_print_meta: vocab type       = BPE
0.00.051.114 I llm_load_print_meta: n_vocab          = 50304
0.00.051.114 I llm_load_print_meta: n_merges         = 50009
0.00.051.115 I llm_load_print_meta: vocab_only       = 0
0.00.051.115 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.115 I llm_load_print_meta: n_embd           = 2048
0.00.051.115 I llm_load_print_meta: n_layer          = 24
0.00.051.118 I llm_load_print_meta: n_head           = 16
0.00.051.119 I llm_load_print_meta: n_head_kv        = 16
0.00.051.119 I llm_load_print_meta: n_rot            = 32
0.00.051.119 I llm_load_print_meta: n_swa            = 0
0.00.051.119 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.119 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.120 I llm_load_print_meta: n_gqa            = 1
0.00.051.121 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.121 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.122 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.122 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.122 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.122 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.123 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.123 I llm_load_print_meta: n_ff             = 8192
0.00.051.124 I llm_load_print_meta: n_expert         = 0
0.00.051.124 I llm_load_print_meta: n_expert_used    = 0
0.00.051.124 I llm_load_print_meta: causal attn      = 1
0.00.051.124 I llm_load_print_meta: pooling type     = 0
0.00.051.124 I llm_load_print_meta: rope type        = 2
0.00.051.124 I llm_load_print_meta: rope scaling     = linear
0.00.051.125 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.125 I llm_load_print_meta: freq_scale_train = 1
0.00.051.125 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.126 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.126 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.126 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.126 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.126 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.126 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.138 I llm_load_print_meta: model type       = 1.4B
0.00.051.138 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.139 I llm_load_print_meta: model params     = 1.41 B
0.00.051.141 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.141 I llm_load_print_meta: general.name     = 1.4B
0.00.051.141 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.141 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.142 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.142 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.142 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.142 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.142 I llm_load_print_meta: max token length = 1024
0.00.053.172 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.172 I llm_load_tensors: offloading output layer to GPU
0.00.053.172 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.182 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.184 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.078 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.079 I llama_new_context_with_model: n_ctx         = 128
0.00.054.079 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.079 I llama_new_context_with_model: n_batch       = 128
0.00.054.079 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.080 I llama_new_context_with_model: flash_attn    = 0
0.00.054.080 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.080 I llama_new_context_with_model: freq_scale    = 1
0.00.054.081 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.081 I ggml_metal_init: allocating
0.00.054.087 I ggml_metal_init: found device: Apple M4
0.00.054.090 I ggml_metal_init: picking default device: Apple M4
0.00.054.612 I ggml_metal_init: using embedded metal library
0.00.056.915 I ggml_metal_init: GPU name:   Apple M4
0.00.056.916 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.917 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.917 I ggml_metal_init: simdgroup reduction   = true
0.00.056.917 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.918 I ggml_metal_init: has bfloat            = true
0.00.056.918 I ggml_metal_init: use bfloat            = true
0.00.056.918 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.919 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.314 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.318 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.331 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.171 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.172 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.172 I llama_new_context_with_model: graph nodes  = 967
0.00.068.172 I llama_new_context_with_model: graph splits = 2
0.00.068.185 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.325.698 I 
0.00.325.730 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.325.744 I perplexity: tokenizing the input ..
0.00.333.829 I perplexity: tokenization took 8.084 ms
0.00.333.842 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.474.475 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.475.886 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.475.908 I llama_perf_context_print:        load time =     315.75 ms
0.00.475.909 I llama_perf_context_print: prompt eval time =     140.31 ms /   128 tokens (    1.10 ms per token,   912.30 tokens per second)
0.00.475.910 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.475.911 I llama_perf_context_print:       total time =     150.21 ms /   129 tokens
0.00.476.377 I ggml_metal_free: deallocating

real	0m0.493s
user	0m0.079s
sys	0m0.075s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.239 I build: 4254 (91c36c26) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.362 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.254 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.258 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.260 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.261 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.261 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.267 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.267 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.282 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.283 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.284 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.288 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.288 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.289 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.292 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.293 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.293 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.016 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.786 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.412 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.414 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.414 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.415 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.415 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.416 I llama_model_loader: - type  f32:  194 tensors
0.00.050.416 I llama_model_loader: - type  f16:   98 tensors
0.00.077.933 I llm_load_vocab: special tokens cache size = 25
0.00.084.417 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.419 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.420 I llm_load_print_meta: arch             = gptneox
0.00.084.420 I llm_load_print_meta: vocab type       = BPE
0.00.084.420 I llm_load_print_meta: n_vocab          = 50304
0.00.084.420 I llm_load_print_meta: n_merges         = 50009
0.00.084.420 I llm_load_print_meta: vocab_only       = 0
0.00.084.421 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.421 I llm_load_print_meta: n_embd           = 2048
0.00.084.421 I llm_load_print_meta: n_layer          = 24
0.00.084.423 I llm_load_print_meta: n_head           = 16
0.00.084.424 I llm_load_print_meta: n_head_kv        = 16
0.00.084.424 I llm_load_print_meta: n_rot            = 32
0.00.084.425 I llm_load_print_meta: n_swa            = 0
0.00.084.425 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.425 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.426 I llm_load_print_meta: n_gqa            = 1
0.00.084.427 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.428 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.429 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.429 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.430 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.430 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.430 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.430 I llm_load_print_meta: n_ff             = 8192
0.00.084.431 I llm_load_print_meta: n_expert         = 0
0.00.084.432 I llm_load_print_meta: n_expert_used    = 0
0.00.084.433 I llm_load_print_meta: causal attn      = 1
0.00.084.433 I llm_load_print_meta: pooling type     = 0
0.00.084.433 I llm_load_print_meta: rope type        = 2
0.00.084.433 I llm_load_print_meta: rope scaling     = linear
0.00.084.433 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.433 I llm_load_print_meta: freq_scale_train = 1
0.00.084.434 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.434 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.434 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.434 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.434 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.434 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.434 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.446 I llm_load_print_meta: model type       = 1.4B
0.00.084.447 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.084.447 I llm_load_print_meta: model params     = 1.41 B
0.00.084.447 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.084.447 I llm_load_print_meta: general.name     = 1.4B
0.00.084.448 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.448 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.448 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.448 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.448 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.084.450 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.450 I llm_load_print_meta: max token length = 1024
0.00.086.892 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.086.892 I llm_load_tensors: offloading output layer to GPU
0.00.086.893 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.086.903 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.086.904 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.087.799 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.800 I llama_new_context_with_model: n_ctx         = 128
0.00.087.800 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.087.800 I llama_new_context_with_model: n_batch       = 128
0.00.087.800 I llama_new_context_with_model: n_ubatch      = 128
0.00.087.800 I llama_new_context_with_model: flash_attn    = 0
0.00.087.801 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.801 I llama_new_context_with_model: freq_scale    = 1
0.00.087.801 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.802 I ggml_metal_init: allocating
0.00.087.805 I ggml_metal_init: found device: Apple M4
0.00.087.807 I ggml_metal_init: picking default device: Apple M4
0.00.088.358 I ggml_metal_init: using embedded metal library
0.00.090.777 I ggml_metal_init: GPU name:   Apple M4
0.00.090.778 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.779 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.779 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.779 I ggml_metal_init: simdgroup reduction   = true
0.00.090.779 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.780 I ggml_metal_init: has bfloat            = true
0.00.090.780 I ggml_metal_init: use bfloat            = true
0.00.090.780 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.753 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.757 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.771 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.589 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.591 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.591 I llama_new_context_with_model: graph nodes  = 967
0.00.101.591 I llama_new_context_with_model: graph splits = 2
0.00.101.603 I 
0.00.101.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.101.638 I compute_imatrix: tokenizing the input ..
0.00.108.528 I compute_imatrix: tokenization took 6.89 ms
0.00.108.530 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.528.508 I compute_imatrix: 1.42 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.531.578 I llama_perf_context_print:        load time =    1507.15 ms
0.01.531.579 I llama_perf_context_print: prompt eval time =    1419.36 ms /   128 tokens (   11.09 ms per token,    90.18 tokens per second)
0.01.531.580 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.531.580 I llama_perf_context_print:       total time =    1510.21 ms /   129 tokens
0.01.532.069 I ggml_metal_free: deallocating

real	0m1.715s
user	0m0.168s
sys	0m0.238s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4254 (91c36c26)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122c0aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122c0b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122c0bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122c0c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122c0c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122c0cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122c0d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122c0d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122c0de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122c0e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122c0e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122c0ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122c0f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122c0fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122c10800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122c10f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122c11640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122c11d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122c12480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122c12c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122c13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122c13a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122c141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122c14a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122c15170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122c15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122c15a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122c166b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122c16bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122c16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122c17350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122c17610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122c17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122c183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122c186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122c18b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122c18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122c19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122c19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122c19dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122c1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122c1a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122c1aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122c1b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122c1b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122c1b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122c1bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122c1c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122c1ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122c1d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122c1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122c1e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122c1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122c1eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122c1f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x122c1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122c1fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122c20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122c206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122c20e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122c21150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122c215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x122c21a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122c21f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x122c223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122c22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122c22d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122c231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122c23650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122c23af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122c23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122c24430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122c248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122c24e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122c25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122c258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122c25e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122c26360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122c268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122c26e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122c27350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122c278a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122c27df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122c28340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122c28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122c28de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122c29330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122c29880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122c29dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122c2a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122c2a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122c2adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122c2b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122c2b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122c2bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122c2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122c2c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122c1c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122c2ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122c2d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122c2d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122c2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122c2e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122c2e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122c2ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122c2f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122c2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122c2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122c30440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122c30990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122c30ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122c31430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122c31980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122c31e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122c322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122c32760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122c32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122c330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122c33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122c339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122c33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122c34320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122c347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122c34c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122c35100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122c355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122c35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122c35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122c36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122c36820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122c36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122c37160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122c37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122c37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122c37f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122c383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122c38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122c38d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122c391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122c39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122c39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122c39fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122c3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122c3a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122c3ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122c3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122c3b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122c3bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122c3c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122c3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122c3c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122c3cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122c3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122c3d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122c3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122c3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122c3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122c3e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122c3ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122c3f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122c3f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122c3fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122c400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122c40560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122c40a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122c40ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122c41340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122c417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122c41c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122c42120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122c425c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122c42a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122c42f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122c433a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122c43840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122c43ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122c44180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122c44620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122c44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122c44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122c45400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122c458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122c45d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122c461e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122c46680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122c46b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122c46fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122c47460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122c47900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122c47da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122c48240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122c486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122c48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122c490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122c49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122c49b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122c4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122c4a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122c4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122c4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122c4b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122c4bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122c4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122c4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122c4cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122c4d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122c4d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122c4dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122c4e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122c4e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122c4ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122c4f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122c4f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122c4fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122c50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122c50870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122c50dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122c51310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122c51860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122c51db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122c52300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122c52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122c52da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122c532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122c53840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122c53d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122c542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122c54830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122c54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122c552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122c55820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122c55d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122c562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122c56810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122c56d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122c572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122c57800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122c57d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122c582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122c587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122c58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122c59290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122c597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122c59d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122c5a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122c5a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122c5ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122c5b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122c5b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122c5bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122c5c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122c5c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122c5cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122c5d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122c5d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122c5dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122c5e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122c5e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122c5ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122c5f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122c5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122c5fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122c60220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122c60770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122c60cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122c61210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122c616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122c61b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122c61ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122c62490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122c62930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122c62dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122c63270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122c63710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122c63bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122c64050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122c644f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122c64990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122c64e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122c65380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122c65aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122c661c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122c668e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122c67000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122c672c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122c67ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122c67d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122c68380 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.137.983 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122c58520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122c58990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122c58e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122c59270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122c596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122c59b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122c59fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122c5a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122c5a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122c5ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122c5b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122c5b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122c5c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122c5c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122c5cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122c5d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122c5dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122c5e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122c5eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122c5f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122c5fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122c602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122c609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122c610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122c617a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122c61c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122c62080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122c624f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122c62960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122c62dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122c63240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122c636b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122c63b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122c63de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122c64250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122c646c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122c64b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122c64fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122c65410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122c65880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122c65cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122c66160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122c665d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122c66a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122c66eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122c67320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122c67790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122c67c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122c68070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122c684e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122c0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122c0bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122c258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122c25b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122c25fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x122c26440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122c268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122c26d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122c27190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122c27600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122c27a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122c27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x122c28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122c287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x122c28c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122c290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122c29510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122c29980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122c29df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122c2a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122c2a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122c2ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122c2afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122c2b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122c2b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122c2bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122c2c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122c2c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122c2ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122c2cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122c2d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122c2d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122c2dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122c2e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122c2e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122c2e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122c2edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122c2f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122c2f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122c2fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122c2ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122c30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122c30870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122c30ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122c31150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122c315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122c31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122c31ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122c32310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122c32780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122c32bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122c33060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122c334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122c33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122c33db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122c34220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122c34690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122c34b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122c34f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122c353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122c35850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122c35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122c36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122c365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122c36a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122c36e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122c372f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122c37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122c37bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122c38040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122c384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122c38920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122c38d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122c39200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122c39670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122c39ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122c39f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122c3a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122c3a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122c3aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122c3b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122c3b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122c3b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122c3be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122c3c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122c3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122c3cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122c3d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122c3d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122c3d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122c3dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122c3e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122c3e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122c3eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122c3ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122c3f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122c3f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122c3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122c400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122c40560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122c409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122c40e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122c412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122c41720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122c41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122c42000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122c42470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122c428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122c42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122c431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122c43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122c43aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122c43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122c44380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122c447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122c44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122c450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122c45540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122c459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122c45e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122c46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122c46700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122c46b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122c46fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122c47450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122c478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122c47d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122c481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122c48610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122c48a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122c48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122c49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122c497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122c49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122c4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122c4a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122c4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122c4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122c4b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122c4b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122c4bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122c4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122c4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122c4c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122c4cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122c4d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122c4d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122c4da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122c4ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122c4e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122c4e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122c4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122c4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122c4f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122c4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122c4fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122c50250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122c506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122c50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122c512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122c51720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122c51b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122c52000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122c52470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122c528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122c52d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122c531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122c53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122c53aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122c53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122c54380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122c547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122c54c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122c550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122c55540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122c559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122c55e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122c56290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122c56700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122c56b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122c56fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122c57450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122c183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122c18690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122c18b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122c18f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122c193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122c19850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122c19cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122c1a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122c1a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122c1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122c1ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122c1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122c1b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122c1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122c1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122c1c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122c1c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122c1cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122c1d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122c1d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122c1dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122c1df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122c1e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122c1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122c1eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122c1f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122c1f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122c1f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122c1fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122c202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122c20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122c20bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122c21020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122c21490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122c21900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122c21d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122c221e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122c22650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122c22ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122c22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122c233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122c23810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122c23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122c240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122c24560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122c249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122c24e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122c252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122c0e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122c0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122c0f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122c0fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122c10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122c104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122c10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122c10da0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122c183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122c18840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122c18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122c19120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122c19590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122c19a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122c19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122c1a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122c1a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122c1abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122c1b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122c1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122c1bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122c1c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122c1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122c1d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122c1dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122c1e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122c1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122c1f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122c1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122c20180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122c20870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122c20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122c21650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122c21ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122c21f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122c223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122c22810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122c22c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122c230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122c23560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122c239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122c23c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122c24100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122c24570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122c249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122c24e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122c252c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122c25a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122c25ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122c26330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122c267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122c26c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122c27080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122c274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122c27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122c27dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122c28240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122c286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122c28b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122c28f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122c29400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122c29870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122c29ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x122c2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122c2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122c2aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122c2aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122c2b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122c2b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122c2bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x122c2c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122c2c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x122c2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122c2cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122c2d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122c2d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122c2db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122c2df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122c2e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122c2e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122c2ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122c2f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122c2f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122c2fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122c2fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122c302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122c30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122c30bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122c31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122c314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122c31920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122c31d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122c32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122c32670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122c32ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122c32f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122c333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122c33830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122c33ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122c34110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122c34580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122c349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122c34e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122c352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122c35740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122c35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122c36020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122c36490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122c36900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122c36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122c371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122c37650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122c37ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122c37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122c383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122c38810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122c38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122c390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122c39560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122c399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122c39e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122c3a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122c3a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122c3ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122c3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122c3b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122c3b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122c3bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122c3c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122c3c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122c3caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122c3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122c3d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122c3d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122c3dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122c3e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122c3e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122c3e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122c3ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122c3f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122c3f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122c3fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122c3ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122c40450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122c408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122c40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122c411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122c41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122c41a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122c41ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122c42360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122c427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122c42c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122c430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122c43520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122c43990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122c43e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122c44270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122c446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122c44b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122c44fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122c45430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122c458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122c45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122c46180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122c465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122c46a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122c46ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122c47340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122c477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122c47c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122c48090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122c48500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122c48970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122c48de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122c49250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122c496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122c49b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122c49fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122c4a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122c4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122c4acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122c4b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122c4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122c4ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122c4beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122c4c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122c4c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122c4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122c4d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122c4d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122c4d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122c4ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122c4e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122c4e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122c4eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122c4ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122c4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122c4f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122c4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122c50140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122c505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122c50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122c50e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122c51300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122c51770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122c51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122c52050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122c524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122c52930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122c52da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122c53210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122c53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122c53af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122c53f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122c543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122c54840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122c54fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122c55430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122c558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122c55d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122c56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122c565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122c56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122c56ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122c57340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122c577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122c0bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122c0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122c58270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122c586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122c58b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122c58fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122c59430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122c598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122c59d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122c5a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122c5a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122c5aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122c5aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122c5b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122c5b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122c5bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122c5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122c5c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122c5c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122c5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122c5d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122c5d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122c5db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122c5dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122c5e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122c5e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122c5ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122c5f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122c5f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122c5fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122c5feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122c60320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122c60790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122c60c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122c61070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122c614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122c61950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122c61dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122c62230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122c626a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122c62b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122c62f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122c633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122c63860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122c63cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122c64140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122c645b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122c64a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122c64e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122c65300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122c65770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122c65be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122c66050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122c664c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122c66930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122c66da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122c67210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122c67680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122c67af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122c67f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122c683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122c0e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122c0ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122c0f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122c0fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122c101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122c10630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122c10aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122c10f10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.777s
user	0m0.290s
sys	0m0.297s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4254 (91c36c26)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125607bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125608330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1256088e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125608e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125609440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1256099f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125609fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12560a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12560ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12560b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12560b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12560ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12560c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12560ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12560d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12560dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12560e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12560ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12560f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12560f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125610050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125610770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125610e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125611730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125611e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125612110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125612720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125613390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1256138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125613b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125614030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1256142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125614b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1256150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125615380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125615820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125615cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125616160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125616600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125616aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125616f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1256173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125617880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125617d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125617fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1256185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125618c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125619520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125619b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12561a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12561a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12561ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12561b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12561b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12561c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12561c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12561cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12561cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12561d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12561db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12561de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12561e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12561e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12561ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12561f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12561f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12561f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12561fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125620330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1256207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125620c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125621110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1256215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125621b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125622050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1256225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125622af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125623040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125623590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125623ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125624030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125624580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125624ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125625020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125625570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125625ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125626010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125626560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125626ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125627000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125627550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125627aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125627ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125628540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125628fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125629530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125619210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1256299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12562a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12562a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12562abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12562b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12562b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12562bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12562c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12562c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12562cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12562d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12562d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12562dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12562e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12562e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12562eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12562efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12562f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12562f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12562fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125630220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1256306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125630b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125631000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1256314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125631940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125631de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125632280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125632720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125632bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125633060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125633500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1256339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125633e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1256342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125634780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125634c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1256350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125635560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125635a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125635ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125636340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1256367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125636c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125637120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1256375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125637a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125637f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1256383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125638840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125638ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125639180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125639620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125639ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125639f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12563a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12563a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12563ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12563b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12563b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12563bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12563bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12563c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12563c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12563cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12563d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12563d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12563db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12563e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12563e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12563e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12563ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12563f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12563f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12563fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125640080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125640520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1256409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125640e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125641300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1256417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125641c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1256420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125642580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125642a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125642ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125643360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125643800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125643ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125644140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1256445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125644a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125644f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1256453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125645860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125645db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125646300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125646850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125646da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125647060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125647670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125647c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125648290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125648a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125648f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1256491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1256497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125649fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12564a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12564a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12564adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12564b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12564bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12564c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12564c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12564cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12564d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12564d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12564daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12564dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12564e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12564ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12564efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12564f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12564fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12564ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125650520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125650a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125650fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125651510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125651a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125651fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125652500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125652a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125652fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1256534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125653a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125653f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1256544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125654a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125654f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1256554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125655a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125655f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1256564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125656a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125656f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1256574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125657a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125657f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1256584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1256589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125658f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125659490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1256599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125659f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12565a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12565a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12565af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12565b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12565b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12565bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12565c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12565c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12565cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12565d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12565d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12565def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12565e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12565e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12565ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12565f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12565f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12565fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12565ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1256603f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125660890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125660d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1256611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125661670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125661b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125662060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125662780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125662ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1256635c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125663ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125663fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125664790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125664a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125665060 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.190 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137004ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137005150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1370055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137005a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137005ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137006310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137006780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137006bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137007060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1370075e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137007a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1370080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137008bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1370093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137009bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13700a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13700a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13700b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13700b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13700c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13700c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13700ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13700d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13700dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13700e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13700e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13700e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13700ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13700f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13700f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13700fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137010010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137010480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137010740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137010bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137011020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137011490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137011900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137011d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1370121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137012650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137012ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137012f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1370133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137013810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137013c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1370140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137014560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1370149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137014e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1370152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137015720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137015b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137016000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137016470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1370168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137016e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137017350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1370177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137017c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1370180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137018510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137018980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137018df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137019260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1370196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137019b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137019fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13701a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13701a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13701ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13701b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13701b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13701ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13701bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13701c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13701c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13701cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13701d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13701d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13701d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13701ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13701e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13701e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13701eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13701ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13701f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13701f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13701fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137020150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1370205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137020a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137020ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137021310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137021780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137021bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137022060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1370224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137022940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137022db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137023220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137023690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137023b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137023f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1370243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137024850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137024cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137025130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1370255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137025a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137025e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1370262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137026760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137026bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137027040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1370274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137027920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137027d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137028200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137028670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137028ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137028f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1370293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137029830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137029ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13702a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13702a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13702a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13702ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13702b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13702b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13702bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13702c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13702c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13702c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13702cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13702d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13702d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13702dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13702df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13702e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13702e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13702ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13702f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13702f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13702f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13702fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1370302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137030720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137030b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137031000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137031470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1370318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137031d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1370321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137032630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137032aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137032f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137033380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1370337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137033c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1370340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137034540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1370349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137034e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137035290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137035700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137035b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137035fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137036450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1370368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137036d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1370371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137037610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137037a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137037ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137038360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1370387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137038c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1370390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137039520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137039990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137039e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13703a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13703a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13703ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13703afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13703b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13703b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13703bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13703c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13703c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13703ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13703ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13703d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13703d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13703dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13703e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13703e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13703e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13703ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13703f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13703f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13703fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13703ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137040410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137040880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137040cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137041160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137041cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137041f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137042250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1370426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137042b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137042fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137043880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137043cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137044160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1370445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137044a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137044eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137045320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137045790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137045c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137046070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1370464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137046950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137046dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137047230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1370476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137047b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137047f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1370483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137048860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137048cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137049140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1370495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137049a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137049e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13704a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13704a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13704abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13704b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13704b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13704b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13704bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13704c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13704c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13704caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13704cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13704d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13704d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13704dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13704e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13704e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13704ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13704ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13704f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13704f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13704fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137050030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1370504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137050910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137050d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1370511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137051660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137051ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137051f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1370523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137052820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137052c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137053100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137053570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1370539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137053e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1370542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137054730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137054ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137055010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137055b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137056270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137056990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1370570b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137057370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137057630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137057aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137057f10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13560a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13560abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13560b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13560b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13560bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13560c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13560c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13560cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13560d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13560d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13560dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13560dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13560e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13560ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13560f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13560fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x135610590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x135610cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1356113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x135611d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1356124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x135612bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1356132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x135613a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x135614120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1356143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1356149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x135615000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x135615610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x135615e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1356162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x135616560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x135616df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x135617330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1356175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1356183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135618870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135618d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1356191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135619650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135619f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13561a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13561a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13561ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13561b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13561ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13561c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13561c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13561ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13561d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13561d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13561e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13561e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13561ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137004880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137004cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137005160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1370055d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137005a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137005eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137006320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137006790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137006c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137007070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1370074e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137007950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137007dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137008230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1370086a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137008b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137008f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1370093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137009860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137009cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13700a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13700a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13700aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13700ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13700b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13700b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13700bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13700c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13700c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13700c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13700cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13700d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13700d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13700daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13700df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13700e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13700e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13700ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13700f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13700f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13700fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13700fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1370102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137010750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137010bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137011030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1370114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137011910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137011d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1370121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137012660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137012ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137012f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1370133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137013820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137013c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137014100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137014570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1370149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137014e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1370152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137015730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137015ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137016010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137016480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1370168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137016d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1370171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137017640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137017ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137017f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137018390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137018800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137018c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1370190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137019550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1370199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137019e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13701a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13701a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13701ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13701aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13701b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13701b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13701bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13701c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13701c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13701ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13701cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13701d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13701d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13701dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13701e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13701e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13701e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13701ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13701f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13701f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13701fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13701ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137020440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1370208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137020d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137021190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137021600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137021a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137021ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137022350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1370227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137022c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1370230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137023510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137023980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137023df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137024260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1370246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137024b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137024fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137025420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137025890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137025d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137026170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1370265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137026a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137026ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137027330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1370277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137027c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137028080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1370284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137028960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137028dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137029240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1370296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137029b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137029f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13702a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13702a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13702ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13702b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13702b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13702ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13702bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13702c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13702c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13702cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13702d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13702d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13702d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13702ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13702e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13702e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13702ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13702f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13702f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13702fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137030140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1370305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137030a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137030e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137031300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137031770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137031be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137032050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1370324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137032930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137032da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137033210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137033680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137033af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137033f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1370343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137034840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137034cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137035120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137035590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137035a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137035e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1370362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137036750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137036bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137037030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1370374a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137037910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137037d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1370381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137038660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137038d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1370391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137039660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137039ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137039f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13703a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13703a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13703ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13703b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13703b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13703b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13703be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13703c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13703c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13703cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13703d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13703d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13703d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13703dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13703e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13703e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13703eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13703ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13703f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13703f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13703fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1370400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137040550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1370409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137040e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1370412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137041710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137041b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137041ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137042460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1370428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137042fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1370436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137043da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137044490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137044900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137044d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1370451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137045650 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.928s
user	0m0.243s
sys	0m0.145s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
