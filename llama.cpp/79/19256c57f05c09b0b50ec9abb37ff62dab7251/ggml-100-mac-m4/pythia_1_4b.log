Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.5s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.791s
user	0m0.881s
sys	0m1.301s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Built target llava
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple-chat
[ 37%] Built target llava_static
[ 37%] Built target llava_shared
[ 37%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-sampling
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-log
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-arg-parser
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-quantize-perf
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-rope
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-barrier
[ 63%] Built target test-chat-template
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-gguf
[ 64%] Built target test-backend-ops
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-autorelease
[ 64%] Built target test-rope
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-barrier
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Built target llama-batched-bench
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-batched
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Built target llama-bench
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-infill
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-lookahead
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-cli
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-parallel
[ 83%] Built target llama-passkey
[ 83%] Built target llama-perplexity
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-quantize
[ 85%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Built target llama-retrieval
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-run
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-run
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-cvector-generator
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.108s
user	0m6.150s
sys	0m9.643s

main: quantize time =  2887.33 ms
main:    total time =  2887.33 ms

main: quantize time =  2276.99 ms
main:    total time =  2276.99 ms

main: quantize time =  1494.44 ms
main:    total time =  1494.44 ms

main: quantize time =  2559.09 ms
main:    total time =  2559.09 ms

main: quantize time =  5360.39 ms
main:    total time =  5360.39 ms

main: quantize time =  5602.05 ms
main:    total time =  5602.05 ms

main: quantize time =  5792.11 ms
main:    total time =  5792.11 ms

main: quantize time =  7083.17 ms
main:    total time =  7083.17 ms

main: quantize time =  6162.64 ms
main:    total time =  6162.64 ms

main: quantize time =  4661.54 ms
main:    total time =  4661.54 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.156 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.333 I main: llama backend init
0.00.000.340 I main: load the model and apply lora adapter, if any
0.00.073.770 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.085.877 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.085.889 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.085.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.085.899 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.085.900 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.085.900 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.085.901 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.085.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.085.904 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.085.905 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.085.906 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.085.907 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.085.907 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.085.908 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.085.913 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.085.913 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.085.914 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.092.722 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.094.866 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.101.577 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.101.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.101.584 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.101.585 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.101.586 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.101.587 I llama_model_loader: - type  f32:  194 tensors
0.00.101.588 I llama_model_loader: - type  f16:   98 tensors
0.00.101.590 I print_info: file format = GGUF V3 (latest)
0.00.101.591 I print_info: file type   = all F32 (guessed)
0.00.101.598 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.120.123 I load: special tokens cache size = 25
0.00.130.600 I load: token to piece cache size = 0.2984 MB
0.00.130.606 I print_info: arch             = gptneox
0.00.130.607 I print_info: vocab_only       = 0
0.00.130.607 I print_info: n_ctx_train      = 2048
0.00.130.607 I print_info: n_embd           = 2048
0.00.130.610 I print_info: n_layer          = 24
0.00.130.615 I print_info: n_head           = 16
0.00.130.619 I print_info: n_head_kv        = 16
0.00.130.620 I print_info: n_rot            = 32
0.00.130.620 I print_info: n_swa            = 0
0.00.130.620 I print_info: n_embd_head_k    = 128
0.00.130.620 I print_info: n_embd_head_v    = 128
0.00.130.621 I print_info: n_gqa            = 1
0.00.130.623 I print_info: n_embd_k_gqa     = 2048
0.00.130.624 I print_info: n_embd_v_gqa     = 2048
0.00.130.625 I print_info: f_norm_eps       = 1.0e-05
0.00.130.625 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.130.625 I print_info: f_clamp_kqv      = 0.0e+00
0.00.130.626 I print_info: f_max_alibi_bias = 0.0e+00
0.00.130.626 I print_info: f_logit_scale    = 0.0e+00
0.00.130.627 I print_info: n_ff             = 8192
0.00.130.627 I print_info: n_expert         = 0
0.00.130.627 I print_info: n_expert_used    = 0
0.00.130.628 I print_info: causal attn      = 1
0.00.130.628 I print_info: pooling type     = 0
0.00.130.628 I print_info: rope type        = 2
0.00.130.628 I print_info: rope scaling     = linear
0.00.130.629 I print_info: freq_base_train  = 10000.0
0.00.130.629 I print_info: freq_scale_train = 1
0.00.130.630 I print_info: n_ctx_orig_yarn  = 2048
0.00.130.630 I print_info: rope_finetuned   = unknown
0.00.130.630 I print_info: ssm_d_conv       = 0
0.00.130.631 I print_info: ssm_d_inner      = 0
0.00.130.631 I print_info: ssm_d_state      = 0
0.00.130.631 I print_info: ssm_dt_rank      = 0
0.00.130.631 I print_info: ssm_dt_b_c_rms   = 0
0.00.130.632 I print_info: model type       = 1.4B
0.00.130.632 I print_info: model params     = 1.41 B
0.00.130.632 I print_info: general.name     = 1.4B
0.00.130.633 I print_info: vocab type       = BPE
0.00.130.633 I print_info: n_vocab          = 50304
0.00.130.634 I print_info: n_merges         = 50009
0.00.130.635 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.130.635 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.130.636 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.130.637 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.130.638 I print_info: LF token         = 128 'Ä'
0.00.130.638 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.130.639 I print_info: max token length = 1024
0.00.186.145 I load_tensors: offloading 24 repeating layers to GPU
0.00.186.149 I load_tensors: offloading output layer to GPU
0.00.186.149 I load_tensors: offloaded 25/25 layers to GPU
0.00.186.175 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.186.176 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.186.791 I llama_init_from_model: n_seq_max     = 1
0.00.186.793 I llama_init_from_model: n_ctx         = 2048
0.00.186.793 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.186.793 I llama_init_from_model: n_batch       = 2048
0.00.186.793 I llama_init_from_model: n_ubatch      = 512
0.00.186.793 I llama_init_from_model: flash_attn    = 0
0.00.186.794 I llama_init_from_model: freq_base     = 10000.0
0.00.186.794 I llama_init_from_model: freq_scale    = 1
0.00.186.795 I ggml_metal_init: allocating
0.00.186.827 I ggml_metal_init: found device: Apple M4
0.00.186.832 I ggml_metal_init: picking default device: Apple M4
0.00.187.454 I ggml_metal_init: using embedded metal library
0.00.203.868 I ggml_metal_init: GPU name:   Apple M4
0.00.203.870 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.203.871 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.203.871 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.203.871 I ggml_metal_init: simdgroup reduction   = true
0.00.203.871 I ggml_metal_init: simdgroup matrix mul. = true
0.00.203.872 I ggml_metal_init: has residency sets    = true
0.00.203.872 I ggml_metal_init: has bfloat            = true
0.00.203.872 I ggml_metal_init: use bfloat            = true
0.00.203.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.203.873 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.251.827 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.283.154 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.283.160 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.283.183 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.288.084 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.288.086 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.288.087 I llama_init_from_model: graph nodes  = 967
0.00.288.087 I llama_init_from_model: graph splits = 2
0.00.288.094 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.288.218 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.288.218 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.353.959 I main: llama threadpool init, n_threads = 4
0.00.353.999 I 
0.00.354.031 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.354.032 I 
0.00.354.191 I sampler seed: 1234
0.00.354.195 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.354.220 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.354.222 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.354.222 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.185.425 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.02.185.426 I llama_perf_context_print:        load time =     279.18 ms
0.02.185.426 I llama_perf_context_print: prompt eval time =      43.72 ms /     7 tokens (    6.25 ms per token,   160.11 tokens per second)
0.02.185.429 I llama_perf_context_print:        eval time =    1784.70 ms /    63 runs   (   28.33 ms per token,    35.30 tokens per second)
0.02.185.429 I llama_perf_context_print:       total time =    1832.46 ms /    70 tokens
0.02.185.718 I ggml_metal_free: deallocating

real	0m2.513s
user	0m0.136s
sys	0m0.153s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.011.103 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.646 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.654 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.655 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.656 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.656 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.658 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.658 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.658 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.659 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.659 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.660 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.662 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.662 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.613 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.552 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.553 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.554 I llama_model_loader: - type  f32:  194 tensors
0.00.037.554 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.555 I print_info: file format = GGUF V3 (latest)
0.00.037.558 I print_info: file type   = Q8_0
0.00.037.560 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.720 I load: special tokens cache size = 25
0.00.054.550 I load: token to piece cache size = 0.2984 MB
0.00.054.554 I print_info: arch             = gptneox
0.00.054.554 I print_info: vocab_only       = 0
0.00.054.554 I print_info: n_ctx_train      = 2048
0.00.054.555 I print_info: n_embd           = 2048
0.00.054.555 I print_info: n_layer          = 24
0.00.054.561 I print_info: n_head           = 16
0.00.054.562 I print_info: n_head_kv        = 16
0.00.054.562 I print_info: n_rot            = 32
0.00.054.562 I print_info: n_swa            = 0
0.00.054.563 I print_info: n_embd_head_k    = 128
0.00.054.563 I print_info: n_embd_head_v    = 128
0.00.054.564 I print_info: n_gqa            = 1
0.00.054.564 I print_info: n_embd_k_gqa     = 2048
0.00.054.565 I print_info: n_embd_v_gqa     = 2048
0.00.054.566 I print_info: f_norm_eps       = 1.0e-05
0.00.054.566 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.566 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.566 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.567 I print_info: f_logit_scale    = 0.0e+00
0.00.054.568 I print_info: n_ff             = 8192
0.00.054.568 I print_info: n_expert         = 0
0.00.054.568 I print_info: n_expert_used    = 0
0.00.054.568 I print_info: causal attn      = 1
0.00.054.568 I print_info: pooling type     = 0
0.00.054.568 I print_info: rope type        = 2
0.00.054.569 I print_info: rope scaling     = linear
0.00.054.569 I print_info: freq_base_train  = 10000.0
0.00.054.569 I print_info: freq_scale_train = 1
0.00.054.570 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.570 I print_info: rope_finetuned   = unknown
0.00.054.570 I print_info: ssm_d_conv       = 0
0.00.054.570 I print_info: ssm_d_inner      = 0
0.00.054.570 I print_info: ssm_d_state      = 0
0.00.054.570 I print_info: ssm_dt_rank      = 0
0.00.054.570 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.571 I print_info: model type       = 1.4B
0.00.054.571 I print_info: model params     = 1.41 B
0.00.054.571 I print_info: general.name     = 1.4B
0.00.054.572 I print_info: vocab type       = BPE
0.00.054.572 I print_info: n_vocab          = 50304
0.00.054.572 I print_info: n_merges         = 50009
0.00.054.575 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.575 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.575 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.575 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.576 I print_info: LF token         = 128 'Ä'
0.00.054.576 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.576 I print_info: max token length = 1024
0.01.138.021 I load_tensors: offloading 24 repeating layers to GPU
0.01.138.027 I load_tensors: offloading output layer to GPU
0.01.138.028 I load_tensors: offloaded 25/25 layers to GPU
0.01.138.051 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.138.052 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.138.978 I llama_init_from_model: n_seq_max     = 1
0.01.138.980 I llama_init_from_model: n_ctx         = 2048
0.01.138.980 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.138.981 I llama_init_from_model: n_batch       = 2048
0.01.138.981 I llama_init_from_model: n_ubatch      = 512
0.01.138.981 I llama_init_from_model: flash_attn    = 0
0.01.138.982 I llama_init_from_model: freq_base     = 10000.0
0.01.138.982 I llama_init_from_model: freq_scale    = 1
0.01.138.984 I ggml_metal_init: allocating
0.01.138.993 I ggml_metal_init: found device: Apple M4
0.01.138.999 I ggml_metal_init: picking default device: Apple M4
0.01.140.167 I ggml_metal_init: using embedded metal library
0.01.145.360 I ggml_metal_init: GPU name:   Apple M4
0.01.145.364 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.145.364 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.145.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.145.366 I ggml_metal_init: simdgroup reduction   = true
0.01.145.366 I ggml_metal_init: simdgroup matrix mul. = true
0.01.145.366 I ggml_metal_init: has residency sets    = true
0.01.145.366 I ggml_metal_init: has bfloat            = true
0.01.145.366 I ggml_metal_init: use bfloat            = true
0.01.145.367 I ggml_metal_init: hasUnifiedMemory      = true
0.01.145.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.161.214 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.217.614 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.217.621 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.217.646 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.222.274 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.222.276 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.222.276 I llama_init_from_model: graph nodes  = 967
0.01.222.276 I llama_init_from_model: graph splits = 2
0.01.222.282 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.222.406 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.222.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.276.862 I main: llama threadpool init, n_threads = 4
0.01.276.907 I 
0.01.276.930 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.276.931 I 
0.01.277.106 I sampler seed: 1234
0.01.277.111 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.277.158 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.277.162 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.277.163 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.356.786 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51636.36 tokens per second)
0.02.356.786 I llama_perf_context_print:        load time =    1264.87 ms
0.02.356.787 I llama_perf_context_print: prompt eval time =      39.81 ms /     7 tokens (    5.69 ms per token,   175.81 tokens per second)
0.02.356.788 I llama_perf_context_print:        eval time =    1036.73 ms /    63 runs   (   16.46 ms per token,    60.77 tokens per second)
0.02.356.789 I llama_perf_context_print:       total time =    1080.81 ms /    70 tokens
0.02.357.005 I ggml_metal_free: deallocating

real	0m2.378s
user	0m0.110s
sys	0m0.280s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.016.375 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.570 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.577 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.578 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.578 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.578 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.579 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.580 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.580 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.580 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.581 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.581 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.582 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.582 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.584 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.585 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.585 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.910 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.024 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.308 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.309 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.310 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.310 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.310 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.311 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.311 I llama_model_loader: - type  f32:  194 tensors
0.00.045.311 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.312 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.312 I print_info: file format = GGUF V3 (latest)
0.00.045.313 I print_info: file type   = Q4_0
0.00.045.314 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.054.507 I load: special tokens cache size = 25
0.00.062.222 I load: token to piece cache size = 0.2984 MB
0.00.062.226 I print_info: arch             = gptneox
0.00.062.226 I print_info: vocab_only       = 0
0.00.062.227 I print_info: n_ctx_train      = 2048
0.00.062.227 I print_info: n_embd           = 2048
0.00.062.227 I print_info: n_layer          = 24
0.00.062.231 I print_info: n_head           = 16
0.00.062.232 I print_info: n_head_kv        = 16
0.00.062.232 I print_info: n_rot            = 32
0.00.062.233 I print_info: n_swa            = 0
0.00.062.233 I print_info: n_embd_head_k    = 128
0.00.062.233 I print_info: n_embd_head_v    = 128
0.00.062.234 I print_info: n_gqa            = 1
0.00.062.235 I print_info: n_embd_k_gqa     = 2048
0.00.062.235 I print_info: n_embd_v_gqa     = 2048
0.00.062.236 I print_info: f_norm_eps       = 1.0e-05
0.00.062.237 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.237 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.237 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.237 I print_info: f_logit_scale    = 0.0e+00
0.00.062.238 I print_info: n_ff             = 8192
0.00.062.238 I print_info: n_expert         = 0
0.00.062.239 I print_info: n_expert_used    = 0
0.00.062.239 I print_info: causal attn      = 1
0.00.062.239 I print_info: pooling type     = 0
0.00.062.239 I print_info: rope type        = 2
0.00.062.239 I print_info: rope scaling     = linear
0.00.062.240 I print_info: freq_base_train  = 10000.0
0.00.062.240 I print_info: freq_scale_train = 1
0.00.062.240 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.241 I print_info: rope_finetuned   = unknown
0.00.062.241 I print_info: ssm_d_conv       = 0
0.00.062.241 I print_info: ssm_d_inner      = 0
0.00.062.241 I print_info: ssm_d_state      = 0
0.00.062.241 I print_info: ssm_dt_rank      = 0
0.00.062.241 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.241 I print_info: model type       = 1.4B
0.00.062.242 I print_info: model params     = 1.41 B
0.00.062.242 I print_info: general.name     = 1.4B
0.00.062.243 I print_info: vocab type       = BPE
0.00.062.243 I print_info: n_vocab          = 50304
0.00.062.243 I print_info: n_merges         = 50009
0.00.062.243 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.244 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.244 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.244 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.244 I print_info: LF token         = 128 'Ä'
0.00.062.245 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.245 I print_info: max token length = 1024
0.00.635.167 I load_tensors: offloading 24 repeating layers to GPU
0.00.635.184 I load_tensors: offloading output layer to GPU
0.00.635.184 I load_tensors: offloaded 25/25 layers to GPU
0.00.635.220 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.635.221 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.636.810 I llama_init_from_model: n_seq_max     = 1
0.00.636.816 I llama_init_from_model: n_ctx         = 2048
0.00.636.817 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.636.817 I llama_init_from_model: n_batch       = 2048
0.00.636.818 I llama_init_from_model: n_ubatch      = 512
0.00.636.818 I llama_init_from_model: flash_attn    = 0
0.00.636.820 I llama_init_from_model: freq_base     = 10000.0
0.00.636.821 I llama_init_from_model: freq_scale    = 1
0.00.636.828 I ggml_metal_init: allocating
0.00.636.934 I ggml_metal_init: found device: Apple M4
0.00.636.948 I ggml_metal_init: picking default device: Apple M4
0.00.638.831 I ggml_metal_init: using embedded metal library
0.00.645.350 I ggml_metal_init: GPU name:   Apple M4
0.00.645.356 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.645.357 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.645.358 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.645.358 I ggml_metal_init: simdgroup reduction   = true
0.00.645.359 I ggml_metal_init: simdgroup matrix mul. = true
0.00.645.359 I ggml_metal_init: has residency sets    = true
0.00.645.359 I ggml_metal_init: has bfloat            = true
0.00.645.359 I ggml_metal_init: use bfloat            = true
0.00.645.360 I ggml_metal_init: hasUnifiedMemory      = true
0.00.645.362 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.562 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.719.717 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.719.724 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.719.759 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.723.981 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.723.983 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.723.983 I llama_init_from_model: graph nodes  = 967
0.00.723.983 I llama_init_from_model: graph splits = 2
0.00.723.988 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.724.121 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.122 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.877 I main: llama threadpool init, n_threads = 4
0.00.778.923 I 
0.00.778.948 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.950 I 
0.00.779.125 I sampler seed: 1234
0.00.779.129 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.186 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.189 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.190 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.456.651 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50605.84 tokens per second)
0.01.456.651 I llama_perf_context_print:        load time =     761.60 ms
0.01.456.652 I llama_perf_context_print: prompt eval time =      49.07 ms /     7 tokens (    7.01 ms per token,   142.66 tokens per second)
0.01.456.653 I llama_perf_context_print:        eval time =     625.49 ms /    63 runs   (    9.93 ms per token,   100.72 tokens per second)
0.01.456.653 I llama_perf_context_print:       total time =     678.67 ms /    70 tokens
0.01.456.943 I ggml_metal_free: deallocating

real	0m1.476s
user	0m0.114s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.886 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.597 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.602 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.607 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.608 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.608 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.609 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.611 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.611 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.612 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.612 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.612 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.616 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.616 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.618 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.618 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.618 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.236 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.209 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.780 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.781 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.781 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.781 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.782 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.782 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.783 I llama_model_loader: - type  f32:  194 tensors
0.00.024.783 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.783 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.784 I print_info: file format = GGUF V3 (latest)
0.00.024.784 I print_info: file type   = Q4_1
0.00.024.785 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.363 I load: special tokens cache size = 25
0.00.038.081 I load: token to piece cache size = 0.2984 MB
0.00.038.084 I print_info: arch             = gptneox
0.00.038.084 I print_info: vocab_only       = 0
0.00.038.084 I print_info: n_ctx_train      = 2048
0.00.038.084 I print_info: n_embd           = 2048
0.00.038.085 I print_info: n_layer          = 24
0.00.038.088 I print_info: n_head           = 16
0.00.038.088 I print_info: n_head_kv        = 16
0.00.038.088 I print_info: n_rot            = 32
0.00.038.089 I print_info: n_swa            = 0
0.00.038.089 I print_info: n_embd_head_k    = 128
0.00.038.089 I print_info: n_embd_head_v    = 128
0.00.038.090 I print_info: n_gqa            = 1
0.00.038.093 I print_info: n_embd_k_gqa     = 2048
0.00.038.093 I print_info: n_embd_v_gqa     = 2048
0.00.038.094 I print_info: f_norm_eps       = 1.0e-05
0.00.038.094 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.094 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.095 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.095 I print_info: f_logit_scale    = 0.0e+00
0.00.038.097 I print_info: n_ff             = 8192
0.00.038.097 I print_info: n_expert         = 0
0.00.038.097 I print_info: n_expert_used    = 0
0.00.038.097 I print_info: causal attn      = 1
0.00.038.098 I print_info: pooling type     = 0
0.00.038.104 I print_info: rope type        = 2
0.00.038.106 I print_info: rope scaling     = linear
0.00.038.106 I print_info: freq_base_train  = 10000.0
0.00.038.107 I print_info: freq_scale_train = 1
0.00.038.107 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.107 I print_info: rope_finetuned   = unknown
0.00.038.107 I print_info: ssm_d_conv       = 0
0.00.038.108 I print_info: ssm_d_inner      = 0
0.00.038.109 I print_info: ssm_d_state      = 0
0.00.038.109 I print_info: ssm_dt_rank      = 0
0.00.038.109 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.109 I print_info: model type       = 1.4B
0.00.038.109 I print_info: model params     = 1.41 B
0.00.038.110 I print_info: general.name     = 1.4B
0.00.038.110 I print_info: vocab type       = BPE
0.00.038.110 I print_info: n_vocab          = 50304
0.00.038.111 I print_info: n_merges         = 50009
0.00.038.111 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.111 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.111 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.111 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.111 I print_info: LF token         = 128 'Ä'
0.00.038.112 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.112 I print_info: max token length = 1024
0.00.650.061 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.074 I load_tensors: offloading output layer to GPU
0.00.650.075 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.112 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.650.113 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.651.442 I llama_init_from_model: n_seq_max     = 1
0.00.651.446 I llama_init_from_model: n_ctx         = 2048
0.00.651.447 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.651.447 I llama_init_from_model: n_batch       = 2048
0.00.651.447 I llama_init_from_model: n_ubatch      = 512
0.00.651.448 I llama_init_from_model: flash_attn    = 0
0.00.651.450 I llama_init_from_model: freq_base     = 10000.0
0.00.651.450 I llama_init_from_model: freq_scale    = 1
0.00.651.453 I ggml_metal_init: allocating
0.00.651.527 I ggml_metal_init: found device: Apple M4
0.00.651.542 I ggml_metal_init: picking default device: Apple M4
0.00.653.308 I ggml_metal_init: using embedded metal library
0.00.660.321 I ggml_metal_init: GPU name:   Apple M4
0.00.660.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.660.327 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.660.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.660.328 I ggml_metal_init: simdgroup reduction   = true
0.00.660.328 I ggml_metal_init: simdgroup matrix mul. = true
0.00.660.329 I ggml_metal_init: has residency sets    = true
0.00.660.329 I ggml_metal_init: has bfloat            = true
0.00.660.329 I ggml_metal_init: use bfloat            = true
0.00.660.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.660.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.587 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.162 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.739.170 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.739.194 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.743.362 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.743.364 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.743.364 I llama_init_from_model: graph nodes  = 967
0.00.743.364 I llama_init_from_model: graph splits = 2
0.00.743.372 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.743.505 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.506 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.674 I main: llama threadpool init, n_threads = 4
0.00.799.723 I 
0.00.799.748 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.749 I 
0.00.799.925 I sampler seed: 1234
0.00.799.930 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.940 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.941 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.945 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.523.922 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.01.523.922 I llama_perf_context_print:        load time =     789.94 ms
0.01.523.923 I llama_perf_context_print: prompt eval time =      48.82 ms /     7 tokens (    6.97 ms per token,   143.39 tokens per second)
0.01.523.924 I llama_perf_context_print:        eval time =     672.35 ms /    63 runs   (   10.67 ms per token,    93.70 tokens per second)
0.01.523.924 I llama_perf_context_print:       total time =     725.10 ms /    70 tokens
0.01.524.154 I ggml_metal_free: deallocating

real	0m1.542s
user	0m0.110s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.406 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.630 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.634 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.635 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.636 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.636 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.636 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.637 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.638 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.638 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.639 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.640 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.642 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.643 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.645 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.645 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.646 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.019 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.021 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.021 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.022 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.022 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.022 I llama_model_loader: - type  f32:  194 tensors
0.00.026.023 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.023 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.024 I print_info: file format = GGUF V3 (latest)
0.00.026.024 I print_info: file type   = Q5_0
0.00.026.025 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.465 I load: special tokens cache size = 25
0.00.039.237 I load: token to piece cache size = 0.2984 MB
0.00.039.239 I print_info: arch             = gptneox
0.00.039.240 I print_info: vocab_only       = 0
0.00.039.240 I print_info: n_ctx_train      = 2048
0.00.039.240 I print_info: n_embd           = 2048
0.00.039.240 I print_info: n_layer          = 24
0.00.039.243 I print_info: n_head           = 16
0.00.039.243 I print_info: n_head_kv        = 16
0.00.039.244 I print_info: n_rot            = 32
0.00.039.244 I print_info: n_swa            = 0
0.00.039.244 I print_info: n_embd_head_k    = 128
0.00.039.244 I print_info: n_embd_head_v    = 128
0.00.039.245 I print_info: n_gqa            = 1
0.00.039.246 I print_info: n_embd_k_gqa     = 2048
0.00.039.246 I print_info: n_embd_v_gqa     = 2048
0.00.039.247 I print_info: f_norm_eps       = 1.0e-05
0.00.039.248 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.249 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.249 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.249 I print_info: f_logit_scale    = 0.0e+00
0.00.039.249 I print_info: n_ff             = 8192
0.00.039.250 I print_info: n_expert         = 0
0.00.039.250 I print_info: n_expert_used    = 0
0.00.039.250 I print_info: causal attn      = 1
0.00.039.250 I print_info: pooling type     = 0
0.00.039.251 I print_info: rope type        = 2
0.00.039.253 I print_info: rope scaling     = linear
0.00.039.253 I print_info: freq_base_train  = 10000.0
0.00.039.254 I print_info: freq_scale_train = 1
0.00.039.254 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.254 I print_info: rope_finetuned   = unknown
0.00.039.254 I print_info: ssm_d_conv       = 0
0.00.039.254 I print_info: ssm_d_inner      = 0
0.00.039.254 I print_info: ssm_d_state      = 0
0.00.039.255 I print_info: ssm_dt_rank      = 0
0.00.039.255 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.255 I print_info: model type       = 1.4B
0.00.039.255 I print_info: model params     = 1.41 B
0.00.039.256 I print_info: general.name     = 1.4B
0.00.039.256 I print_info: vocab type       = BPE
0.00.039.256 I print_info: n_vocab          = 50304
0.00.039.256 I print_info: n_merges         = 50009
0.00.039.257 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.257 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.257 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.257 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.257 I print_info: LF token         = 128 'Ä'
0.00.039.258 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.263 I print_info: max token length = 1024
0.00.690.488 I load_tensors: offloading 24 repeating layers to GPU
0.00.690.499 I load_tensors: offloading output layer to GPU
0.00.690.499 I load_tensors: offloaded 25/25 layers to GPU
0.00.690.535 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.690.540 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.692.055 I llama_init_from_model: n_seq_max     = 1
0.00.692.059 I llama_init_from_model: n_ctx         = 2048
0.00.692.059 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.692.060 I llama_init_from_model: n_batch       = 2048
0.00.692.060 I llama_init_from_model: n_ubatch      = 512
0.00.692.060 I llama_init_from_model: flash_attn    = 0
0.00.692.062 I llama_init_from_model: freq_base     = 10000.0
0.00.692.063 I llama_init_from_model: freq_scale    = 1
0.00.692.069 I ggml_metal_init: allocating
0.00.692.135 I ggml_metal_init: found device: Apple M4
0.00.692.149 I ggml_metal_init: picking default device: Apple M4
0.00.693.906 I ggml_metal_init: using embedded metal library
0.00.700.304 I ggml_metal_init: GPU name:   Apple M4
0.00.700.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.700.309 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.700.310 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.700.311 I ggml_metal_init: simdgroup reduction   = true
0.00.700.311 I ggml_metal_init: simdgroup matrix mul. = true
0.00.700.311 I ggml_metal_init: has residency sets    = true
0.00.700.311 I ggml_metal_init: has bfloat            = true
0.00.700.312 I ggml_metal_init: use bfloat            = true
0.00.700.313 I ggml_metal_init: hasUnifiedMemory      = true
0.00.700.315 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.717.380 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.770.506 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.770.513 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.770.548 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.774.874 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.774.876 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.774.876 I llama_init_from_model: graph nodes  = 967
0.00.774.876 I llama_init_from_model: graph splits = 2
0.00.774.883 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.775.015 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.775.016 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.389 I main: llama threadpool init, n_threads = 4
0.00.836.441 I 
0.00.836.467 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.469 I 
0.00.836.634 I sampler seed: 1234
0.00.836.639 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.650 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.650 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.650 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.623.991 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49719.89 tokens per second)
0.01.623.992 I llama_perf_context_print:        load time =     825.02 ms
0.01.623.994 I llama_perf_context_print: prompt eval time =      53.02 ms /     7 tokens (    7.57 ms per token,   132.04 tokens per second)
0.01.623.995 I llama_perf_context_print:        eval time =     731.80 ms /    63 runs   (   11.62 ms per token,    86.09 tokens per second)
0.01.623.995 I llama_perf_context_print:       total time =     788.56 ms /    70 tokens
0.01.624.285 I ggml_metal_free: deallocating

real	0m1.642s
user	0m0.107s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.965 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.966 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.971 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.973 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.974 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.974 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.974 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.975 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.976 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.977 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.977 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.977 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.977 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.978 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.978 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.980 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.980 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.980 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.760 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.822 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.536 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.537 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.537 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.541 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.541 I llama_model_loader: - type  f32:  194 tensors
0.00.027.543 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.543 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.544 I print_info: file format = GGUF V3 (latest)
0.00.027.544 I print_info: file type   = Q5_1
0.00.027.545 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.486 I load: special tokens cache size = 25
0.00.041.439 I load: token to piece cache size = 0.2984 MB
0.00.041.443 I print_info: arch             = gptneox
0.00.041.443 I print_info: vocab_only       = 0
0.00.041.444 I print_info: n_ctx_train      = 2048
0.00.041.444 I print_info: n_embd           = 2048
0.00.041.444 I print_info: n_layer          = 24
0.00.041.447 I print_info: n_head           = 16
0.00.041.448 I print_info: n_head_kv        = 16
0.00.041.448 I print_info: n_rot            = 32
0.00.041.449 I print_info: n_swa            = 0
0.00.041.450 I print_info: n_embd_head_k    = 128
0.00.041.450 I print_info: n_embd_head_v    = 128
0.00.041.452 I print_info: n_gqa            = 1
0.00.041.453 I print_info: n_embd_k_gqa     = 2048
0.00.041.454 I print_info: n_embd_v_gqa     = 2048
0.00.041.454 I print_info: f_norm_eps       = 1.0e-05
0.00.041.454 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.454 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.456 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.456 I print_info: f_logit_scale    = 0.0e+00
0.00.041.456 I print_info: n_ff             = 8192
0.00.041.456 I print_info: n_expert         = 0
0.00.041.457 I print_info: n_expert_used    = 0
0.00.041.457 I print_info: causal attn      = 1
0.00.041.457 I print_info: pooling type     = 0
0.00.041.457 I print_info: rope type        = 2
0.00.041.457 I print_info: rope scaling     = linear
0.00.041.458 I print_info: freq_base_train  = 10000.0
0.00.041.458 I print_info: freq_scale_train = 1
0.00.041.458 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.479 I print_info: rope_finetuned   = unknown
0.00.041.480 I print_info: ssm_d_conv       = 0
0.00.041.480 I print_info: ssm_d_inner      = 0
0.00.041.480 I print_info: ssm_d_state      = 0
0.00.041.481 I print_info: ssm_dt_rank      = 0
0.00.041.481 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.481 I print_info: model type       = 1.4B
0.00.041.482 I print_info: model params     = 1.41 B
0.00.041.482 I print_info: general.name     = 1.4B
0.00.041.483 I print_info: vocab type       = BPE
0.00.041.483 I print_info: n_vocab          = 50304
0.00.041.483 I print_info: n_merges         = 50009
0.00.041.483 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.483 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.483 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.484 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.484 I print_info: LF token         = 128 'Ä'
0.00.041.484 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.484 I print_info: max token length = 1024
0.00.614.622 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.633 I load_tensors: offloading output layer to GPU
0.00.614.634 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.660 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.614.661 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.615.557 I llama_init_from_model: n_seq_max     = 1
0.00.615.564 I llama_init_from_model: n_ctx         = 2048
0.00.615.564 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.615.565 I llama_init_from_model: n_batch       = 2048
0.00.615.565 I llama_init_from_model: n_ubatch      = 512
0.00.615.565 I llama_init_from_model: flash_attn    = 0
0.00.615.567 I llama_init_from_model: freq_base     = 10000.0
0.00.615.567 I llama_init_from_model: freq_scale    = 1
0.00.615.569 I ggml_metal_init: allocating
0.00.615.619 I ggml_metal_init: found device: Apple M4
0.00.615.631 I ggml_metal_init: picking default device: Apple M4
0.00.616.933 I ggml_metal_init: using embedded metal library
0.00.621.502 I ggml_metal_init: GPU name:   Apple M4
0.00.621.508 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.509 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.510 I ggml_metal_init: simdgroup reduction   = true
0.00.621.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.510 I ggml_metal_init: has residency sets    = true
0.00.621.510 I ggml_metal_init: has bfloat            = true
0.00.621.510 I ggml_metal_init: use bfloat            = true
0.00.621.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.097 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.669.071 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.669.079 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.669.105 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.673.608 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.673.610 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.673.611 I llama_init_from_model: graph nodes  = 967
0.00.673.611 I llama_init_from_model: graph splits = 2
0.00.673.615 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.673.745 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.673.745 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.559 I main: llama threadpool init, n_threads = 4
0.00.732.608 I 
0.00.732.632 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.632 I 
0.00.732.821 I sampler seed: 1234
0.00.732.826 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.732.837 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.732.837 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.732.838 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.569.061 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50354.61 tokens per second)
0.01.569.062 I llama_perf_context_print:        load time =     722.66 ms
0.01.569.063 I llama_perf_context_print: prompt eval time =      50.02 ms /     7 tokens (    7.15 ms per token,   139.93 tokens per second)
0.01.569.063 I llama_perf_context_print:        eval time =     783.60 ms /    63 runs   (   12.44 ms per token,    80.40 tokens per second)
0.01.569.064 I llama_perf_context_print:       total time =     837.44 ms /    70 tokens
0.01.569.302 I ggml_metal_free: deallocating

real	0m1.587s
user	0m0.104s
sys	0m0.190s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.919 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.205 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.210 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.215 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.217 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.218 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.218 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.218 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.219 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.221 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.222 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.222 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.879 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.880 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.476 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.477 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.477 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.478 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.478 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.478 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.479 I llama_model_loader: - type  f32:  194 tensors
0.00.024.479 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.479 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.480 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.480 I print_info: file format = GGUF V3 (latest)
0.00.024.481 I print_info: file type   = Q2_K - Medium
0.00.024.481 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.067 I load: special tokens cache size = 25
0.00.037.898 I load: token to piece cache size = 0.2984 MB
0.00.037.901 I print_info: arch             = gptneox
0.00.037.901 I print_info: vocab_only       = 0
0.00.037.901 I print_info: n_ctx_train      = 2048
0.00.037.901 I print_info: n_embd           = 2048
0.00.037.902 I print_info: n_layer          = 24
0.00.037.904 I print_info: n_head           = 16
0.00.037.905 I print_info: n_head_kv        = 16
0.00.037.905 I print_info: n_rot            = 32
0.00.037.905 I print_info: n_swa            = 0
0.00.037.906 I print_info: n_embd_head_k    = 128
0.00.037.906 I print_info: n_embd_head_v    = 128
0.00.037.906 I print_info: n_gqa            = 1
0.00.037.907 I print_info: n_embd_k_gqa     = 2048
0.00.037.908 I print_info: n_embd_v_gqa     = 2048
0.00.037.909 I print_info: f_norm_eps       = 1.0e-05
0.00.037.909 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.909 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.909 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.909 I print_info: f_logit_scale    = 0.0e+00
0.00.037.910 I print_info: n_ff             = 8192
0.00.037.910 I print_info: n_expert         = 0
0.00.037.910 I print_info: n_expert_used    = 0
0.00.037.911 I print_info: causal attn      = 1
0.00.037.912 I print_info: pooling type     = 0
0.00.037.914 I print_info: rope type        = 2
0.00.037.914 I print_info: rope scaling     = linear
0.00.037.914 I print_info: freq_base_train  = 10000.0
0.00.037.915 I print_info: freq_scale_train = 1
0.00.037.915 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.915 I print_info: rope_finetuned   = unknown
0.00.037.915 I print_info: ssm_d_conv       = 0
0.00.037.915 I print_info: ssm_d_inner      = 0
0.00.037.916 I print_info: ssm_d_state      = 0
0.00.037.916 I print_info: ssm_dt_rank      = 0
0.00.037.916 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.916 I print_info: model type       = 1.4B
0.00.037.917 I print_info: model params     = 1.41 B
0.00.037.917 I print_info: general.name     = 1.4B
0.00.037.917 I print_info: vocab type       = BPE
0.00.037.917 I print_info: n_vocab          = 50304
0.00.037.918 I print_info: n_merges         = 50009
0.00.037.919 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.919 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.919 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.920 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.920 I print_info: LF token         = 128 'Ä'
0.00.037.920 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.920 I print_info: max token length = 1024
0.00.347.205 I load_tensors: offloading 24 repeating layers to GPU
0.00.347.222 I load_tensors: offloading output layer to GPU
0.00.347.223 I load_tensors: offloaded 25/25 layers to GPU
0.00.347.261 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.347.262 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.348.582 I llama_init_from_model: n_seq_max     = 1
0.00.348.587 I llama_init_from_model: n_ctx         = 2048
0.00.348.587 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.348.588 I llama_init_from_model: n_batch       = 2048
0.00.348.588 I llama_init_from_model: n_ubatch      = 512
0.00.348.588 I llama_init_from_model: flash_attn    = 0
0.00.348.590 I llama_init_from_model: freq_base     = 10000.0
0.00.348.595 I llama_init_from_model: freq_scale    = 1
0.00.348.608 I ggml_metal_init: allocating
0.00.348.689 I ggml_metal_init: found device: Apple M4
0.00.348.703 I ggml_metal_init: picking default device: Apple M4
0.00.350.563 I ggml_metal_init: using embedded metal library
0.00.356.115 I ggml_metal_init: GPU name:   Apple M4
0.00.356.134 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.356.134 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.356.135 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.356.136 I ggml_metal_init: simdgroup reduction   = true
0.00.356.136 I ggml_metal_init: simdgroup matrix mul. = true
0.00.356.137 I ggml_metal_init: has residency sets    = true
0.00.356.137 I ggml_metal_init: has bfloat            = true
0.00.356.137 I ggml_metal_init: use bfloat            = true
0.00.356.141 I ggml_metal_init: hasUnifiedMemory      = true
0.00.356.145 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.377.379 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.437.165 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.437.174 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.437.207 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.441.952 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.441.954 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.441.954 I llama_init_from_model: graph nodes  = 967
0.00.441.954 I llama_init_from_model: graph splits = 2
0.00.441.960 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.442.092 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.442.093 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.498.747 I main: llama threadpool init, n_threads = 4
0.00.498.795 I 
0.00.498.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.498.816 I 
0.00.498.988 I sampler seed: 1234
0.00.498.993 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.499.003 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.499.004 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.499.004 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.169.717 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.01.169.718 I llama_perf_context_print:        load time =     487.92 ms
0.01.169.719 I llama_perf_context_print: prompt eval time =      35.46 ms /     7 tokens (    5.07 ms per token,   197.38 tokens per second)
0.01.169.719 I llama_perf_context_print:        eval time =     632.49 ms /    63 runs   (   10.04 ms per token,    99.61 tokens per second)
0.01.169.720 I llama_perf_context_print:       total time =     671.88 ms /    70 tokens
0.01.169.958 I ggml_metal_free: deallocating

real	0m1.188s
user	0m0.110s
sys	0m0.176s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.870 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.042 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.047 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.050 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.051 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.051 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.051 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.051 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.053 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.053 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.054 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.054 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.054 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.055 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.055 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.057 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.057 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.057 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.848 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.864 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.570 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.570 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.571 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.571 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.571 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.571 I llama_model_loader: - type  f32:  194 tensors
0.00.024.572 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.572 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.572 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.572 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.573 I print_info: file format = GGUF V3 (latest)
0.00.024.573 I print_info: file type   = Q3_K - Medium
0.00.024.574 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.345 I load: special tokens cache size = 25
0.00.038.288 I load: token to piece cache size = 0.2984 MB
0.00.038.291 I print_info: arch             = gptneox
0.00.038.291 I print_info: vocab_only       = 0
0.00.038.291 I print_info: n_ctx_train      = 2048
0.00.038.292 I print_info: n_embd           = 2048
0.00.038.292 I print_info: n_layer          = 24
0.00.038.294 I print_info: n_head           = 16
0.00.038.295 I print_info: n_head_kv        = 16
0.00.038.295 I print_info: n_rot            = 32
0.00.038.295 I print_info: n_swa            = 0
0.00.038.295 I print_info: n_embd_head_k    = 128
0.00.038.297 I print_info: n_embd_head_v    = 128
0.00.038.298 I print_info: n_gqa            = 1
0.00.038.299 I print_info: n_embd_k_gqa     = 2048
0.00.038.299 I print_info: n_embd_v_gqa     = 2048
0.00.038.304 I print_info: f_norm_eps       = 1.0e-05
0.00.038.305 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.305 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.305 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.305 I print_info: f_logit_scale    = 0.0e+00
0.00.038.306 I print_info: n_ff             = 8192
0.00.038.306 I print_info: n_expert         = 0
0.00.038.307 I print_info: n_expert_used    = 0
0.00.038.307 I print_info: causal attn      = 1
0.00.038.307 I print_info: pooling type     = 0
0.00.038.307 I print_info: rope type        = 2
0.00.038.307 I print_info: rope scaling     = linear
0.00.038.308 I print_info: freq_base_train  = 10000.0
0.00.038.308 I print_info: freq_scale_train = 1
0.00.038.308 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.308 I print_info: rope_finetuned   = unknown
0.00.038.309 I print_info: ssm_d_conv       = 0
0.00.038.309 I print_info: ssm_d_inner      = 0
0.00.038.309 I print_info: ssm_d_state      = 0
0.00.038.309 I print_info: ssm_dt_rank      = 0
0.00.038.309 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.310 I print_info: model type       = 1.4B
0.00.038.317 I print_info: model params     = 1.41 B
0.00.038.319 I print_info: general.name     = 1.4B
0.00.038.320 I print_info: vocab type       = BPE
0.00.038.320 I print_info: n_vocab          = 50304
0.00.038.321 I print_info: n_merges         = 50009
0.00.038.321 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.321 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.321 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.322 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.322 I print_info: LF token         = 128 'Ä'
0.00.038.322 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.323 I print_info: max token length = 1024
0.00.462.130 I load_tensors: offloading 24 repeating layers to GPU
0.00.462.143 I load_tensors: offloading output layer to GPU
0.00.462.143 I load_tensors: offloaded 25/25 layers to GPU
0.00.462.177 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.462.179 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.463.529 I llama_init_from_model: n_seq_max     = 1
0.00.463.534 I llama_init_from_model: n_ctx         = 2048
0.00.463.534 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.463.535 I llama_init_from_model: n_batch       = 2048
0.00.463.535 I llama_init_from_model: n_ubatch      = 512
0.00.463.536 I llama_init_from_model: flash_attn    = 0
0.00.463.537 I llama_init_from_model: freq_base     = 10000.0
0.00.463.538 I llama_init_from_model: freq_scale    = 1
0.00.463.540 I ggml_metal_init: allocating
0.00.463.612 I ggml_metal_init: found device: Apple M4
0.00.463.627 I ggml_metal_init: picking default device: Apple M4
0.00.465.382 I ggml_metal_init: using embedded metal library
0.00.470.796 I ggml_metal_init: GPU name:   Apple M4
0.00.470.813 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.470.814 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.470.815 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.470.816 I ggml_metal_init: simdgroup reduction   = true
0.00.470.816 I ggml_metal_init: simdgroup matrix mul. = true
0.00.470.816 I ggml_metal_init: has residency sets    = true
0.00.470.817 I ggml_metal_init: has bfloat            = true
0.00.470.817 I ggml_metal_init: use bfloat            = true
0.00.470.822 I ggml_metal_init: hasUnifiedMemory      = true
0.00.470.827 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.490.992 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.549.330 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.549.340 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.549.376 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.553.814 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.553.816 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.553.816 I llama_init_from_model: graph nodes  = 967
0.00.553.816 I llama_init_from_model: graph splits = 2
0.00.553.823 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.553.957 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.553.958 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.448 I main: llama threadpool init, n_threads = 4
0.00.609.494 I 
0.00.609.517 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.518 I 
0.00.609.670 I sampler seed: 1234
0.00.609.675 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.609.715 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.609.719 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.609.719 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.355.057 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53828.66 tokens per second)
0.01.355.058 I llama_perf_context_print:        load time =     599.69 ms
0.01.355.058 I llama_perf_context_print: prompt eval time =      50.28 ms /     7 tokens (    7.18 ms per token,   139.22 tokens per second)
0.01.355.060 I llama_perf_context_print:        eval time =     692.17 ms /    63 runs   (   10.99 ms per token,    91.02 tokens per second)
0.01.355.060 I llama_perf_context_print:       total time =     746.49 ms /    70 tokens
0.01.355.326 I ggml_metal_free: deallocating

real	0m1.372s
user	0m0.111s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.863 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.160 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.165 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.166 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.168 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.168 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.169 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.169 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.170 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.170 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.171 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.171 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.172 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.172 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.908 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.931 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.680 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.682 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.683 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.683 I llama_model_loader: - type  f32:  194 tensors
0.00.024.683 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.683 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.683 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.684 I print_info: file format = GGUF V3 (latest)
0.00.024.684 I print_info: file type   = Q4_K - Medium
0.00.024.686 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.445 I load: special tokens cache size = 25
0.00.038.341 I load: token to piece cache size = 0.2984 MB
0.00.038.344 I print_info: arch             = gptneox
0.00.038.344 I print_info: vocab_only       = 0
0.00.038.344 I print_info: n_ctx_train      = 2048
0.00.038.344 I print_info: n_embd           = 2048
0.00.038.345 I print_info: n_layer          = 24
0.00.038.348 I print_info: n_head           = 16
0.00.038.348 I print_info: n_head_kv        = 16
0.00.038.348 I print_info: n_rot            = 32
0.00.038.349 I print_info: n_swa            = 0
0.00.038.349 I print_info: n_embd_head_k    = 128
0.00.038.349 I print_info: n_embd_head_v    = 128
0.00.038.350 I print_info: n_gqa            = 1
0.00.038.351 I print_info: n_embd_k_gqa     = 2048
0.00.038.351 I print_info: n_embd_v_gqa     = 2048
0.00.038.352 I print_info: f_norm_eps       = 1.0e-05
0.00.038.352 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.352 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.355 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.355 I print_info: f_logit_scale    = 0.0e+00
0.00.038.356 I print_info: n_ff             = 8192
0.00.038.356 I print_info: n_expert         = 0
0.00.038.356 I print_info: n_expert_used    = 0
0.00.038.358 I print_info: causal attn      = 1
0.00.038.359 I print_info: pooling type     = 0
0.00.038.360 I print_info: rope type        = 2
0.00.038.361 I print_info: rope scaling     = linear
0.00.038.361 I print_info: freq_base_train  = 10000.0
0.00.038.361 I print_info: freq_scale_train = 1
0.00.038.362 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.362 I print_info: rope_finetuned   = unknown
0.00.038.362 I print_info: ssm_d_conv       = 0
0.00.038.362 I print_info: ssm_d_inner      = 0
0.00.038.363 I print_info: ssm_d_state      = 0
0.00.038.363 I print_info: ssm_dt_rank      = 0
0.00.038.363 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.363 I print_info: model type       = 1.4B
0.00.038.363 I print_info: model params     = 1.41 B
0.00.038.364 I print_info: general.name     = 1.4B
0.00.038.364 I print_info: vocab type       = BPE
0.00.038.364 I print_info: n_vocab          = 50304
0.00.038.365 I print_info: n_merges         = 50009
0.00.038.365 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.365 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.365 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.365 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.366 I print_info: LF token         = 128 'Ä'
0.00.038.366 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.366 I print_info: max token length = 1024
0.00.538.232 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.244 I load_tensors: offloading output layer to GPU
0.00.538.245 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.275 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.287 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.539.766 I llama_init_from_model: n_seq_max     = 1
0.00.539.775 I llama_init_from_model: n_ctx         = 2048
0.00.539.776 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.539.776 I llama_init_from_model: n_batch       = 2048
0.00.539.777 I llama_init_from_model: n_ubatch      = 512
0.00.539.777 I llama_init_from_model: flash_attn    = 0
0.00.539.778 I llama_init_from_model: freq_base     = 10000.0
0.00.539.782 I llama_init_from_model: freq_scale    = 1
0.00.539.784 I ggml_metal_init: allocating
0.00.539.831 I ggml_metal_init: found device: Apple M4
0.00.539.844 I ggml_metal_init: picking default device: Apple M4
0.00.541.590 I ggml_metal_init: using embedded metal library
0.00.548.360 I ggml_metal_init: GPU name:   Apple M4
0.00.548.365 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.548.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.548.366 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.548.367 I ggml_metal_init: simdgroup reduction   = true
0.00.548.367 I ggml_metal_init: simdgroup matrix mul. = true
0.00.548.368 I ggml_metal_init: has residency sets    = true
0.00.548.368 I ggml_metal_init: has bfloat            = true
0.00.548.368 I ggml_metal_init: use bfloat            = true
0.00.548.369 I ggml_metal_init: hasUnifiedMemory      = true
0.00.548.371 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.566.937 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.188 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.624.196 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.624.219 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.579 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.628.581 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.628.581 I llama_init_from_model: graph nodes  = 967
0.00.628.582 I llama_init_from_model: graph splits = 2
0.00.628.587 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.628.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.628.712 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.324 I main: llama threadpool init, n_threads = 4
0.00.686.368 I 
0.00.686.391 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.392 I 
0.00.686.557 I sampler seed: 1234
0.00.686.561 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.572 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.573 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.573 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.445.652 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53665.91 tokens per second)
0.01.445.652 I llama_perf_context_print:        load time =     676.50 ms
0.01.445.653 I llama_perf_context_print: prompt eval time =      56.83 ms /     7 tokens (    8.12 ms per token,   123.18 tokens per second)
0.01.445.654 I llama_perf_context_print:        eval time =     699.32 ms /    63 runs   (   11.10 ms per token,    90.09 tokens per second)
0.01.445.654 I llama_perf_context_print:       total time =     760.29 ms /    70 tokens
0.01.445.936 I ggml_metal_free: deallocating

real	0m1.464s
user	0m0.110s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.638 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.846 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.853 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.855 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.855 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.855 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.856 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.857 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.859 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.860 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.860 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.860 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.861 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.865 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.865 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.865 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.539 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.535 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.221 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.223 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.223 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.223 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.224 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.224 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.225 I llama_model_loader: - type  f32:  194 tensors
0.00.026.225 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.225 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.226 I print_info: file format = GGUF V3 (latest)
0.00.026.226 I print_info: file type   = Q5_K - Medium
0.00.026.227 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.059 I load: special tokens cache size = 25
0.00.039.861 I load: token to piece cache size = 0.2984 MB
0.00.039.863 I print_info: arch             = gptneox
0.00.039.864 I print_info: vocab_only       = 0
0.00.039.864 I print_info: n_ctx_train      = 2048
0.00.039.864 I print_info: n_embd           = 2048
0.00.039.864 I print_info: n_layer          = 24
0.00.039.867 I print_info: n_head           = 16
0.00.039.868 I print_info: n_head_kv        = 16
0.00.039.868 I print_info: n_rot            = 32
0.00.039.868 I print_info: n_swa            = 0
0.00.039.868 I print_info: n_embd_head_k    = 128
0.00.039.869 I print_info: n_embd_head_v    = 128
0.00.039.869 I print_info: n_gqa            = 1
0.00.039.870 I print_info: n_embd_k_gqa     = 2048
0.00.039.871 I print_info: n_embd_v_gqa     = 2048
0.00.039.871 I print_info: f_norm_eps       = 1.0e-05
0.00.039.872 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.872 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.872 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.872 I print_info: f_logit_scale    = 0.0e+00
0.00.039.873 I print_info: n_ff             = 8192
0.00.039.873 I print_info: n_expert         = 0
0.00.039.873 I print_info: n_expert_used    = 0
0.00.039.873 I print_info: causal attn      = 1
0.00.039.874 I print_info: pooling type     = 0
0.00.039.875 I print_info: rope type        = 2
0.00.039.877 I print_info: rope scaling     = linear
0.00.039.877 I print_info: freq_base_train  = 10000.0
0.00.039.878 I print_info: freq_scale_train = 1
0.00.039.878 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.878 I print_info: rope_finetuned   = unknown
0.00.039.878 I print_info: ssm_d_conv       = 0
0.00.039.878 I print_info: ssm_d_inner      = 0
0.00.039.879 I print_info: ssm_d_state      = 0
0.00.039.879 I print_info: ssm_dt_rank      = 0
0.00.039.879 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.879 I print_info: model type       = 1.4B
0.00.039.879 I print_info: model params     = 1.41 B
0.00.039.880 I print_info: general.name     = 1.4B
0.00.039.880 I print_info: vocab type       = BPE
0.00.039.880 I print_info: n_vocab          = 50304
0.00.039.881 I print_info: n_merges         = 50009
0.00.039.881 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.881 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.885 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.885 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.886 I print_info: LF token         = 128 'Ä'
0.00.039.886 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.886 I print_info: max token length = 1024
0.00.594.322 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.335 I load_tensors: offloading output layer to GPU
0.00.594.336 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.363 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.594.364 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.595.817 I llama_init_from_model: n_seq_max     = 1
0.00.595.826 I llama_init_from_model: n_ctx         = 2048
0.00.595.827 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.595.827 I llama_init_from_model: n_batch       = 2048
0.00.595.828 I llama_init_from_model: n_ubatch      = 512
0.00.595.828 I llama_init_from_model: flash_attn    = 0
0.00.595.829 I llama_init_from_model: freq_base     = 10000.0
0.00.595.829 I llama_init_from_model: freq_scale    = 1
0.00.595.838 I ggml_metal_init: allocating
0.00.595.888 I ggml_metal_init: found device: Apple M4
0.00.595.919 I ggml_metal_init: picking default device: Apple M4
0.00.597.722 I ggml_metal_init: using embedded metal library
0.00.604.504 I ggml_metal_init: GPU name:   Apple M4
0.00.604.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.510 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.511 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.511 I ggml_metal_init: simdgroup reduction   = true
0.00.604.512 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.512 I ggml_metal_init: has residency sets    = true
0.00.604.512 I ggml_metal_init: has bfloat            = true
0.00.604.512 I ggml_metal_init: use bfloat            = true
0.00.604.514 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.522 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.861 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.684.903 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.684.912 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.684.941 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.662 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.689.664 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.689.664 I llama_init_from_model: graph nodes  = 967
0.00.689.664 I llama_init_from_model: graph splits = 2
0.00.689.670 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.689.798 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.689.798 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.215 I main: llama threadpool init, n_threads = 4
0.00.756.259 I 
0.00.756.285 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.285 I 
0.00.756.456 I sampler seed: 1234
0.00.756.460 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.478 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.479 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.479 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.594.967 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53103.96 tokens per second)
0.01.594.968 I llama_perf_context_print:        load time =     744.70 ms
0.01.594.969 I llama_perf_context_print: prompt eval time =      51.23 ms /     7 tokens (    7.32 ms per token,   136.65 tokens per second)
0.01.594.970 I llama_perf_context_print:        eval time =     784.30 ms /    63 runs   (   12.45 ms per token,    80.33 tokens per second)
0.01.594.970 I llama_perf_context_print:       total time =     839.63 ms /    70 tokens
0.01.595.255 I ggml_metal_free: deallocating

real	0m1.614s
user	0m0.109s
sys	0m0.218s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.821 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.560 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.560 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.561 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.563 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.565 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.568 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.568 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.266 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.310 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.975 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.976 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.976 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.977 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.977 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.977 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.978 I llama_model_loader: - type  f32:  194 tensors
0.00.024.978 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.979 I print_info: file format = GGUF V3 (latest)
0.00.024.979 I print_info: file type   = Q6_K
0.00.024.980 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.841 I load: special tokens cache size = 25
0.00.038.679 I load: token to piece cache size = 0.2984 MB
0.00.038.682 I print_info: arch             = gptneox
0.00.038.682 I print_info: vocab_only       = 0
0.00.038.683 I print_info: n_ctx_train      = 2048
0.00.038.683 I print_info: n_embd           = 2048
0.00.038.683 I print_info: n_layer          = 24
0.00.038.686 I print_info: n_head           = 16
0.00.038.686 I print_info: n_head_kv        = 16
0.00.038.687 I print_info: n_rot            = 32
0.00.038.687 I print_info: n_swa            = 0
0.00.038.687 I print_info: n_embd_head_k    = 128
0.00.038.687 I print_info: n_embd_head_v    = 128
0.00.038.688 I print_info: n_gqa            = 1
0.00.038.689 I print_info: n_embd_k_gqa     = 2048
0.00.038.689 I print_info: n_embd_v_gqa     = 2048
0.00.038.690 I print_info: f_norm_eps       = 1.0e-05
0.00.038.690 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.690 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.691 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.691 I print_info: f_logit_scale    = 0.0e+00
0.00.038.692 I print_info: n_ff             = 8192
0.00.038.692 I print_info: n_expert         = 0
0.00.038.692 I print_info: n_expert_used    = 0
0.00.038.692 I print_info: causal attn      = 1
0.00.038.692 I print_info: pooling type     = 0
0.00.038.694 I print_info: rope type        = 2
0.00.038.696 I print_info: rope scaling     = linear
0.00.038.698 I print_info: freq_base_train  = 10000.0
0.00.038.698 I print_info: freq_scale_train = 1
0.00.038.698 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.698 I print_info: rope_finetuned   = unknown
0.00.038.699 I print_info: ssm_d_conv       = 0
0.00.038.699 I print_info: ssm_d_inner      = 0
0.00.038.700 I print_info: ssm_d_state      = 0
0.00.038.700 I print_info: ssm_dt_rank      = 0
0.00.038.700 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.700 I print_info: model type       = 1.4B
0.00.038.701 I print_info: model params     = 1.41 B
0.00.038.701 I print_info: general.name     = 1.4B
0.00.038.701 I print_info: vocab type       = BPE
0.00.038.702 I print_info: n_vocab          = 50304
0.00.038.702 I print_info: n_merges         = 50009
0.00.038.702 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.702 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.702 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.703 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.703 I print_info: LF token         = 128 'Ä'
0.00.038.703 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.704 I print_info: max token length = 1024
0.00.659.375 I load_tensors: offloading 24 repeating layers to GPU
0.00.659.386 I load_tensors: offloading output layer to GPU
0.00.659.386 I load_tensors: offloaded 25/25 layers to GPU
0.00.659.415 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.659.416 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.660.840 I llama_init_from_model: n_seq_max     = 1
0.00.660.846 I llama_init_from_model: n_ctx         = 2048
0.00.660.846 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.660.847 I llama_init_from_model: n_batch       = 2048
0.00.660.847 I llama_init_from_model: n_ubatch      = 512
0.00.660.848 I llama_init_from_model: flash_attn    = 0
0.00.660.849 I llama_init_from_model: freq_base     = 10000.0
0.00.660.850 I llama_init_from_model: freq_scale    = 1
0.00.660.854 I ggml_metal_init: allocating
0.00.660.913 I ggml_metal_init: found device: Apple M4
0.00.660.927 I ggml_metal_init: picking default device: Apple M4
0.00.662.638 I ggml_metal_init: using embedded metal library
0.00.668.399 I ggml_metal_init: GPU name:   Apple M4
0.00.668.405 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.668.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.668.407 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.668.407 I ggml_metal_init: simdgroup reduction   = true
0.00.668.408 I ggml_metal_init: simdgroup matrix mul. = true
0.00.668.408 I ggml_metal_init: has residency sets    = true
0.00.668.408 I ggml_metal_init: has bfloat            = true
0.00.668.408 I ggml_metal_init: use bfloat            = true
0.00.668.410 I ggml_metal_init: hasUnifiedMemory      = true
0.00.668.411 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.687.423 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.750.973 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.750.981 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.751.005 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.755.604 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.755.606 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.755.606 I llama_init_from_model: graph nodes  = 967
0.00.755.607 I llama_init_from_model: graph splits = 2
0.00.755.613 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.755.737 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.755.738 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.825.681 I main: llama threadpool init, n_threads = 4
0.00.825.725 I 
0.00.825.753 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.825.754 I 
0.00.825.925 I sampler seed: 1234
0.00.825.930 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.825.942 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.825.942 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.825.942 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.693.080 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53103.96 tokens per second)
0.01.693.081 I llama_perf_context_print:        load time =     815.99 ms
0.01.693.083 I llama_perf_context_print: prompt eval time =      54.02 ms /     7 tokens (    7.72 ms per token,   129.58 tokens per second)
0.01.693.084 I llama_perf_context_print:        eval time =     810.16 ms /    63 runs   (   12.86 ms per token,    77.76 tokens per second)
0.01.693.084 I llama_perf_context_print:       total time =     868.27 ms /    70 tokens
0.01.693.384 I ggml_metal_free: deallocating

real	0m1.713s
user	0m0.109s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.854 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.033.251 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.047.610 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.620 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.624 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.624 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.625 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.630 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.630 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.631 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.632 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.633 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.634 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.637 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.638 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.639 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.056.585 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.058.883 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.066.467 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.066.469 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.066.470 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.066.470 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.066.470 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.066.471 I llama_model_loader: - type  f32:  194 tensors
0.00.066.471 I llama_model_loader: - type  f16:   98 tensors
0.00.066.473 I print_info: file format = GGUF V3 (latest)
0.00.066.473 I print_info: file type   = all F32 (guessed)
0.00.066.475 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.457 I load: special tokens cache size = 25
0.00.087.990 I load: token to piece cache size = 0.2984 MB
0.00.087.994 I print_info: arch             = gptneox
0.00.087.994 I print_info: vocab_only       = 0
0.00.087.994 I print_info: n_ctx_train      = 2048
0.00.087.994 I print_info: n_embd           = 2048
0.00.087.995 I print_info: n_layer          = 24
0.00.087.998 I print_info: n_head           = 16
0.00.087.999 I print_info: n_head_kv        = 16
0.00.087.999 I print_info: n_rot            = 32
0.00.087.999 I print_info: n_swa            = 0
0.00.088.000 I print_info: n_embd_head_k    = 128
0.00.088.002 I print_info: n_embd_head_v    = 128
0.00.088.003 I print_info: n_gqa            = 1
0.00.088.004 I print_info: n_embd_k_gqa     = 2048
0.00.088.005 I print_info: n_embd_v_gqa     = 2048
0.00.088.006 I print_info: f_norm_eps       = 1.0e-05
0.00.088.007 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.007 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.007 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.007 I print_info: f_logit_scale    = 0.0e+00
0.00.088.008 I print_info: n_ff             = 8192
0.00.088.017 I print_info: n_expert         = 0
0.00.088.019 I print_info: n_expert_used    = 0
0.00.088.019 I print_info: causal attn      = 1
0.00.088.020 I print_info: pooling type     = 0
0.00.088.020 I print_info: rope type        = 2
0.00.088.020 I print_info: rope scaling     = linear
0.00.088.020 I print_info: freq_base_train  = 10000.0
0.00.088.021 I print_info: freq_scale_train = 1
0.00.088.021 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.021 I print_info: rope_finetuned   = unknown
0.00.088.021 I print_info: ssm_d_conv       = 0
0.00.088.022 I print_info: ssm_d_inner      = 0
0.00.088.022 I print_info: ssm_d_state      = 0
0.00.088.022 I print_info: ssm_dt_rank      = 0
0.00.088.022 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.022 I print_info: model type       = 1.4B
0.00.088.023 I print_info: model params     = 1.41 B
0.00.088.025 I print_info: general.name     = 1.4B
0.00.088.025 I print_info: vocab type       = BPE
0.00.088.025 I print_info: n_vocab          = 50304
0.00.088.025 I print_info: n_merges         = 50009
0.00.088.026 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.026 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.027 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.027 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.028 I print_info: LF token         = 128 'Ä'
0.00.088.030 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.030 I print_info: max token length = 1024
0.01.403.167 I load_tensors: offloading 24 repeating layers to GPU
0.01.403.171 I load_tensors: offloading output layer to GPU
0.01.403.171 I load_tensors: offloaded 25/25 layers to GPU
0.01.403.195 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.403.196 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.403.883 I llama_init_from_model: n_seq_max     = 1
0.01.403.885 I llama_init_from_model: n_ctx         = 128
0.01.403.885 I llama_init_from_model: n_ctx_per_seq = 128
0.01.403.886 I llama_init_from_model: n_batch       = 128
0.01.403.886 I llama_init_from_model: n_ubatch      = 128
0.01.403.886 I llama_init_from_model: flash_attn    = 0
0.01.403.887 I llama_init_from_model: freq_base     = 10000.0
0.01.403.887 I llama_init_from_model: freq_scale    = 1
0.01.403.888 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.403.889 I ggml_metal_init: allocating
0.01.403.932 I ggml_metal_init: found device: Apple M4
0.01.403.941 I ggml_metal_init: picking default device: Apple M4
0.01.405.064 I ggml_metal_init: using embedded metal library
0.01.409.702 I ggml_metal_init: GPU name:   Apple M4
0.01.409.704 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.409.705 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.409.706 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.409.706 I ggml_metal_init: simdgroup reduction   = true
0.01.409.706 I ggml_metal_init: simdgroup matrix mul. = true
0.01.409.707 I ggml_metal_init: has residency sets    = true
0.01.409.707 I ggml_metal_init: has bfloat            = true
0.01.409.707 I ggml_metal_init: use bfloat            = true
0.01.409.707 I ggml_metal_init: hasUnifiedMemory      = true
0.01.409.709 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.423.091 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.424.978 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.424.981 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.424.999 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.426.828 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.426.829 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.426.829 I llama_init_from_model: graph nodes  = 967
0.01.426.830 I llama_init_from_model: graph splits = 2
0.01.426.831 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.426.831 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.460.669 I 
0.01.460.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.460.725 I perplexity: tokenizing the input ..
0.01.465.578 I perplexity: tokenization took 4.851 ms
0.01.465.600 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.584.201 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.585.594 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.585.610 I llama_perf_context_print:        load time =    1427.41 ms
0.01.585.611 I llama_perf_context_print: prompt eval time =     118.34 ms /   128 tokens (    0.92 ms per token,  1081.67 tokens per second)
0.01.585.612 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.585.613 I llama_perf_context_print:       total time =     124.94 ms /   129 tokens
0.01.585.948 I ggml_metal_free: deallocating

real	0m1.796s
user	0m0.103s
sys	0m0.237s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.254 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.126 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.113 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.121 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.121 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.127 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.127 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.128 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.129 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.129 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.129 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.130 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.130 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.131 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.133 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.133 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.133 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.277 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.438 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.565 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.567 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.567 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.567 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.567 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.568 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.569 I llama_model_loader: - type  f32:  194 tensors
0.00.026.569 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.570 I print_info: file format = GGUF V3 (latest)
0.00.026.570 I print_info: file type   = Q8_0
0.00.026.571 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.180 I load: special tokens cache size = 25
0.00.041.237 I load: token to piece cache size = 0.2984 MB
0.00.041.242 I print_info: arch             = gptneox
0.00.041.242 I print_info: vocab_only       = 0
0.00.041.242 I print_info: n_ctx_train      = 2048
0.00.041.243 I print_info: n_embd           = 2048
0.00.041.243 I print_info: n_layer          = 24
0.00.041.246 I print_info: n_head           = 16
0.00.041.247 I print_info: n_head_kv        = 16
0.00.041.247 I print_info: n_rot            = 32
0.00.041.248 I print_info: n_swa            = 0
0.00.041.248 I print_info: n_embd_head_k    = 128
0.00.041.248 I print_info: n_embd_head_v    = 128
0.00.041.249 I print_info: n_gqa            = 1
0.00.041.250 I print_info: n_embd_k_gqa     = 2048
0.00.041.250 I print_info: n_embd_v_gqa     = 2048
0.00.041.251 I print_info: f_norm_eps       = 1.0e-05
0.00.041.251 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.251 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.251 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.252 I print_info: f_logit_scale    = 0.0e+00
0.00.041.252 I print_info: n_ff             = 8192
0.00.041.254 I print_info: n_expert         = 0
0.00.041.256 I print_info: n_expert_used    = 0
0.00.041.256 I print_info: causal attn      = 1
0.00.041.256 I print_info: pooling type     = 0
0.00.041.256 I print_info: rope type        = 2
0.00.041.257 I print_info: rope scaling     = linear
0.00.041.257 I print_info: freq_base_train  = 10000.0
0.00.041.257 I print_info: freq_scale_train = 1
0.00.041.258 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.258 I print_info: rope_finetuned   = unknown
0.00.041.258 I print_info: ssm_d_conv       = 0
0.00.041.258 I print_info: ssm_d_inner      = 0
0.00.041.258 I print_info: ssm_d_state      = 0
0.00.041.259 I print_info: ssm_dt_rank      = 0
0.00.041.259 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.259 I print_info: model type       = 1.4B
0.00.041.259 I print_info: model params     = 1.41 B
0.00.041.260 I print_info: general.name     = 1.4B
0.00.041.260 I print_info: vocab type       = BPE
0.00.041.260 I print_info: n_vocab          = 50304
0.00.041.262 I print_info: n_merges         = 50009
0.00.041.262 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.262 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.262 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.263 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.263 I print_info: LF token         = 128 'Ä'
0.00.041.263 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.263 I print_info: max token length = 1024
0.00.852.088 I load_tensors: offloading 24 repeating layers to GPU
0.00.852.093 I load_tensors: offloading output layer to GPU
0.00.852.094 I load_tensors: offloaded 25/25 layers to GPU
0.00.852.122 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.852.124 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.853.222 I llama_init_from_model: n_seq_max     = 1
0.00.853.224 I llama_init_from_model: n_ctx         = 128
0.00.853.224 I llama_init_from_model: n_ctx_per_seq = 128
0.00.853.225 I llama_init_from_model: n_batch       = 128
0.00.853.228 I llama_init_from_model: n_ubatch      = 128
0.00.853.229 I llama_init_from_model: flash_attn    = 0
0.00.853.229 I llama_init_from_model: freq_base     = 10000.0
0.00.853.230 I llama_init_from_model: freq_scale    = 1
0.00.853.231 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.853.232 I ggml_metal_init: allocating
0.00.853.286 I ggml_metal_init: found device: Apple M4
0.00.853.297 I ggml_metal_init: picking default device: Apple M4
0.00.854.564 I ggml_metal_init: using embedded metal library
0.00.859.875 I ggml_metal_init: GPU name:   Apple M4
0.00.859.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.859.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.859.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.859.881 I ggml_metal_init: simdgroup reduction   = true
0.00.859.881 I ggml_metal_init: simdgroup matrix mul. = true
0.00.859.882 I ggml_metal_init: has residency sets    = true
0.00.859.882 I ggml_metal_init: has bfloat            = true
0.00.859.882 I ggml_metal_init: use bfloat            = true
0.00.859.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.859.886 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.876.291 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.879.231 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.879.235 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.879.257 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.881.894 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.881.896 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.881.896 I llama_init_from_model: graph nodes  = 967
0.00.881.896 I llama_init_from_model: graph splits = 2
0.00.881.898 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.881.899 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.909.627 I 
0.00.909.700 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.909.722 I perplexity: tokenizing the input ..
0.00.917.146 I perplexity: tokenization took 7.422 ms
0.00.917.165 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.056.085 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.057.499 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.057.510 I llama_perf_context_print:        load time =     899.49 ms
0.01.057.511 I llama_perf_context_print: prompt eval time =     137.93 ms /   128 tokens (    1.08 ms per token,   928.01 tokens per second)
0.01.057.512 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.057.512 I llama_perf_context_print:       total time =     147.89 ms /   129 tokens
0.01.057.879 I ggml_metal_free: deallocating

real	0m1.075s
user	0m0.078s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.265 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.187 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.325 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.329 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.331 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.332 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.332 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.332 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.333 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.334 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.335 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.338 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.338 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.338 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.341 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.342 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.342 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.065 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.088 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.735 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.736 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.737 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.737 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.737 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.737 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.738 I llama_model_loader: - type  f32:  194 tensors
0.00.025.738 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.739 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.739 I print_info: file format = GGUF V3 (latest)
0.00.025.740 I print_info: file type   = Q4_0
0.00.025.741 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.307 I load: special tokens cache size = 25
0.00.039.135 I load: token to piece cache size = 0.2984 MB
0.00.039.138 I print_info: arch             = gptneox
0.00.039.138 I print_info: vocab_only       = 0
0.00.039.138 I print_info: n_ctx_train      = 2048
0.00.039.138 I print_info: n_embd           = 2048
0.00.039.139 I print_info: n_layer          = 24
0.00.039.142 I print_info: n_head           = 16
0.00.039.145 I print_info: n_head_kv        = 16
0.00.039.145 I print_info: n_rot            = 32
0.00.039.145 I print_info: n_swa            = 0
0.00.039.145 I print_info: n_embd_head_k    = 128
0.00.039.145 I print_info: n_embd_head_v    = 128
0.00.039.146 I print_info: n_gqa            = 1
0.00.039.147 I print_info: n_embd_k_gqa     = 2048
0.00.039.148 I print_info: n_embd_v_gqa     = 2048
0.00.039.148 I print_info: f_norm_eps       = 1.0e-05
0.00.039.149 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.149 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.149 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.149 I print_info: f_logit_scale    = 0.0e+00
0.00.039.150 I print_info: n_ff             = 8192
0.00.039.150 I print_info: n_expert         = 0
0.00.039.150 I print_info: n_expert_used    = 0
0.00.039.151 I print_info: causal attn      = 1
0.00.039.151 I print_info: pooling type     = 0
0.00.039.151 I print_info: rope type        = 2
0.00.039.151 I print_info: rope scaling     = linear
0.00.039.152 I print_info: freq_base_train  = 10000.0
0.00.039.152 I print_info: freq_scale_train = 1
0.00.039.152 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.152 I print_info: rope_finetuned   = unknown
0.00.039.158 I print_info: ssm_d_conv       = 0
0.00.039.161 I print_info: ssm_d_inner      = 0
0.00.039.161 I print_info: ssm_d_state      = 0
0.00.039.161 I print_info: ssm_dt_rank      = 0
0.00.039.161 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.161 I print_info: model type       = 1.4B
0.00.039.163 I print_info: model params     = 1.41 B
0.00.039.163 I print_info: general.name     = 1.4B
0.00.039.163 I print_info: vocab type       = BPE
0.00.039.164 I print_info: n_vocab          = 50304
0.00.039.164 I print_info: n_merges         = 50009
0.00.039.164 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.165 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.165 I print_info: LF token         = 128 'Ä'
0.00.039.165 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.165 I print_info: max token length = 1024
0.00.555.274 I load_tensors: offloading 24 repeating layers to GPU
0.00.555.287 I load_tensors: offloading output layer to GPU
0.00.555.287 I load_tensors: offloaded 25/25 layers to GPU
0.00.555.319 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.555.320 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.556.700 I llama_init_from_model: n_seq_max     = 1
0.00.556.704 I llama_init_from_model: n_ctx         = 128
0.00.556.705 I llama_init_from_model: n_ctx_per_seq = 128
0.00.556.710 I llama_init_from_model: n_batch       = 128
0.00.556.711 I llama_init_from_model: n_ubatch      = 128
0.00.556.711 I llama_init_from_model: flash_attn    = 0
0.00.556.713 I llama_init_from_model: freq_base     = 10000.0
0.00.556.714 I llama_init_from_model: freq_scale    = 1
0.00.556.714 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.556.716 I ggml_metal_init: allocating
0.00.556.793 I ggml_metal_init: found device: Apple M4
0.00.556.807 I ggml_metal_init: picking default device: Apple M4
0.00.558.632 I ggml_metal_init: using embedded metal library
0.00.565.395 I ggml_metal_init: GPU name:   Apple M4
0.00.565.400 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.565.401 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.565.402 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.565.402 I ggml_metal_init: simdgroup reduction   = true
0.00.565.403 I ggml_metal_init: simdgroup matrix mul. = true
0.00.565.403 I ggml_metal_init: has residency sets    = true
0.00.565.403 I ggml_metal_init: has bfloat            = true
0.00.565.403 I ggml_metal_init: use bfloat            = true
0.00.565.405 I ggml_metal_init: hasUnifiedMemory      = true
0.00.565.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.583.915 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.587.489 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.587.494 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.587.548 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.590.718 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.590.720 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.590.720 I llama_init_from_model: graph nodes  = 967
0.00.590.721 I llama_init_from_model: graph splits = 2
0.00.590.724 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.590.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.869 I 
0.00.616.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.962 I perplexity: tokenizing the input ..
0.00.624.730 I perplexity: tokenization took 7.766 ms
0.00.624.751 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.760.722 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.762.073 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.762.091 I llama_perf_context_print:        load time =     606.67 ms
0.00.762.092 I llama_perf_context_print: prompt eval time =     135.00 ms /   128 tokens (    1.05 ms per token,   948.11 tokens per second)
0.00.762.093 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.093 I llama_perf_context_print:       total time =     145.23 ms /   129 tokens
0.00.762.541 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.080s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.725 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.587 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.593 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.599 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.599 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.600 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.600 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.600 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.603 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.603 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.603 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.604 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.604 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.604 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.605 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.607 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.607 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.608 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.174 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.142 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.786 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.788 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.788 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.789 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.789 I llama_model_loader: - type  f32:  194 tensors
0.00.023.789 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.790 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.791 I print_info: file format = GGUF V3 (latest)
0.00.023.795 I print_info: file type   = Q4_1
0.00.023.796 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.031.334 I load: special tokens cache size = 25
0.00.037.089 I load: token to piece cache size = 0.2984 MB
0.00.037.092 I print_info: arch             = gptneox
0.00.037.092 I print_info: vocab_only       = 0
0.00.037.093 I print_info: n_ctx_train      = 2048
0.00.037.093 I print_info: n_embd           = 2048
0.00.037.093 I print_info: n_layer          = 24
0.00.037.097 I print_info: n_head           = 16
0.00.037.098 I print_info: n_head_kv        = 16
0.00.037.098 I print_info: n_rot            = 32
0.00.037.099 I print_info: n_swa            = 0
0.00.037.099 I print_info: n_embd_head_k    = 128
0.00.037.099 I print_info: n_embd_head_v    = 128
0.00.037.101 I print_info: n_gqa            = 1
0.00.037.101 I print_info: n_embd_k_gqa     = 2048
0.00.037.102 I print_info: n_embd_v_gqa     = 2048
0.00.037.103 I print_info: f_norm_eps       = 1.0e-05
0.00.037.103 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.103 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.103 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.104 I print_info: f_logit_scale    = 0.0e+00
0.00.037.104 I print_info: n_ff             = 8192
0.00.037.105 I print_info: n_expert         = 0
0.00.037.105 I print_info: n_expert_used    = 0
0.00.037.105 I print_info: causal attn      = 1
0.00.037.105 I print_info: pooling type     = 0
0.00.037.105 I print_info: rope type        = 2
0.00.037.105 I print_info: rope scaling     = linear
0.00.037.106 I print_info: freq_base_train  = 10000.0
0.00.037.106 I print_info: freq_scale_train = 1
0.00.037.106 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.107 I print_info: rope_finetuned   = unknown
0.00.037.107 I print_info: ssm_d_conv       = 0
0.00.037.107 I print_info: ssm_d_inner      = 0
0.00.037.107 I print_info: ssm_d_state      = 0
0.00.037.107 I print_info: ssm_dt_rank      = 0
0.00.037.107 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.108 I print_info: model type       = 1.4B
0.00.037.108 I print_info: model params     = 1.41 B
0.00.037.108 I print_info: general.name     = 1.4B
0.00.037.109 I print_info: vocab type       = BPE
0.00.037.109 I print_info: n_vocab          = 50304
0.00.037.109 I print_info: n_merges         = 50009
0.00.037.109 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.110 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.110 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.110 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.110 I print_info: LF token         = 128 'Ä'
0.00.037.111 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.111 I print_info: max token length = 1024
0.00.646.224 I load_tensors: offloading 24 repeating layers to GPU
0.00.646.232 I load_tensors: offloading output layer to GPU
0.00.646.233 I load_tensors: offloaded 25/25 layers to GPU
0.00.646.269 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.646.270 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.647.712 I llama_init_from_model: n_seq_max     = 1
0.00.647.715 I llama_init_from_model: n_ctx         = 128
0.00.647.715 I llama_init_from_model: n_ctx_per_seq = 128
0.00.647.716 I llama_init_from_model: n_batch       = 128
0.00.647.716 I llama_init_from_model: n_ubatch      = 128
0.00.647.717 I llama_init_from_model: flash_attn    = 0
0.00.647.719 I llama_init_from_model: freq_base     = 10000.0
0.00.647.719 I llama_init_from_model: freq_scale    = 1
0.00.647.720 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.647.722 I ggml_metal_init: allocating
0.00.647.827 I ggml_metal_init: found device: Apple M4
0.00.647.845 I ggml_metal_init: picking default device: Apple M4
0.00.649.724 I ggml_metal_init: using embedded metal library
0.00.656.351 I ggml_metal_init: GPU name:   Apple M4
0.00.656.357 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.656.358 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.656.359 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.656.359 I ggml_metal_init: simdgroup reduction   = true
0.00.656.360 I ggml_metal_init: simdgroup matrix mul. = true
0.00.656.360 I ggml_metal_init: has residency sets    = true
0.00.656.360 I ggml_metal_init: has bfloat            = true
0.00.656.360 I ggml_metal_init: use bfloat            = true
0.00.656.361 I ggml_metal_init: hasUnifiedMemory      = true
0.00.656.363 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.517 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.091 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.678.095 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.678.122 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.681.270 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.681.271 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.681.272 I llama_init_from_model: graph nodes  = 967
0.00.681.272 I llama_init_from_model: graph splits = 2
0.00.681.275 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.681.276 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.836 I 
0.00.706.907 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.925 I perplexity: tokenizing the input ..
0.00.713.719 I perplexity: tokenization took 6.791 ms
0.00.713.737 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.285 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.851.661 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.851.676 I llama_perf_context_print:        load time =     698.10 ms
0.00.851.677 I llama_perf_context_print: prompt eval time =     136.00 ms /   128 tokens (    1.06 ms per token,   941.17 tokens per second)
0.00.851.678 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.679 I llama_perf_context_print:       total time =     144.84 ms /   129 tokens
0.00.852.061 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.077s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.097 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.103 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.103 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.104 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.104 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.106 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.107 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.107 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.108 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.109 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.110 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.110 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.748 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.716 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.331 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.332 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.333 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.333 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.334 I llama_model_loader: - type  f32:  194 tensors
0.00.028.334 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.334 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.335 I print_info: file format = GGUF V3 (latest)
0.00.028.335 I print_info: file type   = Q5_0
0.00.028.336 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.082 I load: special tokens cache size = 25
0.00.041.888 I load: token to piece cache size = 0.2984 MB
0.00.041.891 I print_info: arch             = gptneox
0.00.041.891 I print_info: vocab_only       = 0
0.00.041.891 I print_info: n_ctx_train      = 2048
0.00.041.891 I print_info: n_embd           = 2048
0.00.041.892 I print_info: n_layer          = 24
0.00.041.894 I print_info: n_head           = 16
0.00.041.895 I print_info: n_head_kv        = 16
0.00.041.895 I print_info: n_rot            = 32
0.00.041.898 I print_info: n_swa            = 0
0.00.041.898 I print_info: n_embd_head_k    = 128
0.00.041.898 I print_info: n_embd_head_v    = 128
0.00.041.899 I print_info: n_gqa            = 1
0.00.041.900 I print_info: n_embd_k_gqa     = 2048
0.00.041.901 I print_info: n_embd_v_gqa     = 2048
0.00.041.901 I print_info: f_norm_eps       = 1.0e-05
0.00.041.901 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.902 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.902 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.902 I print_info: f_logit_scale    = 0.0e+00
0.00.041.903 I print_info: n_ff             = 8192
0.00.041.903 I print_info: n_expert         = 0
0.00.041.903 I print_info: n_expert_used    = 0
0.00.041.903 I print_info: causal attn      = 1
0.00.041.903 I print_info: pooling type     = 0
0.00.041.907 I print_info: rope type        = 2
0.00.041.908 I print_info: rope scaling     = linear
0.00.041.908 I print_info: freq_base_train  = 10000.0
0.00.041.908 I print_info: freq_scale_train = 1
0.00.041.909 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.909 I print_info: rope_finetuned   = unknown
0.00.041.909 I print_info: ssm_d_conv       = 0
0.00.041.909 I print_info: ssm_d_inner      = 0
0.00.041.909 I print_info: ssm_d_state      = 0
0.00.041.911 I print_info: ssm_dt_rank      = 0
0.00.041.911 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.911 I print_info: model type       = 1.4B
0.00.041.912 I print_info: model params     = 1.41 B
0.00.041.912 I print_info: general.name     = 1.4B
0.00.041.912 I print_info: vocab type       = BPE
0.00.041.912 I print_info: n_vocab          = 50304
0.00.041.913 I print_info: n_merges         = 50009
0.00.041.913 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.913 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.913 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.913 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.914 I print_info: LF token         = 128 'Ä'
0.00.041.914 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.915 I print_info: max token length = 1024
0.00.696.864 I load_tensors: offloading 24 repeating layers to GPU
0.00.696.877 I load_tensors: offloading output layer to GPU
0.00.696.878 I load_tensors: offloaded 25/25 layers to GPU
0.00.696.912 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.696.933 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.698.534 I llama_init_from_model: n_seq_max     = 1
0.00.698.537 I llama_init_from_model: n_ctx         = 128
0.00.698.537 I llama_init_from_model: n_ctx_per_seq = 128
0.00.698.538 I llama_init_from_model: n_batch       = 128
0.00.698.539 I llama_init_from_model: n_ubatch      = 128
0.00.698.539 I llama_init_from_model: flash_attn    = 0
0.00.698.541 I llama_init_from_model: freq_base     = 10000.0
0.00.698.542 I llama_init_from_model: freq_scale    = 1
0.00.698.542 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.698.549 I ggml_metal_init: allocating
0.00.698.634 I ggml_metal_init: found device: Apple M4
0.00.698.647 I ggml_metal_init: picking default device: Apple M4
0.00.700.386 I ggml_metal_init: using embedded metal library
0.00.707.040 I ggml_metal_init: GPU name:   Apple M4
0.00.707.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.707.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.707.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.707.047 I ggml_metal_init: simdgroup reduction   = true
0.00.707.048 I ggml_metal_init: simdgroup matrix mul. = true
0.00.707.048 I ggml_metal_init: has residency sets    = true
0.00.707.048 I ggml_metal_init: has bfloat            = true
0.00.707.048 I ggml_metal_init: use bfloat            = true
0.00.707.049 I ggml_metal_init: hasUnifiedMemory      = true
0.00.707.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.724.497 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.727.894 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.727.901 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.727.949 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.731.109 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.731.111 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.731.112 I llama_init_from_model: graph nodes  = 967
0.00.731.112 I llama_init_from_model: graph splits = 2
0.00.731.116 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.731.116 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.988 I 
0.00.760.064 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.082 I perplexity: tokenizing the input ..
0.00.766.954 I perplexity: tokenization took 6.871 ms
0.00.766.966 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.901.124 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.902.463 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.902.478 I llama_perf_context_print:        load time =     750.06 ms
0.00.902.479 I llama_perf_context_print: prompt eval time =     133.93 ms /   128 tokens (    1.05 ms per token,   955.73 tokens per second)
0.00.902.480 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.902.484 I llama_perf_context_print:       total time =     142.49 ms /   129 tokens
0.00.902.915 I ggml_metal_free: deallocating

real	0m0.919s
user	0m0.077s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.864 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.661 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.666 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.667 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.673 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.674 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.674 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.675 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.675 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.676 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.676 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.677 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.677 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.678 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.680 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.680 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.680 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.262 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.232 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.918 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.920 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.920 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.921 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.921 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.921 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.922 I llama_model_loader: - type  f32:  194 tensors
0.00.023.923 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.923 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.924 I print_info: file format = GGUF V3 (latest)
0.00.023.924 I print_info: file type   = Q5_1
0.00.023.925 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.031.833 I load: special tokens cache size = 25
0.00.037.679 I load: token to piece cache size = 0.2984 MB
0.00.037.682 I print_info: arch             = gptneox
0.00.037.682 I print_info: vocab_only       = 0
0.00.037.682 I print_info: n_ctx_train      = 2048
0.00.037.683 I print_info: n_embd           = 2048
0.00.037.683 I print_info: n_layer          = 24
0.00.037.687 I print_info: n_head           = 16
0.00.037.687 I print_info: n_head_kv        = 16
0.00.037.688 I print_info: n_rot            = 32
0.00.037.690 I print_info: n_swa            = 0
0.00.037.690 I print_info: n_embd_head_k    = 128
0.00.037.691 I print_info: n_embd_head_v    = 128
0.00.037.691 I print_info: n_gqa            = 1
0.00.037.692 I print_info: n_embd_k_gqa     = 2048
0.00.037.693 I print_info: n_embd_v_gqa     = 2048
0.00.037.693 I print_info: f_norm_eps       = 1.0e-05
0.00.037.694 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.694 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.694 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.694 I print_info: f_logit_scale    = 0.0e+00
0.00.037.695 I print_info: n_ff             = 8192
0.00.037.695 I print_info: n_expert         = 0
0.00.037.695 I print_info: n_expert_used    = 0
0.00.037.696 I print_info: causal attn      = 1
0.00.037.696 I print_info: pooling type     = 0
0.00.037.696 I print_info: rope type        = 2
0.00.037.697 I print_info: rope scaling     = linear
0.00.037.697 I print_info: freq_base_train  = 10000.0
0.00.037.697 I print_info: freq_scale_train = 1
0.00.037.698 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.698 I print_info: rope_finetuned   = unknown
0.00.037.698 I print_info: ssm_d_conv       = 0
0.00.037.698 I print_info: ssm_d_inner      = 0
0.00.037.698 I print_info: ssm_d_state      = 0
0.00.037.698 I print_info: ssm_dt_rank      = 0
0.00.037.699 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.699 I print_info: model type       = 1.4B
0.00.037.699 I print_info: model params     = 1.41 B
0.00.037.699 I print_info: general.name     = 1.4B
0.00.037.700 I print_info: vocab type       = BPE
0.00.037.700 I print_info: n_vocab          = 50304
0.00.037.700 I print_info: n_merges         = 50009
0.00.037.701 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.701 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.702 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.702 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.703 I print_info: LF token         = 128 'Ä'
0.00.037.703 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.703 I print_info: max token length = 1024
0.00.590.075 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.090 I load_tensors: offloading output layer to GPU
0.00.590.091 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.123 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.590.130 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.591.194 I llama_init_from_model: n_seq_max     = 1
0.00.591.199 I llama_init_from_model: n_ctx         = 128
0.00.591.200 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.200 I llama_init_from_model: n_batch       = 128
0.00.591.201 I llama_init_from_model: n_ubatch      = 128
0.00.591.201 I llama_init_from_model: flash_attn    = 0
0.00.591.203 I llama_init_from_model: freq_base     = 10000.0
0.00.591.204 I llama_init_from_model: freq_scale    = 1
0.00.591.204 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.207 I ggml_metal_init: allocating
0.00.591.279 I ggml_metal_init: found device: Apple M4
0.00.591.293 I ggml_metal_init: picking default device: Apple M4
0.00.593.034 I ggml_metal_init: using embedded metal library
0.00.599.668 I ggml_metal_init: GPU name:   Apple M4
0.00.599.672 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.673 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.674 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.674 I ggml_metal_init: simdgroup reduction   = true
0.00.599.674 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.675 I ggml_metal_init: has residency sets    = true
0.00.599.675 I ggml_metal_init: has bfloat            = true
0.00.599.675 I ggml_metal_init: use bfloat            = true
0.00.599.676 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.534 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.880 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.619.884 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.921 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.253 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.255 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.256 I llama_init_from_model: graph nodes  = 967
0.00.623.256 I llama_init_from_model: graph splits = 2
0.00.623.259 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.259 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.067 I 
0.00.654.150 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.169 I perplexity: tokenizing the input ..
0.00.661.846 I perplexity: tokenization took 7.673 ms
0.00.661.868 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.838 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.812.186 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.812.205 I llama_perf_context_print:        load time =     645.19 ms
0.00.812.206 I llama_perf_context_print: prompt eval time =     148.04 ms /   128 tokens (    1.16 ms per token,   864.61 tokens per second)
0.00.812.207 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.207 I llama_perf_context_print:       total time =     158.14 ms /   129 tokens
0.00.812.613 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.079s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.022 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.576 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.584 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.587 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.588 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.171 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.142 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.760 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.761 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.761 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.761 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.762 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.762 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.762 I llama_model_loader: - type  f32:  194 tensors
0.00.026.763 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.763 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.763 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.763 I print_info: file format = GGUF V3 (latest)
0.00.026.764 I print_info: file type   = Q2_K - Medium
0.00.026.764 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.231 I load: special tokens cache size = 25
0.00.040.028 I load: token to piece cache size = 0.2984 MB
0.00.040.031 I print_info: arch             = gptneox
0.00.040.031 I print_info: vocab_only       = 0
0.00.040.032 I print_info: n_ctx_train      = 2048
0.00.040.032 I print_info: n_embd           = 2048
0.00.040.032 I print_info: n_layer          = 24
0.00.040.034 I print_info: n_head           = 16
0.00.040.035 I print_info: n_head_kv        = 16
0.00.040.035 I print_info: n_rot            = 32
0.00.040.035 I print_info: n_swa            = 0
0.00.040.035 I print_info: n_embd_head_k    = 128
0.00.040.038 I print_info: n_embd_head_v    = 128
0.00.040.038 I print_info: n_gqa            = 1
0.00.040.039 I print_info: n_embd_k_gqa     = 2048
0.00.040.044 I print_info: n_embd_v_gqa     = 2048
0.00.040.045 I print_info: f_norm_eps       = 1.0e-05
0.00.040.045 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.045 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.045 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.046 I print_info: f_logit_scale    = 0.0e+00
0.00.040.046 I print_info: n_ff             = 8192
0.00.040.047 I print_info: n_expert         = 0
0.00.040.047 I print_info: n_expert_used    = 0
0.00.040.047 I print_info: causal attn      = 1
0.00.040.048 I print_info: pooling type     = 0
0.00.040.048 I print_info: rope type        = 2
0.00.040.048 I print_info: rope scaling     = linear
0.00.040.049 I print_info: freq_base_train  = 10000.0
0.00.040.049 I print_info: freq_scale_train = 1
0.00.040.050 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.051 I print_info: rope_finetuned   = unknown
0.00.040.051 I print_info: ssm_d_conv       = 0
0.00.040.051 I print_info: ssm_d_inner      = 0
0.00.040.051 I print_info: ssm_d_state      = 0
0.00.040.051 I print_info: ssm_dt_rank      = 0
0.00.040.051 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.052 I print_info: model type       = 1.4B
0.00.040.052 I print_info: model params     = 1.41 B
0.00.040.052 I print_info: general.name     = 1.4B
0.00.040.053 I print_info: vocab type       = BPE
0.00.040.053 I print_info: n_vocab          = 50304
0.00.040.054 I print_info: n_merges         = 50009
0.00.040.055 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.055 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.055 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.055 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.056 I print_info: LF token         = 128 'Ä'
0.00.040.056 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.056 I print_info: max token length = 1024
0.00.340.731 I load_tensors: offloading 24 repeating layers to GPU
0.00.340.745 I load_tensors: offloading output layer to GPU
0.00.340.746 I load_tensors: offloaded 25/25 layers to GPU
0.00.340.775 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.340.776 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.342.152 I llama_init_from_model: n_seq_max     = 1
0.00.342.159 I llama_init_from_model: n_ctx         = 128
0.00.342.159 I llama_init_from_model: n_ctx_per_seq = 128
0.00.342.160 I llama_init_from_model: n_batch       = 128
0.00.342.160 I llama_init_from_model: n_ubatch      = 128
0.00.342.161 I llama_init_from_model: flash_attn    = 0
0.00.342.163 I llama_init_from_model: freq_base     = 10000.0
0.00.342.164 I llama_init_from_model: freq_scale    = 1
0.00.342.164 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.342.167 I ggml_metal_init: allocating
0.00.342.226 I ggml_metal_init: found device: Apple M4
0.00.342.238 I ggml_metal_init: picking default device: Apple M4
0.00.343.866 I ggml_metal_init: using embedded metal library
0.00.349.309 I ggml_metal_init: GPU name:   Apple M4
0.00.349.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.349.327 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.349.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.349.328 I ggml_metal_init: simdgroup reduction   = true
0.00.349.328 I ggml_metal_init: simdgroup matrix mul. = true
0.00.349.328 I ggml_metal_init: has residency sets    = true
0.00.349.328 I ggml_metal_init: has bfloat            = true
0.00.349.329 I ggml_metal_init: use bfloat            = true
0.00.349.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.349.335 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.371.439 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.375.122 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.375.130 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.375.171 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.378.646 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.378.649 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.378.650 I llama_init_from_model: graph nodes  = 967
0.00.378.650 I llama_init_from_model: graph splits = 2
0.00.378.654 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.378.657 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.410.992 I 
0.00.411.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.411.095 I perplexity: tokenizing the input ..
0.00.418.111 I perplexity: tokenization took 7.012 ms
0.00.418.130 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.552.087 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.553.540 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.553.557 I llama_perf_context_print:        load time =     400.96 ms
0.00.553.558 I llama_perf_context_print: prompt eval time =     132.92 ms /   128 tokens (    1.04 ms per token,   962.98 tokens per second)
0.00.553.558 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.553.560 I llama_perf_context_print:       total time =     142.57 ms /   129 tokens
0.00.553.935 I ggml_metal_free: deallocating

real	0m0.570s
user	0m0.082s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.630 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.447 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.453 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.453 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.454 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.454 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.454 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.455 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.455 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.456 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.456 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.457 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.458 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.458 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.460 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.460 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.461 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.781 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.783 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.783 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.783 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.783 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.784 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.784 I llama_model_loader: - type  f32:  194 tensors
0.00.024.784 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.785 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.785 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.785 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.785 I print_info: file format = GGUF V3 (latest)
0.00.024.786 I print_info: file type   = Q3_K - Medium
0.00.024.787 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.294 I load: special tokens cache size = 25
0.00.038.102 I load: token to piece cache size = 0.2984 MB
0.00.038.105 I print_info: arch             = gptneox
0.00.038.105 I print_info: vocab_only       = 0
0.00.038.105 I print_info: n_ctx_train      = 2048
0.00.038.106 I print_info: n_embd           = 2048
0.00.038.106 I print_info: n_layer          = 24
0.00.038.109 I print_info: n_head           = 16
0.00.038.109 I print_info: n_head_kv        = 16
0.00.038.110 I print_info: n_rot            = 32
0.00.038.110 I print_info: n_swa            = 0
0.00.038.110 I print_info: n_embd_head_k    = 128
0.00.038.110 I print_info: n_embd_head_v    = 128
0.00.038.111 I print_info: n_gqa            = 1
0.00.038.112 I print_info: n_embd_k_gqa     = 2048
0.00.038.115 I print_info: n_embd_v_gqa     = 2048
0.00.038.116 I print_info: f_norm_eps       = 1.0e-05
0.00.038.117 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.117 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.117 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.117 I print_info: f_logit_scale    = 0.0e+00
0.00.038.118 I print_info: n_ff             = 8192
0.00.038.119 I print_info: n_expert         = 0
0.00.038.119 I print_info: n_expert_used    = 0
0.00.038.120 I print_info: causal attn      = 1
0.00.038.120 I print_info: pooling type     = 0
0.00.038.120 I print_info: rope type        = 2
0.00.038.120 I print_info: rope scaling     = linear
0.00.038.121 I print_info: freq_base_train  = 10000.0
0.00.038.121 I print_info: freq_scale_train = 1
0.00.038.121 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.121 I print_info: rope_finetuned   = unknown
0.00.038.121 I print_info: ssm_d_conv       = 0
0.00.038.122 I print_info: ssm_d_inner      = 0
0.00.038.122 I print_info: ssm_d_state      = 0
0.00.038.122 I print_info: ssm_dt_rank      = 0
0.00.038.122 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.126 I print_info: model type       = 1.4B
0.00.038.126 I print_info: model params     = 1.41 B
0.00.038.126 I print_info: general.name     = 1.4B
0.00.038.127 I print_info: vocab type       = BPE
0.00.038.127 I print_info: n_vocab          = 50304
0.00.038.127 I print_info: n_merges         = 50009
0.00.038.128 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.128 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.128 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.128 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.128 I print_info: LF token         = 128 'Ä'
0.00.038.128 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.129 I print_info: max token length = 1024
0.00.440.200 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.216 I load_tensors: offloading output layer to GPU
0.00.440.216 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.251 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.252 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.441.680 I llama_init_from_model: n_seq_max     = 1
0.00.441.685 I llama_init_from_model: n_ctx         = 128
0.00.441.685 I llama_init_from_model: n_ctx_per_seq = 128
0.00.441.686 I llama_init_from_model: n_batch       = 128
0.00.441.686 I llama_init_from_model: n_ubatch      = 128
0.00.441.686 I llama_init_from_model: flash_attn    = 0
0.00.441.688 I llama_init_from_model: freq_base     = 10000.0
0.00.441.689 I llama_init_from_model: freq_scale    = 1
0.00.441.690 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.441.697 I ggml_metal_init: allocating
0.00.441.793 I ggml_metal_init: found device: Apple M4
0.00.441.807 I ggml_metal_init: picking default device: Apple M4
0.00.443.571 I ggml_metal_init: using embedded metal library
0.00.449.043 I ggml_metal_init: GPU name:   Apple M4
0.00.449.047 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.449.048 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.449.049 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.449.050 I ggml_metal_init: simdgroup reduction   = true
0.00.449.051 I ggml_metal_init: simdgroup matrix mul. = true
0.00.449.051 I ggml_metal_init: has residency sets    = true
0.00.449.051 I ggml_metal_init: has bfloat            = true
0.00.449.052 I ggml_metal_init: use bfloat            = true
0.00.449.052 I ggml_metal_init: hasUnifiedMemory      = true
0.00.449.054 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.468.762 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.472.343 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.472.346 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.472.373 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.475.606 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.475.608 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.475.609 I llama_init_from_model: graph nodes  = 967
0.00.475.609 I llama_init_from_model: graph splits = 2
0.00.475.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.475.624 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.504.781 I 
0.00.504.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.504.888 I perplexity: tokenizing the input ..
0.00.512.061 I perplexity: tokenization took 7.167 ms
0.00.512.087 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.653.698 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.655.049 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.655.065 I llama_perf_context_print:        load time =     495.14 ms
0.00.655.066 I llama_perf_context_print: prompt eval time =     140.73 ms /   128 tokens (    1.10 ms per token,   909.56 tokens per second)
0.00.655.066 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.655.067 I llama_perf_context_print:       total time =     150.29 ms /   129 tokens
0.00.655.450 I ggml_metal_free: deallocating

real	0m0.669s
user	0m0.078s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.045 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.005 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.010 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.015 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.015 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.016 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.016 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.016 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.017 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.017 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.018 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.018 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.019 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.019 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.019 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.021 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.021 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.022 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.622 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.282 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.283 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.283 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.284 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.284 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.284 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.285 I llama_model_loader: - type  f32:  194 tensors
0.00.024.285 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.285 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.285 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.286 I print_info: file format = GGUF V3 (latest)
0.00.024.286 I print_info: file type   = Q4_K - Medium
0.00.024.287 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.031.705 I load: special tokens cache size = 25
0.00.037.451 I load: token to piece cache size = 0.2984 MB
0.00.037.453 I print_info: arch             = gptneox
0.00.037.454 I print_info: vocab_only       = 0
0.00.037.454 I print_info: n_ctx_train      = 2048
0.00.037.454 I print_info: n_embd           = 2048
0.00.037.454 I print_info: n_layer          = 24
0.00.037.457 I print_info: n_head           = 16
0.00.037.458 I print_info: n_head_kv        = 16
0.00.037.458 I print_info: n_rot            = 32
0.00.037.458 I print_info: n_swa            = 0
0.00.037.458 I print_info: n_embd_head_k    = 128
0.00.037.458 I print_info: n_embd_head_v    = 128
0.00.037.459 I print_info: n_gqa            = 1
0.00.037.460 I print_info: n_embd_k_gqa     = 2048
0.00.037.460 I print_info: n_embd_v_gqa     = 2048
0.00.037.461 I print_info: f_norm_eps       = 1.0e-05
0.00.037.461 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.461 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.462 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.462 I print_info: f_logit_scale    = 0.0e+00
0.00.037.462 I print_info: n_ff             = 8192
0.00.037.463 I print_info: n_expert         = 0
0.00.037.463 I print_info: n_expert_used    = 0
0.00.037.463 I print_info: causal attn      = 1
0.00.037.463 I print_info: pooling type     = 0
0.00.037.463 I print_info: rope type        = 2
0.00.037.463 I print_info: rope scaling     = linear
0.00.037.464 I print_info: freq_base_train  = 10000.0
0.00.037.464 I print_info: freq_scale_train = 1
0.00.037.464 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.464 I print_info: rope_finetuned   = unknown
0.00.037.464 I print_info: ssm_d_conv       = 0
0.00.037.465 I print_info: ssm_d_inner      = 0
0.00.037.466 I print_info: ssm_d_state      = 0
0.00.037.466 I print_info: ssm_dt_rank      = 0
0.00.037.466 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.468 I print_info: model type       = 1.4B
0.00.037.468 I print_info: model params     = 1.41 B
0.00.037.469 I print_info: general.name     = 1.4B
0.00.037.469 I print_info: vocab type       = BPE
0.00.037.469 I print_info: n_vocab          = 50304
0.00.037.470 I print_info: n_merges         = 50009
0.00.037.470 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.474 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.474 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.474 I print_info: LF token         = 128 'Ä'
0.00.037.475 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.477 I print_info: max token length = 1024
0.00.521.477 I load_tensors: offloading 24 repeating layers to GPU
0.00.521.490 I load_tensors: offloading output layer to GPU
0.00.521.490 I load_tensors: offloaded 25/25 layers to GPU
0.00.521.529 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.521.530 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.523.138 I llama_init_from_model: n_seq_max     = 1
0.00.523.143 I llama_init_from_model: n_ctx         = 128
0.00.523.147 I llama_init_from_model: n_ctx_per_seq = 128
0.00.523.147 I llama_init_from_model: n_batch       = 128
0.00.523.148 I llama_init_from_model: n_ubatch      = 128
0.00.523.148 I llama_init_from_model: flash_attn    = 0
0.00.523.151 I llama_init_from_model: freq_base     = 10000.0
0.00.523.151 I llama_init_from_model: freq_scale    = 1
0.00.523.155 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.523.160 I ggml_metal_init: allocating
0.00.523.293 I ggml_metal_init: found device: Apple M4
0.00.523.307 I ggml_metal_init: picking default device: Apple M4
0.00.525.250 I ggml_metal_init: using embedded metal library
0.00.531.746 I ggml_metal_init: GPU name:   Apple M4
0.00.531.749 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.531.750 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.531.751 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.531.752 I ggml_metal_init: simdgroup reduction   = true
0.00.531.752 I ggml_metal_init: simdgroup matrix mul. = true
0.00.531.752 I ggml_metal_init: has residency sets    = true
0.00.531.753 I ggml_metal_init: has bfloat            = true
0.00.531.753 I ggml_metal_init: use bfloat            = true
0.00.531.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.531.756 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.022 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.552.499 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.552.503 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.552.534 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.555.716 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.555.718 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.555.719 I llama_init_from_model: graph nodes  = 967
0.00.555.719 I llama_init_from_model: graph splits = 2
0.00.555.722 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.555.722 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.585.282 I 
0.00.585.369 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.585.391 I perplexity: tokenizing the input ..
0.00.592.598 I perplexity: tokenization took 7.203 ms
0.00.592.620 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.735.017 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.736.359 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.736.375 I llama_perf_context_print:        load time =     576.23 ms
0.00.736.376 I llama_perf_context_print: prompt eval time =     141.44 ms /   128 tokens (    1.10 ms per token,   904.99 tokens per second)
0.00.736.376 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.736.377 I llama_perf_context_print:       total time =     151.10 ms /   129 tokens
0.00.736.778 I ggml_metal_free: deallocating

real	0m0.751s
user	0m0.078s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.172 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.943 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.950 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.950 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.950 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.951 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.952 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.953 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.953 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.954 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.954 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.955 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.958 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.958 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.958 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.562 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.150 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.151 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.151 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.152 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.152 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.152 I llama_model_loader: - type  f32:  194 tensors
0.00.025.153 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.153 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.153 I print_info: file format = GGUF V3 (latest)
0.00.025.153 I print_info: file type   = Q5_K - Medium
0.00.025.154 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.681 I load: special tokens cache size = 25
0.00.038.501 I load: token to piece cache size = 0.2984 MB
0.00.038.504 I print_info: arch             = gptneox
0.00.038.504 I print_info: vocab_only       = 0
0.00.038.505 I print_info: n_ctx_train      = 2048
0.00.038.505 I print_info: n_embd           = 2048
0.00.038.505 I print_info: n_layer          = 24
0.00.038.508 I print_info: n_head           = 16
0.00.038.508 I print_info: n_head_kv        = 16
0.00.038.509 I print_info: n_rot            = 32
0.00.038.509 I print_info: n_swa            = 0
0.00.038.509 I print_info: n_embd_head_k    = 128
0.00.038.509 I print_info: n_embd_head_v    = 128
0.00.038.510 I print_info: n_gqa            = 1
0.00.038.511 I print_info: n_embd_k_gqa     = 2048
0.00.038.511 I print_info: n_embd_v_gqa     = 2048
0.00.038.512 I print_info: f_norm_eps       = 1.0e-05
0.00.038.512 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.513 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.513 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.513 I print_info: f_logit_scale    = 0.0e+00
0.00.038.514 I print_info: n_ff             = 8192
0.00.038.514 I print_info: n_expert         = 0
0.00.038.514 I print_info: n_expert_used    = 0
0.00.038.514 I print_info: causal attn      = 1
0.00.038.514 I print_info: pooling type     = 0
0.00.038.514 I print_info: rope type        = 2
0.00.038.515 I print_info: rope scaling     = linear
0.00.038.515 I print_info: freq_base_train  = 10000.0
0.00.038.515 I print_info: freq_scale_train = 1
0.00.038.515 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.516 I print_info: rope_finetuned   = unknown
0.00.038.516 I print_info: ssm_d_conv       = 0
0.00.038.516 I print_info: ssm_d_inner      = 0
0.00.038.516 I print_info: ssm_d_state      = 0
0.00.038.516 I print_info: ssm_dt_rank      = 0
0.00.038.517 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.517 I print_info: model type       = 1.4B
0.00.038.517 I print_info: model params     = 1.41 B
0.00.038.517 I print_info: general.name     = 1.4B
0.00.038.518 I print_info: vocab type       = BPE
0.00.038.518 I print_info: n_vocab          = 50304
0.00.038.518 I print_info: n_merges         = 50009
0.00.038.519 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.519 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.519 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.519 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.519 I print_info: LF token         = 128 'Ä'
0.00.038.520 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.520 I print_info: max token length = 1024
0.00.611.052 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.066 I load_tensors: offloading output layer to GPU
0.00.611.067 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.098 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.611.100 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.612.481 I llama_init_from_model: n_seq_max     = 1
0.00.612.485 I llama_init_from_model: n_ctx         = 128
0.00.612.485 I llama_init_from_model: n_ctx_per_seq = 128
0.00.612.486 I llama_init_from_model: n_batch       = 128
0.00.612.486 I llama_init_from_model: n_ubatch      = 128
0.00.612.487 I llama_init_from_model: flash_attn    = 0
0.00.612.489 I llama_init_from_model: freq_base     = 10000.0
0.00.612.489 I llama_init_from_model: freq_scale    = 1
0.00.612.490 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.612.493 I ggml_metal_init: allocating
0.00.612.585 I ggml_metal_init: found device: Apple M4
0.00.612.600 I ggml_metal_init: picking default device: Apple M4
0.00.614.388 I ggml_metal_init: using embedded metal library
0.00.621.113 I ggml_metal_init: GPU name:   Apple M4
0.00.621.117 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.118 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.119 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.120 I ggml_metal_init: simdgroup reduction   = true
0.00.621.120 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.120 I ggml_metal_init: has residency sets    = true
0.00.621.120 I ggml_metal_init: has bfloat            = true
0.00.621.121 I ggml_metal_init: use bfloat            = true
0.00.621.121 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.123 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.616 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.089 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.096 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.133 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.645.411 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.645.413 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.645.413 I llama_init_from_model: graph nodes  = 967
0.00.645.414 I llama_init_from_model: graph splits = 2
0.00.645.417 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.645.417 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.993 I 
0.00.683.084 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.105 I perplexity: tokenizing the input ..
0.00.689.869 I perplexity: tokenization took 6.763 ms
0.00.689.883 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.841.745 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.843.078 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.843.093 I llama_perf_context_print:        load time =     672.81 ms
0.00.843.094 I llama_perf_context_print: prompt eval time =     151.63 ms /   128 tokens (    1.18 ms per token,   844.16 tokens per second)
0.00.843.095 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.843.095 I llama_perf_context_print:       total time =     160.11 ms /   129 tokens
0.00.843.442 I ggml_metal_free: deallocating

real	0m0.859s
user	0m0.076s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.941 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.522 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.526 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.532 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.532 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.533 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.533 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.534 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.535 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.536 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.536 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.537 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.540 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.541 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.542 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.542 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.543 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.314 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.053 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.054 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.054 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.055 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.055 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.055 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.055 I llama_model_loader: - type  f32:  194 tensors
0.00.024.056 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.056 I print_info: file format = GGUF V3 (latest)
0.00.024.057 I print_info: file type   = Q6_K
0.00.024.057 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.864 I load: special tokens cache size = 25
0.00.037.705 I load: token to piece cache size = 0.2984 MB
0.00.037.707 I print_info: arch             = gptneox
0.00.037.708 I print_info: vocab_only       = 0
0.00.037.708 I print_info: n_ctx_train      = 2048
0.00.037.708 I print_info: n_embd           = 2048
0.00.037.708 I print_info: n_layer          = 24
0.00.037.711 I print_info: n_head           = 16
0.00.037.712 I print_info: n_head_kv        = 16
0.00.037.714 I print_info: n_rot            = 32
0.00.037.714 I print_info: n_swa            = 0
0.00.037.714 I print_info: n_embd_head_k    = 128
0.00.037.714 I print_info: n_embd_head_v    = 128
0.00.037.715 I print_info: n_gqa            = 1
0.00.037.716 I print_info: n_embd_k_gqa     = 2048
0.00.037.717 I print_info: n_embd_v_gqa     = 2048
0.00.037.717 I print_info: f_norm_eps       = 1.0e-05
0.00.037.718 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.718 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.718 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.718 I print_info: f_logit_scale    = 0.0e+00
0.00.037.719 I print_info: n_ff             = 8192
0.00.037.719 I print_info: n_expert         = 0
0.00.037.719 I print_info: n_expert_used    = 0
0.00.037.719 I print_info: causal attn      = 1
0.00.037.720 I print_info: pooling type     = 0
0.00.037.720 I print_info: rope type        = 2
0.00.037.720 I print_info: rope scaling     = linear
0.00.037.720 I print_info: freq_base_train  = 10000.0
0.00.037.721 I print_info: freq_scale_train = 1
0.00.037.721 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.721 I print_info: rope_finetuned   = unknown
0.00.037.725 I print_info: ssm_d_conv       = 0
0.00.037.726 I print_info: ssm_d_inner      = 0
0.00.037.726 I print_info: ssm_d_state      = 0
0.00.037.726 I print_info: ssm_dt_rank      = 0
0.00.037.726 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.726 I print_info: model type       = 1.4B
0.00.037.727 I print_info: model params     = 1.41 B
0.00.037.727 I print_info: general.name     = 1.4B
0.00.037.728 I print_info: vocab type       = BPE
0.00.037.728 I print_info: n_vocab          = 50304
0.00.037.728 I print_info: n_merges         = 50009
0.00.037.728 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.728 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.729 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.729 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.729 I print_info: LF token         = 128 'Ä'
0.00.037.729 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.729 I print_info: max token length = 1024
0.00.583.270 I load_tensors: offloading 24 repeating layers to GPU
0.00.583.273 I load_tensors: offloading output layer to GPU
0.00.583.274 I load_tensors: offloaded 25/25 layers to GPU
0.00.583.298 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.583.302 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.584.746 I llama_init_from_model: n_seq_max     = 1
0.00.584.748 I llama_init_from_model: n_ctx         = 128
0.00.584.749 I llama_init_from_model: n_ctx_per_seq = 128
0.00.584.749 I llama_init_from_model: n_batch       = 128
0.00.584.753 I llama_init_from_model: n_ubatch      = 128
0.00.584.754 I llama_init_from_model: flash_attn    = 0
0.00.584.755 I llama_init_from_model: freq_base     = 10000.0
0.00.584.758 I llama_init_from_model: freq_scale    = 1
0.00.584.759 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.584.763 I ggml_metal_init: allocating
0.00.584.824 I ggml_metal_init: found device: Apple M4
0.00.584.846 I ggml_metal_init: picking default device: Apple M4
0.00.586.473 I ggml_metal_init: using embedded metal library
0.00.592.337 I ggml_metal_init: GPU name:   Apple M4
0.00.592.340 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.592.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.592.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.592.343 I ggml_metal_init: simdgroup reduction   = true
0.00.592.343 I ggml_metal_init: simdgroup matrix mul. = true
0.00.592.343 I ggml_metal_init: has residency sets    = true
0.00.592.343 I ggml_metal_init: has bfloat            = true
0.00.592.344 I ggml_metal_init: use bfloat            = true
0.00.592.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.592.353 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.609.465 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.612.870 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.612.874 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.612.898 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.616.025 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.616.026 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.616.027 I llama_init_from_model: graph nodes  = 967
0.00.616.027 I llama_init_from_model: graph splits = 2
0.00.616.030 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.616.030 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.594 I 
0.00.649.678 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.697 I perplexity: tokenizing the input ..
0.00.657.022 I perplexity: tokenization took 7.321 ms
0.00.657.050 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.067 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.799.408 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.799.427 I llama_perf_context_print:        load time =     640.64 ms
0.00.799.428 I llama_perf_context_print: prompt eval time =     140.14 ms /   128 tokens (    1.09 ms per token,   913.37 tokens per second)
0.00.799.429 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.429 I llama_perf_context_print:       total time =     149.84 ms /   129 tokens
0.00.799.843 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.077s
sys	0m0.132s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.271 I build: 4591 (7919256c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.072 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.312 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.322 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.325 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.326 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.327 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.328 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.328 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.330 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.331 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.332 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.333 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.333 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.334 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.335 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.339 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.339 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.627 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.219 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.221 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.222 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.222 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.223 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.224 I llama_model_loader: - type  f32:  194 tensors
0.00.057.224 I llama_model_loader: - type  f16:   98 tensors
0.00.057.225 I print_info: file format = GGUF V3 (latest)
0.00.057.228 I print_info: file type   = all F32 (guessed)
0.00.057.230 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.601 I load: special tokens cache size = 25
0.00.078.991 I load: token to piece cache size = 0.2984 MB
0.00.078.994 I print_info: arch             = gptneox
0.00.078.995 I print_info: vocab_only       = 0
0.00.078.995 I print_info: n_ctx_train      = 2048
0.00.078.995 I print_info: n_embd           = 2048
0.00.078.995 I print_info: n_layer          = 24
0.00.078.998 I print_info: n_head           = 16
0.00.079.000 I print_info: n_head_kv        = 16
0.00.079.000 I print_info: n_rot            = 32
0.00.079.000 I print_info: n_swa            = 0
0.00.079.000 I print_info: n_embd_head_k    = 128
0.00.079.000 I print_info: n_embd_head_v    = 128
0.00.079.001 I print_info: n_gqa            = 1
0.00.079.002 I print_info: n_embd_k_gqa     = 2048
0.00.079.003 I print_info: n_embd_v_gqa     = 2048
0.00.079.004 I print_info: f_norm_eps       = 1.0e-05
0.00.079.004 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.004 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.004 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.004 I print_info: f_logit_scale    = 0.0e+00
0.00.079.005 I print_info: n_ff             = 8192
0.00.079.005 I print_info: n_expert         = 0
0.00.079.005 I print_info: n_expert_used    = 0
0.00.079.008 I print_info: causal attn      = 1
0.00.079.008 I print_info: pooling type     = 0
0.00.079.008 I print_info: rope type        = 2
0.00.079.008 I print_info: rope scaling     = linear
0.00.079.009 I print_info: freq_base_train  = 10000.0
0.00.079.010 I print_info: freq_scale_train = 1
0.00.079.010 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.011 I print_info: rope_finetuned   = unknown
0.00.079.011 I print_info: ssm_d_conv       = 0
0.00.079.011 I print_info: ssm_d_inner      = 0
0.00.079.011 I print_info: ssm_d_state      = 0
0.00.079.011 I print_info: ssm_dt_rank      = 0
0.00.079.011 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.012 I print_info: model type       = 1.4B
0.00.079.012 I print_info: model params     = 1.41 B
0.00.079.012 I print_info: general.name     = 1.4B
0.00.079.013 I print_info: vocab type       = BPE
0.00.079.013 I print_info: n_vocab          = 50304
0.00.079.013 I print_info: n_merges         = 50009
0.00.079.013 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.014 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.015 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.019 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.019 I print_info: LF token         = 128 'Ä'
0.00.079.019 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.019 I print_info: max token length = 1024
0.01.308.898 I load_tensors: offloading 24 repeating layers to GPU
0.01.308.903 I load_tensors: offloading output layer to GPU
0.01.308.904 I load_tensors: offloaded 25/25 layers to GPU
0.01.308.939 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.308.940 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.309.537 I llama_init_from_model: n_seq_max     = 1
0.01.309.538 I llama_init_from_model: n_ctx         = 128
0.01.309.539 I llama_init_from_model: n_ctx_per_seq = 128
0.01.309.539 I llama_init_from_model: n_batch       = 128
0.01.309.539 I llama_init_from_model: n_ubatch      = 128
0.01.309.539 I llama_init_from_model: flash_attn    = 0
0.01.309.540 I llama_init_from_model: freq_base     = 10000.0
0.01.309.540 I llama_init_from_model: freq_scale    = 1
0.01.309.540 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.309.544 I ggml_metal_init: allocating
0.01.309.603 I ggml_metal_init: found device: Apple M4
0.01.309.608 I ggml_metal_init: picking default device: Apple M4
0.01.310.725 I ggml_metal_init: using embedded metal library
0.01.314.550 I ggml_metal_init: GPU name:   Apple M4
0.01.314.553 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.314.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.314.554 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.314.554 I ggml_metal_init: simdgroup reduction   = true
0.01.314.554 I ggml_metal_init: simdgroup matrix mul. = true
0.01.314.554 I ggml_metal_init: has residency sets    = true
0.01.314.554 I ggml_metal_init: has bfloat            = true
0.01.314.554 I ggml_metal_init: use bfloat            = true
0.01.314.555 I ggml_metal_init: hasUnifiedMemory      = true
0.01.314.556 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.325.121 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.326.843 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.326.846 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.326.859 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.328.444 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.328.445 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.328.446 I llama_init_from_model: graph nodes  = 967
0.01.328.446 I llama_init_from_model: graph splits = 2
0.01.328.447 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.328.448 I 
0.01.328.483 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.328.485 I compute_imatrix: tokenizing the input ..
0.01.332.523 I compute_imatrix: tokenization took 4.037 ms
0.01.332.525 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.604.092 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.607.227 I llama_perf_context_print:        load time =    1580.02 ms
0.01.607.228 I llama_perf_context_print: prompt eval time =     269.84 ms /   128 tokens (    2.11 ms per token,   474.36 tokens per second)
0.01.607.229 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.607.229 I llama_perf_context_print:       total time =    1583.14 ms /   129 tokens
0.01.607.784 I ggml_metal_free: deallocating

real	0m1.814s
user	0m0.128s
sys	0m0.253s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4591 (7919256c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107605260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1076085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107608a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107608ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107609340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1076097b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107609c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10760a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10760a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10760a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10760ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10760b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10760bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10760c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10760cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10760d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10760dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10760e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10760ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10760f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10760fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1076101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107610910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1076111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1076118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107611b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107611e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1076122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1076129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107612e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107613410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107613920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107613d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107614050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1076144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107614930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107614e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107615390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107615890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107615d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107616290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107616790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107616c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107617190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107617f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1076183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107618b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107618fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107619450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1076198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107619d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10761a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10761a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10761ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10761b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10761b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10761ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10761c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10761c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10761c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10761ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10761d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10761d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10761dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10761e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10761e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10761ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10761eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10761f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10761f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10761fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1076201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107620740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107620c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1076211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107621c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1076221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107622720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107622c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1076231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107623710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107623c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1076241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107624700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107624c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1076251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1076256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107625c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107626190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1076266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107626c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107627180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1076276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107627c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1076186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107628090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107628840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107628d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1076292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107629830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107629d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10762a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10762a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10762ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10762b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10762b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10762bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10762c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10762c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10762cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10762d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10762d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10762db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10762dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10762e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10762e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10762edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10762f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10762f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10762fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1076302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107630560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107630a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107630f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107631460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107631960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107631e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107632360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107632860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107632d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107633260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107633760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107633c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107634160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107634660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107634b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107635060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107635560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107635a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107635f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107636460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107636e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107637360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107637860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107637d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107638260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107638760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107638c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107639160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107639660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107639b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10763a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10763a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10763aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10763af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10763b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10763b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10763be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10763c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10763c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10763cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10763d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10763d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10763dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10763e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10763e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10763eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10763f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10763f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10763fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10763ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107640460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107640960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107640e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107641360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107641860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107641d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107642260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107642760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107642c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107643160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107643660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107643b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107644060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107644560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107644a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107644f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107645460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107645960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107645e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107646410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1076469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107646f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107647520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107647b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107648140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107648750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107648f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1076493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1076496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107649cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10764a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10764aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10764af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10764b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10764b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10764c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10764c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10764cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10764d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10764d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10764dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10764e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10764e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10764eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10764f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10764f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10764fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107650000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107650550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107650aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107650ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107651540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107651a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107651fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107652530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107652a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107652fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107653520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107653a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107653fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107654510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107654a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107654fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107655500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107655a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107655fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1076564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107656a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107656f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1076574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107657a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107657f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1076584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107658a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107658f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1076594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107659a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107659f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10765a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10765aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10765af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10765b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10765b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10765bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10765c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10765c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10765cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10765d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10765d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10765df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10765e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10765e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10765ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10765f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10765f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10765fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1076600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107660580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107660a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107660ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107661360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107661800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107661ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107662140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1076625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107662a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107662f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107663470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107663b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1076642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1076649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1076650f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1076653b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107665ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107665e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107666470 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.717.812 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.717.816 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107704bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107705030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1077054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107705910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107705d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1077061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107706660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107706ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107706f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1077073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107707820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107707ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107708a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1077091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1077099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10770a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10770a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10770af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10770b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10770be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10770c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10770cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10770d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10770da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10770e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10770e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10770e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10770eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10770f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10770f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10770f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10770fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107710290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107710550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1077109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107710e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1077112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107711710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107711b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107711ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107712460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1077128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107712d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1077131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107713620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107713a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107713f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107714370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1077147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107714c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1077150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107715530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1077159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107715e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107716280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1077166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107716c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107717160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1077175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107717a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107717eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107718320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107718790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107718c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107719070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1077194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107719950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107719dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10771a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10771a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10771ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10771af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10771b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10771b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10771bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10771c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10771c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10771ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10771ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10771d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10771d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10771dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10771e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10771e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10771e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10771eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10771f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10771f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10771faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10771ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1077203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107720840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107720cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107721120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107721590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107721a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107721e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1077222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107722750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107722bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107723030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1077234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107723910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107723d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1077241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107724660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107724ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107724f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1077253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107725820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107725c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107726100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107726570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1077269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107726e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1077272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107727730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107727ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107728010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107728480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1077288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107728d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1077291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107729640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107729ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107729f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10772a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10772a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10772ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10772b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10772b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10772b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10772be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10772c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10772c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10772cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10772cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10772d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10772d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10772dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10772e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10772e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10772ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10772ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10772f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10772f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10772fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1077300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107730530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1077309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107730e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107731280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1077316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107731b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107731fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107732440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1077328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107732d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107733190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107733600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107733a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107733ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107734350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1077347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107734c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1077350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107735cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107735f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107736250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1077366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107736b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107736fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107737410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107737880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107737cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107738160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1077385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107738a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107738eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107739320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107739790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107739c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10773a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10773a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10773a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10773adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10773b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10773b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10773bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10773bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10773c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10773c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10773ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10773d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10773d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10773da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10773de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10773e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10773e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10773ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10773f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10773f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10773fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10773ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1077403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107740810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107740c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1077410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107741610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107741b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107742690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107742950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107742f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1077434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107743a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107744050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107744610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107744bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107745190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107745750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107745d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1077462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107746890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107746e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107747410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1077479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107747f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107748550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107748b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1077490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107749690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107749c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10774a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10774a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10774ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10774b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10774b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10774bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10774c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10774ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10774d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10774d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10774db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10774e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10774e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10774ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10774f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10774f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10774fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1077503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107750990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107750f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107751510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107751ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107752090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107752650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107752c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1077531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107753790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107753d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107754310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1077548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107754e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107755450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107755a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107755fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107756590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107756b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107757050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107757550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107757a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107757f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107758450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107758950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107758e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107759350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107759850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107759d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10775a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10775a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10775ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10775b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10775b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10775c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10775c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10775cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10775d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10775d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10775e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10775e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10775e940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1073086f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107306500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107308d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107309180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1073095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107309ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10730a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10730a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10730acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10730b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10730b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10730bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10730c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10730ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10730d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10730ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10730e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10730ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10730f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10730fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107310200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107310920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107311040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107311760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107311e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107312140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107312750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107312d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107313370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107313b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107314000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1073142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107314b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107315090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107315350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1073157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107315c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107316130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1073165d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107316a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107316f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1073173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107317850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107317cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107317fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1073185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107318bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1073191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1073197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107319e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10731a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10731aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10731b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10731b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10731be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10731c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10731c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10731ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10731d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10731d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10731dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10731e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10731e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10731eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10731ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10731f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10731f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10731fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1073201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107320670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107320b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107320fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107321450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1073219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107321ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107322440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107322990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107322ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107323430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107323980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107323ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107324420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107324970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107324ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107325410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107325960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107325eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107326400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107326950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107326ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1073273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107327940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107327e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1073283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107328930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107328e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1073293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107329920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107329e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10732a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10732a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10732ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10732b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10732b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10732be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10732c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10732c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10732ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10732d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10732d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10732de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10732e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10732e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10732ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10732f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10732f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10732fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10732fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107330490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107330930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107330dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107331270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107331710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107331bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107332050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1073324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107332990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107332e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1073332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107333770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107333c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1073340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107334550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1073349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107334e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107335330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1073357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107335c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107336110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1073365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107336a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107336ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107337390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107337830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107337cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107338170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107338610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107338ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107338f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1073393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107339890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107339d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10733a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10733a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10733ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10733afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10733b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10733b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10733bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10733c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10733c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10733cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10733d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10733d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10733d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10733ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10733e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10733e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10733ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10733f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10733f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10733f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10733fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1073402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107340790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107340c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1073410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107341570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107341a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107341eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107342350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1073427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107342c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107343130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1073435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107343a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107343f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1073443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107344850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107344cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107345190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107345630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107345ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107346020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107346570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107346ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107347010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1073472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1073478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107347ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107348500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107348cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107349190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107349450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107349a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10734a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10734a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10734ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10734b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10734b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10734bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10734c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10734c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10734cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10734d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10734d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10734ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10734e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10734e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10734edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10734f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10734f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10734fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107350300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107350850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107350da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1073512f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107351840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107351d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1073522e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107352830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107352d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1073532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107353820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107353d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1073542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107354810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107354d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1073552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107355800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107355d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1073562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1073567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107356d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107357290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1073577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107357d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107358280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1073587d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107358d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107359270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1073597c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107359d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10735a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10735a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10735ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10735b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10735b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10735bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10735c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10735c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10735cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10735d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10735d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10735dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10735e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10735e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10735ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10735f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10735f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10735f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10735fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107360330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1073607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107360c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107361110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1073615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107361a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107361ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107362390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107362830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107362cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107363220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107363940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107364060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107364780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107364ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107365160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107365950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107365c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107366220 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.809s
user	0m0.284s
sys	0m0.317s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4591 (7919256c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131f0d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131f0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131f0e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131f0e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131f0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131f0f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131f0f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131f0fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131f10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131f10880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131f10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131f11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131f11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131f12550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131f12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131f13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131f13ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131f142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131f149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131f151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131f158d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131f15ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131f16710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131f16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131f176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131f17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131f17fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131f18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131f19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131f19410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131f198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131f19b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131f1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131f1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131f1ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131f1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131f1b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131f1b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131f1be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131f1c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131f1c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131f1cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131f1d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131f1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131f1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131f1de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131f1e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131f1eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131f1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131f1f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131f1ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131f205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131f20bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131f21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131f219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131f21e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131f22330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131f225f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131f22c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131f233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131f236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131f23b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131f23ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131f24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131f24930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131f24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131f25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131f25bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131f26050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131f264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131f26990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131f26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131f27380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131f278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131f27e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131f288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131f28e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131f29360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131f298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131f29e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131f2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131f2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131f2adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131f2b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131f2b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131f2bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131f2c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131f2c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131f2cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131f2d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131f2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131f2ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131f2e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131f2e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131f2edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131f1ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131f2f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131f2f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131f2ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131f30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131f309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131f30f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131f31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131f319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131f31f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131f32450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131f329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131f32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131f33440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131f33990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131f33ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131f34380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131f34820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131f34cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131f35160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131f35600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131f35aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131f35f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131f363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131f36880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131f36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131f371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131f37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131f37b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131f37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131f38440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131f388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131f38d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131f39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131f396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131f39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131f3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131f3a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131f3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131f3ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131f3b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131f3b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131f3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131f3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131f3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131f3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131f3ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131f3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131f3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131f3dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131f3e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131f3e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131f3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131f3eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131f3f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131f3f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131f3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131f40120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131f405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131f40a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131f40f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131f413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131f41840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131f41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131f42180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131f42620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131f42ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131f42f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131f43400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131f438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131f43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131f441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131f44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131f44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131f44fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131f45460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131f45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131f45da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131f46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131f466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131f46b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131f47020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131f474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131f47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131f47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131f482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131f48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131f48be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131f49080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131f49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131f499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131f49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131f4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131f4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131f4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131f4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131f4b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131f4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131f4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131f4c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131f4c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131f4cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131f4d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131f4db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131f4e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131f4e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131f4ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131f4f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131f4f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131f4fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131f50310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131f507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131f50c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131f51950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131f51ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131f523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131f52940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131f52e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131f533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131f53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131f53e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131f543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131f54920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131f54e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131f553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131f55910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131f55e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131f563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131f56900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131f56e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131f573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131f578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131f58390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131f588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131f58e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131f59380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131f598d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131f59e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131f5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131f5a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131f5ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131f5b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131f5b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131f5be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131f5c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131f5c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131f5cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131f5d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131f5d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131f5dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131f5e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131f5e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131f5edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131f5f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131f5f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131f5fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131f60310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131f60860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131f60db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131f61300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131f61850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131f61da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131f622f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131f62840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131f62d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131f632e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131f63830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131f63d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131f64220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131f646c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131f64b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131f65000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131f654a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131f65940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131f65de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131f66280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131f66720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131f66bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131f67060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131f67500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131f679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131f67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131f682e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131f68830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131f68f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131f69670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131f69d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131f6a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131f6a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131f6af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131f6b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131f6b830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.651 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131f6b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131f4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131f4cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131f4d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131f208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131f20290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131f228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131f4f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131f17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131f1e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131f1f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131f1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131f1db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131f1fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131f16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131f22ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131f2f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131f6aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131f19e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131f1a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131f4f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131f4ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131f18260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131f18520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131f187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131f6bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131f6bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131f6c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131f6c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131f6c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131f6ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131f6cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131f6cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131f6d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131f6d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131f6d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131f6dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131f6dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131f6e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131f6e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131f6e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131f6e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131f6eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131f6ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131f6f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131f6f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131f6f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131f6f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131f6fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131f6fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131f70150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131f70410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131f706d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131f70990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131f70c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131f70f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131f711d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131f71490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131f71750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131f71a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131f71cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131f71f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131f72250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131f72510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131f727d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131f72a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131f72d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131f73010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131f732d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131f73590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131f73850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131f73b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131f73dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131f74090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131f74350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131f74610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131f748d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131f74b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131f74e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131f75110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131f753d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131f75690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131f75950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131f75c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131f75ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131f76190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131f76450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131f76710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131f769d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131f76c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131f76f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131f77210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131f774d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131f77790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131f77a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131f77d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131f77fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131f78290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131f78550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131f78810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131f78ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131f78d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131f79050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131f79310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131f795d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131f79890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131f79b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131f79e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131f7a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131f7a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131f7a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131f7a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131f7abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131f7ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131f7b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131f7b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131f7b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131f7b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131f7bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131f7bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131f7c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131f7c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131f7c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131f7ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131f7ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131f7cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131f7d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131f7d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131f7d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131f7da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131f7dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131f7e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131f7e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131f7e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131f7e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131f7eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131f7edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131f7f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131f7f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131f7f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131f7f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131f7fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131f7fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131f80110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131f803d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131f80690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131f80950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131f80c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131f80ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131f81190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131f81450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131f81710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131f819d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131f81c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131f81f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131f82210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131f824d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131f82790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131f82a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131f82d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131f82fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131f83290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131f83550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131f83810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131f83ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131f83d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131f84050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131f84310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131f845d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131f84890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131f84b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131f84e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131f850d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131f85390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131f85650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131f85910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131f85bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131f85e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131f86150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131f86410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131f866d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131f86990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131f86c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131f86f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131f871d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131f87490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131f87750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131f87a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131f87cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131f87f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131f88250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131f88510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131f887d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131f88a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131f88d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131f89010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131f892d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131f89590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131f89850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131f89b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131f89dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131f8a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131f8a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131f8a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131f8a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131f8ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131f8b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131f8b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131f8ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131f8beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131f8c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131f8c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131f8cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131f8d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131f8d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131f8d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131f8dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131f8e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131f8e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131f8eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131f8ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131f8f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131f8f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131f8fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131f90120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131f90590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131f90a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131f90e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131f912e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131f91750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131f91bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131f92030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131f924a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131f92910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131f92d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131f931f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131f93660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131f93ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131f93f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131f943b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131f94820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131f94c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131f95100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131f95570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131f959e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131f95e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131f962c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131f96730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131f96ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131f97010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131f97480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131f978f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131f97d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131f981d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131f98640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131f98ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131f98f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131f99390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131f99800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131f99c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131f9a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131f9a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131f9a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131f9ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131f9b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131f9b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131f9bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131f9bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131f9c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131f9c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131f9cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131f9d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131f9d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131f9da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131f9df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131f9e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131f9e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131f9ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131f9f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131f9f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131f9f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131f9fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131fa0280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131fa0cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131fa1410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131fa1b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131fa2250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131fa2510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131fa2d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131fa2fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131fa35d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136e046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136e04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136e04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136e05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136e058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136e05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136e06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136e065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136e06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136e06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136e07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136e079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136e08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136e08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136e094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136e09be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136e0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136e0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136e0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136e0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136e0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136e0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136e0ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136e0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136e0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136e0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136e0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136e0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136e0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136e0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136e0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136e0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136e0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136e10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136e104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136e10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136e10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136e11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136e11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136e11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136e11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136e123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136e12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136e12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136e13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136e13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136e13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136e13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136e142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136e14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136e14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136e15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136e154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136e15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136e15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136e161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136e16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136e16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136e170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136e17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136e179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136e17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136e18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136e18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136e18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136e18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136e19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136e198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136e19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136e1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136e1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136e1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136e1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136e1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136e1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136e1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136e1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136e1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136e1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136e1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136e1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136e1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136e1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136e1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136e1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136e1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136e1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136e1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136e1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136e1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136e1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136e20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136e207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136e20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136e21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136e21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136e21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136e21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136e22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136e226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136e22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136e22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136e23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136e23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136e23f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136e243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136e24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136e24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136e25120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136e25590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136e25a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136e25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136e262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136e26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136e26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136e27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136e274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136e27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136e27d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136e281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136e28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136e28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136e28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136e293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136e29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136e29c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136e2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136e2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136e2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136e2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136e2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136e2b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136e2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136e2c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136e2c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136e2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136e2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136e2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136e2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136e2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136e2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136e2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136e2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136e2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136e2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136e2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136e2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136e2fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136e302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136e30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136e30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136e30ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136e31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136e318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136e31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136e321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136e32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136e32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136e32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136e33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136e337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136e33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136e340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136e34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136e349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136e34e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136e35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136e356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136e35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136e35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136e36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136e368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136e36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136e37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136e37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136e37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136e37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136e38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136e387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136e38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136e390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136e39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136e39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136e39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136e3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136e3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136e3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136e3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136e3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136e3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136e3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136e3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136e3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136e3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136e3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136e3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136e3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136e3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136e3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136e3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136e3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136e3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136e3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136e3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136e3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136e3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136e40400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136e40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136e40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136e41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136e41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136e41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136e42250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136e426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136e42b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136e42fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136e43410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136e43880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136e43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136e44160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136e445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136e44a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136e44eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136e45320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136e45790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136e45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136e46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136e464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136e46950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136e46dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136e47230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136e476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136e47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136e47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136e483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136e48860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136e48cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136e49140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136e495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136e49a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136e49e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136e4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136e4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136e4abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136e4b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136e4b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136e4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136e4bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136e4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136e4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136e4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136e4cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136e4d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136e4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136e4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136e4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136e4e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136e4ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136e4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136e4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136e4f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136e4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136e50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136e504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136e50910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136e50d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136e511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136e51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136e51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136e51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136e523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136e52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136e52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136e53100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136e53570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136e539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136e53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136e542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136e54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136e54ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136e55010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136e55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136e558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136e56360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136e56a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136e571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136e578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136e57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136e57ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136e585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136e58c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.961s
user	0m0.234s
sys	0m0.188s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
