### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.13 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.48 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.29 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.23 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.68 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.26 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    2.87 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.00 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  190.57 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.90 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.45 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.32 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 234.65 sec*proc (28 tests)

Total Test time (real) = 234.66 sec

real	3m54.774s
user	8m3.418s
sys	0m7.337s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.20 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.16 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.96 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.17 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.43 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.41 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   30.90 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.36 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.12 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  53.09 sec*proc (28 tests)

Total Test time (real) =  53.10 sec

real	0m53.115s
user	1m15.704s
sys	0m6.378s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.068 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.062 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.999 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.008 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.008 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.009 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.010 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.010 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.012 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.012 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.013 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.013 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.014 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.017 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.017 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.018 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.019 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.019 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.020 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.020 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.000 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.002 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.002 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.003 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.003 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.004 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.025.004 I llama_model_loader: - type  f32:  124 tensors
0.00.025.005 I llama_model_loader: - type  f16:   73 tensors
0.00.025.005 I print_info: file format = GGUF V3 (latest)
0.00.025.006 I print_info: file type   = F16
0.00.025.007 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.029.194 I load: special tokens cache size = 5
0.00.031.274 I load: token to piece cache size = 0.2032 MB
0.00.031.277 I print_info: arch             = bert
0.00.031.278 I print_info: vocab_only       = 0
0.00.031.278 I print_info: n_ctx_train      = 512
0.00.031.278 I print_info: n_embd           = 384
0.00.031.279 I print_info: n_layer          = 12
0.00.031.282 I print_info: n_head           = 12
0.00.031.283 I print_info: n_head_kv        = 12
0.00.031.283 I print_info: n_rot            = 32
0.00.031.283 I print_info: n_swa            = 0
0.00.031.284 I print_info: n_embd_head_k    = 32
0.00.031.284 I print_info: n_embd_head_v    = 32
0.00.031.285 I print_info: n_gqa            = 1
0.00.031.286 I print_info: n_embd_k_gqa     = 384
0.00.031.286 I print_info: n_embd_v_gqa     = 384
0.00.031.287 I print_info: f_norm_eps       = 1.0e-12
0.00.031.288 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.031.288 I print_info: f_clamp_kqv      = 0.0e+00
0.00.031.288 I print_info: f_max_alibi_bias = 0.0e+00
0.00.031.288 I print_info: f_logit_scale    = 0.0e+00
0.00.031.289 I print_info: n_ff             = 1536
0.00.031.290 I print_info: n_expert         = 0
0.00.031.292 I print_info: n_expert_used    = 0
0.00.031.292 I print_info: causal attn      = 0
0.00.031.293 I print_info: pooling type     = 2
0.00.031.293 I print_info: rope type        = 2
0.00.031.293 I print_info: rope scaling     = linear
0.00.031.294 I print_info: freq_base_train  = 10000.0
0.00.031.294 I print_info: freq_scale_train = 1
0.00.031.294 I print_info: n_ctx_orig_yarn  = 512
0.00.031.295 I print_info: rope_finetuned   = unknown
0.00.031.295 I print_info: ssm_d_conv       = 0
0.00.031.295 I print_info: ssm_d_inner      = 0
0.00.031.295 I print_info: ssm_d_state      = 0
0.00.031.295 I print_info: ssm_dt_rank      = 0
0.00.031.295 I print_info: ssm_dt_b_c_rms   = 0
0.00.031.296 I print_info: model type       = 33M
0.00.031.296 I print_info: model params     = 33.21 M
0.00.031.297 I print_info: general.name     = Bge Small
0.00.031.297 I print_info: vocab type       = WPM
0.00.031.297 I print_info: n_vocab          = 30522
0.00.031.298 I print_info: n_merges         = 0
0.00.031.298 I print_info: BOS token        = 101 '[CLS]'
0.00.031.298 I print_info: UNK token        = 100 '[UNK]'
0.00.031.298 I print_info: SEP token        = 102 '[SEP]'
0.00.031.299 I print_info: PAD token        = 0 '[PAD]'
0.00.031.300 I print_info: MASK token       = 103 '[MASK]'
0.00.031.300 I print_info: LF token         = 0 '[PAD]'
0.00.031.300 I print_info: max token length = 21
0.00.034.475 I load_tensors: offloading 12 repeating layers to GPU
0.00.034.477 I load_tensors: offloading output layer to GPU
0.00.034.477 I load_tensors: offloaded 13/13 layers to GPU
0.00.034.502 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.504 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.034.815 I llama_init_from_model: n_seq_max     = 1
0.00.034.817 I llama_init_from_model: n_ctx         = 512
0.00.034.817 I llama_init_from_model: n_ctx_per_seq = 512
0.00.034.817 I llama_init_from_model: n_batch       = 2048
0.00.034.817 I llama_init_from_model: n_ubatch      = 2048
0.00.034.818 I llama_init_from_model: flash_attn    = 0
0.00.034.818 I llama_init_from_model: freq_base     = 10000.0
0.00.034.819 I llama_init_from_model: freq_scale    = 1
0.00.034.819 I ggml_metal_init: allocating
0.00.034.834 I ggml_metal_init: found device: Apple M4
0.00.034.839 I ggml_metal_init: picking default device: Apple M4
0.00.035.514 I ggml_metal_init: using embedded metal library
0.00.039.318 I ggml_metal_init: GPU name:   Apple M4
0.00.039.320 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.321 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.322 I ggml_metal_init: simdgroup reduction   = true
0.00.039.322 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.322 I ggml_metal_init: has residency sets    = true
0.00.039.322 I ggml_metal_init: has bfloat            = true
0.00.039.322 I ggml_metal_init: use bfloat            = true
0.00.039.323 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.050.697 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.051.386 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.388 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.391 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.052.663 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.052.665 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.052.665 I llama_init_from_model: graph nodes  = 429
0.00.052.665 I llama_init_from_model: graph splits = 2
0.00.052.667 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.052.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.058.125 I 
0.00.058.150 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.058.788 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.063.897 I llama_perf_context_print:        load time =      43.06 ms
0.00.063.900 I llama_perf_context_print: prompt eval time =       4.96 ms /     9 tokens (    0.55 ms per token,  1816.35 tokens per second)
0.00.063.900 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.063.901 I llama_perf_context_print:       total time =       5.77 ms /    10 tokens
0.00.064.040 I ggml_metal_free: deallocating

real	0m0.239s
user	0m0.045s
sys	0m0.026s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.034 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.606 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.212 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.216 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.217 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.218 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.218 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.218 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.219 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.219 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.220 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.220 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.220 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.221 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.223 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.223 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.224 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.224 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.224 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.225 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.610 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.288 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.290 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.290 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.290 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.291 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.291 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.291 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.292 I llama_model_loader: - type  f32:  124 tensors
0.00.015.292 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.293 I print_info: file format = GGUF V3 (latest)
0.00.015.293 I print_info: file type   = Q8_0
0.00.015.294 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.693 I load: special tokens cache size = 5
0.00.018.964 I load: token to piece cache size = 0.2032 MB
0.00.018.968 I print_info: arch             = bert
0.00.018.968 I print_info: vocab_only       = 0
0.00.018.968 I print_info: n_ctx_train      = 512
0.00.018.968 I print_info: n_embd           = 384
0.00.018.969 I print_info: n_layer          = 12
0.00.018.972 I print_info: n_head           = 12
0.00.018.973 I print_info: n_head_kv        = 12
0.00.018.973 I print_info: n_rot            = 32
0.00.018.974 I print_info: n_swa            = 0
0.00.018.974 I print_info: n_embd_head_k    = 32
0.00.018.974 I print_info: n_embd_head_v    = 32
0.00.018.975 I print_info: n_gqa            = 1
0.00.018.975 I print_info: n_embd_k_gqa     = 384
0.00.018.976 I print_info: n_embd_v_gqa     = 384
0.00.018.977 I print_info: f_norm_eps       = 1.0e-12
0.00.018.978 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.978 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.979 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.979 I print_info: f_logit_scale    = 0.0e+00
0.00.018.980 I print_info: n_ff             = 1536
0.00.018.980 I print_info: n_expert         = 0
0.00.018.980 I print_info: n_expert_used    = 0
0.00.018.980 I print_info: causal attn      = 0
0.00.018.980 I print_info: pooling type     = 2
0.00.018.981 I print_info: rope type        = 2
0.00.018.981 I print_info: rope scaling     = linear
0.00.018.981 I print_info: freq_base_train  = 10000.0
0.00.018.982 I print_info: freq_scale_train = 1
0.00.018.982 I print_info: n_ctx_orig_yarn  = 512
0.00.018.982 I print_info: rope_finetuned   = unknown
0.00.018.982 I print_info: ssm_d_conv       = 0
0.00.018.983 I print_info: ssm_d_inner      = 0
0.00.018.983 I print_info: ssm_d_state      = 0
0.00.018.983 I print_info: ssm_dt_rank      = 0
0.00.018.983 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.983 I print_info: model type       = 33M
0.00.018.984 I print_info: model params     = 33.21 M
0.00.018.984 I print_info: general.name     = Bge Small
0.00.018.984 I print_info: vocab type       = WPM
0.00.018.985 I print_info: n_vocab          = 30522
0.00.018.985 I print_info: n_merges         = 0
0.00.018.985 I print_info: BOS token        = 101 '[CLS]'
0.00.018.985 I print_info: UNK token        = 100 '[UNK]'
0.00.018.986 I print_info: SEP token        = 102 '[SEP]'
0.00.018.986 I print_info: PAD token        = 0 '[PAD]'
0.00.018.986 I print_info: MASK token       = 103 '[MASK]'
0.00.018.986 I print_info: LF token         = 0 '[PAD]'
0.00.018.987 I print_info: max token length = 21
0.00.020.840 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.841 I load_tensors: offloading output layer to GPU
0.00.020.841 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.849 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.849 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.047 I llama_init_from_model: n_seq_max     = 1
0.00.021.048 I llama_init_from_model: n_ctx         = 512
0.00.021.048 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.048 I llama_init_from_model: n_batch       = 2048
0.00.021.049 I llama_init_from_model: n_ubatch      = 2048
0.00.021.049 I llama_init_from_model: flash_attn    = 0
0.00.021.049 I llama_init_from_model: freq_base     = 10000.0
0.00.021.049 I llama_init_from_model: freq_scale    = 1
0.00.021.052 I ggml_metal_init: allocating
0.00.021.071 I ggml_metal_init: found device: Apple M4
0.00.021.076 I ggml_metal_init: picking default device: Apple M4
0.00.021.609 I ggml_metal_init: using embedded metal library
0.00.024.034 I ggml_metal_init: GPU name:   Apple M4
0.00.024.036 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.037 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.038 I ggml_metal_init: simdgroup reduction   = true
0.00.024.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.038 I ggml_metal_init: has residency sets    = true
0.00.024.038 I ggml_metal_init: has bfloat            = true
0.00.024.038 I ggml_metal_init: use bfloat            = true
0.00.024.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.041 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.463 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.092 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.095 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.099 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.238 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.239 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.239 I llama_init_from_model: graph nodes  = 429
0.00.036.239 I llama_init_from_model: graph splits = 2
0.00.036.241 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.241 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.329 I 
0.00.040.355 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.914 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.395 I llama_perf_context_print:        load time =      30.72 ms
0.00.045.396 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2072.30 tokens per second)
0.00.045.397 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.398 I llama_perf_context_print:       total time =       5.07 ms /    10 tokens
0.00.045.615 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.030s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.192 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.711 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.150 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.156 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.158 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.167 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.168 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.168 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.170 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.170 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.171 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.171 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.172 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.176 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.176 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.177 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.427 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.312 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.314 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.315 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.315 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.315 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.316 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.316 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.316 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.317 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.317 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.318 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.318 I llama_model_loader: - type  f32:   40 tensors
0.00.048.318 I llama_model_loader: - type  f16:   30 tensors
0.00.048.324 I print_info: file format = GGUF V3 (latest)
0.00.048.325 I print_info: file type   = F16
0.00.048.329 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.603 W load: empty token at index 5
0.00.057.603 W load: model vocab missing newline token, using special_pad_id instead
0.00.058.999 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.033 I load: special tokens cache size = 5
0.00.327.820 I load: token to piece cache size = 1.5060 MB
0.00.327.833 I print_info: arch             = jina-bert-v2
0.00.327.833 I print_info: vocab_only       = 0
0.00.327.834 I print_info: n_ctx_train      = 8192
0.00.327.834 I print_info: n_embd           = 384
0.00.327.834 I print_info: n_layer          = 4
0.00.327.839 I print_info: n_head           = 12
0.00.327.840 I print_info: n_head_kv        = 12
0.00.327.840 I print_info: n_rot            = 32
0.00.327.840 I print_info: n_swa            = 0
0.00.327.840 I print_info: n_embd_head_k    = 32
0.00.327.840 I print_info: n_embd_head_v    = 32
0.00.327.841 I print_info: n_gqa            = 1
0.00.327.841 I print_info: n_embd_k_gqa     = 384
0.00.327.842 I print_info: n_embd_v_gqa     = 384
0.00.327.842 I print_info: f_norm_eps       = 1.0e-12
0.00.327.843 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.327.843 I print_info: f_clamp_kqv      = 0.0e+00
0.00.327.843 I print_info: f_max_alibi_bias = 8.0e+00
0.00.327.843 I print_info: f_logit_scale    = 0.0e+00
0.00.327.844 I print_info: n_ff             = 1536
0.00.327.844 I print_info: n_expert         = 0
0.00.327.844 I print_info: n_expert_used    = 0
0.00.327.844 I print_info: causal attn      = 0
0.00.327.845 I print_info: pooling type     = -1
0.00.327.845 I print_info: rope type        = -1
0.00.327.845 I print_info: rope scaling     = linear
0.00.327.845 I print_info: freq_base_train  = 10000.0
0.00.327.845 I print_info: freq_scale_train = 1
0.00.327.846 I print_info: n_ctx_orig_yarn  = 8192
0.00.327.846 I print_info: rope_finetuned   = unknown
0.00.327.846 I print_info: ssm_d_conv       = 0
0.00.327.846 I print_info: ssm_d_inner      = 0
0.00.327.846 I print_info: ssm_d_state      = 0
0.00.327.846 I print_info: ssm_dt_rank      = 0
0.00.327.846 I print_info: ssm_dt_b_c_rms   = 0
0.00.327.847 I print_info: model type       = 33M
0.00.327.847 I print_info: model params     = 32.90 M
0.00.327.847 I print_info: general.name     = Jina Bert Implementation
0.00.327.848 I print_info: vocab type       = BPE
0.00.327.848 I print_info: n_vocab          = 61056
0.00.327.848 I print_info: n_merges         = 39382
0.00.327.848 I print_info: BOS token        = 0 '<s>'
0.00.327.849 I print_info: EOS token        = 2 '</s>'
0.00.327.850 I print_info: UNK token        = 3 '<unk>'
0.00.327.850 I print_info: SEP token        = 2 '</s>'
0.00.327.852 I print_info: PAD token        = 1 '<pad>'
0.00.327.852 I print_info: MASK token       = 4 '<mask>'
0.00.327.853 I print_info: EOG token        = 2 '</s>'
0.00.327.853 I print_info: max token length = 45
0.00.329.217 I load_tensors: offloading 4 repeating layers to GPU
0.00.329.218 I load_tensors: offloading output layer to GPU
0.00.329.218 I load_tensors: offloaded 5/5 layers to GPU
0.00.329.234 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.329.236 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.329.388 I llama_init_from_model: n_seq_max     = 1
0.00.329.389 I llama_init_from_model: n_ctx         = 8192
0.00.329.389 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.329.389 I llama_init_from_model: n_batch       = 2048
0.00.329.389 I llama_init_from_model: n_ubatch      = 2048
0.00.329.390 I llama_init_from_model: flash_attn    = 0
0.00.329.390 I llama_init_from_model: freq_base     = 10000.0
0.00.329.390 I llama_init_from_model: freq_scale    = 1
0.00.329.391 I ggml_metal_init: allocating
0.00.329.395 I ggml_metal_init: found device: Apple M4
0.00.329.399 I ggml_metal_init: picking default device: Apple M4
0.00.329.925 I ggml_metal_init: using embedded metal library
0.00.332.464 I ggml_metal_init: GPU name:   Apple M4
0.00.332.466 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.332.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.332.467 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.332.467 I ggml_metal_init: simdgroup reduction   = true
0.00.332.467 I ggml_metal_init: simdgroup matrix mul. = true
0.00.332.468 I ggml_metal_init: has residency sets    = true
0.00.332.468 I ggml_metal_init: has bfloat            = true
0.00.332.468 I ggml_metal_init: use bfloat            = true
0.00.332.468 I ggml_metal_init: hasUnifiedMemory      = true
0.00.332.469 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.342.774 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.345.810 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.345.813 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.345.818 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.352.772 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.352.775 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.352.775 I llama_init_from_model: graph nodes  = 154
0.00.352.775 I llama_init_from_model: graph splits = 2
0.00.352.777 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.352.777 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.359.870 I 
0.00.359.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.129 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.360.130 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.360.134 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.360.136 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.360.146 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.360.146 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.360.643 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.147 I llama_perf_context_print:        load time =     338.15 ms
0.00.364.148 I llama_perf_context_print: prompt eval time =       3.48 ms /    62 tokens (    0.06 ms per token, 17810.97 tokens per second)
0.00.364.149 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.149 I llama_perf_context_print:       total time =       4.28 ms /    63 tokens
0.00.364.407 I ggml_metal_free: deallocating

real	0m1.101s
user	0m0.336s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.161 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.378 I main: llama backend init
0.00.000.427 I main: load the model and apply lora adapter, if any
0.00.032.731 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.046.183 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.046.196 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.046.200 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.046.201 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.046.202 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.046.203 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.046.203 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.046.206 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.046.207 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.046.207 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.046.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.046.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.046.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.046.211 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.046.214 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.046.215 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.046.215 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.057.054 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.064.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.064.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.064.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.064.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.064.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.064.337 I llama_model_loader: - type  f32:  194 tensors
0.00.064.337 I llama_model_loader: - type  f16:   98 tensors
0.00.064.338 I print_info: file format = GGUF V3 (latest)
0.00.064.339 I print_info: file type   = all F32 (guessed)
0.00.064.343 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.077.164 I load: special tokens cache size = 25
0.00.085.326 I load: token to piece cache size = 0.2984 MB
0.00.085.329 I print_info: arch             = gptneox
0.00.085.329 I print_info: vocab_only       = 0
0.00.085.330 I print_info: n_ctx_train      = 2048
0.00.085.330 I print_info: n_embd           = 2048
0.00.085.330 I print_info: n_layer          = 24
0.00.085.333 I print_info: n_head           = 16
0.00.085.334 I print_info: n_head_kv        = 16
0.00.085.334 I print_info: n_rot            = 32
0.00.085.334 I print_info: n_swa            = 0
0.00.085.335 I print_info: n_embd_head_k    = 128
0.00.085.335 I print_info: n_embd_head_v    = 128
0.00.085.335 I print_info: n_gqa            = 1
0.00.085.336 I print_info: n_embd_k_gqa     = 2048
0.00.085.337 I print_info: n_embd_v_gqa     = 2048
0.00.085.338 I print_info: f_norm_eps       = 1.0e-05
0.00.085.338 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.338 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.338 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.340 I print_info: f_logit_scale    = 0.0e+00
0.00.085.341 I print_info: n_ff             = 8192
0.00.085.341 I print_info: n_expert         = 0
0.00.085.341 I print_info: n_expert_used    = 0
0.00.085.342 I print_info: causal attn      = 1
0.00.085.342 I print_info: pooling type     = 0
0.00.085.342 I print_info: rope type        = 2
0.00.085.342 I print_info: rope scaling     = linear
0.00.085.343 I print_info: freq_base_train  = 10000.0
0.00.085.343 I print_info: freq_scale_train = 1
0.00.085.343 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.344 I print_info: rope_finetuned   = unknown
0.00.085.345 I print_info: ssm_d_conv       = 0
0.00.085.345 I print_info: ssm_d_inner      = 0
0.00.085.345 I print_info: ssm_d_state      = 0
0.00.085.345 I print_info: ssm_dt_rank      = 0
0.00.085.345 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.346 I print_info: model type       = 1.4B
0.00.085.346 I print_info: model params     = 1.41 B
0.00.085.347 I print_info: general.name     = 1.4B
0.00.085.348 I print_info: vocab type       = BPE
0.00.085.348 I print_info: n_vocab          = 50304
0.00.085.349 I print_info: n_merges         = 50009
0.00.085.349 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.349 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.349 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.349 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.350 I print_info: LF token         = 128 'Ä'
0.00.085.350 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.350 I print_info: max token length = 1024
0.00.122.976 I load_tensors: offloading 24 repeating layers to GPU
0.00.122.980 I load_tensors: offloading output layer to GPU
0.00.122.980 I load_tensors: offloaded 25/25 layers to GPU
0.00.123.002 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.123.004 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.123.272 I llama_init_from_model: n_seq_max     = 1
0.00.123.273 I llama_init_from_model: n_ctx         = 2048
0.00.123.273 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.123.273 I llama_init_from_model: n_batch       = 2048
0.00.123.274 I llama_init_from_model: n_ubatch      = 512
0.00.123.274 I llama_init_from_model: flash_attn    = 0
0.00.123.274 I llama_init_from_model: freq_base     = 10000.0
0.00.123.274 I llama_init_from_model: freq_scale    = 1
0.00.123.275 I ggml_metal_init: allocating
0.00.123.289 I ggml_metal_init: found device: Apple M4
0.00.123.294 I ggml_metal_init: picking default device: Apple M4
0.00.123.863 I ggml_metal_init: using embedded metal library
0.00.142.652 I ggml_metal_init: GPU name:   Apple M4
0.00.142.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.142.654 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.142.655 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.142.655 I ggml_metal_init: simdgroup reduction   = true
0.00.142.655 I ggml_metal_init: simdgroup matrix mul. = true
0.00.142.655 I ggml_metal_init: has residency sets    = true
0.00.142.655 I ggml_metal_init: has bfloat            = true
0.00.142.655 I ggml_metal_init: use bfloat            = true
0.00.142.656 I ggml_metal_init: hasUnifiedMemory      = true
0.00.142.657 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.188.973 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.217.290 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.217.298 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.217.321 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.221.552 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.221.555 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.221.555 I llama_init_from_model: graph nodes  = 967
0.00.221.555 I llama_init_from_model: graph splits = 2
0.00.221.559 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.221.692 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.221.692 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.289.265 I main: llama threadpool init, n_threads = 4
0.00.289.301 I 
0.00.289.337 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.289.338 I 
0.00.289.380 I sampler seed: 1234
0.00.289.384 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.289.409 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.289.409 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.289.409 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.122.598 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.02.122.599 I llama_perf_context_print:        load time =     255.48 ms
0.02.122.600 I llama_perf_context_print: prompt eval time =      44.01 ms /     7 tokens (    6.29 ms per token,   159.07 tokens per second)
0.02.122.600 I llama_perf_context_print:        eval time =    1786.33 ms /    63 runs   (   28.35 ms per token,    35.27 tokens per second)
0.02.122.601 I llama_perf_context_print:       total time =    1834.38 ms /    70 tokens
0.02.122.874 I ggml_metal_free: deallocating

real	0m2.430s
user	0m0.133s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.484 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.050 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.197 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.202 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.204 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.206 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.206 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.206 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.207 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.209 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.210 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.211 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.213 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.213 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.213 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.775 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.334 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.336 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.337 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.337 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.338 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.338 I llama_model_loader: - type  f32:  194 tensors
0.00.055.339 I llama_model_loader: - type  f16:   98 tensors
0.00.055.340 I print_info: file format = GGUF V3 (latest)
0.00.055.348 I print_info: file type   = all F32 (guessed)
0.00.055.349 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.763 I load: special tokens cache size = 25
0.00.077.192 I load: token to piece cache size = 0.2984 MB
0.00.077.195 I print_info: arch             = gptneox
0.00.077.196 I print_info: vocab_only       = 0
0.00.077.196 I print_info: n_ctx_train      = 2048
0.00.077.196 I print_info: n_embd           = 2048
0.00.077.196 I print_info: n_layer          = 24
0.00.077.199 I print_info: n_head           = 16
0.00.077.200 I print_info: n_head_kv        = 16
0.00.077.201 I print_info: n_rot            = 32
0.00.077.201 I print_info: n_swa            = 0
0.00.077.201 I print_info: n_embd_head_k    = 128
0.00.077.201 I print_info: n_embd_head_v    = 128
0.00.077.202 I print_info: n_gqa            = 1
0.00.077.203 I print_info: n_embd_k_gqa     = 2048
0.00.077.204 I print_info: n_embd_v_gqa     = 2048
0.00.077.204 I print_info: f_norm_eps       = 1.0e-05
0.00.077.205 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.205 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.205 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.205 I print_info: f_logit_scale    = 0.0e+00
0.00.077.206 I print_info: n_ff             = 8192
0.00.077.206 I print_info: n_expert         = 0
0.00.077.206 I print_info: n_expert_used    = 0
0.00.077.206 I print_info: causal attn      = 1
0.00.077.207 I print_info: pooling type     = 0
0.00.077.207 I print_info: rope type        = 2
0.00.077.207 I print_info: rope scaling     = linear
0.00.077.213 I print_info: freq_base_train  = 10000.0
0.00.077.213 I print_info: freq_scale_train = 1
0.00.077.213 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.215 I print_info: rope_finetuned   = unknown
0.00.077.215 I print_info: ssm_d_conv       = 0
0.00.077.215 I print_info: ssm_d_inner      = 0
0.00.077.215 I print_info: ssm_d_state      = 0
0.00.077.216 I print_info: ssm_dt_rank      = 0
0.00.077.216 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.216 I print_info: model type       = 1.4B
0.00.077.216 I print_info: model params     = 1.41 B
0.00.077.216 I print_info: general.name     = 1.4B
0.00.077.217 I print_info: vocab type       = BPE
0.00.077.217 I print_info: n_vocab          = 50304
0.00.077.217 I print_info: n_merges         = 50009
0.00.077.218 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.218 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.218 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.218 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.218 I print_info: LF token         = 128 'Ä'
0.00.077.219 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.219 I print_info: max token length = 1024
0.00.934.269 I load_tensors: offloading 24 repeating layers to GPU
0.00.934.275 I load_tensors: offloading output layer to GPU
0.00.934.275 I load_tensors: offloaded 25/25 layers to GPU
0.00.934.305 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.934.307 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.934.869 I llama_init_from_model: n_seq_max     = 1
0.00.934.870 I llama_init_from_model: n_ctx         = 128
0.00.934.870 I llama_init_from_model: n_ctx_per_seq = 128
0.00.934.870 I llama_init_from_model: n_batch       = 128
0.00.934.871 I llama_init_from_model: n_ubatch      = 128
0.00.934.871 I llama_init_from_model: flash_attn    = 0
0.00.934.871 I llama_init_from_model: freq_base     = 10000.0
0.00.934.872 I llama_init_from_model: freq_scale    = 1
0.00.934.872 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.934.873 I ggml_metal_init: allocating
0.00.934.934 I ggml_metal_init: found device: Apple M4
0.00.934.942 I ggml_metal_init: picking default device: Apple M4
0.00.936.042 I ggml_metal_init: using embedded metal library
0.00.939.947 I ggml_metal_init: GPU name:   Apple M4
0.00.939.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.939.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.939.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.939.951 I ggml_metal_init: simdgroup reduction   = true
0.00.939.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.939.951 I ggml_metal_init: has residency sets    = true
0.00.939.951 I ggml_metal_init: has bfloat            = true
0.00.939.952 I ggml_metal_init: use bfloat            = true
0.00.939.952 I ggml_metal_init: hasUnifiedMemory      = true
0.00.939.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.951.867 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.953.671 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.953.674 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.953.688 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.955.418 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.955.419 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.955.420 I llama_init_from_model: graph nodes  = 967
0.00.955.420 I llama_init_from_model: graph splits = 2
0.00.955.421 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.955.421 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.990.792 I 
0.00.990.835 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.990.857 I perplexity: tokenizing the input ..
0.00.996.573 I perplexity: tokenization took 5.713 ms
0.00.996.596 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.115.406 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.116.765 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.116.779 I llama_perf_context_print:        load time =     966.73 ms
0.01.116.780 I llama_perf_context_print: prompt eval time =     118.51 ms /   128 tokens (    0.93 ms per token,  1080.11 tokens per second)
0.01.116.780 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.116.781 I llama_perf_context_print:       total time =     125.99 ms /   129 tokens
0.01.117.147 I ggml_metal_free: deallocating

real	0m1.302s
user	0m0.100s
sys	0m0.206s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.008 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.842 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.849 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.853 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.854 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.854 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.854 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.856 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.856 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.857 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.857 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.858 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.860 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.860 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.861 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.763 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.807 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.757 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.759 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.759 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.759 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.760 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.760 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.761 I llama_model_loader: - type  f32:  194 tensors
0.00.036.761 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.762 I print_info: file format = GGUF V3 (latest)
0.00.036.762 I print_info: file type   = Q8_0
0.00.036.767 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.785 I load: special tokens cache size = 25
0.00.052.852 I load: token to piece cache size = 0.2984 MB
0.00.052.856 I print_info: arch             = gptneox
0.00.052.856 I print_info: vocab_only       = 0
0.00.052.857 I print_info: n_ctx_train      = 2048
0.00.052.859 I print_info: n_embd           = 2048
0.00.052.859 I print_info: n_layer          = 24
0.00.052.865 I print_info: n_head           = 16
0.00.052.866 I print_info: n_head_kv        = 16
0.00.052.866 I print_info: n_rot            = 32
0.00.052.866 I print_info: n_swa            = 0
0.00.052.866 I print_info: n_embd_head_k    = 128
0.00.052.866 I print_info: n_embd_head_v    = 128
0.00.052.867 I print_info: n_gqa            = 1
0.00.052.868 I print_info: n_embd_k_gqa     = 2048
0.00.052.868 I print_info: n_embd_v_gqa     = 2048
0.00.052.869 I print_info: f_norm_eps       = 1.0e-05
0.00.052.870 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.870 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.870 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.870 I print_info: f_logit_scale    = 0.0e+00
0.00.052.871 I print_info: n_ff             = 8192
0.00.052.871 I print_info: n_expert         = 0
0.00.052.872 I print_info: n_expert_used    = 0
0.00.052.872 I print_info: causal attn      = 1
0.00.052.872 I print_info: pooling type     = 0
0.00.052.872 I print_info: rope type        = 2
0.00.052.872 I print_info: rope scaling     = linear
0.00.052.875 I print_info: freq_base_train  = 10000.0
0.00.052.875 I print_info: freq_scale_train = 1
0.00.052.875 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.875 I print_info: rope_finetuned   = unknown
0.00.052.875 I print_info: ssm_d_conv       = 0
0.00.052.876 I print_info: ssm_d_inner      = 0
0.00.052.876 I print_info: ssm_d_state      = 0
0.00.052.876 I print_info: ssm_dt_rank      = 0
0.00.052.876 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.876 I print_info: model type       = 1.4B
0.00.052.876 I print_info: model params     = 1.41 B
0.00.052.877 I print_info: general.name     = 1.4B
0.00.052.877 I print_info: vocab type       = BPE
0.00.052.877 I print_info: n_vocab          = 50304
0.00.052.878 I print_info: n_merges         = 50009
0.00.052.878 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.878 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.882 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.883 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.884 I print_info: LF token         = 128 'Ä'
0.00.052.885 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.885 I print_info: max token length = 1024
0.01.034.302 I load_tensors: offloading 24 repeating layers to GPU
0.01.034.307 I load_tensors: offloading output layer to GPU
0.01.034.309 I load_tensors: offloaded 25/25 layers to GPU
0.01.034.332 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.034.334 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.035.148 I llama_init_from_model: n_seq_max     = 1
0.01.035.150 I llama_init_from_model: n_ctx         = 2048
0.01.035.150 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.035.151 I llama_init_from_model: n_batch       = 2048
0.01.035.151 I llama_init_from_model: n_ubatch      = 512
0.01.035.151 I llama_init_from_model: flash_attn    = 0
0.01.035.152 I llama_init_from_model: freq_base     = 10000.0
0.01.035.152 I llama_init_from_model: freq_scale    = 1
0.01.035.154 I ggml_metal_init: allocating
0.01.035.170 I ggml_metal_init: found device: Apple M4
0.01.035.178 I ggml_metal_init: picking default device: Apple M4
0.01.036.296 I ggml_metal_init: using embedded metal library
0.01.041.468 I ggml_metal_init: GPU name:   Apple M4
0.01.041.472 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.041.473 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.041.474 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.041.474 I ggml_metal_init: simdgroup reduction   = true
0.01.041.475 I ggml_metal_init: simdgroup matrix mul. = true
0.01.041.475 I ggml_metal_init: has residency sets    = true
0.01.041.475 I ggml_metal_init: has bfloat            = true
0.01.041.475 I ggml_metal_init: use bfloat            = true
0.01.041.476 I ggml_metal_init: hasUnifiedMemory      = true
0.01.041.477 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.057.445 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.115.300 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.115.305 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.115.336 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.120.118 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.120.120 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.120.121 I llama_init_from_model: graph nodes  = 967
0.01.120.121 I llama_init_from_model: graph splits = 2
0.01.120.127 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.120.256 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.120.257 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.176.953 I main: llama threadpool init, n_threads = 4
0.01.176.993 I 
0.01.177.036 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.177.040 I 
0.01.177.307 I sampler seed: 1234
0.01.177.316 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.177.331 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.177.333 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.177.333 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.266.201 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55642.63 tokens per second)
0.02.266.202 I llama_perf_context_print:        load time =    1166.02 ms
0.02.266.203 I llama_perf_context_print: prompt eval time =      50.07 ms /     7 tokens (    7.15 ms per token,   139.80 tokens per second)
0.02.266.204 I llama_perf_context_print:        eval time =    1035.89 ms /    63 runs   (   16.44 ms per token,    60.82 tokens per second)
0.02.266.206 I llama_perf_context_print:       total time =    1090.18 ms /    70 tokens
0.02.266.438 I ggml_metal_free: deallocating

real	0m2.286s
user	0m0.110s
sys	0m0.245s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.021 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.337 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.343 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.345 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.351 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.352 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.352 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.352 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.353 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.354 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.354 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.354 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.354 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.356 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.357 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.359 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.359 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.359 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.288 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.142 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.143 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.144 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.144 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.144 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.145 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.145 I llama_model_loader: - type  f32:  194 tensors
0.00.025.145 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.146 I print_info: file format = GGUF V3 (latest)
0.00.025.146 I print_info: file type   = Q8_0
0.00.025.151 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.290 I load: special tokens cache size = 25
0.00.040.200 I load: token to piece cache size = 0.2984 MB
0.00.040.204 I print_info: arch             = gptneox
0.00.040.204 I print_info: vocab_only       = 0
0.00.040.204 I print_info: n_ctx_train      = 2048
0.00.040.205 I print_info: n_embd           = 2048
0.00.040.205 I print_info: n_layer          = 24
0.00.040.210 I print_info: n_head           = 16
0.00.040.210 I print_info: n_head_kv        = 16
0.00.040.211 I print_info: n_rot            = 32
0.00.040.211 I print_info: n_swa            = 0
0.00.040.213 I print_info: n_embd_head_k    = 128
0.00.040.213 I print_info: n_embd_head_v    = 128
0.00.040.214 I print_info: n_gqa            = 1
0.00.040.215 I print_info: n_embd_k_gqa     = 2048
0.00.040.216 I print_info: n_embd_v_gqa     = 2048
0.00.040.216 I print_info: f_norm_eps       = 1.0e-05
0.00.040.217 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.217 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.217 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.217 I print_info: f_logit_scale    = 0.0e+00
0.00.040.218 I print_info: n_ff             = 8192
0.00.040.218 I print_info: n_expert         = 0
0.00.040.218 I print_info: n_expert_used    = 0
0.00.040.219 I print_info: causal attn      = 1
0.00.040.219 I print_info: pooling type     = 0
0.00.040.219 I print_info: rope type        = 2
0.00.040.219 I print_info: rope scaling     = linear
0.00.040.220 I print_info: freq_base_train  = 10000.0
0.00.040.220 I print_info: freq_scale_train = 1
0.00.040.220 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.220 I print_info: rope_finetuned   = unknown
0.00.040.222 I print_info: ssm_d_conv       = 0
0.00.040.222 I print_info: ssm_d_inner      = 0
0.00.040.222 I print_info: ssm_d_state      = 0
0.00.040.222 I print_info: ssm_dt_rank      = 0
0.00.040.223 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.223 I print_info: model type       = 1.4B
0.00.040.223 I print_info: model params     = 1.41 B
0.00.040.223 I print_info: general.name     = 1.4B
0.00.040.224 I print_info: vocab type       = BPE
0.00.040.224 I print_info: n_vocab          = 50304
0.00.040.224 I print_info: n_merges         = 50009
0.00.040.225 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.225 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.225 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.225 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.226 I print_info: LF token         = 128 'Ä'
0.00.040.226 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.226 I print_info: max token length = 1024
0.00.818.269 I load_tensors: offloading 24 repeating layers to GPU
0.00.818.274 I load_tensors: offloading output layer to GPU
0.00.818.276 I load_tensors: offloaded 25/25 layers to GPU
0.00.818.300 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.818.302 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.819.584 I llama_init_from_model: n_seq_max     = 1
0.00.819.586 I llama_init_from_model: n_ctx         = 128
0.00.819.587 I llama_init_from_model: n_ctx_per_seq = 128
0.00.819.587 I llama_init_from_model: n_batch       = 128
0.00.819.587 I llama_init_from_model: n_ubatch      = 128
0.00.819.588 I llama_init_from_model: flash_attn    = 0
0.00.819.589 I llama_init_from_model: freq_base     = 10000.0
0.00.819.589 I llama_init_from_model: freq_scale    = 1
0.00.819.590 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.819.591 I ggml_metal_init: allocating
0.00.819.609 I ggml_metal_init: found device: Apple M4
0.00.819.616 I ggml_metal_init: picking default device: Apple M4
0.00.820.850 I ggml_metal_init: using embedded metal library
0.00.826.117 I ggml_metal_init: GPU name:   Apple M4
0.00.826.122 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.826.122 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.826.123 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.826.123 I ggml_metal_init: simdgroup reduction   = true
0.00.826.123 I ggml_metal_init: simdgroup matrix mul. = true
0.00.826.124 I ggml_metal_init: has residency sets    = true
0.00.826.124 I ggml_metal_init: has bfloat            = true
0.00.826.124 I ggml_metal_init: use bfloat            = true
0.00.826.125 I ggml_metal_init: hasUnifiedMemory      = true
0.00.826.129 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.841.665 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.845.075 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.845.078 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.845.106 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.848.208 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.848.210 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.848.210 I llama_init_from_model: graph nodes  = 967
0.00.848.210 I llama_init_from_model: graph splits = 2
0.00.848.213 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.848.213 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.876.782 I 
0.00.876.854 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.876.875 I perplexity: tokenizing the input ..
0.00.884.475 I perplexity: tokenization took 7.597 ms
0.00.884.496 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.023.738 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.025.118 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.025.134 I llama_perf_context_print:        load time =     867.75 ms
0.01.025.135 I llama_perf_context_print: prompt eval time =     138.36 ms /   128 tokens (    1.08 ms per token,   925.10 tokens per second)
0.01.025.135 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.025.136 I llama_perf_context_print:       total time =     148.36 ms /   129 tokens
0.01.025.527 I ggml_metal_free: deallocating

real	0m1.040s
user	0m0.079s
sys	0m0.162s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.891 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.886 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.891 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.899 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.900 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.900 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.900 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.901 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.903 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.904 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.905 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.907 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.907 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.908 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.657 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.448 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.450 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.451 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.451 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.451 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.452 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.452 I llama_model_loader: - type  f32:  194 tensors
0.00.027.453 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.453 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.454 I print_info: file format = GGUF V3 (latest)
0.00.027.454 I print_info: file type   = Q4_0
0.00.027.455 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.555 I load: special tokens cache size = 25
0.00.041.295 I load: token to piece cache size = 0.2984 MB
0.00.041.299 I print_info: arch             = gptneox
0.00.041.299 I print_info: vocab_only       = 0
0.00.041.299 I print_info: n_ctx_train      = 2048
0.00.041.299 I print_info: n_embd           = 2048
0.00.041.300 I print_info: n_layer          = 24
0.00.041.304 I print_info: n_head           = 16
0.00.041.305 I print_info: n_head_kv        = 16
0.00.041.305 I print_info: n_rot            = 32
0.00.041.305 I print_info: n_swa            = 0
0.00.041.305 I print_info: n_embd_head_k    = 128
0.00.041.305 I print_info: n_embd_head_v    = 128
0.00.041.306 I print_info: n_gqa            = 1
0.00.041.307 I print_info: n_embd_k_gqa     = 2048
0.00.041.307 I print_info: n_embd_v_gqa     = 2048
0.00.041.308 I print_info: f_norm_eps       = 1.0e-05
0.00.041.308 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.308 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.309 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.309 I print_info: f_logit_scale    = 0.0e+00
0.00.041.310 I print_info: n_ff             = 8192
0.00.041.311 I print_info: n_expert         = 0
0.00.041.311 I print_info: n_expert_used    = 0
0.00.041.312 I print_info: causal attn      = 1
0.00.041.312 I print_info: pooling type     = 0
0.00.041.312 I print_info: rope type        = 2
0.00.041.312 I print_info: rope scaling     = linear
0.00.041.313 I print_info: freq_base_train  = 10000.0
0.00.041.313 I print_info: freq_scale_train = 1
0.00.041.313 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.313 I print_info: rope_finetuned   = unknown
0.00.041.313 I print_info: ssm_d_conv       = 0
0.00.041.315 I print_info: ssm_d_inner      = 0
0.00.041.315 I print_info: ssm_d_state      = 0
0.00.041.315 I print_info: ssm_dt_rank      = 0
0.00.041.315 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.315 I print_info: model type       = 1.4B
0.00.041.316 I print_info: model params     = 1.41 B
0.00.041.317 I print_info: general.name     = 1.4B
0.00.041.317 I print_info: vocab type       = BPE
0.00.041.318 I print_info: n_vocab          = 50304
0.00.041.318 I print_info: n_merges         = 50009
0.00.041.318 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.318 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.318 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.319 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.319 I print_info: LF token         = 128 'Ä'
0.00.041.319 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.319 I print_info: max token length = 1024
0.00.592.560 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.577 I load_tensors: offloading output layer to GPU
0.00.592.578 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.612 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.592.613 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.594.111 I llama_init_from_model: n_seq_max     = 1
0.00.594.117 I llama_init_from_model: n_ctx         = 2048
0.00.594.117 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.594.118 I llama_init_from_model: n_batch       = 2048
0.00.594.118 I llama_init_from_model: n_ubatch      = 512
0.00.594.119 I llama_init_from_model: flash_attn    = 0
0.00.594.121 I llama_init_from_model: freq_base     = 10000.0
0.00.594.121 I llama_init_from_model: freq_scale    = 1
0.00.594.127 I ggml_metal_init: allocating
0.00.594.207 I ggml_metal_init: found device: Apple M4
0.00.594.220 I ggml_metal_init: picking default device: Apple M4
0.00.596.037 I ggml_metal_init: using embedded metal library
0.00.602.469 I ggml_metal_init: GPU name:   Apple M4
0.00.602.473 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.474 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.476 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.476 I ggml_metal_init: simdgroup reduction   = true
0.00.602.477 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.477 I ggml_metal_init: has residency sets    = true
0.00.602.477 I ggml_metal_init: has bfloat            = true
0.00.602.477 I ggml_metal_init: use bfloat            = true
0.00.602.479 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.480 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.670 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.679.620 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.679.626 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.679.648 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.683.889 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.683.891 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.683.891 I llama_init_from_model: graph nodes  = 967
0.00.683.891 I llama_init_from_model: graph splits = 2
0.00.683.896 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.684.026 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.478 I main: llama threadpool init, n_threads = 4
0.00.738.522 I 
0.00.738.546 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.546 I 
0.00.738.724 I sampler seed: 1234
0.00.738.729 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.740 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.740 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.741 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.418.283 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51900.58 tokens per second)
0.01.418.284 I llama_perf_context_print:        load time =     726.68 ms
0.01.418.286 I llama_perf_context_print: prompt eval time =      48.91 ms /     7 tokens (    6.99 ms per token,   143.12 tokens per second)
0.01.418.286 I llama_perf_context_print:        eval time =     627.69 ms /    63 runs   (    9.96 ms per token,   100.37 tokens per second)
0.01.418.287 I llama_perf_context_print:       total time =     680.70 ms /    70 tokens
0.01.418.549 I ggml_metal_free: deallocating

real	0m1.437s
user	0m0.109s
sys	0m0.228s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.952 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.851 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.858 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.858 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.859 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.859 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.859 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.860 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.861 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.861 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.861 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.862 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.862 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.863 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.865 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.865 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.865 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.689 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.772 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.597 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.599 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.599 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.599 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.600 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.600 I llama_model_loader: - type  f32:  194 tensors
0.00.025.600 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.601 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.601 I print_info: file format = GGUF V3 (latest)
0.00.025.602 I print_info: file type   = Q4_0
0.00.025.604 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.779 I load: special tokens cache size = 25
0.00.039.773 I load: token to piece cache size = 0.2984 MB
0.00.039.776 I print_info: arch             = gptneox
0.00.039.776 I print_info: vocab_only       = 0
0.00.039.776 I print_info: n_ctx_train      = 2048
0.00.039.777 I print_info: n_embd           = 2048
0.00.039.777 I print_info: n_layer          = 24
0.00.039.780 I print_info: n_head           = 16
0.00.039.781 I print_info: n_head_kv        = 16
0.00.039.781 I print_info: n_rot            = 32
0.00.039.781 I print_info: n_swa            = 0
0.00.039.781 I print_info: n_embd_head_k    = 128
0.00.039.782 I print_info: n_embd_head_v    = 128
0.00.039.782 I print_info: n_gqa            = 1
0.00.039.783 I print_info: n_embd_k_gqa     = 2048
0.00.039.784 I print_info: n_embd_v_gqa     = 2048
0.00.039.785 I print_info: f_norm_eps       = 1.0e-05
0.00.039.785 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.790 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.790 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.790 I print_info: f_logit_scale    = 0.0e+00
0.00.039.791 I print_info: n_ff             = 8192
0.00.039.791 I print_info: n_expert         = 0
0.00.039.791 I print_info: n_expert_used    = 0
0.00.039.791 I print_info: causal attn      = 1
0.00.039.792 I print_info: pooling type     = 0
0.00.039.792 I print_info: rope type        = 2
0.00.039.792 I print_info: rope scaling     = linear
0.00.039.792 I print_info: freq_base_train  = 10000.0
0.00.039.793 I print_info: freq_scale_train = 1
0.00.039.793 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.793 I print_info: rope_finetuned   = unknown
0.00.039.793 I print_info: ssm_d_conv       = 0
0.00.039.795 I print_info: ssm_d_inner      = 0
0.00.039.795 I print_info: ssm_d_state      = 0
0.00.039.795 I print_info: ssm_dt_rank      = 0
0.00.039.795 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.796 I print_info: model type       = 1.4B
0.00.039.796 I print_info: model params     = 1.41 B
0.00.039.796 I print_info: general.name     = 1.4B
0.00.039.797 I print_info: vocab type       = BPE
0.00.039.797 I print_info: n_vocab          = 50304
0.00.039.797 I print_info: n_merges         = 50009
0.00.039.797 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.797 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.800 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.800 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.800 I print_info: LF token         = 128 'Ä'
0.00.039.801 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.801 I print_info: max token length = 1024
0.00.565.170 I load_tensors: offloading 24 repeating layers to GPU
0.00.565.180 I load_tensors: offloading output layer to GPU
0.00.565.181 I load_tensors: offloaded 25/25 layers to GPU
0.00.565.210 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.565.211 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.566.380 I llama_init_from_model: n_seq_max     = 1
0.00.566.385 I llama_init_from_model: n_ctx         = 128
0.00.566.386 I llama_init_from_model: n_ctx_per_seq = 128
0.00.566.387 I llama_init_from_model: n_batch       = 128
0.00.566.387 I llama_init_from_model: n_ubatch      = 128
0.00.566.388 I llama_init_from_model: flash_attn    = 0
0.00.566.389 I llama_init_from_model: freq_base     = 10000.0
0.00.566.390 I llama_init_from_model: freq_scale    = 1
0.00.566.391 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.566.397 I ggml_metal_init: allocating
0.00.566.459 I ggml_metal_init: found device: Apple M4
0.00.566.472 I ggml_metal_init: picking default device: Apple M4
0.00.568.332 I ggml_metal_init: using embedded metal library
0.00.574.867 I ggml_metal_init: GPU name:   Apple M4
0.00.574.871 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.574.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.574.873 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.574.873 I ggml_metal_init: simdgroup reduction   = true
0.00.574.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.574.873 I ggml_metal_init: has residency sets    = true
0.00.574.874 I ggml_metal_init: has bfloat            = true
0.00.574.874 I ggml_metal_init: use bfloat            = true
0.00.574.875 I ggml_metal_init: hasUnifiedMemory      = true
0.00.574.885 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.592.257 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.595.823 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.595.829 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.595.870 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.599.230 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.599.232 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.599.233 I llama_init_from_model: graph nodes  = 967
0.00.599.233 I llama_init_from_model: graph splits = 2
0.00.599.235 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.599.235 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.624.304 I 
0.00.624.384 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.624.404 I perplexity: tokenizing the input ..
0.00.630.970 I perplexity: tokenization took 6.563 ms
0.00.630.986 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.766.878 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.768.291 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.768.316 I llama_perf_context_print:        load time =     614.34 ms
0.00.768.317 I llama_perf_context_print: prompt eval time =     135.51 ms /   128 tokens (    1.06 ms per token,   944.60 tokens per second)
0.00.768.318 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.768.318 I llama_perf_context_print:       total time =     144.02 ms /   129 tokens
0.00.768.698 I ggml_metal_free: deallocating

real	0m0.784s
user	0m0.078s
sys	0m0.129s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.803 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.496 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.501 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.503 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.503 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.504 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.504 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.507 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.507 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.508 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.514 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.514 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.515 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.399 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.406 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.206 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.207 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.208 I llama_model_loader: - type  f32:  194 tensors
0.00.026.208 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.208 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.209 I print_info: file format = GGUF V3 (latest)
0.00.026.209 I print_info: file type   = Q4_1
0.00.026.214 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.393 I load: special tokens cache size = 25
0.00.040.354 I load: token to piece cache size = 0.2984 MB
0.00.040.357 I print_info: arch             = gptneox
0.00.040.357 I print_info: vocab_only       = 0
0.00.040.358 I print_info: n_ctx_train      = 2048
0.00.040.358 I print_info: n_embd           = 2048
0.00.040.358 I print_info: n_layer          = 24
0.00.040.361 I print_info: n_head           = 16
0.00.040.361 I print_info: n_head_kv        = 16
0.00.040.362 I print_info: n_rot            = 32
0.00.040.363 I print_info: n_swa            = 0
0.00.040.364 I print_info: n_embd_head_k    = 128
0.00.040.364 I print_info: n_embd_head_v    = 128
0.00.040.364 I print_info: n_gqa            = 1
0.00.040.365 I print_info: n_embd_k_gqa     = 2048
0.00.040.375 I print_info: n_embd_v_gqa     = 2048
0.00.040.376 I print_info: f_norm_eps       = 1.0e-05
0.00.040.376 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.376 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.377 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.377 I print_info: f_logit_scale    = 0.0e+00
0.00.040.381 I print_info: n_ff             = 8192
0.00.040.381 I print_info: n_expert         = 0
0.00.040.381 I print_info: n_expert_used    = 0
0.00.040.381 I print_info: causal attn      = 1
0.00.040.381 I print_info: pooling type     = 0
0.00.040.382 I print_info: rope type        = 2
0.00.040.384 I print_info: rope scaling     = linear
0.00.040.384 I print_info: freq_base_train  = 10000.0
0.00.040.384 I print_info: freq_scale_train = 1
0.00.040.385 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.385 I print_info: rope_finetuned   = unknown
0.00.040.386 I print_info: ssm_d_conv       = 0
0.00.040.386 I print_info: ssm_d_inner      = 0
0.00.040.386 I print_info: ssm_d_state      = 0
0.00.040.386 I print_info: ssm_dt_rank      = 0
0.00.040.387 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.387 I print_info: model type       = 1.4B
0.00.040.387 I print_info: model params     = 1.41 B
0.00.040.387 I print_info: general.name     = 1.4B
0.00.040.388 I print_info: vocab type       = BPE
0.00.040.388 I print_info: n_vocab          = 50304
0.00.040.388 I print_info: n_merges         = 50009
0.00.040.389 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.389 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.389 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.390 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.390 I print_info: LF token         = 128 'Ä'
0.00.040.390 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.390 I print_info: max token length = 1024
0.00.620.039 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.049 I load_tensors: offloading output layer to GPU
0.00.620.050 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.082 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.620.083 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.621.614 I llama_init_from_model: n_seq_max     = 1
0.00.621.620 I llama_init_from_model: n_ctx         = 2048
0.00.621.620 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.621.620 I llama_init_from_model: n_batch       = 2048
0.00.621.621 I llama_init_from_model: n_ubatch      = 512
0.00.621.621 I llama_init_from_model: flash_attn    = 0
0.00.621.622 I llama_init_from_model: freq_base     = 10000.0
0.00.621.623 I llama_init_from_model: freq_scale    = 1
0.00.621.629 I ggml_metal_init: allocating
0.00.621.677 I ggml_metal_init: found device: Apple M4
0.00.621.687 I ggml_metal_init: picking default device: Apple M4
0.00.623.837 I ggml_metal_init: using embedded metal library
0.00.630.067 I ggml_metal_init: GPU name:   Apple M4
0.00.630.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.076 I ggml_metal_init: simdgroup reduction   = true
0.00.630.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.076 I ggml_metal_init: has residency sets    = true
0.00.630.076 I ggml_metal_init: has bfloat            = true
0.00.630.077 I ggml_metal_init: use bfloat            = true
0.00.630.078 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.083 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.649.909 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.711.128 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.711.133 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.711.154 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.715.528 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.715.529 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.715.529 I llama_init_from_model: graph nodes  = 967
0.00.715.530 I llama_init_from_model: graph splits = 2
0.00.715.535 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.715.673 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.715.674 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.572 I main: llama threadpool init, n_threads = 4
0.00.769.616 I 
0.00.769.639 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.640 I 
0.00.769.794 I sampler seed: 1234
0.00.769.798 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.838 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.841 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.841 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.499.351 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.01.499.352 I llama_perf_context_print:        load time =     758.86 ms
0.01.499.352 I llama_perf_context_print: prompt eval time =      50.43 ms /     7 tokens (    7.20 ms per token,   138.80 tokens per second)
0.01.499.354 I llama_perf_context_print:        eval time =     676.30 ms /    63 runs   (   10.73 ms per token,    93.15 tokens per second)
0.01.499.355 I llama_perf_context_print:       total time =     730.69 ms /    70 tokens
0.01.499.583 I ggml_metal_free: deallocating

real	0m1.519s
user	0m0.112s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.961 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.336 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.340 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.342 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.343 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.343 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.344 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.344 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.345 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.345 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.346 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.346 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.347 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.347 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.352 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.352 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.164 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.235 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.990 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.991 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.991 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.992 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.992 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.992 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.993 I llama_model_loader: - type  f32:  194 tensors
0.00.024.993 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.994 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.994 I print_info: file format = GGUF V3 (latest)
0.00.024.995 I print_info: file type   = Q4_1
0.00.024.995 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.969 I load: special tokens cache size = 25
0.00.038.789 I load: token to piece cache size = 0.2984 MB
0.00.038.793 I print_info: arch             = gptneox
0.00.038.793 I print_info: vocab_only       = 0
0.00.038.793 I print_info: n_ctx_train      = 2048
0.00.038.793 I print_info: n_embd           = 2048
0.00.038.793 I print_info: n_layer          = 24
0.00.038.796 I print_info: n_head           = 16
0.00.038.797 I print_info: n_head_kv        = 16
0.00.038.797 I print_info: n_rot            = 32
0.00.038.798 I print_info: n_swa            = 0
0.00.038.798 I print_info: n_embd_head_k    = 128
0.00.038.798 I print_info: n_embd_head_v    = 128
0.00.038.799 I print_info: n_gqa            = 1
0.00.038.800 I print_info: n_embd_k_gqa     = 2048
0.00.038.800 I print_info: n_embd_v_gqa     = 2048
0.00.038.801 I print_info: f_norm_eps       = 1.0e-05
0.00.038.801 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.801 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.802 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.802 I print_info: f_logit_scale    = 0.0e+00
0.00.038.802 I print_info: n_ff             = 8192
0.00.038.803 I print_info: n_expert         = 0
0.00.038.803 I print_info: n_expert_used    = 0
0.00.038.803 I print_info: causal attn      = 1
0.00.038.803 I print_info: pooling type     = 0
0.00.038.803 I print_info: rope type        = 2
0.00.038.803 I print_info: rope scaling     = linear
0.00.038.804 I print_info: freq_base_train  = 10000.0
0.00.038.804 I print_info: freq_scale_train = 1
0.00.038.804 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.804 I print_info: rope_finetuned   = unknown
0.00.038.805 I print_info: ssm_d_conv       = 0
0.00.038.805 I print_info: ssm_d_inner      = 0
0.00.038.807 I print_info: ssm_d_state      = 0
0.00.038.807 I print_info: ssm_dt_rank      = 0
0.00.038.807 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.808 I print_info: model type       = 1.4B
0.00.038.808 I print_info: model params     = 1.41 B
0.00.038.808 I print_info: general.name     = 1.4B
0.00.038.809 I print_info: vocab type       = BPE
0.00.038.809 I print_info: n_vocab          = 50304
0.00.038.809 I print_info: n_merges         = 50009
0.00.038.809 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.810 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.810 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.810 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.810 I print_info: LF token         = 128 'Ä'
0.00.038.810 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.811 I print_info: max token length = 1024
0.00.625.891 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.907 I load_tensors: offloading output layer to GPU
0.00.625.908 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.940 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.625.941 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.627.274 I llama_init_from_model: n_seq_max     = 1
0.00.627.279 I llama_init_from_model: n_ctx         = 128
0.00.627.280 I llama_init_from_model: n_ctx_per_seq = 128
0.00.627.281 I llama_init_from_model: n_batch       = 128
0.00.627.281 I llama_init_from_model: n_ubatch      = 128
0.00.627.282 I llama_init_from_model: flash_attn    = 0
0.00.627.284 I llama_init_from_model: freq_base     = 10000.0
0.00.627.284 I llama_init_from_model: freq_scale    = 1
0.00.627.285 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.627.292 I ggml_metal_init: allocating
0.00.627.383 I ggml_metal_init: found device: Apple M4
0.00.627.397 I ggml_metal_init: picking default device: Apple M4
0.00.629.187 I ggml_metal_init: using embedded metal library
0.00.636.058 I ggml_metal_init: GPU name:   Apple M4
0.00.636.063 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.063 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.064 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.065 I ggml_metal_init: simdgroup reduction   = true
0.00.636.065 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.065 I ggml_metal_init: has residency sets    = true
0.00.636.065 I ggml_metal_init: has bfloat            = true
0.00.636.066 I ggml_metal_init: use bfloat            = true
0.00.636.067 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.068 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.653.632 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.657.030 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.657.033 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.657.071 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.660.379 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.660.381 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.660.381 I llama_init_from_model: graph nodes  = 967
0.00.660.381 I llama_init_from_model: graph splits = 2
0.00.660.384 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.660.385 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.531 I 
0.00.688.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.632 I perplexity: tokenizing the input ..
0.00.695.043 I perplexity: tokenization took 6.409 ms
0.00.695.063 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.830.870 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.832.219 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.832.235 I llama_perf_context_print:        load time =     679.56 ms
0.00.832.236 I llama_perf_context_print: prompt eval time =     135.26 ms /   128 tokens (    1.06 ms per token,   946.35 tokens per second)
0.00.832.237 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.832.237 I llama_perf_context_print:       total time =     143.71 ms /   129 tokens
0.00.832.614 I ggml_metal_free: deallocating

real	0m0.847s
user	0m0.078s
sys	0m0.124s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.515 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.028 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.033 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.035 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.037 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.038 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.039 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.040 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.045 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.045 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.046 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.843 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.652 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.654 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.654 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.655 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.655 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.655 I llama_model_loader: - type  f32:  194 tensors
0.00.027.656 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.656 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.657 I print_info: file format = GGUF V3 (latest)
0.00.027.657 I print_info: file type   = Q5_0
0.00.027.658 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.555 I load: special tokens cache size = 25
0.00.041.374 I load: token to piece cache size = 0.2984 MB
0.00.041.377 I print_info: arch             = gptneox
0.00.041.377 I print_info: vocab_only       = 0
0.00.041.377 I print_info: n_ctx_train      = 2048
0.00.041.377 I print_info: n_embd           = 2048
0.00.041.378 I print_info: n_layer          = 24
0.00.041.381 I print_info: n_head           = 16
0.00.041.381 I print_info: n_head_kv        = 16
0.00.041.382 I print_info: n_rot            = 32
0.00.041.382 I print_info: n_swa            = 0
0.00.041.382 I print_info: n_embd_head_k    = 128
0.00.041.382 I print_info: n_embd_head_v    = 128
0.00.041.383 I print_info: n_gqa            = 1
0.00.041.384 I print_info: n_embd_k_gqa     = 2048
0.00.041.384 I print_info: n_embd_v_gqa     = 2048
0.00.041.385 I print_info: f_norm_eps       = 1.0e-05
0.00.041.386 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.386 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.386 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.386 I print_info: f_logit_scale    = 0.0e+00
0.00.041.387 I print_info: n_ff             = 8192
0.00.041.387 I print_info: n_expert         = 0
0.00.041.387 I print_info: n_expert_used    = 0
0.00.041.387 I print_info: causal attn      = 1
0.00.041.387 I print_info: pooling type     = 0
0.00.041.389 I print_info: rope type        = 2
0.00.041.391 I print_info: rope scaling     = linear
0.00.041.391 I print_info: freq_base_train  = 10000.0
0.00.041.391 I print_info: freq_scale_train = 1
0.00.041.392 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.392 I print_info: rope_finetuned   = unknown
0.00.041.392 I print_info: ssm_d_conv       = 0
0.00.041.392 I print_info: ssm_d_inner      = 0
0.00.041.392 I print_info: ssm_d_state      = 0
0.00.041.393 I print_info: ssm_dt_rank      = 0
0.00.041.393 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.393 I print_info: model type       = 1.4B
0.00.041.393 I print_info: model params     = 1.41 B
0.00.041.393 I print_info: general.name     = 1.4B
0.00.041.395 I print_info: vocab type       = BPE
0.00.041.396 I print_info: n_vocab          = 50304
0.00.041.396 I print_info: n_merges         = 50009
0.00.041.396 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.396 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.396 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.397 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.397 I print_info: LF token         = 128 'Ä'
0.00.041.397 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.397 I print_info: max token length = 1024
0.00.650.243 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.257 I load_tensors: offloading output layer to GPU
0.00.650.258 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.296 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.650.297 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.651.880 I llama_init_from_model: n_seq_max     = 1
0.00.651.886 I llama_init_from_model: n_ctx         = 2048
0.00.651.887 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.651.888 I llama_init_from_model: n_batch       = 2048
0.00.651.888 I llama_init_from_model: n_ubatch      = 512
0.00.651.888 I llama_init_from_model: flash_attn    = 0
0.00.651.891 I llama_init_from_model: freq_base     = 10000.0
0.00.651.891 I llama_init_from_model: freq_scale    = 1
0.00.651.898 I ggml_metal_init: allocating
0.00.651.978 I ggml_metal_init: found device: Apple M4
0.00.651.992 I ggml_metal_init: picking default device: Apple M4
0.00.653.707 I ggml_metal_init: using embedded metal library
0.00.660.339 I ggml_metal_init: GPU name:   Apple M4
0.00.660.343 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.660.345 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.660.345 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.660.346 I ggml_metal_init: simdgroup reduction   = true
0.00.660.346 I ggml_metal_init: simdgroup matrix mul. = true
0.00.660.346 I ggml_metal_init: has residency sets    = true
0.00.660.346 I ggml_metal_init: has bfloat            = true
0.00.660.347 I ggml_metal_init: use bfloat            = true
0.00.660.348 I ggml_metal_init: hasUnifiedMemory      = true
0.00.660.349 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.625 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.825 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.730.841 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.869 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.923 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.734.925 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.734.925 I llama_init_from_model: graph nodes  = 967
0.00.734.926 I llama_init_from_model: graph splits = 2
0.00.734.932 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.735.077 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.078 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.489 I main: llama threadpool init, n_threads = 4
0.00.795.530 I 
0.00.795.558 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.558 I 
0.00.795.731 I sampler seed: 1234
0.00.795.735 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.755 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.756 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.756 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.594.268 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.01.594.268 I llama_perf_context_print:        load time =     783.06 ms
0.01.594.269 I llama_perf_context_print: prompt eval time =      54.00 ms /     7 tokens (    7.71 ms per token,   129.63 tokens per second)
0.01.594.271 I llama_perf_context_print:        eval time =     741.69 ms /    63 runs   (   11.77 ms per token,    84.94 tokens per second)
0.01.594.271 I llama_perf_context_print:       total time =     799.69 ms /    70 tokens
0.01.594.556 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.108s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.706 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.527 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.531 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.533 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.533 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.534 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.534 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.534 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.535 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.536 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.536 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.537 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.537 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.538 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.540 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.366 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.371 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.167 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.168 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.168 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.168 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.168 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.169 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.169 I llama_model_loader: - type  f32:  194 tensors
0.00.026.170 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.170 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.170 I print_info: file format = GGUF V3 (latest)
0.00.026.171 I print_info: file type   = Q5_0
0.00.026.172 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.089 I load: special tokens cache size = 25
0.00.040.112 I load: token to piece cache size = 0.2984 MB
0.00.040.115 I print_info: arch             = gptneox
0.00.040.115 I print_info: vocab_only       = 0
0.00.040.115 I print_info: n_ctx_train      = 2048
0.00.040.115 I print_info: n_embd           = 2048
0.00.040.115 I print_info: n_layer          = 24
0.00.040.118 I print_info: n_head           = 16
0.00.040.119 I print_info: n_head_kv        = 16
0.00.040.119 I print_info: n_rot            = 32
0.00.040.120 I print_info: n_swa            = 0
0.00.040.120 I print_info: n_embd_head_k    = 128
0.00.040.120 I print_info: n_embd_head_v    = 128
0.00.040.121 I print_info: n_gqa            = 1
0.00.040.121 I print_info: n_embd_k_gqa     = 2048
0.00.040.122 I print_info: n_embd_v_gqa     = 2048
0.00.040.123 I print_info: f_norm_eps       = 1.0e-05
0.00.040.123 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.123 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.123 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.123 I print_info: f_logit_scale    = 0.0e+00
0.00.040.124 I print_info: n_ff             = 8192
0.00.040.124 I print_info: n_expert         = 0
0.00.040.124 I print_info: n_expert_used    = 0
0.00.040.125 I print_info: causal attn      = 1
0.00.040.125 I print_info: pooling type     = 0
0.00.040.125 I print_info: rope type        = 2
0.00.040.125 I print_info: rope scaling     = linear
0.00.040.126 I print_info: freq_base_train  = 10000.0
0.00.040.126 I print_info: freq_scale_train = 1
0.00.040.126 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.126 I print_info: rope_finetuned   = unknown
0.00.040.127 I print_info: ssm_d_conv       = 0
0.00.040.127 I print_info: ssm_d_inner      = 0
0.00.040.127 I print_info: ssm_d_state      = 0
0.00.040.127 I print_info: ssm_dt_rank      = 0
0.00.040.127 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.127 I print_info: model type       = 1.4B
0.00.040.128 I print_info: model params     = 1.41 B
0.00.040.128 I print_info: general.name     = 1.4B
0.00.040.128 I print_info: vocab type       = BPE
0.00.040.129 I print_info: n_vocab          = 50304
0.00.040.129 I print_info: n_merges         = 50009
0.00.040.129 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.129 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.130 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.130 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.130 I print_info: LF token         = 128 'Ä'
0.00.040.130 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.131 I print_info: max token length = 1024
0.00.648.953 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.967 I load_tensors: offloading output layer to GPU
0.00.648.968 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.003 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.649.015 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.650.463 I llama_init_from_model: n_seq_max     = 1
0.00.650.467 I llama_init_from_model: n_ctx         = 128
0.00.650.467 I llama_init_from_model: n_ctx_per_seq = 128
0.00.650.468 I llama_init_from_model: n_batch       = 128
0.00.650.469 I llama_init_from_model: n_ubatch      = 128
0.00.650.469 I llama_init_from_model: flash_attn    = 0
0.00.650.471 I llama_init_from_model: freq_base     = 10000.0
0.00.650.471 I llama_init_from_model: freq_scale    = 1
0.00.650.472 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.650.475 I ggml_metal_init: allocating
0.00.650.545 I ggml_metal_init: found device: Apple M4
0.00.650.558 I ggml_metal_init: picking default device: Apple M4
0.00.652.252 I ggml_metal_init: using embedded metal library
0.00.658.928 I ggml_metal_init: GPU name:   Apple M4
0.00.658.932 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.933 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.935 I ggml_metal_init: simdgroup reduction   = true
0.00.658.935 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.935 I ggml_metal_init: has residency sets    = true
0.00.658.936 I ggml_metal_init: has bfloat            = true
0.00.658.936 I ggml_metal_init: use bfloat            = true
0.00.658.937 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.938 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.676.614 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.240 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.680.244 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.680.269 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.683.728 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.683.730 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.683.730 I llama_init_from_model: graph nodes  = 967
0.00.683.730 I llama_init_from_model: graph splits = 2
0.00.683.733 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.683.734 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.386 I 
0.00.711.460 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.480 I perplexity: tokenizing the input ..
0.00.718.804 I perplexity: tokenization took 7.32 ms
0.00.718.830 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.854.899 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.856.224 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.856.238 I llama_perf_context_print:        load time =     700.67 ms
0.00.856.240 I llama_perf_context_print: prompt eval time =     135.16 ms /   128 tokens (    1.06 ms per token,   946.99 tokens per second)
0.00.856.241 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.856.241 I llama_perf_context_print:       total time =     144.86 ms /   129 tokens
0.00.856.646 I ggml_metal_free: deallocating

real	0m0.872s
user	0m0.080s
sys	0m0.139s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.329 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.220 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.222 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.222 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.223 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.223 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.223 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.226 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.226 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.228 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.229 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.229 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.229 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.233 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.116 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.895 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.896 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.896 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.896 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.897 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.897 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.898 I llama_model_loader: - type  f32:  194 tensors
0.00.025.898 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.898 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.899 I print_info: file format = GGUF V3 (latest)
0.00.025.899 I print_info: file type   = Q5_1
0.00.025.905 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.026 I load: special tokens cache size = 25
0.00.040.007 I load: token to piece cache size = 0.2984 MB
0.00.040.010 I print_info: arch             = gptneox
0.00.040.010 I print_info: vocab_only       = 0
0.00.040.011 I print_info: n_ctx_train      = 2048
0.00.040.011 I print_info: n_embd           = 2048
0.00.040.011 I print_info: n_layer          = 24
0.00.040.014 I print_info: n_head           = 16
0.00.040.015 I print_info: n_head_kv        = 16
0.00.040.015 I print_info: n_rot            = 32
0.00.040.015 I print_info: n_swa            = 0
0.00.040.015 I print_info: n_embd_head_k    = 128
0.00.040.016 I print_info: n_embd_head_v    = 128
0.00.040.017 I print_info: n_gqa            = 1
0.00.040.017 I print_info: n_embd_k_gqa     = 2048
0.00.040.018 I print_info: n_embd_v_gqa     = 2048
0.00.040.019 I print_info: f_norm_eps       = 1.0e-05
0.00.040.019 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.019 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.019 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.019 I print_info: f_logit_scale    = 0.0e+00
0.00.040.020 I print_info: n_ff             = 8192
0.00.040.020 I print_info: n_expert         = 0
0.00.040.021 I print_info: n_expert_used    = 0
0.00.040.021 I print_info: causal attn      = 1
0.00.040.021 I print_info: pooling type     = 0
0.00.040.021 I print_info: rope type        = 2
0.00.040.021 I print_info: rope scaling     = linear
0.00.040.022 I print_info: freq_base_train  = 10000.0
0.00.040.022 I print_info: freq_scale_train = 1
0.00.040.022 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.022 I print_info: rope_finetuned   = unknown
0.00.040.024 I print_info: ssm_d_conv       = 0
0.00.040.024 I print_info: ssm_d_inner      = 0
0.00.040.024 I print_info: ssm_d_state      = 0
0.00.040.024 I print_info: ssm_dt_rank      = 0
0.00.040.025 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.025 I print_info: model type       = 1.4B
0.00.040.025 I print_info: model params     = 1.41 B
0.00.040.025 I print_info: general.name     = 1.4B
0.00.040.026 I print_info: vocab type       = BPE
0.00.040.026 I print_info: n_vocab          = 50304
0.00.040.026 I print_info: n_merges         = 50009
0.00.040.027 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.027 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.027 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.027 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.027 I print_info: LF token         = 128 'Ä'
0.00.040.028 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.028 I print_info: max token length = 1024
0.00.681.387 I load_tensors: offloading 24 repeating layers to GPU
0.00.681.396 I load_tensors: offloading output layer to GPU
0.00.681.396 I load_tensors: offloaded 25/25 layers to GPU
0.00.681.428 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.681.429 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.682.749 I llama_init_from_model: n_seq_max     = 1
0.00.682.753 I llama_init_from_model: n_ctx         = 2048
0.00.682.754 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.682.754 I llama_init_from_model: n_batch       = 2048
0.00.682.754 I llama_init_from_model: n_ubatch      = 512
0.00.682.755 I llama_init_from_model: flash_attn    = 0
0.00.682.757 I llama_init_from_model: freq_base     = 10000.0
0.00.682.757 I llama_init_from_model: freq_scale    = 1
0.00.682.765 I ggml_metal_init: allocating
0.00.682.845 I ggml_metal_init: found device: Apple M4
0.00.682.858 I ggml_metal_init: picking default device: Apple M4
0.00.684.920 I ggml_metal_init: using embedded metal library
0.00.691.486 I ggml_metal_init: GPU name:   Apple M4
0.00.691.490 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.691.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.691.492 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.691.492 I ggml_metal_init: simdgroup reduction   = true
0.00.691.493 I ggml_metal_init: simdgroup matrix mul. = true
0.00.691.493 I ggml_metal_init: has residency sets    = true
0.00.691.493 I ggml_metal_init: has bfloat            = true
0.00.691.493 I ggml_metal_init: use bfloat            = true
0.00.691.494 I ggml_metal_init: hasUnifiedMemory      = true
0.00.691.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.711.243 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.769.355 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.769.361 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.769.383 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.773.660 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.773.662 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.773.662 I llama_init_from_model: graph nodes  = 967
0.00.773.662 I llama_init_from_model: graph splits = 2
0.00.773.668 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.773.784 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.773.784 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.834.047 I main: llama threadpool init, n_threads = 4
0.00.834.086 I 
0.00.834.109 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.834.109 I 
0.00.834.270 I sampler seed: 1234
0.00.834.275 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.834.285 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.834.286 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.834.286 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.677.410 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53303.30 tokens per second)
0.01.677.411 I llama_perf_context_print:        load time =     823.81 ms
0.01.677.413 I llama_perf_context_print: prompt eval time =      50.27 ms /     7 tokens (    7.18 ms per token,   139.25 tokens per second)
0.01.677.413 I llama_perf_context_print:        eval time =     790.01 ms /    63 runs   (   12.54 ms per token,    79.75 tokens per second)
0.01.677.414 I llama_perf_context_print:       total time =     844.27 ms /    70 tokens
0.01.677.667 I ggml_metal_free: deallocating

real	0m1.696s
user	0m0.111s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.340 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.345 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.347 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.348 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.348 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.348 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.349 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.350 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.350 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.350 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.351 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.351 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.351 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.355 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.220 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.230 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.000 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.001 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.001 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.002 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.002 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.002 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.003 I llama_model_loader: - type  f32:  194 tensors
0.00.025.003 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.004 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.004 I print_info: file format = GGUF V3 (latest)
0.00.025.004 I print_info: file type   = Q5_1
0.00.025.005 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.093 I load: special tokens cache size = 25
0.00.039.091 I load: token to piece cache size = 0.2984 MB
0.00.039.094 I print_info: arch             = gptneox
0.00.039.094 I print_info: vocab_only       = 0
0.00.039.094 I print_info: n_ctx_train      = 2048
0.00.039.094 I print_info: n_embd           = 2048
0.00.039.094 I print_info: n_layer          = 24
0.00.039.098 I print_info: n_head           = 16
0.00.039.099 I print_info: n_head_kv        = 16
0.00.039.099 I print_info: n_rot            = 32
0.00.039.099 I print_info: n_swa            = 0
0.00.039.099 I print_info: n_embd_head_k    = 128
0.00.039.102 I print_info: n_embd_head_v    = 128
0.00.039.102 I print_info: n_gqa            = 1
0.00.039.103 I print_info: n_embd_k_gqa     = 2048
0.00.039.104 I print_info: n_embd_v_gqa     = 2048
0.00.039.105 I print_info: f_norm_eps       = 1.0e-05
0.00.039.106 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.106 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.106 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.106 I print_info: f_logit_scale    = 0.0e+00
0.00.039.107 I print_info: n_ff             = 8192
0.00.039.107 I print_info: n_expert         = 0
0.00.039.107 I print_info: n_expert_used    = 0
0.00.039.108 I print_info: causal attn      = 1
0.00.039.108 I print_info: pooling type     = 0
0.00.039.108 I print_info: rope type        = 2
0.00.039.108 I print_info: rope scaling     = linear
0.00.039.108 I print_info: freq_base_train  = 10000.0
0.00.039.109 I print_info: freq_scale_train = 1
0.00.039.109 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.109 I print_info: rope_finetuned   = unknown
0.00.039.109 I print_info: ssm_d_conv       = 0
0.00.039.109 I print_info: ssm_d_inner      = 0
0.00.039.110 I print_info: ssm_d_state      = 0
0.00.039.110 I print_info: ssm_dt_rank      = 0
0.00.039.110 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.110 I print_info: model type       = 1.4B
0.00.039.110 I print_info: model params     = 1.41 B
0.00.039.111 I print_info: general.name     = 1.4B
0.00.039.112 I print_info: vocab type       = BPE
0.00.039.112 I print_info: n_vocab          = 50304
0.00.039.112 I print_info: n_merges         = 50009
0.00.039.113 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.114 I print_info: LF token         = 128 'Ä'
0.00.039.114 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.114 I print_info: max token length = 1024
0.00.704.913 I load_tensors: offloading 24 repeating layers to GPU
0.00.704.926 I load_tensors: offloading output layer to GPU
0.00.704.927 I load_tensors: offloaded 25/25 layers to GPU
0.00.704.962 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.704.963 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.706.506 I llama_init_from_model: n_seq_max     = 1
0.00.706.510 I llama_init_from_model: n_ctx         = 128
0.00.706.511 I llama_init_from_model: n_ctx_per_seq = 128
0.00.706.515 I llama_init_from_model: n_batch       = 128
0.00.706.515 I llama_init_from_model: n_ubatch      = 128
0.00.706.516 I llama_init_from_model: flash_attn    = 0
0.00.706.519 I llama_init_from_model: freq_base     = 10000.0
0.00.706.519 I llama_init_from_model: freq_scale    = 1
0.00.706.520 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.706.523 I ggml_metal_init: allocating
0.00.706.632 I ggml_metal_init: found device: Apple M4
0.00.706.645 I ggml_metal_init: picking default device: Apple M4
0.00.708.477 I ggml_metal_init: using embedded metal library
0.00.714.861 I ggml_metal_init: GPU name:   Apple M4
0.00.714.865 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.714.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.714.867 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.714.867 I ggml_metal_init: simdgroup reduction   = true
0.00.714.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.714.868 I ggml_metal_init: has residency sets    = true
0.00.714.868 I ggml_metal_init: has bfloat            = true
0.00.714.868 I ggml_metal_init: use bfloat            = true
0.00.714.869 I ggml_metal_init: hasUnifiedMemory      = true
0.00.714.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.732.364 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.899 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.735.902 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.735.932 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.739.033 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.739.035 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.739.035 I llama_init_from_model: graph nodes  = 967
0.00.739.035 I llama_init_from_model: graph splits = 2
0.00.739.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.739.038 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.321 I 
0.00.770.397 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.418 I perplexity: tokenizing the input ..
0.00.777.296 I perplexity: tokenization took 6.875 ms
0.00.777.315 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.924.778 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.926.108 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.926.124 I llama_perf_context_print:        load time =     761.50 ms
0.00.926.125 I llama_perf_context_print: prompt eval time =     146.59 ms /   128 tokens (    1.15 ms per token,   873.15 tokens per second)
0.00.926.126 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.926.126 I llama_perf_context_print:       total time =     155.81 ms /   129 tokens
0.00.926.489 I ggml_metal_free: deallocating

real	0m0.941s
user	0m0.079s
sys	0m0.150s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.673 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.403 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.408 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.409 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.409 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.410 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.410 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.410 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.411 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.412 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.412 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.413 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.413 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.413 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.414 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.146 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.155 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.864 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.865 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.865 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.866 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.866 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.866 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.867 I llama_model_loader: - type  f32:  194 tensors
0.00.024.867 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.867 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.868 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.868 I print_info: file format = GGUF V3 (latest)
0.00.024.869 I print_info: file type   = Q2_K - Medium
0.00.024.870 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.776 I load: special tokens cache size = 25
0.00.038.683 I load: token to piece cache size = 0.2984 MB
0.00.038.685 I print_info: arch             = gptneox
0.00.038.686 I print_info: vocab_only       = 0
0.00.038.686 I print_info: n_ctx_train      = 2048
0.00.038.686 I print_info: n_embd           = 2048
0.00.038.686 I print_info: n_layer          = 24
0.00.038.689 I print_info: n_head           = 16
0.00.038.690 I print_info: n_head_kv        = 16
0.00.038.690 I print_info: n_rot            = 32
0.00.038.690 I print_info: n_swa            = 0
0.00.038.690 I print_info: n_embd_head_k    = 128
0.00.038.693 I print_info: n_embd_head_v    = 128
0.00.038.693 I print_info: n_gqa            = 1
0.00.038.694 I print_info: n_embd_k_gqa     = 2048
0.00.038.695 I print_info: n_embd_v_gqa     = 2048
0.00.038.696 I print_info: f_norm_eps       = 1.0e-05
0.00.038.696 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.696 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.696 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.696 I print_info: f_logit_scale    = 0.0e+00
0.00.038.697 I print_info: n_ff             = 8192
0.00.038.697 I print_info: n_expert         = 0
0.00.038.697 I print_info: n_expert_used    = 0
0.00.038.698 I print_info: causal attn      = 1
0.00.038.698 I print_info: pooling type     = 0
0.00.038.698 I print_info: rope type        = 2
0.00.038.698 I print_info: rope scaling     = linear
0.00.038.699 I print_info: freq_base_train  = 10000.0
0.00.038.699 I print_info: freq_scale_train = 1
0.00.038.699 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.699 I print_info: rope_finetuned   = unknown
0.00.038.699 I print_info: ssm_d_conv       = 0
0.00.038.700 I print_info: ssm_d_inner      = 0
0.00.038.700 I print_info: ssm_d_state      = 0
0.00.038.700 I print_info: ssm_dt_rank      = 0
0.00.038.701 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.701 I print_info: model type       = 1.4B
0.00.038.701 I print_info: model params     = 1.41 B
0.00.038.701 I print_info: general.name     = 1.4B
0.00.038.702 I print_info: vocab type       = BPE
0.00.038.706 I print_info: n_vocab          = 50304
0.00.038.706 I print_info: n_merges         = 50009
0.00.038.708 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.708 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.708 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.708 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.708 I print_info: LF token         = 128 'Ä'
0.00.038.709 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.709 I print_info: max token length = 1024
0.00.377.397 I load_tensors: offloading 24 repeating layers to GPU
0.00.377.410 I load_tensors: offloading output layer to GPU
0.00.377.410 I load_tensors: offloaded 25/25 layers to GPU
0.00.377.446 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.377.447 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.378.878 I llama_init_from_model: n_seq_max     = 1
0.00.378.884 I llama_init_from_model: n_ctx         = 2048
0.00.378.884 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.378.885 I llama_init_from_model: n_batch       = 2048
0.00.378.885 I llama_init_from_model: n_ubatch      = 512
0.00.378.885 I llama_init_from_model: flash_attn    = 0
0.00.378.887 I llama_init_from_model: freq_base     = 10000.0
0.00.378.888 I llama_init_from_model: freq_scale    = 1
0.00.378.890 I ggml_metal_init: allocating
0.00.378.979 I ggml_metal_init: found device: Apple M4
0.00.378.993 I ggml_metal_init: picking default device: Apple M4
0.00.380.753 I ggml_metal_init: using embedded metal library
0.00.386.279 I ggml_metal_init: GPU name:   Apple M4
0.00.386.297 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.386.298 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.386.299 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.386.299 I ggml_metal_init: simdgroup reduction   = true
0.00.386.300 I ggml_metal_init: simdgroup matrix mul. = true
0.00.386.300 I ggml_metal_init: has residency sets    = true
0.00.386.300 I ggml_metal_init: has bfloat            = true
0.00.386.300 I ggml_metal_init: use bfloat            = true
0.00.386.302 I ggml_metal_init: hasUnifiedMemory      = true
0.00.386.306 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.407.193 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.463.128 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.463.136 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.463.159 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.467.232 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.467.234 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.467.235 I llama_init_from_model: graph nodes  = 967
0.00.467.235 I llama_init_from_model: graph splits = 2
0.00.467.241 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.467.351 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.467.352 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.516.928 I main: llama threadpool init, n_threads = 4
0.00.516.966 I 
0.00.516.991 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.516.992 I 
0.00.517.124 I sampler seed: 1234
0.00.517.128 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.517.138 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.517.139 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.517.140 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.192.562 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54699.54 tokens per second)
0.01.192.563 I llama_perf_context_print:        load time =     506.36 ms
0.01.192.564 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.33 tokens per second)
0.01.192.564 I llama_perf_context_print:        eval time =     636.70 ms /    63 runs   (   10.11 ms per token,    98.95 tokens per second)
0.01.192.565 I llama_perf_context_print:       total time =     676.52 ms /    70 tokens
0.01.192.826 I ggml_metal_free: deallocating

real	0m1.210s
user	0m0.113s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.671 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.677 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.678 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.679 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.679 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.679 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.680 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.680 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.681 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.681 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.681 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.684 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.684 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.685 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.686 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.687 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.687 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.545 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.422 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.423 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.423 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.424 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.424 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.425 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.425 I llama_model_loader: - type  f32:  194 tensors
0.00.026.425 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.426 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.426 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.426 I print_info: file format = GGUF V3 (latest)
0.00.026.431 I print_info: file type   = Q2_K - Medium
0.00.026.432 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.713 I load: special tokens cache size = 25
0.00.040.663 I load: token to piece cache size = 0.2984 MB
0.00.040.666 I print_info: arch             = gptneox
0.00.040.666 I print_info: vocab_only       = 0
0.00.040.666 I print_info: n_ctx_train      = 2048
0.00.040.666 I print_info: n_embd           = 2048
0.00.040.666 I print_info: n_layer          = 24
0.00.040.669 I print_info: n_head           = 16
0.00.040.670 I print_info: n_head_kv        = 16
0.00.040.670 I print_info: n_rot            = 32
0.00.040.670 I print_info: n_swa            = 0
0.00.040.672 I print_info: n_embd_head_k    = 128
0.00.040.673 I print_info: n_embd_head_v    = 128
0.00.040.674 I print_info: n_gqa            = 1
0.00.040.675 I print_info: n_embd_k_gqa     = 2048
0.00.040.676 I print_info: n_embd_v_gqa     = 2048
0.00.040.676 I print_info: f_norm_eps       = 1.0e-05
0.00.040.677 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.677 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.677 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.677 I print_info: f_logit_scale    = 0.0e+00
0.00.040.678 I print_info: n_ff             = 8192
0.00.040.678 I print_info: n_expert         = 0
0.00.040.678 I print_info: n_expert_used    = 0
0.00.040.678 I print_info: causal attn      = 1
0.00.040.678 I print_info: pooling type     = 0
0.00.040.679 I print_info: rope type        = 2
0.00.040.679 I print_info: rope scaling     = linear
0.00.040.679 I print_info: freq_base_train  = 10000.0
0.00.040.680 I print_info: freq_scale_train = 1
0.00.040.680 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.681 I print_info: rope_finetuned   = unknown
0.00.040.682 I print_info: ssm_d_conv       = 0
0.00.040.682 I print_info: ssm_d_inner      = 0
0.00.040.682 I print_info: ssm_d_state      = 0
0.00.040.682 I print_info: ssm_dt_rank      = 0
0.00.040.682 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.683 I print_info: model type       = 1.4B
0.00.040.683 I print_info: model params     = 1.41 B
0.00.040.683 I print_info: general.name     = 1.4B
0.00.040.684 I print_info: vocab type       = BPE
0.00.040.684 I print_info: n_vocab          = 50304
0.00.040.684 I print_info: n_merges         = 50009
0.00.040.684 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.685 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.685 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.685 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.692 I print_info: LF token         = 128 'Ä'
0.00.040.692 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.693 I print_info: max token length = 1024
0.00.377.806 I load_tensors: offloading 24 repeating layers to GPU
0.00.377.821 I load_tensors: offloading output layer to GPU
0.00.377.822 I load_tensors: offloaded 25/25 layers to GPU
0.00.377.854 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.377.856 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.379.235 I llama_init_from_model: n_seq_max     = 1
0.00.379.240 I llama_init_from_model: n_ctx         = 128
0.00.379.241 I llama_init_from_model: n_ctx_per_seq = 128
0.00.379.241 I llama_init_from_model: n_batch       = 128
0.00.379.242 I llama_init_from_model: n_ubatch      = 128
0.00.379.242 I llama_init_from_model: flash_attn    = 0
0.00.379.245 I llama_init_from_model: freq_base     = 10000.0
0.00.379.245 I llama_init_from_model: freq_scale    = 1
0.00.379.246 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.379.253 I ggml_metal_init: allocating
0.00.379.348 I ggml_metal_init: found device: Apple M4
0.00.379.361 I ggml_metal_init: picking default device: Apple M4
0.00.381.118 I ggml_metal_init: using embedded metal library
0.00.386.617 I ggml_metal_init: GPU name:   Apple M4
0.00.386.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.386.630 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.386.631 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.386.631 I ggml_metal_init: simdgroup reduction   = true
0.00.386.632 I ggml_metal_init: simdgroup matrix mul. = true
0.00.386.632 I ggml_metal_init: has residency sets    = true
0.00.386.632 I ggml_metal_init: has bfloat            = true
0.00.386.632 I ggml_metal_init: use bfloat            = true
0.00.386.637 I ggml_metal_init: hasUnifiedMemory      = true
0.00.386.641 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.408.114 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.411.590 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.411.596 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.411.631 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.414.871 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.414.873 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.414.873 I llama_init_from_model: graph nodes  = 967
0.00.414.874 I llama_init_from_model: graph splits = 2
0.00.414.877 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.414.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.445.040 I 
0.00.445.121 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.445.139 I perplexity: tokenizing the input ..
0.00.451.805 I perplexity: tokenization took 6.663 ms
0.00.451.817 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.583.386 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.584.716 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.584.734 I llama_perf_context_print:        load time =     434.30 ms
0.00.584.735 I llama_perf_context_print: prompt eval time =     131.34 ms /   128 tokens (    1.03 ms per token,   974.59 tokens per second)
0.00.584.736 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.584.736 I llama_perf_context_print:       total time =     139.70 ms /   129 tokens
0.00.585.123 I ggml_metal_free: deallocating

real	0m0.600s
user	0m0.080s
sys	0m0.088s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.840 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.836 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.842 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.845 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.846 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.847 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.852 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.852 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.852 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.748 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.505 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.507 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.508 I llama_model_loader: - type  f32:  194 tensors
0.00.025.509 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.509 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.509 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.509 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.510 I print_info: file format = GGUF V3 (latest)
0.00.025.511 I print_info: file type   = Q3_K - Medium
0.00.025.511 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.759 I load: special tokens cache size = 25
0.00.039.769 I load: token to piece cache size = 0.2984 MB
0.00.039.772 I print_info: arch             = gptneox
0.00.039.772 I print_info: vocab_only       = 0
0.00.039.772 I print_info: n_ctx_train      = 2048
0.00.039.772 I print_info: n_embd           = 2048
0.00.039.773 I print_info: n_layer          = 24
0.00.039.775 I print_info: n_head           = 16
0.00.039.776 I print_info: n_head_kv        = 16
0.00.039.776 I print_info: n_rot            = 32
0.00.039.776 I print_info: n_swa            = 0
0.00.039.776 I print_info: n_embd_head_k    = 128
0.00.039.776 I print_info: n_embd_head_v    = 128
0.00.039.777 I print_info: n_gqa            = 1
0.00.039.778 I print_info: n_embd_k_gqa     = 2048
0.00.039.779 I print_info: n_embd_v_gqa     = 2048
0.00.039.779 I print_info: f_norm_eps       = 1.0e-05
0.00.039.779 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.781 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.782 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.782 I print_info: f_logit_scale    = 0.0e+00
0.00.039.782 I print_info: n_ff             = 8192
0.00.039.783 I print_info: n_expert         = 0
0.00.039.783 I print_info: n_expert_used    = 0
0.00.039.783 I print_info: causal attn      = 1
0.00.039.784 I print_info: pooling type     = 0
0.00.039.785 I print_info: rope type        = 2
0.00.039.785 I print_info: rope scaling     = linear
0.00.039.785 I print_info: freq_base_train  = 10000.0
0.00.039.785 I print_info: freq_scale_train = 1
0.00.039.786 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.786 I print_info: rope_finetuned   = unknown
0.00.039.787 I print_info: ssm_d_conv       = 0
0.00.039.787 I print_info: ssm_d_inner      = 0
0.00.039.788 I print_info: ssm_d_state      = 0
0.00.039.788 I print_info: ssm_dt_rank      = 0
0.00.039.788 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.788 I print_info: model type       = 1.4B
0.00.039.789 I print_info: model params     = 1.41 B
0.00.039.789 I print_info: general.name     = 1.4B
0.00.039.790 I print_info: vocab type       = BPE
0.00.039.790 I print_info: n_vocab          = 50304
0.00.039.790 I print_info: n_merges         = 50009
0.00.039.790 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.791 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.791 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.791 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.793 I print_info: LF token         = 128 'Ä'
0.00.039.793 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.793 I print_info: max token length = 1024
0.00.433.481 I load_tensors: offloading 24 repeating layers to GPU
0.00.433.496 I load_tensors: offloading output layer to GPU
0.00.433.497 I load_tensors: offloaded 25/25 layers to GPU
0.00.433.535 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.433.537 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.434.873 I llama_init_from_model: n_seq_max     = 1
0.00.434.879 I llama_init_from_model: n_ctx         = 2048
0.00.434.879 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.434.880 I llama_init_from_model: n_batch       = 2048
0.00.434.880 I llama_init_from_model: n_ubatch      = 512
0.00.434.880 I llama_init_from_model: flash_attn    = 0
0.00.434.882 I llama_init_from_model: freq_base     = 10000.0
0.00.434.883 I llama_init_from_model: freq_scale    = 1
0.00.434.885 I ggml_metal_init: allocating
0.00.434.956 I ggml_metal_init: found device: Apple M4
0.00.434.970 I ggml_metal_init: picking default device: Apple M4
0.00.436.717 I ggml_metal_init: using embedded metal library
0.00.442.178 I ggml_metal_init: GPU name:   Apple M4
0.00.442.197 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.198 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.200 I ggml_metal_init: simdgroup reduction   = true
0.00.442.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.200 I ggml_metal_init: has residency sets    = true
0.00.442.201 I ggml_metal_init: has bfloat            = true
0.00.442.201 I ggml_metal_init: use bfloat            = true
0.00.442.203 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.207 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.462.730 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.516.835 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.516.841 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.516.866 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.522.149 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.522.152 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.522.152 I llama_init_from_model: graph nodes  = 967
0.00.522.153 I llama_init_from_model: graph splits = 2
0.00.522.159 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.522.268 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.522.269 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.578.460 I main: llama threadpool init, n_threads = 4
0.00.578.508 I 
0.00.578.535 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.578.537 I 
0.00.578.687 I sampler seed: 1234
0.00.578.691 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.578.731 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.578.733 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.578.734 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.329.246 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48763.74 tokens per second)
0.01.329.248 I llama_perf_context_print:        load time =     568.71 ms
0.01.329.248 I llama_perf_context_print: prompt eval time =      50.07 ms /     7 tokens (    7.15 ms per token,   139.80 tokens per second)
0.01.329.249 I llama_perf_context_print:        eval time =     697.62 ms /    63 runs   (   11.07 ms per token,    90.31 tokens per second)
0.01.329.249 I llama_perf_context_print:       total time =     751.70 ms /    70 tokens
0.01.329.465 I ggml_metal_free: deallocating

real	0m1.345s
user	0m0.110s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.062 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.309 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.314 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.320 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.321 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.321 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.322 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.323 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.323 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.324 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.324 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.324 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.325 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.326 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.327 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.327 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.139 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.144 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.931 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.932 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.932 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.933 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.933 I llama_model_loader: - type  f32:  194 tensors
0.00.024.934 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.934 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.934 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.934 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.935 I print_info: file format = GGUF V3 (latest)
0.00.024.938 I print_info: file type   = Q3_K - Medium
0.00.024.939 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.800 I load: special tokens cache size = 25
0.00.038.536 I load: token to piece cache size = 0.2984 MB
0.00.038.538 I print_info: arch             = gptneox
0.00.038.539 I print_info: vocab_only       = 0
0.00.038.539 I print_info: n_ctx_train      = 2048
0.00.038.539 I print_info: n_embd           = 2048
0.00.038.539 I print_info: n_layer          = 24
0.00.038.542 I print_info: n_head           = 16
0.00.038.543 I print_info: n_head_kv        = 16
0.00.038.543 I print_info: n_rot            = 32
0.00.038.544 I print_info: n_swa            = 0
0.00.038.544 I print_info: n_embd_head_k    = 128
0.00.038.544 I print_info: n_embd_head_v    = 128
0.00.038.545 I print_info: n_gqa            = 1
0.00.038.546 I print_info: n_embd_k_gqa     = 2048
0.00.038.546 I print_info: n_embd_v_gqa     = 2048
0.00.038.548 I print_info: f_norm_eps       = 1.0e-05
0.00.038.548 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.548 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.549 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.549 I print_info: f_logit_scale    = 0.0e+00
0.00.038.550 I print_info: n_ff             = 8192
0.00.038.550 I print_info: n_expert         = 0
0.00.038.550 I print_info: n_expert_used    = 0
0.00.038.550 I print_info: causal attn      = 1
0.00.038.550 I print_info: pooling type     = 0
0.00.038.550 I print_info: rope type        = 2
0.00.038.550 I print_info: rope scaling     = linear
0.00.038.551 I print_info: freq_base_train  = 10000.0
0.00.038.551 I print_info: freq_scale_train = 1
0.00.038.551 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.552 I print_info: rope_finetuned   = unknown
0.00.038.552 I print_info: ssm_d_conv       = 0
0.00.038.552 I print_info: ssm_d_inner      = 0
0.00.038.552 I print_info: ssm_d_state      = 0
0.00.038.554 I print_info: ssm_dt_rank      = 0
0.00.038.554 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.554 I print_info: model type       = 1.4B
0.00.038.555 I print_info: model params     = 1.41 B
0.00.038.555 I print_info: general.name     = 1.4B
0.00.038.555 I print_info: vocab type       = BPE
0.00.038.555 I print_info: n_vocab          = 50304
0.00.038.555 I print_info: n_merges         = 50009
0.00.038.556 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.556 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.556 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.556 I print_info: LF token         = 128 'Ä'
0.00.038.557 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.557 I print_info: max token length = 1024
0.00.431.727 I load_tensors: offloading 24 repeating layers to GPU
0.00.431.741 I load_tensors: offloading output layer to GPU
0.00.431.741 I load_tensors: offloaded 25/25 layers to GPU
0.00.431.779 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.431.780 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.433.388 I llama_init_from_model: n_seq_max     = 1
0.00.433.397 I llama_init_from_model: n_ctx         = 128
0.00.433.398 I llama_init_from_model: n_ctx_per_seq = 128
0.00.433.398 I llama_init_from_model: n_batch       = 128
0.00.433.399 I llama_init_from_model: n_ubatch      = 128
0.00.433.399 I llama_init_from_model: flash_attn    = 0
0.00.433.401 I llama_init_from_model: freq_base     = 10000.0
0.00.433.405 I llama_init_from_model: freq_scale    = 1
0.00.433.405 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.433.412 I ggml_metal_init: allocating
0.00.433.549 I ggml_metal_init: found device: Apple M4
0.00.433.562 I ggml_metal_init: picking default device: Apple M4
0.00.435.443 I ggml_metal_init: using embedded metal library
0.00.440.839 I ggml_metal_init: GPU name:   Apple M4
0.00.440.843 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.440.844 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.440.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.440.846 I ggml_metal_init: simdgroup reduction   = true
0.00.440.846 I ggml_metal_init: simdgroup matrix mul. = true
0.00.440.847 I ggml_metal_init: has residency sets    = true
0.00.440.847 I ggml_metal_init: has bfloat            = true
0.00.440.847 I ggml_metal_init: use bfloat            = true
0.00.440.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.440.858 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.460.057 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.463.584 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.463.590 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.463.630 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.466.941 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.466.943 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.466.943 I llama_init_from_model: graph nodes  = 967
0.00.466.944 I llama_init_from_model: graph splits = 2
0.00.466.946 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.466.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.239 I 
0.00.494.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.345 I perplexity: tokenizing the input ..
0.00.501.364 I perplexity: tokenization took 7.017 ms
0.00.501.378 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.645.656 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.064 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.078 I llama_perf_context_print:        load time =     485.17 ms
0.00.647.079 I llama_perf_context_print: prompt eval time =     144.05 ms /   128 tokens (    1.13 ms per token,   888.57 tokens per second)
0.00.647.079 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.080 I llama_perf_context_print:       total time =     152.84 ms /   129 tokens
0.00.647.425 I ggml_metal_free: deallocating

real	0m0.661s
user	0m0.077s
sys	0m0.103s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.171 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.881 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.887 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.888 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.889 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.889 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.890 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.890 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.891 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.891 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.892 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.892 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.893 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.144 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.218 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.220 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.220 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.220 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.221 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.221 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.222 I llama_model_loader: - type  f32:  194 tensors
0.00.026.222 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.222 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.223 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.223 I print_info: file format = GGUF V3 (latest)
0.00.026.224 I print_info: file type   = Q4_K - Medium
0.00.026.230 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.718 I load: special tokens cache size = 25
0.00.040.791 I load: token to piece cache size = 0.2984 MB
0.00.040.798 I print_info: arch             = gptneox
0.00.040.798 I print_info: vocab_only       = 0
0.00.040.798 I print_info: n_ctx_train      = 2048
0.00.040.799 I print_info: n_embd           = 2048
0.00.040.799 I print_info: n_layer          = 24
0.00.040.802 I print_info: n_head           = 16
0.00.040.803 I print_info: n_head_kv        = 16
0.00.040.803 I print_info: n_rot            = 32
0.00.040.804 I print_info: n_swa            = 0
0.00.040.804 I print_info: n_embd_head_k    = 128
0.00.040.804 I print_info: n_embd_head_v    = 128
0.00.040.805 I print_info: n_gqa            = 1
0.00.040.805 I print_info: n_embd_k_gqa     = 2048
0.00.040.806 I print_info: n_embd_v_gqa     = 2048
0.00.040.809 I print_info: f_norm_eps       = 1.0e-05
0.00.040.809 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.809 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.809 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.809 I print_info: f_logit_scale    = 0.0e+00
0.00.040.811 I print_info: n_ff             = 8192
0.00.040.811 I print_info: n_expert         = 0
0.00.040.811 I print_info: n_expert_used    = 0
0.00.040.811 I print_info: causal attn      = 1
0.00.040.814 I print_info: pooling type     = 0
0.00.040.815 I print_info: rope type        = 2
0.00.040.815 I print_info: rope scaling     = linear
0.00.040.816 I print_info: freq_base_train  = 10000.0
0.00.040.816 I print_info: freq_scale_train = 1
0.00.040.817 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.817 I print_info: rope_finetuned   = unknown
0.00.040.817 I print_info: ssm_d_conv       = 0
0.00.040.817 I print_info: ssm_d_inner      = 0
0.00.040.817 I print_info: ssm_d_state      = 0
0.00.040.817 I print_info: ssm_dt_rank      = 0
0.00.040.817 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.818 I print_info: model type       = 1.4B
0.00.040.818 I print_info: model params     = 1.41 B
0.00.040.818 I print_info: general.name     = 1.4B
0.00.040.819 I print_info: vocab type       = BPE
0.00.040.819 I print_info: n_vocab          = 50304
0.00.040.819 I print_info: n_merges         = 50009
0.00.040.820 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.820 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.820 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.820 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.820 I print_info: LF token         = 128 'Ä'
0.00.040.821 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.821 I print_info: max token length = 1024
0.00.507.084 I load_tensors: offloading 24 repeating layers to GPU
0.00.507.094 I load_tensors: offloading output layer to GPU
0.00.507.095 I load_tensors: offloaded 25/25 layers to GPU
0.00.507.120 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.507.121 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.508.215 I llama_init_from_model: n_seq_max     = 1
0.00.508.231 I llama_init_from_model: n_ctx         = 2048
0.00.508.232 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.508.232 I llama_init_from_model: n_batch       = 2048
0.00.508.232 I llama_init_from_model: n_ubatch      = 512
0.00.508.233 I llama_init_from_model: flash_attn    = 0
0.00.508.235 I llama_init_from_model: freq_base     = 10000.0
0.00.508.245 I llama_init_from_model: freq_scale    = 1
0.00.508.248 I ggml_metal_init: allocating
0.00.508.323 I ggml_metal_init: found device: Apple M4
0.00.508.337 I ggml_metal_init: picking default device: Apple M4
0.00.510.130 I ggml_metal_init: using embedded metal library
0.00.514.791 I ggml_metal_init: GPU name:   Apple M4
0.00.514.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.514.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.514.799 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.514.799 I ggml_metal_init: simdgroup reduction   = true
0.00.514.800 I ggml_metal_init: simdgroup matrix mul. = true
0.00.514.800 I ggml_metal_init: has residency sets    = true
0.00.514.801 I ggml_metal_init: has bfloat            = true
0.00.514.801 I ggml_metal_init: use bfloat            = true
0.00.514.802 I ggml_metal_init: hasUnifiedMemory      = true
0.00.514.805 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.527.268 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.557.586 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.557.592 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.557.615 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.561.990 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.561.992 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.561.993 I llama_init_from_model: graph nodes  = 967
0.00.561.993 I llama_init_from_model: graph splits = 2
0.00.561.997 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.562.126 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.562.127 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.008 I main: llama threadpool init, n_threads = 4
0.00.615.044 I 
0.00.615.071 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.071 I 
0.00.615.248 I sampler seed: 1234
0.00.615.252 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.615.288 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.615.290 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.615.290 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.375.655 I llama_perf_sampler_print:    sampling time =       1.52 ms /    71 runs   (    0.02 ms per token, 46772.07 tokens per second)
0.01.375.655 I llama_perf_context_print:        load time =     604.95 ms
0.01.375.656 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.59 tokens per second)
0.01.375.657 I llama_perf_context_print:        eval time =     710.70 ms /    63 runs   (   11.28 ms per token,    88.65 tokens per second)
0.01.375.657 I llama_perf_context_print:       total time =     761.53 ms /    70 tokens
0.01.375.898 I ggml_metal_free: deallocating

real	0m1.395s
user	0m0.105s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.112 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.308 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.310 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.311 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.311 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.313 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.313 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.313 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.314 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.314 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.315 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.316 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.317 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.317 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.119 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.166 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.954 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.955 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.955 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.956 I llama_model_loader: - type  f32:  194 tensors
0.00.025.956 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.956 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.957 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.957 I print_info: file format = GGUF V3 (latest)
0.00.025.958 I print_info: file type   = Q4_K - Medium
0.00.025.963 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.264 I load: special tokens cache size = 25
0.00.040.236 I load: token to piece cache size = 0.2984 MB
0.00.040.241 I print_info: arch             = gptneox
0.00.040.241 I print_info: vocab_only       = 0
0.00.040.241 I print_info: n_ctx_train      = 2048
0.00.040.242 I print_info: n_embd           = 2048
0.00.040.243 I print_info: n_layer          = 24
0.00.040.246 I print_info: n_head           = 16
0.00.040.246 I print_info: n_head_kv        = 16
0.00.040.247 I print_info: n_rot            = 32
0.00.040.247 I print_info: n_swa            = 0
0.00.040.247 I print_info: n_embd_head_k    = 128
0.00.040.247 I print_info: n_embd_head_v    = 128
0.00.040.248 I print_info: n_gqa            = 1
0.00.040.249 I print_info: n_embd_k_gqa     = 2048
0.00.040.249 I print_info: n_embd_v_gqa     = 2048
0.00.040.250 I print_info: f_norm_eps       = 1.0e-05
0.00.040.250 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.250 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.251 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.251 I print_info: f_logit_scale    = 0.0e+00
0.00.040.251 I print_info: n_ff             = 8192
0.00.040.252 I print_info: n_expert         = 0
0.00.040.252 I print_info: n_expert_used    = 0
0.00.040.252 I print_info: causal attn      = 1
0.00.040.252 I print_info: pooling type     = 0
0.00.040.252 I print_info: rope type        = 2
0.00.040.252 I print_info: rope scaling     = linear
0.00.040.253 I print_info: freq_base_train  = 10000.0
0.00.040.253 I print_info: freq_scale_train = 1
0.00.040.253 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.254 I print_info: rope_finetuned   = unknown
0.00.040.254 I print_info: ssm_d_conv       = 0
0.00.040.254 I print_info: ssm_d_inner      = 0
0.00.040.256 I print_info: ssm_d_state      = 0
0.00.040.256 I print_info: ssm_dt_rank      = 0
0.00.040.256 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.256 I print_info: model type       = 1.4B
0.00.040.257 I print_info: model params     = 1.41 B
0.00.040.257 I print_info: general.name     = 1.4B
0.00.040.257 I print_info: vocab type       = BPE
0.00.040.257 I print_info: n_vocab          = 50304
0.00.040.257 I print_info: n_merges         = 50009
0.00.040.258 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.258 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.258 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.258 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.262 I print_info: LF token         = 128 'Ä'
0.00.040.262 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.262 I print_info: max token length = 1024
0.00.528.459 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.475 I load_tensors: offloading output layer to GPU
0.00.528.475 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.512 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.513 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.529.909 I llama_init_from_model: n_seq_max     = 1
0.00.529.916 I llama_init_from_model: n_ctx         = 128
0.00.529.916 I llama_init_from_model: n_ctx_per_seq = 128
0.00.529.917 I llama_init_from_model: n_batch       = 128
0.00.529.917 I llama_init_from_model: n_ubatch      = 128
0.00.529.917 I llama_init_from_model: flash_attn    = 0
0.00.529.919 I llama_init_from_model: freq_base     = 10000.0
0.00.529.920 I llama_init_from_model: freq_scale    = 1
0.00.529.920 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.529.923 I ggml_metal_init: allocating
0.00.529.996 I ggml_metal_init: found device: Apple M4
0.00.530.011 I ggml_metal_init: picking default device: Apple M4
0.00.531.758 I ggml_metal_init: using embedded metal library
0.00.538.385 I ggml_metal_init: GPU name:   Apple M4
0.00.538.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.538.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.538.392 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.538.393 I ggml_metal_init: simdgroup reduction   = true
0.00.538.393 I ggml_metal_init: simdgroup matrix mul. = true
0.00.538.393 I ggml_metal_init: has residency sets    = true
0.00.538.393 I ggml_metal_init: has bfloat            = true
0.00.538.394 I ggml_metal_init: use bfloat            = true
0.00.538.395 I ggml_metal_init: hasUnifiedMemory      = true
0.00.538.396 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.274 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.559.864 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.559.871 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.559.910 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.563.135 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.563.137 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.563.138 I llama_init_from_model: graph nodes  = 967
0.00.563.138 I llama_init_from_model: graph splits = 2
0.00.563.141 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.563.141 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.030 I 
0.00.594.111 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.594.131 I perplexity: tokenizing the input ..
0.00.601.461 I perplexity: tokenization took 7.326 ms
0.00.601.484 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.748.767 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.750.166 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.750.179 I llama_perf_context_print:        load time =     583.91 ms
0.00.750.180 I llama_perf_context_print: prompt eval time =     146.36 ms /   128 tokens (    1.14 ms per token,   874.57 tokens per second)
0.00.750.180 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.181 I llama_perf_context_print:       total time =     156.15 ms /   129 tokens
0.00.750.573 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.080s
sys	0m0.132s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.939 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.195 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.027.202 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.209 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.209 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.210 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.212 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.212 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.212 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.213 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.213 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.215 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.072 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.126 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.966 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.968 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.969 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.035.969 I llama_model_loader: - type  f32:  194 tensors
0.00.035.970 I llama_model_loader: - type q5_K:   61 tensors
0.00.035.970 I llama_model_loader: - type q6_K:   37 tensors
0.00.035.971 I print_info: file format = GGUF V3 (latest)
0.00.035.971 I print_info: file type   = Q5_K - Medium
0.00.035.972 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.608 I load: special tokens cache size = 25
0.00.051.310 I load: token to piece cache size = 0.2984 MB
0.00.051.312 I print_info: arch             = gptneox
0.00.051.313 I print_info: vocab_only       = 0
0.00.051.313 I print_info: n_ctx_train      = 2048
0.00.051.313 I print_info: n_embd           = 2048
0.00.051.313 I print_info: n_layer          = 24
0.00.051.316 I print_info: n_head           = 16
0.00.051.317 I print_info: n_head_kv        = 16
0.00.051.318 I print_info: n_rot            = 32
0.00.051.318 I print_info: n_swa            = 0
0.00.051.318 I print_info: n_embd_head_k    = 128
0.00.051.318 I print_info: n_embd_head_v    = 128
0.00.051.319 I print_info: n_gqa            = 1
0.00.051.320 I print_info: n_embd_k_gqa     = 2048
0.00.051.320 I print_info: n_embd_v_gqa     = 2048
0.00.051.321 I print_info: f_norm_eps       = 1.0e-05
0.00.051.321 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.322 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.322 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.322 I print_info: f_logit_scale    = 0.0e+00
0.00.051.322 I print_info: n_ff             = 8192
0.00.051.323 I print_info: n_expert         = 0
0.00.051.323 I print_info: n_expert_used    = 0
0.00.051.323 I print_info: causal attn      = 1
0.00.051.323 I print_info: pooling type     = 0
0.00.051.323 I print_info: rope type        = 2
0.00.051.323 I print_info: rope scaling     = linear
0.00.051.326 I print_info: freq_base_train  = 10000.0
0.00.051.326 I print_info: freq_scale_train = 1
0.00.051.326 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.326 I print_info: rope_finetuned   = unknown
0.00.051.327 I print_info: ssm_d_conv       = 0
0.00.051.327 I print_info: ssm_d_inner      = 0
0.00.051.327 I print_info: ssm_d_state      = 0
0.00.051.327 I print_info: ssm_dt_rank      = 0
0.00.051.327 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.327 I print_info: model type       = 1.4B
0.00.051.328 I print_info: model params     = 1.41 B
0.00.051.328 I print_info: general.name     = 1.4B
0.00.051.330 I print_info: vocab type       = BPE
0.00.051.330 I print_info: n_vocab          = 50304
0.00.051.330 I print_info: n_merges         = 50009
0.00.051.330 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.330 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.331 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.331 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.331 I print_info: LF token         = 128 'Ä'
0.00.051.331 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.332 I print_info: max token length = 1024
0.00.596.458 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.471 I load_tensors: offloading output layer to GPU
0.00.596.472 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.505 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.596.506 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.597.740 I llama_init_from_model: n_seq_max     = 1
0.00.597.746 I llama_init_from_model: n_ctx         = 2048
0.00.597.746 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.597.747 I llama_init_from_model: n_batch       = 2048
0.00.597.747 I llama_init_from_model: n_ubatch      = 512
0.00.597.748 I llama_init_from_model: flash_attn    = 0
0.00.597.749 I llama_init_from_model: freq_base     = 10000.0
0.00.597.749 I llama_init_from_model: freq_scale    = 1
0.00.597.752 I ggml_metal_init: allocating
0.00.597.825 I ggml_metal_init: found device: Apple M4
0.00.597.839 I ggml_metal_init: picking default device: Apple M4
0.00.599.450 I ggml_metal_init: using embedded metal library
0.00.605.848 I ggml_metal_init: GPU name:   Apple M4
0.00.605.851 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.852 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.853 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.854 I ggml_metal_init: simdgroup reduction   = true
0.00.605.854 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.854 I ggml_metal_init: has residency sets    = true
0.00.605.854 I ggml_metal_init: has bfloat            = true
0.00.605.855 I ggml_metal_init: use bfloat            = true
0.00.605.855 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.857 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.350 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.559 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.675.566 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.675.589 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.823 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.679.825 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.679.825 I llama_init_from_model: graph nodes  = 967
0.00.679.826 I llama_init_from_model: graph splits = 2
0.00.679.831 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.679.959 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.679.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.725 I main: llama threadpool init, n_threads = 4
0.00.742.774 I 
0.00.742.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.798 I 
0.00.742.947 I sampler seed: 1234
0.00.742.952 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.021 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.025 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.025 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.596.568 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.596.568 I llama_perf_context_print:        load time =     732.88 ms
0.01.596.571 I llama_perf_context_print: prompt eval time =      51.54 ms /     7 tokens (    7.36 ms per token,   135.83 tokens per second)
0.01.596.572 I llama_perf_context_print:        eval time =     799.12 ms /    63 runs   (   12.68 ms per token,    78.84 tokens per second)
0.01.596.572 I llama_perf_context_print:       total time =     854.75 ms /    70 tokens
0.01.596.818 I ggml_metal_free: deallocating

real	0m1.615s
user	0m0.112s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.267 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.042 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.053 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.054 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.054 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.055 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.056 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.056 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.056 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.057 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.060 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.002 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.943 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.944 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.945 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.945 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.945 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.946 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.946 I llama_model_loader: - type  f32:  194 tensors
0.00.025.947 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.947 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.948 I print_info: file format = GGUF V3 (latest)
0.00.025.948 I print_info: file type   = Q5_K - Medium
0.00.025.949 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.116 I load: special tokens cache size = 25
0.00.040.267 I load: token to piece cache size = 0.2984 MB
0.00.040.272 I print_info: arch             = gptneox
0.00.040.273 I print_info: vocab_only       = 0
0.00.040.273 I print_info: n_ctx_train      = 2048
0.00.040.273 I print_info: n_embd           = 2048
0.00.040.273 I print_info: n_layer          = 24
0.00.040.277 I print_info: n_head           = 16
0.00.040.278 I print_info: n_head_kv        = 16
0.00.040.280 I print_info: n_rot            = 32
0.00.040.280 I print_info: n_swa            = 0
0.00.040.280 I print_info: n_embd_head_k    = 128
0.00.040.280 I print_info: n_embd_head_v    = 128
0.00.040.282 I print_info: n_gqa            = 1
0.00.040.283 I print_info: n_embd_k_gqa     = 2048
0.00.040.283 I print_info: n_embd_v_gqa     = 2048
0.00.040.284 I print_info: f_norm_eps       = 1.0e-05
0.00.040.284 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.284 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.284 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.284 I print_info: f_logit_scale    = 0.0e+00
0.00.040.285 I print_info: n_ff             = 8192
0.00.040.285 I print_info: n_expert         = 0
0.00.040.285 I print_info: n_expert_used    = 0
0.00.040.285 I print_info: causal attn      = 1
0.00.040.285 I print_info: pooling type     = 0
0.00.040.285 I print_info: rope type        = 2
0.00.040.286 I print_info: rope scaling     = linear
0.00.040.288 I print_info: freq_base_train  = 10000.0
0.00.040.289 I print_info: freq_scale_train = 1
0.00.040.289 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.289 I print_info: rope_finetuned   = unknown
0.00.040.289 I print_info: ssm_d_conv       = 0
0.00.040.290 I print_info: ssm_d_inner      = 0
0.00.040.290 I print_info: ssm_d_state      = 0
0.00.040.290 I print_info: ssm_dt_rank      = 0
0.00.040.290 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.290 I print_info: model type       = 1.4B
0.00.040.291 I print_info: model params     = 1.41 B
0.00.040.291 I print_info: general.name     = 1.4B
0.00.040.291 I print_info: vocab type       = BPE
0.00.040.291 I print_info: n_vocab          = 50304
0.00.040.291 I print_info: n_merges         = 50009
0.00.040.292 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.292 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.292 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.292 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.292 I print_info: LF token         = 128 'Ä'
0.00.040.293 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.293 I print_info: max token length = 1024
0.00.567.688 I load_tensors: offloading 24 repeating layers to GPU
0.00.567.700 I load_tensors: offloading output layer to GPU
0.00.567.701 I load_tensors: offloaded 25/25 layers to GPU
0.00.567.735 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.567.736 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.568.769 I llama_init_from_model: n_seq_max     = 1
0.00.568.776 I llama_init_from_model: n_ctx         = 128
0.00.568.776 I llama_init_from_model: n_ctx_per_seq = 128
0.00.568.777 I llama_init_from_model: n_batch       = 128
0.00.568.778 I llama_init_from_model: n_ubatch      = 128
0.00.568.778 I llama_init_from_model: flash_attn    = 0
0.00.568.780 I llama_init_from_model: freq_base     = 10000.0
0.00.568.780 I llama_init_from_model: freq_scale    = 1
0.00.568.781 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.568.783 I ggml_metal_init: allocating
0.00.568.884 I ggml_metal_init: found device: Apple M4
0.00.568.906 I ggml_metal_init: picking default device: Apple M4
0.00.570.623 I ggml_metal_init: using embedded metal library
0.00.577.059 I ggml_metal_init: GPU name:   Apple M4
0.00.577.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.577.065 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.577.066 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.577.067 I ggml_metal_init: simdgroup reduction   = true
0.00.577.067 I ggml_metal_init: simdgroup matrix mul. = true
0.00.577.067 I ggml_metal_init: has residency sets    = true
0.00.577.068 I ggml_metal_init: has bfloat            = true
0.00.577.068 I ggml_metal_init: use bfloat            = true
0.00.577.069 I ggml_metal_init: hasUnifiedMemory      = true
0.00.577.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.593.943 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.597.418 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.597.423 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.597.458 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.600.622 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.600.624 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.600.625 I llama_init_from_model: graph nodes  = 967
0.00.600.625 I llama_init_from_model: graph splits = 2
0.00.600.628 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.600.628 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.439 I 
0.00.633.525 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.547 I perplexity: tokenizing the input ..
0.00.640.123 I perplexity: tokenization took 6.574 ms
0.00.640.150 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.191 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.781.622 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.781.636 I llama_perf_context_print:        load time =     624.16 ms
0.00.781.637 I llama_perf_context_print: prompt eval time =     139.77 ms /   128 tokens (    1.09 ms per token,   915.81 tokens per second)
0.00.781.638 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.638 I llama_perf_context_print:       total time =     148.20 ms /   129 tokens
0.00.781.970 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.077s
sys	0m0.116s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.013.411 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.806 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.021.810 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.812 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.812 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.813 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.813 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.813 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.814 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.814 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.815 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.815 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.815 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.816 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.816 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.817 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.818 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.818 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.131 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.585 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.587 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.587 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.588 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.588 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.588 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.031.589 I llama_model_loader: - type  f32:  194 tensors
0.00.031.589 I llama_model_loader: - type q6_K:   98 tensors
0.00.031.590 I print_info: file format = GGUF V3 (latest)
0.00.031.590 I print_info: file type   = Q6_K
0.00.031.591 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.041.264 I load: special tokens cache size = 25
0.00.049.127 I load: token to piece cache size = 0.2984 MB
0.00.049.131 I print_info: arch             = gptneox
0.00.049.131 I print_info: vocab_only       = 0
0.00.049.131 I print_info: n_ctx_train      = 2048
0.00.049.132 I print_info: n_embd           = 2048
0.00.049.132 I print_info: n_layer          = 24
0.00.049.135 I print_info: n_head           = 16
0.00.049.136 I print_info: n_head_kv        = 16
0.00.049.136 I print_info: n_rot            = 32
0.00.049.136 I print_info: n_swa            = 0
0.00.049.137 I print_info: n_embd_head_k    = 128
0.00.049.137 I print_info: n_embd_head_v    = 128
0.00.049.138 I print_info: n_gqa            = 1
0.00.049.138 I print_info: n_embd_k_gqa     = 2048
0.00.049.139 I print_info: n_embd_v_gqa     = 2048
0.00.049.140 I print_info: f_norm_eps       = 1.0e-05
0.00.049.140 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.140 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.140 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.141 I print_info: f_logit_scale    = 0.0e+00
0.00.049.141 I print_info: n_ff             = 8192
0.00.049.141 I print_info: n_expert         = 0
0.00.049.142 I print_info: n_expert_used    = 0
0.00.049.142 I print_info: causal attn      = 1
0.00.049.142 I print_info: pooling type     = 0
0.00.049.142 I print_info: rope type        = 2
0.00.049.143 I print_info: rope scaling     = linear
0.00.049.146 I print_info: freq_base_train  = 10000.0
0.00.049.146 I print_info: freq_scale_train = 1
0.00.049.147 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.147 I print_info: rope_finetuned   = unknown
0.00.049.147 I print_info: ssm_d_conv       = 0
0.00.049.147 I print_info: ssm_d_inner      = 0
0.00.049.147 I print_info: ssm_d_state      = 0
0.00.049.147 I print_info: ssm_dt_rank      = 0
0.00.049.148 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.148 I print_info: model type       = 1.4B
0.00.049.148 I print_info: model params     = 1.41 B
0.00.049.148 I print_info: general.name     = 1.4B
0.00.049.149 I print_info: vocab type       = BPE
0.00.049.149 I print_info: n_vocab          = 50304
0.00.049.149 I print_info: n_merges         = 50009
0.00.049.150 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.150 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.150 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.150 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.150 I print_info: LF token         = 128 'Ä'
0.00.049.151 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.151 I print_info: max token length = 1024
0.00.656.916 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.932 I load_tensors: offloading output layer to GPU
0.00.656.933 I load_tensors: offloaded 25/25 layers to GPU
0.00.656.968 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.656.969 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.658.359 I llama_init_from_model: n_seq_max     = 1
0.00.658.362 I llama_init_from_model: n_ctx         = 2048
0.00.658.363 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.658.363 I llama_init_from_model: n_batch       = 2048
0.00.658.364 I llama_init_from_model: n_ubatch      = 512
0.00.658.365 I llama_init_from_model: flash_attn    = 0
0.00.658.366 I llama_init_from_model: freq_base     = 10000.0
0.00.658.366 I llama_init_from_model: freq_scale    = 1
0.00.658.367 I ggml_metal_init: allocating
0.00.658.381 I ggml_metal_init: found device: Apple M4
0.00.658.393 I ggml_metal_init: picking default device: Apple M4
0.00.659.785 I ggml_metal_init: using embedded metal library
0.00.666.082 I ggml_metal_init: GPU name:   Apple M4
0.00.666.086 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.087 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.088 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.088 I ggml_metal_init: simdgroup reduction   = true
0.00.666.088 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.089 I ggml_metal_init: has residency sets    = true
0.00.666.089 I ggml_metal_init: has bfloat            = true
0.00.666.089 I ggml_metal_init: use bfloat            = true
0.00.666.090 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.091 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.949 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.741.741 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.741.749 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.741.781 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.746.536 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.746.539 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.746.539 I llama_init_from_model: graph nodes  = 967
0.00.746.539 I llama_init_from_model: graph splits = 2
0.00.746.544 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.746.670 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.746.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.295 I main: llama threadpool init, n_threads = 4
0.00.807.333 I 
0.00.807.354 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.355 I 
0.00.807.470 I sampler seed: 1234
0.00.807.475 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.484 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.485 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.485 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.691.337 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.691.338 I llama_perf_context_print:        load time =     793.02 ms
0.01.691.339 I llama_perf_context_print: prompt eval time =      54.43 ms /     7 tokens (    7.78 ms per token,   128.60 tokens per second)
0.01.691.340 I llama_perf_context_print:        eval time =     826.50 ms /    63 runs   (   13.12 ms per token,    76.22 tokens per second)
0.01.691.341 I llama_perf_context_print:       total time =     884.90 ms /    70 tokens
0.01.691.587 I ggml_metal_free: deallocating

real	0m1.721s
user	0m0.115s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4579 (794fe23f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.464 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.377 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.382 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.383 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.384 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.384 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.385 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.386 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.387 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.387 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.387 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.388 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.388 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.389 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.390 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.390 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.391 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.224 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.998 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.000 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.000 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.000 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.001 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.001 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.002 I llama_model_loader: - type  f32:  194 tensors
0.00.026.002 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.003 I print_info: file format = GGUF V3 (latest)
0.00.026.003 I print_info: file type   = Q6_K
0.00.026.005 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.969 I load: special tokens cache size = 25
0.00.040.026 I load: token to piece cache size = 0.2984 MB
0.00.040.029 I print_info: arch             = gptneox
0.00.040.029 I print_info: vocab_only       = 0
0.00.040.029 I print_info: n_ctx_train      = 2048
0.00.040.030 I print_info: n_embd           = 2048
0.00.040.030 I print_info: n_layer          = 24
0.00.040.033 I print_info: n_head           = 16
0.00.040.034 I print_info: n_head_kv        = 16
0.00.040.034 I print_info: n_rot            = 32
0.00.040.034 I print_info: n_swa            = 0
0.00.040.034 I print_info: n_embd_head_k    = 128
0.00.040.034 I print_info: n_embd_head_v    = 128
0.00.040.035 I print_info: n_gqa            = 1
0.00.040.036 I print_info: n_embd_k_gqa     = 2048
0.00.040.038 I print_info: n_embd_v_gqa     = 2048
0.00.040.039 I print_info: f_norm_eps       = 1.0e-05
0.00.040.039 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.041 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.041 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.041 I print_info: f_logit_scale    = 0.0e+00
0.00.040.042 I print_info: n_ff             = 8192
0.00.040.042 I print_info: n_expert         = 0
0.00.040.042 I print_info: n_expert_used    = 0
0.00.040.042 I print_info: causal attn      = 1
0.00.040.042 I print_info: pooling type     = 0
0.00.040.043 I print_info: rope type        = 2
0.00.040.043 I print_info: rope scaling     = linear
0.00.040.043 I print_info: freq_base_train  = 10000.0
0.00.040.045 I print_info: freq_scale_train = 1
0.00.040.045 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.045 I print_info: rope_finetuned   = unknown
0.00.040.045 I print_info: ssm_d_conv       = 0
0.00.040.046 I print_info: ssm_d_inner      = 0
0.00.040.046 I print_info: ssm_d_state      = 0
0.00.040.046 I print_info: ssm_dt_rank      = 0
0.00.040.046 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.046 I print_info: model type       = 1.4B
0.00.040.047 I print_info: model params     = 1.41 B
0.00.040.047 I print_info: general.name     = 1.4B
0.00.040.047 I print_info: vocab type       = BPE
0.00.040.047 I print_info: n_vocab          = 50304
0.00.040.048 I print_info: n_merges         = 50009
0.00.040.048 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.052 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.052 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.052 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.052 I print_info: LF token         = 128 'Ä'
0.00.040.053 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.053 I print_info: max token length = 1024
0.00.196.338 I load_tensors: offloading 24 repeating layers to GPU
0.00.196.344 I load_tensors: offloading output layer to GPU
0.00.196.345 I load_tensors: offloaded 25/25 layers to GPU
0.00.196.371 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.196.374 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.197.209 I llama_init_from_model: n_seq_max     = 1
0.00.197.211 I llama_init_from_model: n_ctx         = 128
0.00.197.212 I llama_init_from_model: n_ctx_per_seq = 128
0.00.197.212 I llama_init_from_model: n_batch       = 128
0.00.197.212 I llama_init_from_model: n_ubatch      = 128
0.00.197.212 I llama_init_from_model: flash_attn    = 0
0.00.197.213 I llama_init_from_model: freq_base     = 10000.0
0.00.197.214 I llama_init_from_model: freq_scale    = 1
0.00.197.214 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.197.215 I ggml_metal_init: allocating
0.00.197.241 I ggml_metal_init: found device: Apple M4
0.00.197.249 I ggml_metal_init: picking default device: Apple M4
0.00.198.206 I ggml_metal_init: using embedded metal library
0.00.203.063 I ggml_metal_init: GPU name:   Apple M4
0.00.203.066 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.203.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.203.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.203.068 I ggml_metal_init: simdgroup reduction   = true
0.00.203.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.203.069 I ggml_metal_init: has residency sets    = true
0.00.203.069 I ggml_metal_init: has bfloat            = true
0.00.203.069 I ggml_metal_init: use bfloat            = true
0.00.203.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.203.071 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.217.132 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.219.153 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.219.156 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.219.173 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.221.174 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.221.176 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.221.176 I llama_init_from_model: graph nodes  = 967
0.00.221.177 I llama_init_from_model: graph splits = 2
0.00.221.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.221.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.252.791 I 
0.00.252.835 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.252.847 I perplexity: tokenizing the input ..
0.00.258.095 I perplexity: tokenization took 5.247 ms
0.00.258.106 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.397.345 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.398.681 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.398.693 I llama_perf_context_print:        load time =     242.32 ms
0.00.398.694 I llama_perf_context_print: prompt eval time =     139.01 ms /   128 tokens (    1.09 ms per token,   920.81 tokens per second)
0.00.398.694 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.398.695 I llama_perf_context_print:       total time =     145.90 ms /   129 tokens
0.00.399.100 I ggml_metal_free: deallocating

real	0m0.415s
user	0m0.071s
sys	0m0.075s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4579 (794fe23f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f107860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f107f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f108530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f108ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f109090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f109640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f109bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f10a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f10a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f10ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f10b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f10b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f10c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f10c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f10d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f10d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f10df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f10e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f10edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f10f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f10fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f1103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f110ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f111380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f111aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f111d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f112370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f112fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f113520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f1137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f113c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f113f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f1147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f114d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f114fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f115470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f115910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f115db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f116250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f1166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f116b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f117030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f1174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f117970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f117c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f118240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f118850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f119170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f119780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f119d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f11a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f11a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f11afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f11b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f11bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f11c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f11c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f11c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f11cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f11d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f11da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f11df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f11e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f11e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f11ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f11f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f11f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f11fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f11ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f120420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f1208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f120d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f121200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f121750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f121ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f1221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f122740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f122c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f1231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f123730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f123c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f1241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f124720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f124c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f1251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f125710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f125c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f1261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f126700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f126c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f1271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f1276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f127c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f128190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f1286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f128c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f129180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f118e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f1295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f129da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f12a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f12a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f12ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f12b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f12b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f12bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f12c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f12c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f12cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f12d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f12d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f12dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f12e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f12e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f12ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f12f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f12f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f12f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f12fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f130310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f1307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f130c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f1310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f131590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f131a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f131ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f132370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f132810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f132cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f133150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f1335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f133a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f133f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f1343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f134870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f134d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f1351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f135650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f135af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f135f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f136430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f1368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f136d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f137210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f1376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f137b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f137ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f138490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f138930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f138dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f139270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f139710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f139bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f13a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f13a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f13a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f13ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f13b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f13b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f13bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f13c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f13c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f13c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f13ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f13d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f13d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f13dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f13e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f13e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f13ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f13eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f13f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f13f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f13fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f140170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f140610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f140ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f140f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f1413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f141890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f141d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f1421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f142670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f142b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f142fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f143450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f1438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f143d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f144230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f1446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f144b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f145010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f1454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f145a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f145f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f1464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f1469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f146cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f1472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f1478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f147ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f1486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f148b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f148e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f149440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f149a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f14a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f14a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f14ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f14b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f14b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f14bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f14c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f14c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f14cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f14d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f14d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f14dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f14e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f14e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f14ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f14f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f14f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f14fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f150230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f150780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f150cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f151220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f151770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f151cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f152210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f152760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f152cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f153200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f153750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f153ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f1541f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f154740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f154c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f1551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f155730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f155c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f1561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f156720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f156c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f1571c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f157710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f157c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f1581b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f158700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f158c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f1591a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f1596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f159c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f15a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f15a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f15ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f15b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f15b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f15bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f15c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f15c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f15cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f15d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f15d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f15dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f15e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f15e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f15ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f15ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f15f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f15f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f15fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f1601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f160650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f160af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f160f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f161430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f1618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f161d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f162210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f1626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f162c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f163320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f163a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f164160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f164880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f164b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f165330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f1655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f165c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.694.425 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.694.430 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10e704b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10e704f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10e705400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10e705870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10e705ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10e706150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10e7065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10e706a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10e706ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10e707310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10e707780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10e707e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10e708990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10e709140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10e709950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10e70a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10e70a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10e70aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10e70b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10e70bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10e70c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10e70cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10e70d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10e70d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10e70e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10e70e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10e70e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10e70ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10e70ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10e70f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10e70f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10e70fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10e710180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10e710440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10e7108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10e710d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10e711190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10e711600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10e711a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10e711ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10e712350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10e7127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10e712c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10e7130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10e713510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10e713980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10e713df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10e714260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10e7146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10e714b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10e714fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10e715420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10e715890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10e715d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10e716170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10e7165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10e716b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10e717050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10e7174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10e717930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10e717da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10e718210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10e718680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10e718af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10e718f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10e7193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10e719840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10e719cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10e71a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10e71a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10e71aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10e71ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10e71b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10e71b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10e71bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10e71c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10e71c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10e71c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10e71cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10e71d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10e71d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10e71dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10e71df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10e71e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10e71e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10e71ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10e71f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10e71f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10e71f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10e71fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10e7202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10e720730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10e720ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10e721010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10e721480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10e7218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10e721d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10e7221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10e722640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10e722ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10e722f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10e723390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10e723800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10e723c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10e7240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10e724550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10e7249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10e724e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10e7252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10e725710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10e725b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10e725ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10e726460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10e7268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10e726d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10e7271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10e727620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10e727a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10e727f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10e728370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10e7287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10e728c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10e7290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10e729530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10e7299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10e729e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10e72a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10e72a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10e72ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10e72afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10e72b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10e72b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10e72bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10e72c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10e72c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10e72ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10e72cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10e72d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10e72d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10e72dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10e72e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10e72e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10e72e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10e72edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10e72f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10e72f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10e72fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10e72ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10e730420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10e730890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10e730d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10e731170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10e7315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10e731a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10e731ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10e732330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10e7327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10e732c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10e733080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10e7334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10e733960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10e733dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10e734240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10e7346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10e734b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10e734f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10e735bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10e735e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10e736140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10e7365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10e736a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10e736e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10e737300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10e737770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10e737be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10e738050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10e7384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10e738930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10e738da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10e739210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10e739680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10e739af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10e739f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10e73a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10e73a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10e73acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10e73b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10e73b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10e73ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10e73be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10e73c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10e73c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10e73cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10e73d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10e73d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10e73d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10e73dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10e73e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10e73e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10e73ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10e73ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10e73f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10e73f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10e73fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10e740290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10e740700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10e740b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10e740fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10e741500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10e741a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10e742580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10e742840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10e742e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10e7433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10e743980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10e743f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10e744500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10e744ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10e745080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10e745640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10e745c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10e7461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10e746780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10e746d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10e747300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10e7478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10e747e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10e748440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10e748a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10e748fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10e749580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10e749b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10e74a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10e74a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10e74ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10e74b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10e74b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10e74bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10e74c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10e74c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10e74cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10e74d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10e74da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10e74e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10e74e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10e74ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10e74f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10e74f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10e74fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10e7502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10e750880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10e750e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10e751400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10e7519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10e751f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10e752540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10e752b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10e7530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10e753680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10e753c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10e754200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10e7547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10e754d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10e755340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10e755900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10e755ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10e756480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10e756a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10e756f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10e757440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10e757940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10e757e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10e758340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10e758840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10e758d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10e759240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10e759740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10e759c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10e75a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10e75a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10e75ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10e75b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10e75b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10e75bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10e75c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10e75cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10e75d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10e75d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10e75df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10e75e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10e75e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10e75b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10e74c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10e74b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10e748140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10e745900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10e755040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10e752800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10e750580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10e74e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10e746480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10e743c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10e748cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10e749e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10e74f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10e74c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10e753f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10e747b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10e751100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10e74a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10e74cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10e7475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10e755600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10e7447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10e7430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10e745340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10e755bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10e74af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10e753380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10e749280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10e74bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10e74fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10e747000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10e74ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10e7516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10e745ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10e7544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10e751c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10e74d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10e756740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10e744d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10e756180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10e744200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10e754a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10e74e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10e750b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10e753940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10e752240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10e74a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10e741cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10e704680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10e75da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10e70b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10e75ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10e75f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10e75f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10e75f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10e75fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10e75fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10e75ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10e760250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10e760510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10e7607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10e760a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10e760d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10e761010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10e7612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10e761590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10e761850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10e761b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10e761dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10e762090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10e762350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10e762610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10e7628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10e762b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10e762e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10e763110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10e7633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10e763690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10e763950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10e763c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10e763ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10e764190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10e764450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10e764710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10e7649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10e764c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10e764f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10e765210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10e7654d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10e765790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10e765a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10e765d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10e765fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10e766290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10e766550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10e766810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10e766ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10e766d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10e767050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10e767310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10e7675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10e767890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10e767b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10e767e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10e7680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10e768390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10e768650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10e768910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10e768bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10e768e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10e769150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10e769410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10e7696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10e769990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10e769c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10e769f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10e76a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10e76a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10e76a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10e76aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10e76acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10e76af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10e76b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10e76b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10e76b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10e76ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10e76bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10e76c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10e76c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10e76c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10e76c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10e76cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10e76cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10e76d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10e76d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10e76d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10e76d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10e76db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10e76de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10e76e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10e76e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10e76e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10e76e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10e76ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10e76eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10e76f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10e76f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10e76f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10e76f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10e76fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10e76ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10e770210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10e7704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10e770790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10e770a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10e770d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10e770fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10e771290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10e771550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10e771810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10e771ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10e771d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10e772050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10e772310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10e7725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10e772890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10e772b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10e772e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10e7730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10e773390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10e773650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10e773910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10e773bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10e773e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10e774150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10e774410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10e7746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10e774990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10e774c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10e774f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10e7751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10e775490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10e775750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10e775a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10e775cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10e775f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10e776250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10e776510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10e7767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10e776a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10e776d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10e777010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10e7772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10e777590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10e777850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10e777b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10e777dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10e778090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10e778350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10e778610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10e7788d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10e778b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10e778e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10e779110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10e7793d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10e779690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10e779950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10e779c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10e779ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10e77a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10e77a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10e77aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10e77ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10e77afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10e77b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10e77ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10e77bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10e77c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10e77ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10e77cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10e77d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10e77da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10e77df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10e77e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10e77ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10e77ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10e77f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10e77fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10e77ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10e7804a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10e7809f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10e780f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10e781490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10e7819e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10e781f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10e782480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10e7829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10e782f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10e783470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10e7839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10e783f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10e784460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10e7849b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10e784f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10e785450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10e7859a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10e785ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10e786440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10e786990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10e786ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10e787430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10e787980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10e787ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10e788420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10e788970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10e788ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10e789410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10e789960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10e789eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10e78a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10e78a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10e78aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10e78b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10e78b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10e78be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10e78c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10e78c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10e78c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10e78cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10e78d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10e78d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10e78d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10e78dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10e78e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10e78e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10e78eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10e78efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10e78f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10e78f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10e78fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10e790160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10e7905d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10e790a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10e791730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10e791e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10e792570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10e792830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10e792ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10e7932a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10e7938b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.750s
user	0m0.280s
sys	0m0.320s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4579 (794fe23f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157e085a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157e08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157e09260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157e09810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157e09dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157e0a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157e0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157e0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157e0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157e0b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157e0be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157e0c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157e0cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157e0d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157e0de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157e0e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157e0eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157e0f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157e0fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157e102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157e109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157e110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157e11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157e120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157e127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157e12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157e130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157e13d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157e14250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157e14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157e149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157e14c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157e15500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157e15a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157e15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157e161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157e16640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157e16ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157e16f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157e17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157e178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157e17d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157e18200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157e186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157e18960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157e18f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157e19580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157e19ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157e1a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157e1aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157e1b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157e1b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157e1bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157e1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157e1caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157e1cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157e1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157e1d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157e1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157e1e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157e1e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157e1ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157e1f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157e1f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157e1fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157e1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157e20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157e20810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157e20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157e21150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157e215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157e21a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157e21f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157e22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157e229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157e22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157e23470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157e239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157e23f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157e24460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157e249b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157e24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157e25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157e259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157e25ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157e26440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157e26990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157e26ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157e27430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157e27980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157e27ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157e28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157e28970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157e28ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157e29410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157e29960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157e29eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157e19b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157e2a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157e2aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157e2b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157e2b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157e2bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157e2c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157e2c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157e2cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157e2d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157e2d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157e2daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157e2dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157e2e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157e2ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157e2efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157e2f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157e2f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157e2fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157e30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157e30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157e30ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157e31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157e314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157e31980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157e31e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157e322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157e32760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157e32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157e330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157e33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157e339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157e33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157e34320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157e347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157e34c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157e35100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157e355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157e35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157e35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157e36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157e36820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157e36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157e37160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157e37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157e37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157e37f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157e383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157e38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157e38d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157e391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157e39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157e39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157e39fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157e3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157e3a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157e3ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157e3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157e3b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157e3bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157e3c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157e3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157e3c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157e3cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157e3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157e3d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157e3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157e3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157e3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157e3e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157e3ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157e3f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157e3f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157e3fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157e400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157e40560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157e40a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157e40ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157e41340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157e417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157e41c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157e42120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157e425c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157e42a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157e42f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157e433a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157e43840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157e43ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157e44180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157e44620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157e44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157e44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157e45400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157e458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157e45d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157e461e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157e46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157e46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157e471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157e47720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157e479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157e47ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157e48600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157e48c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157e49400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157e498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157e49b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157e4a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157e4a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157e4af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157e4b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157e4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157e4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157e4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157e4ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157e4cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157e4d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157e4da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157e4df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157e4e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157e4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157e4ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157e4f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157e4fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157e4ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157e504c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157e50a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157e50f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157e514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157e51a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157e51f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157e524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157e529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157e52f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157e53490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157e539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157e53f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157e54480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157e549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157e54f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157e55470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157e559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157e55f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157e56460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157e569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157e56f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157e57450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157e579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157e57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157e58440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157e58990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157e58ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157e59430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157e59980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157e59ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157e5a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157e5a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157e5aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157e5b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157e5b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157e5beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157e5c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157e5c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157e5cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157e5d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157e5d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157e5de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157e5e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157e5e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157e5ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157e5f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157e5f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157e5fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157e60100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157e605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157e60a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157e60ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157e61380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157e61820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157e61cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157e62160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157e62600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157e62aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157e62f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157e633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157e63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157e64050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157e64770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157e64e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157e655b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157e65870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157e66060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157e66320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157e66930 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.098.128 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.133 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157e665e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157e482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157e47ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157e488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157e1b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157e1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157e1d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157e4a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157e12d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157e19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157e1a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157e1a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157e18c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157e1ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157e11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157e1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157e2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157e65b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157e14f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157e151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157e4aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157e48ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157e13360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157e13620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157e138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157e66d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157e67050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157e67310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157e675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157e67890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157e67b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157e67e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157e680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157e68390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157e68650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157e68910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157e68bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157e68e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157e69150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157e69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157e696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157e69990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157e69c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157e69f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157e6a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157e6a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157e6a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157e6aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157e6acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157e6af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157e6b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157e6b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157e6b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157e6ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157e6bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157e6c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157e6c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157e6c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157e6c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157e6cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157e6cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157e6d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157e6d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157e6d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157e6d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157e6db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157e6de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157e6e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157e6e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157e6e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157e6e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157e6ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157e6eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157e6f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157e6f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157e6f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157e6f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157e6fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157e6ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157e70210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157e704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157e70790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157e70a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157e70d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157e70fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157e71290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157e71550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157e71810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157e71ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157e71d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157e72050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157e72310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157e725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157e72890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157e72b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157e72e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157e730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157e73390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157e73650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157e73910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157e73bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157e73e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157e74150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157e74410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157e746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157e74990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157e74c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157e74f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157e751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157e75490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157e75750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157e75a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157e75cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157e75f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157e76250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157e76510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157e767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157e76a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157e76d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157e77010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157e772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157e77590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157e77850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157e77b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157e77dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157e78090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157e78350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157e78610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157e788d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157e78b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157e78e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157e79110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157e793d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157e79690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157e79950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157e79c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157e79ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157e7a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157e7a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157e7a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157e7a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157e7ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157e7af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157e7b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157e7b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157e7b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157e7ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157e7bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157e7bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157e7c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157e7c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157e7c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157e7cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157e7cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157e7d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157e7d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157e7d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157e7d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157e7db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157e7de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157e7e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157e7e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157e7e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157e7e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157e7ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157e7ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157e7f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157e7f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157e7f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157e7f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157e7fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157e7ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157e801d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157e80490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157e80750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157e80a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157e80cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157e80f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157e81250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157e81510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157e817d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157e81a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157e81d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157e82010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157e822d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157e82590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157e82850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157e82b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157e82dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157e83090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157e83350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157e83610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157e838d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157e83b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157e83e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157e84110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157e843d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157e84690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157e84950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157e84c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157e84ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157e85190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157e85450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157e85710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157e859d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157e85f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157e861d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157e86670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157e86b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157e86fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157e87760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157e87a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157e87ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157e88150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157e885c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157e88a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157e88ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157e89310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157e89780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157e89bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157e8a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157e8a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157e8a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157e8adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157e8b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157e8b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157e8bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157e8bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157e8c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157e8c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157e8ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157e8d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157e8d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157e8da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157e8de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157e8e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157e8e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157e8ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157e8f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157e8f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157e8f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157e8fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157e90200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157e90670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157e90ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157e90f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157e913c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157e91830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157e91ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157e92110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157e92580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157e929f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157e92e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157e932d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157e93740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157e93bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157e94020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157e94490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157e94900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157e94d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157e951e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157e95650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157e95ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157e95f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157e963a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157e96810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157e96c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157e970f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157e97560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157e979d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157e97e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157e982b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157e98720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157e98b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157e99000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157e99470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157e998e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157e99d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157e9a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157e9a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157e9aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157e9af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157e9b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157e9bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157e9c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157e9cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157e9d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157e9d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157e9de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157e9e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157e9e6d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147e07130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147e075a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147e07a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147e07e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147e082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147e08760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147e08bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147e09040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147e094b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147e09920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147e09d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147e0a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147e0afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147e0b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147e0bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147e0c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147e0cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147e0d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147e0dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147e0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147e0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147e0f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147e0f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147e0ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147e106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147e109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147e10c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147e110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147e11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147e119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147e11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147e12360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147e127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147e12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147e12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147e13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147e137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147e13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147e140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147e14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147e149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147e14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147e15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147e156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147e15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147e15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147e16440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147e168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147e16d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147e17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147e17600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147e17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147e17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147e18350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147e187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147e18c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147e191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147e196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147e19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147e19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147e1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147e1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147e1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147e1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147e1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147e1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147e1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147e1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147e1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147e1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147e1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147e1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147e1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147e1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147e1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147e1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147e1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147e1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147e1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147e1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147e1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147e20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147e20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147e20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147e20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147e212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147e21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147e21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147e22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147e224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147e22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147e22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147e231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147e23660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147e23ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147e23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147e243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147e24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147e24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147e25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147e25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147e259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147e25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147e266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147e269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147e26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147e27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147e276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147e27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147e27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147e28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147e288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147e28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147e29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147e29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147e29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147e29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147e2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147e2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147e2ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147e2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147e2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147e2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147e2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147e2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147e2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147e2cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147e2cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147e2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147e2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147e2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147e2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147e2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147e2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147e2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147e2f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147e2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147e2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147e30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147e304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147e30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147e30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147e31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147e316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147e31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147e31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147e32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147e32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147e32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147e33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147e335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147e33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147e33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147e34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147e34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147e34bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147e35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147e354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147e35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147e35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147e36220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147e36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147e36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147e36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147e373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147e37850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147e37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147e38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147e385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147e38a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147e38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147e392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147e39760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147e39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147e3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147e3a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147e3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147e3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147e3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147e3b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147e3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147e3bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147e3c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147e3c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147e3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147e3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147e3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147e3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147e3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147e3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147e3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147e3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147e3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147e3f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147e3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147e3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147e401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147e40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147e40ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147e40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147e413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147e41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147e41c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147e420f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147e42560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147e429d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147e42e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147e432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147e43720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147e43b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147e44710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147e449d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147e44c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147e45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147e45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147e459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147e45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147e462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147e46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147e46ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147e47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147e47480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147e478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147e47d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147e481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147e48640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147e48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147e48f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147e49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147e49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147e49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147e4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147e4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147e4a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147e4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147e4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147e4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147e4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147e4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147e4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147e4c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147e4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147e4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147e4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147e4da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147e4df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147e4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147e4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147e4ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147e4f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147e4f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147e4f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147e4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147e50280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147e506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147e50b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147e50fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147e51440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147e518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147e51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147e52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147e52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147e52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147e52ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147e53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147e537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147e53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147e540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147e54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147e54980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147e54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147e55260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147e556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147e55b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147e55fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147e56420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147e56890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147e56d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147e57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147e575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147e57a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147e57ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147e58330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147e58da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147e594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147e59be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147e5a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147e5a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147e5aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147e5b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147e5b640 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.960s
user	0m0.235s
sys	0m0.186s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.43 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    1.78 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.22 sec*proc (2 tests)

Total Test time (real) =   2.23 sec
        2.25 real         0.52 user         0.27 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.23 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.13 user         0.08 sys
```
