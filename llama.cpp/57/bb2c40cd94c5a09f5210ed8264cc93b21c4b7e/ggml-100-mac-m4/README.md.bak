### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.40 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.26 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.26 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.23 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.58 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  177.40 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.31 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 220.89 sec*proc (28 tests)

Total Test time (real) = 220.90 sec

real	3m40.928s
user	7m29.555s
sys	0m6.440s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.34 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.29 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.39 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.12 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.43 sec*proc (28 tests)

Total Test time (real) =  51.44 sec

real	0m51.449s
user	1m11.519s
sys	0m5.736s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.110 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.623 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.543 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.550 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.553 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.554 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.554 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.555 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.556 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.557 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.558 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.559 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.559 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.562 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.565 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.566 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.567 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.567 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.568 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.568 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.570 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.521 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.031.742 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.744 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.031.744 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.031.745 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.031.745 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.031.746 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.031.746 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.031.747 I llama_model_loader: - type  f32:  124 tensors
0.00.031.747 I llama_model_loader: - type  f16:   73 tensors
0.00.036.360 I llm_load_vocab: special tokens cache size = 5
0.00.038.834 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.038.838 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.038.839 I llm_load_print_meta: arch             = bert
0.00.038.839 I llm_load_print_meta: vocab type       = WPM
0.00.038.839 I llm_load_print_meta: n_vocab          = 30522
0.00.038.840 I llm_load_print_meta: n_merges         = 0
0.00.038.840 I llm_load_print_meta: vocab_only       = 0
0.00.038.840 I llm_load_print_meta: n_ctx_train      = 512
0.00.038.840 I llm_load_print_meta: n_embd           = 384
0.00.038.841 I llm_load_print_meta: n_layer          = 12
0.00.038.844 I llm_load_print_meta: n_head           = 12
0.00.038.845 I llm_load_print_meta: n_head_kv        = 12
0.00.038.845 I llm_load_print_meta: n_rot            = 32
0.00.038.848 I llm_load_print_meta: n_swa            = 0
0.00.038.848 I llm_load_print_meta: n_embd_head_k    = 32
0.00.038.848 I llm_load_print_meta: n_embd_head_v    = 32
0.00.038.849 I llm_load_print_meta: n_gqa            = 1
0.00.038.850 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.038.851 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.038.852 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.038.852 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.038.852 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.038.853 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.038.853 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.038.854 I llm_load_print_meta: n_ff             = 1536
0.00.038.854 I llm_load_print_meta: n_expert         = 0
0.00.038.854 I llm_load_print_meta: n_expert_used    = 0
0.00.038.855 I llm_load_print_meta: causal attn      = 0
0.00.038.855 I llm_load_print_meta: pooling type     = 2
0.00.038.855 I llm_load_print_meta: rope type        = 2
0.00.038.855 I llm_load_print_meta: rope scaling     = linear
0.00.038.856 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.038.857 I llm_load_print_meta: freq_scale_train = 1
0.00.038.857 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.038.857 I llm_load_print_meta: rope_finetuned   = unknown
0.00.038.857 I llm_load_print_meta: ssm_d_conv       = 0
0.00.038.857 I llm_load_print_meta: ssm_d_inner      = 0
0.00.038.859 I llm_load_print_meta: ssm_d_state      = 0
0.00.038.859 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.038.861 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.038.861 I llm_load_print_meta: model type       = 33M
0.00.038.862 I llm_load_print_meta: model ftype      = F16
0.00.038.862 I llm_load_print_meta: model params     = 33.21 M
0.00.038.863 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.038.864 I llm_load_print_meta: general.name     = Bge Small
0.00.038.865 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.038.865 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.038.865 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.038.865 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.038.866 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.038.866 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.038.866 I llm_load_print_meta: max token length = 21
0.00.040.942 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.040.943 I llm_load_tensors: offloading output layer to GPU
0.00.040.944 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.040.969 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.971 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.578 I llama_new_context_with_model: n_seq_max     = 1
0.00.041.580 I llama_new_context_with_model: n_ctx         = 512
0.00.041.580 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.041.580 I llama_new_context_with_model: n_batch       = 2048
0.00.041.580 I llama_new_context_with_model: n_ubatch      = 2048
0.00.041.581 I llama_new_context_with_model: flash_attn    = 0
0.00.041.581 I llama_new_context_with_model: freq_base     = 10000.0
0.00.041.582 I llama_new_context_with_model: freq_scale    = 1
0.00.041.583 I ggml_metal_init: allocating
0.00.041.595 I ggml_metal_init: found device: Apple M4
0.00.041.602 I ggml_metal_init: picking default device: Apple M4
0.00.042.534 I ggml_metal_init: using embedded metal library
0.00.047.035 I ggml_metal_init: GPU name:   Apple M4
0.00.047.038 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.047.039 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.047.039 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.047.040 I ggml_metal_init: simdgroup reduction   = true
0.00.047.040 I ggml_metal_init: simdgroup matrix mul. = true
0.00.047.040 I ggml_metal_init: has bfloat            = true
0.00.047.040 I ggml_metal_init: use bfloat            = true
0.00.047.041 I ggml_metal_init: hasUnifiedMemory      = true
0.00.047.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.059.978 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.060.688 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.060.690 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.060.693 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.061.537 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.061.538 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.061.539 I llama_new_context_with_model: graph nodes  = 429
0.00.061.539 I llama_new_context_with_model: graph splits = 2
0.00.061.562 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.061.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.067.932 I 
0.00.067.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.068.618 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.073.417 I llama_perf_context_print:        load time =      47.30 ms
0.00.073.418 I llama_perf_context_print: prompt eval time =       4.65 ms /     9 tokens (    0.52 ms per token,  1937.57 tokens per second)
0.00.073.419 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.073.420 I llama_perf_context_print:       total time =       5.48 ms /    10 tokens
0.00.073.539 I ggml_metal_free: deallocating

real	0m0.258s
user	0m0.051s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.421 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.556 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.560 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.561 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.562 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.562 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.563 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.564 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.564 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.564 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.565 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.565 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.569 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.569 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.573 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.574 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.574 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.574 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.574 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.322 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.051 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.052 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.052 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.053 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.053 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.053 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.053 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.054 I llama_model_loader: - type  f32:  124 tensors
0.00.015.054 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.714 I llm_load_vocab: special tokens cache size = 5
0.00.018.998 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.019.001 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.019.001 I llm_load_print_meta: arch             = bert
0.00.019.002 I llm_load_print_meta: vocab type       = WPM
0.00.019.002 I llm_load_print_meta: n_vocab          = 30522
0.00.019.002 I llm_load_print_meta: n_merges         = 0
0.00.019.002 I llm_load_print_meta: vocab_only       = 0
0.00.019.003 I llm_load_print_meta: n_ctx_train      = 512
0.00.019.003 I llm_load_print_meta: n_embd           = 384
0.00.019.003 I llm_load_print_meta: n_layer          = 12
0.00.019.006 I llm_load_print_meta: n_head           = 12
0.00.019.006 I llm_load_print_meta: n_head_kv        = 12
0.00.019.006 I llm_load_print_meta: n_rot            = 32
0.00.019.006 I llm_load_print_meta: n_swa            = 0
0.00.019.007 I llm_load_print_meta: n_embd_head_k    = 32
0.00.019.007 I llm_load_print_meta: n_embd_head_v    = 32
0.00.019.008 I llm_load_print_meta: n_gqa            = 1
0.00.019.009 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.019.009 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.019.010 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.019.010 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.019.011 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.019.011 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.019.011 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.019.011 I llm_load_print_meta: n_ff             = 1536
0.00.019.011 I llm_load_print_meta: n_expert         = 0
0.00.019.012 I llm_load_print_meta: n_expert_used    = 0
0.00.019.012 I llm_load_print_meta: causal attn      = 0
0.00.019.012 I llm_load_print_meta: pooling type     = 2
0.00.019.012 I llm_load_print_meta: rope type        = 2
0.00.019.012 I llm_load_print_meta: rope scaling     = linear
0.00.019.013 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.019.013 I llm_load_print_meta: freq_scale_train = 1
0.00.019.015 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.019.015 I llm_load_print_meta: rope_finetuned   = unknown
0.00.019.015 I llm_load_print_meta: ssm_d_conv       = 0
0.00.019.016 I llm_load_print_meta: ssm_d_inner      = 0
0.00.019.016 I llm_load_print_meta: ssm_d_state      = 0
0.00.019.016 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.019.016 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.019.016 I llm_load_print_meta: model type       = 33M
0.00.019.016 I llm_load_print_meta: model ftype      = Q8_0
0.00.019.017 I llm_load_print_meta: model params     = 33.21 M
0.00.019.017 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.019.018 I llm_load_print_meta: general.name     = Bge Small
0.00.019.018 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.019.018 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.019.018 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.019.018 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.019.018 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.019.019 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.019.019 I llm_load_print_meta: max token length = 21
0.00.020.291 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.292 I llm_load_tensors: offloading output layer to GPU
0.00.020.292 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.297 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.298 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.652 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.653 I llama_new_context_with_model: n_ctx         = 512
0.00.020.653 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.653 I llama_new_context_with_model: n_batch       = 2048
0.00.020.654 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.654 I llama_new_context_with_model: flash_attn    = 0
0.00.020.654 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.655 I llama_new_context_with_model: freq_scale    = 1
0.00.020.655 I ggml_metal_init: allocating
0.00.020.658 I ggml_metal_init: found device: Apple M4
0.00.020.660 I ggml_metal_init: picking default device: Apple M4
0.00.021.277 I ggml_metal_init: using embedded metal library
0.00.023.817 I ggml_metal_init: GPU name:   Apple M4
0.00.023.819 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.819 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.819 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.820 I ggml_metal_init: simdgroup reduction   = true
0.00.023.820 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.820 I ggml_metal_init: has bfloat            = true
0.00.023.820 I ggml_metal_init: use bfloat            = true
0.00.023.820 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.821 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.892 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.034.364 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.366 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.367 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.992 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.993 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.993 I llama_new_context_with_model: graph nodes  = 429
0.00.034.994 I llama_new_context_with_model: graph splits = 2
0.00.035.007 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.008 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.187 I 
0.00.040.214 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.786 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.243 I llama_perf_context_print:        load time =      30.76 ms
0.00.045.244 I llama_perf_context_print: prompt eval time =       4.32 ms /     9 tokens (    0.48 ms per token,  2082.37 tokens per second)
0.00.045.245 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.245 I llama_perf_context_print:       total time =       5.06 ms /    10 tokens
0.00.045.449 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.088 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.040 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.556 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.560 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.562 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.024.563 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.569 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.024.569 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.024.570 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.024.571 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.024.571 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.024.572 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.024.574 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.024.574 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.024.580 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.581 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.581 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.024.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.582 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.029.877 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.031.452 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.988 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.034.990 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.990 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.034.990 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.034.991 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.034.991 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.034.991 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.034.991 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.034.992 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.034.992 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.034.992 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.034.992 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.034.993 I llama_model_loader: - type  f32:   40 tensors
0.00.034.993 I llama_model_loader: - type  f16:   30 tensors
0.00.049.041 W llm_load_vocab: empty token at index 5
0.00.052.987 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.054.180 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.054.213 I llm_load_vocab: special tokens cache size = 5
0.00.337.425 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.337.432 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.337.433 I llm_load_print_meta: arch             = jina-bert-v2
0.00.337.433 I llm_load_print_meta: vocab type       = BPE
0.00.337.433 I llm_load_print_meta: n_vocab          = 61056
0.00.337.433 I llm_load_print_meta: n_merges         = 39382
0.00.337.434 I llm_load_print_meta: vocab_only       = 0
0.00.337.434 I llm_load_print_meta: n_ctx_train      = 8192
0.00.337.434 I llm_load_print_meta: n_embd           = 384
0.00.337.436 I llm_load_print_meta: n_layer          = 4
0.00.337.439 I llm_load_print_meta: n_head           = 12
0.00.337.440 I llm_load_print_meta: n_head_kv        = 12
0.00.337.440 I llm_load_print_meta: n_rot            = 32
0.00.337.440 I llm_load_print_meta: n_swa            = 0
0.00.337.440 I llm_load_print_meta: n_embd_head_k    = 32
0.00.337.441 I llm_load_print_meta: n_embd_head_v    = 32
0.00.337.441 I llm_load_print_meta: n_gqa            = 1
0.00.337.442 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.337.442 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.337.443 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.337.444 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.337.449 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.337.450 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.337.451 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.337.458 I llm_load_print_meta: n_ff             = 1536
0.00.337.459 I llm_load_print_meta: n_expert         = 0
0.00.337.459 I llm_load_print_meta: n_expert_used    = 0
0.00.337.459 I llm_load_print_meta: causal attn      = 0
0.00.337.459 I llm_load_print_meta: pooling type     = -1
0.00.337.459 I llm_load_print_meta: rope type        = -1
0.00.337.460 I llm_load_print_meta: rope scaling     = linear
0.00.337.461 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.337.461 I llm_load_print_meta: freq_scale_train = 1
0.00.337.461 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.337.461 I llm_load_print_meta: rope_finetuned   = unknown
0.00.337.461 I llm_load_print_meta: ssm_d_conv       = 0
0.00.337.461 I llm_load_print_meta: ssm_d_inner      = 0
0.00.337.462 I llm_load_print_meta: ssm_d_state      = 0
0.00.337.462 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.337.462 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.337.462 I llm_load_print_meta: model type       = 33M
0.00.337.463 I llm_load_print_meta: model ftype      = F16
0.00.337.463 I llm_load_print_meta: model params     = 32.90 M
0.00.337.464 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.337.464 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.337.465 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.337.465 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.337.465 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.337.466 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.337.466 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.337.466 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.337.466 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.337.466 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.337.467 I llm_load_print_meta: max token length = 45
0.00.338.430 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.338.431 I llm_load_tensors: offloading output layer to GPU
0.00.338.431 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.338.451 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.338.452 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.339.170 I llama_new_context_with_model: n_seq_max     = 1
0.00.339.170 I llama_new_context_with_model: n_ctx         = 8192
0.00.339.170 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.339.171 I llama_new_context_with_model: n_batch       = 2048
0.00.339.171 I llama_new_context_with_model: n_ubatch      = 2048
0.00.339.171 I llama_new_context_with_model: flash_attn    = 0
0.00.339.171 I llama_new_context_with_model: freq_base     = 10000.0
0.00.339.171 I llama_new_context_with_model: freq_scale    = 1
0.00.339.172 I ggml_metal_init: allocating
0.00.339.175 I ggml_metal_init: found device: Apple M4
0.00.339.177 I ggml_metal_init: picking default device: Apple M4
0.00.339.824 I ggml_metal_init: using embedded metal library
0.00.342.380 I ggml_metal_init: GPU name:   Apple M4
0.00.342.382 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.342.382 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.342.383 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.342.383 I ggml_metal_init: simdgroup reduction   = true
0.00.342.383 I ggml_metal_init: simdgroup matrix mul. = true
0.00.342.383 I ggml_metal_init: has bfloat            = true
0.00.342.383 I ggml_metal_init: use bfloat            = true
0.00.342.384 I ggml_metal_init: hasUnifiedMemory      = true
0.00.342.385 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.352.657 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.355.181 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.355.185 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.355.187 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.355.707 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.355.708 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.355.708 I llama_new_context_with_model: graph nodes  = 154
0.00.355.709 I llama_new_context_with_model: graph splits = 2
0.00.355.723 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.355.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.367.383 I 
0.00.367.426 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.367.675 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.367.675 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.367.678 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.367.678 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.367.686 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.367.686 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.368.171 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.371.720 I llama_perf_context_print:        load time =     349.34 ms
0.00.371.725 I llama_perf_context_print: prompt eval time =       3.54 ms /    62 tokens (    0.06 ms per token, 17514.12 tokens per second)
0.00.371.726 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.371.727 I llama_perf_context_print:       total time =       4.34 ms /    63 tokens
0.00.371.962 I ggml_metal_free: deallocating

real	0m1.107s
user	0m0.347s
sys	0m0.038s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.137 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.289 I main: llama backend init
0.00.000.302 I main: load the model and apply lora adapter, if any
0.00.035.301 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.046.417 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.046.434 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.046.438 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.046.438 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.046.439 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.046.439 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.046.440 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.046.442 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.046.442 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.046.443 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.046.447 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.046.449 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.046.453 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.046.454 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.046.461 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.046.462 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.046.470 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.055.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.057.780 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.065.528 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.065.531 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.065.531 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.065.532 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.065.532 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.065.534 I llama_model_loader: - type  f32:  194 tensors
0.00.065.534 I llama_model_loader: - type  f16:   98 tensors
0.00.095.979 I llm_load_vocab: special tokens cache size = 25
0.00.103.004 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.103.007 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.103.007 I llm_load_print_meta: arch             = gptneox
0.00.103.007 I llm_load_print_meta: vocab type       = BPE
0.00.103.008 I llm_load_print_meta: n_vocab          = 50304
0.00.103.008 I llm_load_print_meta: n_merges         = 50009
0.00.103.008 I llm_load_print_meta: vocab_only       = 0
0.00.103.008 I llm_load_print_meta: n_ctx_train      = 2048
0.00.103.008 I llm_load_print_meta: n_embd           = 2048
0.00.103.008 I llm_load_print_meta: n_layer          = 24
0.00.103.011 I llm_load_print_meta: n_head           = 16
0.00.103.012 I llm_load_print_meta: n_head_kv        = 16
0.00.103.012 I llm_load_print_meta: n_rot            = 32
0.00.103.012 I llm_load_print_meta: n_swa            = 0
0.00.103.015 I llm_load_print_meta: n_embd_head_k    = 128
0.00.103.015 I llm_load_print_meta: n_embd_head_v    = 128
0.00.103.015 I llm_load_print_meta: n_gqa            = 1
0.00.103.016 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.103.017 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.103.017 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.103.018 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.103.018 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.103.018 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.103.018 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.103.019 I llm_load_print_meta: n_ff             = 8192
0.00.103.019 I llm_load_print_meta: n_expert         = 0
0.00.103.019 I llm_load_print_meta: n_expert_used    = 0
0.00.103.019 I llm_load_print_meta: causal attn      = 1
0.00.103.019 I llm_load_print_meta: pooling type     = 0
0.00.103.020 I llm_load_print_meta: rope type        = 2
0.00.103.020 I llm_load_print_meta: rope scaling     = linear
0.00.103.020 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.103.020 I llm_load_print_meta: freq_scale_train = 1
0.00.103.020 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.103.021 I llm_load_print_meta: rope_finetuned   = unknown
0.00.103.021 I llm_load_print_meta: ssm_d_conv       = 0
0.00.103.021 I llm_load_print_meta: ssm_d_inner      = 0
0.00.103.021 I llm_load_print_meta: ssm_d_state      = 0
0.00.103.022 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.103.023 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.103.023 I llm_load_print_meta: model type       = 1.4B
0.00.103.024 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.103.024 I llm_load_print_meta: model params     = 1.41 B
0.00.103.024 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.103.025 I llm_load_print_meta: general.name     = 1.4B
0.00.103.025 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.103.025 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.103.025 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.103.025 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.103.026 I llm_load_print_meta: LF token         = 128 ''
0.00.103.026 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.103.027 I llm_load_print_meta: max token length = 1024
0.00.105.070 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.105.071 I llm_load_tensors: offloading output layer to GPU
0.00.105.071 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.105.084 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.105.085 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.106.015 I llama_new_context_with_model: n_seq_max     = 1
0.00.106.016 I llama_new_context_with_model: n_ctx         = 2048
0.00.106.016 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.106.016 I llama_new_context_with_model: n_batch       = 2048
0.00.106.016 I llama_new_context_with_model: n_ubatch      = 512
0.00.106.016 I llama_new_context_with_model: flash_attn    = 0
0.00.106.017 I llama_new_context_with_model: freq_base     = 10000.0
0.00.106.017 I llama_new_context_with_model: freq_scale    = 1
0.00.106.018 I ggml_metal_init: allocating
0.00.106.020 I ggml_metal_init: found device: Apple M4
0.00.106.022 I ggml_metal_init: picking default device: Apple M4
0.00.106.724 I ggml_metal_init: using embedded metal library
0.00.116.665 I ggml_metal_init: GPU name:   Apple M4
0.00.116.667 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.116.667 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.116.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.116.668 I ggml_metal_init: simdgroup reduction   = true
0.00.116.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.116.668 I ggml_metal_init: has bfloat            = true
0.00.116.668 I ggml_metal_init: use bfloat            = true
0.00.116.669 I ggml_metal_init: hasUnifiedMemory      = true
0.00.116.669 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.141.206 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.161.708 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.161.713 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.161.735 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.162.712 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.162.713 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.162.714 I llama_new_context_with_model: graph nodes  = 967
0.00.162.714 I llama_new_context_with_model: graph splits = 2
0.00.162.738 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.162.884 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.162.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.244.272 I main: llama threadpool init, n_threads = 4
0.00.244.304 I 
0.00.244.338 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.244.340 I 
0.00.244.412 I sampler seed: 1234
0.00.244.417 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.244.453 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.244.455 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.244.455 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.091.629 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.02.091.630 I llama_perf_context_print:        load time =     208.96 ms
0.02.091.631 I llama_perf_context_print: prompt eval time =      44.03 ms /     7 tokens (    6.29 ms per token,   158.97 tokens per second)
0.02.091.633 I llama_perf_context_print:        eval time =    1800.34 ms /    63 runs   (   28.58 ms per token,    34.99 tokens per second)
0.02.091.633 I llama_perf_context_print:       total time =    1847.36 ms /    70 tokens
0.02.091.798 I ggml_metal_free: deallocating

real	0m2.399s
user	0m0.145s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.441 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.601 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.288 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.299 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.301 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.302 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.303 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.303 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.304 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.305 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.305 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.306 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.306 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.307 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.307 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.308 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.311 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.311 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.312 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.035 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.116 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.893 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.894 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.895 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.895 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.896 I llama_model_loader: - type  f32:  194 tensors
0.00.039.896 I llama_model_loader: - type  f16:   98 tensors
0.00.060.851 I llm_load_vocab: special tokens cache size = 25
0.00.067.246 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.252 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.254 I llm_load_print_meta: arch             = gptneox
0.00.067.255 I llm_load_print_meta: vocab type       = BPE
0.00.067.257 I llm_load_print_meta: n_vocab          = 50304
0.00.067.257 I llm_load_print_meta: n_merges         = 50009
0.00.067.257 I llm_load_print_meta: vocab_only       = 0
0.00.067.258 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.258 I llm_load_print_meta: n_embd           = 2048
0.00.067.258 I llm_load_print_meta: n_layer          = 24
0.00.067.262 I llm_load_print_meta: n_head           = 16
0.00.067.263 I llm_load_print_meta: n_head_kv        = 16
0.00.067.263 I llm_load_print_meta: n_rot            = 32
0.00.067.263 I llm_load_print_meta: n_swa            = 0
0.00.067.263 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.263 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.264 I llm_load_print_meta: n_gqa            = 1
0.00.067.265 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.265 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.266 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.266 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.266 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.266 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.267 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.267 I llm_load_print_meta: n_ff             = 8192
0.00.067.267 I llm_load_print_meta: n_expert         = 0
0.00.067.267 I llm_load_print_meta: n_expert_used    = 0
0.00.067.268 I llm_load_print_meta: causal attn      = 1
0.00.067.268 I llm_load_print_meta: pooling type     = 0
0.00.067.268 I llm_load_print_meta: rope type        = 2
0.00.067.268 I llm_load_print_meta: rope scaling     = linear
0.00.067.269 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.269 I llm_load_print_meta: freq_scale_train = 1
0.00.067.269 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.269 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.269 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.269 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.269 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.270 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.270 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.270 I llm_load_print_meta: model type       = 1.4B
0.00.067.270 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.067.271 I llm_load_print_meta: model params     = 1.41 B
0.00.067.271 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.067.273 I llm_load_print_meta: general.name     = 1.4B
0.00.067.273 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.273 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.273 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.273 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.274 I llm_load_print_meta: LF token         = 128 ''
0.00.067.274 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.274 I llm_load_print_meta: max token length = 1024
0.00.069.735 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.736 I llm_load_tensors: offloading output layer to GPU
0.00.069.737 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.747 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.069.748 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.070.681 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.682 I llama_new_context_with_model: n_ctx         = 128
0.00.070.682 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.070.682 I llama_new_context_with_model: n_batch       = 128
0.00.070.683 I llama_new_context_with_model: n_ubatch      = 128
0.00.070.683 I llama_new_context_with_model: flash_attn    = 0
0.00.070.683 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.684 I llama_new_context_with_model: freq_scale    = 1
0.00.070.684 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.684 I ggml_metal_init: allocating
0.00.070.688 I ggml_metal_init: found device: Apple M4
0.00.070.690 I ggml_metal_init: picking default device: Apple M4
0.00.071.301 I ggml_metal_init: using embedded metal library
0.00.073.821 I ggml_metal_init: GPU name:   Apple M4
0.00.073.823 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.824 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.824 I ggml_metal_init: simdgroup reduction   = true
0.00.073.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.824 I ggml_metal_init: has bfloat            = true
0.00.073.825 I ggml_metal_init: use bfloat            = true
0.00.073.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.179 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.493 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.496 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.511 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.367 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.085.368 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.085.368 I llama_new_context_with_model: graph nodes  = 967
0.00.085.369 I llama_new_context_with_model: graph splits = 2
0.00.085.381 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.381 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.864.432 I 
0.00.864.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.864.526 I perplexity: tokenizing the input ..
0.00.876.368 I perplexity: tokenization took 11.84 ms
0.00.876.373 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.997.007 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.998.779 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.998.797 I llama_perf_context_print:        load time =     845.82 ms
0.00.998.798 I llama_perf_context_print: prompt eval time =     120.26 ms /   128 tokens (    0.94 ms per token,  1064.39 tokens per second)
0.00.998.801 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.998.802 I llama_perf_context_print:       total time =     134.37 ms /   129 tokens
0.00.999.470 I ggml_metal_free: deallocating

real	0m1.186s
user	0m0.102s
sys	0m0.179s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.784 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.101 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.106 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.108 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.109 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.109 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.110 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.110 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.111 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.112 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.112 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.113 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.113 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.113 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.114 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.117 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.117 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.117 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.127 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.023 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.025 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.025 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.026 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.027 I llama_model_loader: - type  f32:  194 tensors
0.00.030.027 I llama_model_loader: - type q8_0:   98 tensors
0.00.051.999 I llm_load_vocab: special tokens cache size = 25
0.00.058.136 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.141 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.141 I llm_load_print_meta: arch             = gptneox
0.00.058.142 I llm_load_print_meta: vocab type       = BPE
0.00.058.142 I llm_load_print_meta: n_vocab          = 50304
0.00.058.142 I llm_load_print_meta: n_merges         = 50009
0.00.058.144 I llm_load_print_meta: vocab_only       = 0
0.00.058.144 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.146 I llm_load_print_meta: n_embd           = 2048
0.00.058.146 I llm_load_print_meta: n_layer          = 24
0.00.058.151 I llm_load_print_meta: n_head           = 16
0.00.058.157 I llm_load_print_meta: n_head_kv        = 16
0.00.058.157 I llm_load_print_meta: n_rot            = 32
0.00.058.157 I llm_load_print_meta: n_swa            = 0
0.00.058.157 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.157 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.158 I llm_load_print_meta: n_gqa            = 1
0.00.058.159 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.160 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.161 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.161 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.163 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.163 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.163 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.164 I llm_load_print_meta: n_ff             = 8192
0.00.058.164 I llm_load_print_meta: n_expert         = 0
0.00.058.164 I llm_load_print_meta: n_expert_used    = 0
0.00.058.164 I llm_load_print_meta: causal attn      = 1
0.00.058.164 I llm_load_print_meta: pooling type     = 0
0.00.058.165 I llm_load_print_meta: rope type        = 2
0.00.058.165 I llm_load_print_meta: rope scaling     = linear
0.00.058.165 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.166 I llm_load_print_meta: freq_scale_train = 1
0.00.058.166 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.166 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.166 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.166 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.166 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.166 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.166 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.167 I llm_load_print_meta: model type       = 1.4B
0.00.058.167 I llm_load_print_meta: model ftype      = Q8_0
0.00.058.168 I llm_load_print_meta: model params     = 1.41 B
0.00.058.168 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.058.168 I llm_load_print_meta: general.name     = 1.4B
0.00.058.168 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.169 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.169 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.169 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.169 I llm_load_print_meta: LF token         = 128 ''
0.00.058.169 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.170 I llm_load_print_meta: max token length = 1024
0.00.060.667 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.667 I llm_load_tensors: offloading output layer to GPU
0.00.060.668 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.679 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.060.680 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.061.627 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.628 I llama_new_context_with_model: n_ctx         = 2048
0.00.061.628 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.061.628 I llama_new_context_with_model: n_batch       = 2048
0.00.061.628 I llama_new_context_with_model: n_ubatch      = 512
0.00.061.628 I llama_new_context_with_model: flash_attn    = 0
0.00.061.629 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.629 I llama_new_context_with_model: freq_scale    = 1
0.00.061.630 I ggml_metal_init: allocating
0.00.061.635 I ggml_metal_init: found device: Apple M4
0.00.061.637 I ggml_metal_init: picking default device: Apple M4
0.00.062.351 I ggml_metal_init: using embedded metal library
0.00.064.891 I ggml_metal_init: GPU name:   Apple M4
0.00.064.892 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.893 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.893 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.894 I ggml_metal_init: simdgroup reduction   = true
0.00.064.894 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.894 I ggml_metal_init: has bfloat            = true
0.00.064.894 I ggml_metal_init: use bfloat            = true
0.00.064.895 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.422 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.100.373 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.385 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.412 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.463 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.101.466 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.101.466 I llama_new_context_with_model: graph nodes  = 967
0.00.101.466 I llama_new_context_with_model: graph splits = 2
0.00.101.484 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.101.599 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.342.975 I main: llama threadpool init, n_threads = 4
0.01.343.022 I 
0.01.343.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.343.057 I 
0.01.343.289 I sampler seed: 1234
0.01.343.293 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.343.338 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.343.341 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.343.341 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.451.378 I llama_perf_sampler_print:    sampling time =       1.63 ms /    71 runs   (    0.02 ms per token, 43531.58 tokens per second)
0.02.451.379 I llama_perf_context_print:        load time =    1333.18 ms
0.02.451.380 I llama_perf_context_print: prompt eval time =      49.57 ms /     7 tokens (    7.08 ms per token,   141.23 tokens per second)
0.02.451.380 I llama_perf_context_print:        eval time =    1055.25 ms /    63 runs   (   16.75 ms per token,    59.70 tokens per second)
0.02.451.381 I llama_perf_context_print:       total time =    1108.41 ms /    70 tokens
0.02.451.581 I ggml_metal_free: deallocating

real	0m2.470s
user	0m0.119s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.149 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.383 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.446 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.453 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.461 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.462 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.463 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.463 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.465 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.467 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.467 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.468 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.468 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.468 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.469 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.472 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.472 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.473 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.181 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.649 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.651 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.651 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.652 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.652 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.654 I llama_model_loader: - type  f32:  194 tensors
0.00.038.654 I llama_model_loader: - type q8_0:   98 tensors
0.00.065.654 I llm_load_vocab: special tokens cache size = 25
0.00.071.936 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.938 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.939 I llm_load_print_meta: arch             = gptneox
0.00.071.939 I llm_load_print_meta: vocab type       = BPE
0.00.071.939 I llm_load_print_meta: n_vocab          = 50304
0.00.071.939 I llm_load_print_meta: n_merges         = 50009
0.00.071.940 I llm_load_print_meta: vocab_only       = 0
0.00.071.940 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.940 I llm_load_print_meta: n_embd           = 2048
0.00.071.940 I llm_load_print_meta: n_layer          = 24
0.00.071.944 I llm_load_print_meta: n_head           = 16
0.00.071.944 I llm_load_print_meta: n_head_kv        = 16
0.00.071.945 I llm_load_print_meta: n_rot            = 32
0.00.071.945 I llm_load_print_meta: n_swa            = 0
0.00.071.945 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.945 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.946 I llm_load_print_meta: n_gqa            = 1
0.00.071.947 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.947 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.948 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.948 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.948 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.948 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.949 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.949 I llm_load_print_meta: n_ff             = 8192
0.00.071.950 I llm_load_print_meta: n_expert         = 0
0.00.071.950 I llm_load_print_meta: n_expert_used    = 0
0.00.071.950 I llm_load_print_meta: causal attn      = 1
0.00.071.950 I llm_load_print_meta: pooling type     = 0
0.00.071.950 I llm_load_print_meta: rope type        = 2
0.00.071.950 I llm_load_print_meta: rope scaling     = linear
0.00.071.950 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.951 I llm_load_print_meta: freq_scale_train = 1
0.00.071.951 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.951 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.951 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.951 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.952 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.952 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.952 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.954 I llm_load_print_meta: model type       = 1.4B
0.00.071.955 I llm_load_print_meta: model ftype      = Q8_0
0.00.071.955 I llm_load_print_meta: model params     = 1.41 B
0.00.071.955 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.071.956 I llm_load_print_meta: general.name     = 1.4B
0.00.071.956 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.957 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.957 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.960 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.961 I llm_load_print_meta: LF token         = 128 ''
0.00.071.961 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.961 I llm_load_print_meta: max token length = 1024
0.00.074.313 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.314 I llm_load_tensors: offloading output layer to GPU
0.00.074.314 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.325 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.074.326 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.075.236 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.236 I llama_new_context_with_model: n_ctx         = 128
0.00.075.236 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.075.237 I llama_new_context_with_model: n_batch       = 128
0.00.075.237 I llama_new_context_with_model: n_ubatch      = 128
0.00.075.237 I llama_new_context_with_model: flash_attn    = 0
0.00.075.237 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.238 I llama_new_context_with_model: freq_scale    = 1
0.00.075.238 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.075.238 I ggml_metal_init: allocating
0.00.075.242 I ggml_metal_init: found device: Apple M4
0.00.075.244 I ggml_metal_init: picking default device: Apple M4
0.00.075.865 I ggml_metal_init: using embedded metal library
0.00.078.517 I ggml_metal_init: GPU name:   Apple M4
0.00.078.519 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.520 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.520 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.520 I ggml_metal_init: simdgroup reduction   = true
0.00.078.520 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.521 I ggml_metal_init: has bfloat            = true
0.00.078.521 I ggml_metal_init: use bfloat            = true
0.00.078.521 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.522 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.696 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.090.034 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.090.036 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.090.054 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.021 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.091.022 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.091.022 I llama_new_context_with_model: graph nodes  = 967
0.00.091.022 I llama_new_context_with_model: graph splits = 2
0.00.091.035 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.091.036 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.868.260 I 
0.00.868.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.868.331 I perplexity: tokenizing the input ..
0.00.875.966 I perplexity: tokenization took 7.633 ms
0.00.875.972 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.000.005 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.001.182 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.001.201 I llama_perf_context_print:        load time =     854.87 ms
0.01.001.202 I llama_perf_context_print: prompt eval time =     123.81 ms /   128 tokens (    0.97 ms per token,  1033.88 tokens per second)
0.01.001.203 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.001.204 I llama_perf_context_print:       total time =     132.94 ms /   129 tokens
0.01.001.637 I ggml_metal_free: deallocating

real	0m1.022s
user	0m0.100s
sys	0m0.150s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.015.590 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.906 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.912 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.914 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.915 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.915 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.915 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.918 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.919 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.919 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.920 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.921 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.921 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.925 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.926 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.926 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.381 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.623 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.017 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.019 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.019 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.020 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.041.021 I llama_model_loader: - type  f32:  194 tensors
0.00.041.021 I llama_model_loader: - type q4_0:   97 tensors
0.00.041.021 I llama_model_loader: - type q6_K:    1 tensors
0.00.068.778 I llm_load_vocab: special tokens cache size = 25
0.00.076.310 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.076.313 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.076.313 I llm_load_print_meta: arch             = gptneox
0.00.076.313 I llm_load_print_meta: vocab type       = BPE
0.00.076.313 I llm_load_print_meta: n_vocab          = 50304
0.00.076.314 I llm_load_print_meta: n_merges         = 50009
0.00.076.314 I llm_load_print_meta: vocab_only       = 0
0.00.076.314 I llm_load_print_meta: n_ctx_train      = 2048
0.00.076.314 I llm_load_print_meta: n_embd           = 2048
0.00.076.314 I llm_load_print_meta: n_layer          = 24
0.00.076.319 I llm_load_print_meta: n_head           = 16
0.00.076.319 I llm_load_print_meta: n_head_kv        = 16
0.00.076.320 I llm_load_print_meta: n_rot            = 32
0.00.076.322 I llm_load_print_meta: n_swa            = 0
0.00.076.322 I llm_load_print_meta: n_embd_head_k    = 128
0.00.076.323 I llm_load_print_meta: n_embd_head_v    = 128
0.00.076.323 I llm_load_print_meta: n_gqa            = 1
0.00.076.324 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.076.325 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.076.326 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.076.326 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.076.326 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.076.327 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.076.327 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.076.328 I llm_load_print_meta: n_ff             = 8192
0.00.076.328 I llm_load_print_meta: n_expert         = 0
0.00.076.329 I llm_load_print_meta: n_expert_used    = 0
0.00.076.331 I llm_load_print_meta: causal attn      = 1
0.00.076.331 I llm_load_print_meta: pooling type     = 0
0.00.076.331 I llm_load_print_meta: rope type        = 2
0.00.076.332 I llm_load_print_meta: rope scaling     = linear
0.00.076.332 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.076.332 I llm_load_print_meta: freq_scale_train = 1
0.00.076.332 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.076.332 I llm_load_print_meta: rope_finetuned   = unknown
0.00.076.333 I llm_load_print_meta: ssm_d_conv       = 0
0.00.076.333 I llm_load_print_meta: ssm_d_inner      = 0
0.00.076.333 I llm_load_print_meta: ssm_d_state      = 0
0.00.076.333 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.076.333 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.076.333 I llm_load_print_meta: model type       = 1.4B
0.00.076.334 I llm_load_print_meta: model ftype      = Q4_0
0.00.076.334 I llm_load_print_meta: model params     = 1.41 B
0.00.076.335 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.076.335 I llm_load_print_meta: general.name     = 1.4B
0.00.076.335 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.076.339 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.076.339 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.076.340 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.076.340 I llm_load_print_meta: LF token         = 128 ''
0.00.076.340 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.076.340 I llm_load_print_meta: max token length = 1024
0.00.078.737 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.078.738 I llm_load_tensors: offloading output layer to GPU
0.00.078.739 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.078.750 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.078.751 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.079.844 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.845 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.845 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.845 I llama_new_context_with_model: n_batch       = 2048
0.00.079.845 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.846 I llama_new_context_with_model: flash_attn    = 0
0.00.079.846 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.846 I llama_new_context_with_model: freq_scale    = 1
0.00.079.847 I ggml_metal_init: allocating
0.00.079.850 I ggml_metal_init: found device: Apple M4
0.00.079.852 I ggml_metal_init: picking default device: Apple M4
0.00.080.850 I ggml_metal_init: using embedded metal library
0.00.085.018 I ggml_metal_init: GPU name:   Apple M4
0.00.085.021 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.022 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.022 I ggml_metal_init: simdgroup reduction   = true
0.00.085.022 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.022 I ggml_metal_init: has bfloat            = true
0.00.085.024 I ggml_metal_init: use bfloat            = true
0.00.085.025 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.026 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.187 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.126.170 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.126.182 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.126.212 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.127.333 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.127.335 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.127.335 I llama_new_context_with_model: graph nodes  = 967
0.00.127.336 I llama_new_context_with_model: graph splits = 2
0.00.127.353 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.127.494 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.127.495 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.829.500 I main: llama threadpool init, n_threads = 4
0.00.829.541 I 
0.00.829.579 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.829.580 I 
0.00.829.803 I sampler seed: 1234
0.00.829.810 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.829.827 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.829.829 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.829.829 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.503.569 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59915.61 tokens per second)
0.01.503.570 I llama_perf_context_print:        load time =     813.90 ms
0.01.503.571 I llama_perf_context_print: prompt eval time =      39.78 ms /     7 tokens (    5.68 ms per token,   175.98 tokens per second)
0.01.503.572 I llama_perf_context_print:        eval time =     630.98 ms /    63 runs   (   10.02 ms per token,    99.84 tokens per second)
0.01.503.572 I llama_perf_context_print:       total time =     674.07 ms /    70 tokens
0.01.503.794 I ggml_metal_free: deallocating

real	0m1.526s
user	0m0.127s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.552 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.572 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.585 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.585 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.585 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.586 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.587 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.588 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.593 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.625 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.626 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.627 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.627 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.627 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.628 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.628 I llama_model_loader: - type  f32:  194 tensors
0.00.024.628 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.629 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.855 I llm_load_vocab: special tokens cache size = 25
0.00.051.877 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.879 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.880 I llm_load_print_meta: arch             = gptneox
0.00.051.880 I llm_load_print_meta: vocab type       = BPE
0.00.051.880 I llm_load_print_meta: n_vocab          = 50304
0.00.051.880 I llm_load_print_meta: n_merges         = 50009
0.00.051.881 I llm_load_print_meta: vocab_only       = 0
0.00.051.881 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.881 I llm_load_print_meta: n_embd           = 2048
0.00.051.881 I llm_load_print_meta: n_layer          = 24
0.00.051.884 I llm_load_print_meta: n_head           = 16
0.00.051.885 I llm_load_print_meta: n_head_kv        = 16
0.00.051.885 I llm_load_print_meta: n_rot            = 32
0.00.051.885 I llm_load_print_meta: n_swa            = 0
0.00.051.886 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.886 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.886 I llm_load_print_meta: n_gqa            = 1
0.00.051.887 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.888 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.889 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.889 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.889 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.890 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.890 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.894 I llm_load_print_meta: n_ff             = 8192
0.00.051.894 I llm_load_print_meta: n_expert         = 0
0.00.051.894 I llm_load_print_meta: n_expert_used    = 0
0.00.051.894 I llm_load_print_meta: causal attn      = 1
0.00.051.894 I llm_load_print_meta: pooling type     = 0
0.00.051.895 I llm_load_print_meta: rope type        = 2
0.00.051.895 I llm_load_print_meta: rope scaling     = linear
0.00.051.895 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.896 I llm_load_print_meta: freq_scale_train = 1
0.00.051.896 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.896 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.896 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.896 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.896 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.897 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.897 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.897 I llm_load_print_meta: model type       = 1.4B
0.00.051.901 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.902 I llm_load_print_meta: model params     = 1.41 B
0.00.051.902 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.903 I llm_load_print_meta: general.name     = 1.4B
0.00.051.903 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.903 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.903 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.904 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.904 I llm_load_print_meta: LF token         = 128 ''
0.00.051.904 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.904 I llm_load_print_meta: max token length = 1024
0.00.053.911 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.912 I llm_load_tensors: offloading output layer to GPU
0.00.053.912 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.922 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.924 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.848 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.848 I llama_new_context_with_model: n_ctx         = 128
0.00.054.849 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.849 I llama_new_context_with_model: n_batch       = 128
0.00.054.849 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.849 I llama_new_context_with_model: flash_attn    = 0
0.00.054.850 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.850 I llama_new_context_with_model: freq_scale    = 1
0.00.054.850 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.851 I ggml_metal_init: allocating
0.00.054.857 I ggml_metal_init: found device: Apple M4
0.00.054.859 I ggml_metal_init: picking default device: Apple M4
0.00.055.418 I ggml_metal_init: using embedded metal library
0.00.057.768 I ggml_metal_init: GPU name:   Apple M4
0.00.057.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.770 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.770 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.770 I ggml_metal_init: simdgroup reduction   = true
0.00.057.771 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.771 I ggml_metal_init: has bfloat            = true
0.00.057.771 I ggml_metal_init: use bfloat            = true
0.00.057.771 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.772 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.597 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.858 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.862 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.877 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.765 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.766 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.766 I llama_new_context_with_model: graph nodes  = 967
0.00.069.767 I llama_new_context_with_model: graph splits = 2
0.00.069.779 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.780 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.359 I 
0.00.610.396 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.408 I perplexity: tokenizing the input ..
0.00.618.211 I perplexity: tokenization took 7.801 ms
0.00.618.215 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.740.998 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.742.222 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.742.239 I llama_perf_context_print:        load time =     600.80 ms
0.00.742.240 I llama_perf_context_print: prompt eval time =     122.55 ms /   128 tokens (    0.96 ms per token,  1044.47 tokens per second)
0.00.742.242 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.742.243 I llama_perf_context_print:       total time =     131.88 ms /   129 tokens
0.00.742.730 I ggml_metal_free: deallocating

real	0m0.758s
user	0m0.079s
sys	0m0.102s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.015.679 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.224 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.033.228 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.230 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.230 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.231 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.231 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.233 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.234 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.237 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.238 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.238 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.239 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.243 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.245 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.246 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.470 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.621 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.839 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.840 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.840 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.841 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.841 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.841 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.042.842 I llama_model_loader: - type  f32:  194 tensors
0.00.042.842 I llama_model_loader: - type q4_1:   97 tensors
0.00.042.843 I llama_model_loader: - type q6_K:    1 tensors
0.00.068.390 I llm_load_vocab: special tokens cache size = 25
0.00.075.644 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.647 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.647 I llm_load_print_meta: arch             = gptneox
0.00.075.648 I llm_load_print_meta: vocab type       = BPE
0.00.075.648 I llm_load_print_meta: n_vocab          = 50304
0.00.075.648 I llm_load_print_meta: n_merges         = 50009
0.00.075.648 I llm_load_print_meta: vocab_only       = 0
0.00.075.649 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.649 I llm_load_print_meta: n_embd           = 2048
0.00.075.649 I llm_load_print_meta: n_layer          = 24
0.00.075.651 I llm_load_print_meta: n_head           = 16
0.00.075.652 I llm_load_print_meta: n_head_kv        = 16
0.00.075.652 I llm_load_print_meta: n_rot            = 32
0.00.075.653 I llm_load_print_meta: n_swa            = 0
0.00.075.653 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.653 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.654 I llm_load_print_meta: n_gqa            = 1
0.00.075.654 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.655 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.656 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.656 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.656 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.656 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.656 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.657 I llm_load_print_meta: n_ff             = 8192
0.00.075.657 I llm_load_print_meta: n_expert         = 0
0.00.075.657 I llm_load_print_meta: n_expert_used    = 0
0.00.075.659 I llm_load_print_meta: causal attn      = 1
0.00.075.660 I llm_load_print_meta: pooling type     = 0
0.00.075.660 I llm_load_print_meta: rope type        = 2
0.00.075.661 I llm_load_print_meta: rope scaling     = linear
0.00.075.661 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.661 I llm_load_print_meta: freq_scale_train = 1
0.00.075.662 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.662 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.662 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.662 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.662 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.662 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.664 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.664 I llm_load_print_meta: model type       = 1.4B
0.00.075.664 I llm_load_print_meta: model ftype      = Q4_1
0.00.075.665 I llm_load_print_meta: model params     = 1.41 B
0.00.075.665 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.075.665 I llm_load_print_meta: general.name     = 1.4B
0.00.075.665 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.666 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.666 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.666 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.670 I llm_load_print_meta: LF token         = 128 ''
0.00.075.670 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.671 I llm_load_print_meta: max token length = 1024
0.00.077.908 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.077.909 I llm_load_tensors: offloading output layer to GPU
0.00.077.909 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.920 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.077.921 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.078.972 I llama_new_context_with_model: n_seq_max     = 1
0.00.078.973 I llama_new_context_with_model: n_ctx         = 2048
0.00.078.973 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.078.973 I llama_new_context_with_model: n_batch       = 2048
0.00.078.973 I llama_new_context_with_model: n_ubatch      = 512
0.00.078.973 I llama_new_context_with_model: flash_attn    = 0
0.00.078.974 I llama_new_context_with_model: freq_base     = 10000.0
0.00.078.974 I llama_new_context_with_model: freq_scale    = 1
0.00.078.974 I ggml_metal_init: allocating
0.00.078.977 I ggml_metal_init: found device: Apple M4
0.00.078.979 I ggml_metal_init: picking default device: Apple M4
0.00.079.660 I ggml_metal_init: using embedded metal library
0.00.082.503 I ggml_metal_init: GPU name:   Apple M4
0.00.082.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.505 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.506 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.506 I ggml_metal_init: simdgroup reduction   = true
0.00.082.506 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.506 I ggml_metal_init: has bfloat            = true
0.00.082.508 I ggml_metal_init: use bfloat            = true
0.00.082.508 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.510 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.952 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.116.653 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.116.657 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.674 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.668 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.117.669 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.117.669 I llama_new_context_with_model: graph nodes  = 967
0.00.117.670 I llama_new_context_with_model: graph splits = 2
0.00.117.684 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.117.826 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.117.827 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.565 I main: llama threadpool init, n_threads = 4
0.00.707.612 I 
0.00.707.639 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.639 I 
0.00.707.865 I sampler seed: 1234
0.00.707.870 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.902 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.903 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.903 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.438.363 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65137.61 tokens per second)
0.01.438.364 I llama_perf_context_print:        load time =     691.88 ms
0.01.438.365 I llama_perf_context_print: prompt eval time =      39.63 ms /     7 tokens (    5.66 ms per token,   176.63 tokens per second)
0.01.438.365 I llama_perf_context_print:        eval time =     688.00 ms /    63 runs   (   10.92 ms per token,    91.57 tokens per second)
0.01.438.366 I llama_perf_context_print:       total time =     730.80 ms /    70 tokens
0.01.438.557 I ggml_metal_free: deallocating

real	0m1.459s
user	0m0.120s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.092 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.089 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.095 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.097 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.097 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.097 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.101 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.102 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.102 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.102 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.103 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.104 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.104 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.105 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.106 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.107 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.109 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.029 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.073 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.985 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.986 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.986 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.987 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.987 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.987 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.988 I llama_model_loader: - type  f32:  194 tensors
0.00.023.988 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.989 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.267 I llm_load_vocab: special tokens cache size = 25
0.00.050.245 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.248 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.248 I llm_load_print_meta: arch             = gptneox
0.00.050.249 I llm_load_print_meta: vocab type       = BPE
0.00.050.249 I llm_load_print_meta: n_vocab          = 50304
0.00.050.249 I llm_load_print_meta: n_merges         = 50009
0.00.050.249 I llm_load_print_meta: vocab_only       = 0
0.00.050.250 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.250 I llm_load_print_meta: n_embd           = 2048
0.00.050.250 I llm_load_print_meta: n_layer          = 24
0.00.050.252 I llm_load_print_meta: n_head           = 16
0.00.050.253 I llm_load_print_meta: n_head_kv        = 16
0.00.050.253 I llm_load_print_meta: n_rot            = 32
0.00.050.253 I llm_load_print_meta: n_swa            = 0
0.00.050.254 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.254 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.255 I llm_load_print_meta: n_gqa            = 1
0.00.050.255 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.256 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.257 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.257 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.257 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.257 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.258 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.258 I llm_load_print_meta: n_ff             = 8192
0.00.050.260 I llm_load_print_meta: n_expert         = 0
0.00.050.260 I llm_load_print_meta: n_expert_used    = 0
0.00.050.260 I llm_load_print_meta: causal attn      = 1
0.00.050.261 I llm_load_print_meta: pooling type     = 0
0.00.050.261 I llm_load_print_meta: rope type        = 2
0.00.050.261 I llm_load_print_meta: rope scaling     = linear
0.00.050.261 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.262 I llm_load_print_meta: freq_scale_train = 1
0.00.050.262 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.262 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.262 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.262 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.262 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.262 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.263 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.263 I llm_load_print_meta: model type       = 1.4B
0.00.050.263 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.264 I llm_load_print_meta: model params     = 1.41 B
0.00.050.264 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.264 I llm_load_print_meta: general.name     = 1.4B
0.00.050.265 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.265 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.265 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.269 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.269 I llm_load_print_meta: LF token         = 128 ''
0.00.050.270 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.270 I llm_load_print_meta: max token length = 1024
0.00.052.015 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.016 I llm_load_tensors: offloading output layer to GPU
0.00.052.016 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.022 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.022 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.417 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.417 I llama_new_context_with_model: n_ctx         = 128
0.00.053.418 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.418 I llama_new_context_with_model: n_batch       = 128
0.00.053.418 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.418 I llama_new_context_with_model: flash_attn    = 0
0.00.053.419 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.419 I llama_new_context_with_model: freq_scale    = 1
0.00.053.419 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.420 I ggml_metal_init: allocating
0.00.053.425 I ggml_metal_init: found device: Apple M4
0.00.053.428 I ggml_metal_init: picking default device: Apple M4
0.00.053.992 I ggml_metal_init: using embedded metal library
0.00.056.346 I ggml_metal_init: GPU name:   Apple M4
0.00.056.348 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.348 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.349 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.349 I ggml_metal_init: simdgroup reduction   = true
0.00.056.349 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.349 I ggml_metal_init: has bfloat            = true
0.00.056.349 I ggml_metal_init: use bfloat            = true
0.00.056.350 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.350 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.912 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.150 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.154 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.168 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.017 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.019 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.019 I llama_new_context_with_model: graph nodes  = 967
0.00.068.019 I llama_new_context_with_model: graph splits = 2
0.00.068.027 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.327 I 
0.00.628.372 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.384 I perplexity: tokenizing the input ..
0.00.635.961 I perplexity: tokenization took 7.576 ms
0.00.635.965 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.758.352 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.759.565 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.759.578 I llama_perf_context_print:        load time =     619.23 ms
0.00.759.579 I llama_perf_context_print: prompt eval time =     122.16 ms /   128 tokens (    0.95 ms per token,  1047.78 tokens per second)
0.00.759.580 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.759.581 I llama_perf_context_print:       total time =     131.25 ms /   129 tokens
0.00.760.037 I ggml_metal_free: deallocating

real	0m0.774s
user	0m0.079s
sys	0m0.100s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.012.828 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.969 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.026.973 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.974 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.975 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.975 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.975 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.975 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.976 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.976 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.977 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.977 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.977 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.978 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.978 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.981 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.981 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.982 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.857 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.033 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.091 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.092 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.093 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.093 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.036.093 I llama_model_loader: - type  f32:  194 tensors
0.00.036.094 I llama_model_loader: - type q5_0:   97 tensors
0.00.036.094 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.431 I llm_load_vocab: special tokens cache size = 25
0.00.065.958 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.961 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.961 I llm_load_print_meta: arch             = gptneox
0.00.065.962 I llm_load_print_meta: vocab type       = BPE
0.00.065.962 I llm_load_print_meta: n_vocab          = 50304
0.00.065.962 I llm_load_print_meta: n_merges         = 50009
0.00.065.962 I llm_load_print_meta: vocab_only       = 0
0.00.065.962 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.962 I llm_load_print_meta: n_embd           = 2048
0.00.065.963 I llm_load_print_meta: n_layer          = 24
0.00.065.966 I llm_load_print_meta: n_head           = 16
0.00.065.966 I llm_load_print_meta: n_head_kv        = 16
0.00.065.966 I llm_load_print_meta: n_rot            = 32
0.00.065.969 I llm_load_print_meta: n_swa            = 0
0.00.065.969 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.969 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.970 I llm_load_print_meta: n_gqa            = 1
0.00.065.970 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.971 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.972 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.972 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.973 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.973 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.973 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.974 I llm_load_print_meta: n_ff             = 8192
0.00.065.974 I llm_load_print_meta: n_expert         = 0
0.00.065.974 I llm_load_print_meta: n_expert_used    = 0
0.00.065.975 I llm_load_print_meta: causal attn      = 1
0.00.065.976 I llm_load_print_meta: pooling type     = 0
0.00.065.976 I llm_load_print_meta: rope type        = 2
0.00.065.977 I llm_load_print_meta: rope scaling     = linear
0.00.065.977 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.977 I llm_load_print_meta: freq_scale_train = 1
0.00.065.977 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.978 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.978 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.978 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.978 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.982 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.982 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.982 I llm_load_print_meta: model type       = 1.4B
0.00.065.983 I llm_load_print_meta: model ftype      = Q5_0
0.00.065.983 I llm_load_print_meta: model params     = 1.41 B
0.00.065.984 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.065.984 I llm_load_print_meta: general.name     = 1.4B
0.00.065.984 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.984 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.984 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.985 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.985 I llm_load_print_meta: LF token         = 128 ''
0.00.065.985 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.985 I llm_load_print_meta: max token length = 1024
0.00.068.073 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.073 I llm_load_tensors: offloading output layer to GPU
0.00.068.073 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.084 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.068.085 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.069.020 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.021 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.021 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.021 I llama_new_context_with_model: n_batch       = 2048
0.00.069.021 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.022 I llama_new_context_with_model: flash_attn    = 0
0.00.069.022 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.022 I llama_new_context_with_model: freq_scale    = 1
0.00.069.023 I ggml_metal_init: allocating
0.00.069.025 I ggml_metal_init: found device: Apple M4
0.00.069.028 I ggml_metal_init: picking default device: Apple M4
0.00.069.654 I ggml_metal_init: using embedded metal library
0.00.072.340 I ggml_metal_init: GPU name:   Apple M4
0.00.072.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.342 I ggml_metal_init: simdgroup reduction   = true
0.00.072.343 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.343 I ggml_metal_init: has bfloat            = true
0.00.072.343 I ggml_metal_init: use bfloat            = true
0.00.072.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.345 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.289 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.105.917 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.924 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.942 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.041 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.042 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.043 I llama_new_context_with_model: graph nodes  = 967
0.00.107.043 I llama_new_context_with_model: graph splits = 2
0.00.107.058 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.207 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.207 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.871.760 I main: llama threadpool init, n_threads = 4
0.00.871.798 I 
0.00.871.830 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.871.830 I 
0.00.872.085 I sampler seed: 1234
0.00.872.089 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.872.125 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.872.126 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.872.126 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.660.098 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.660.099 I llama_perf_context_print:        load time =     858.93 ms
0.01.660.100 I llama_perf_context_print: prompt eval time =      43.08 ms /     7 tokens (    6.15 ms per token,   162.51 tokens per second)
0.01.660.101 I llama_perf_context_print:        eval time =     741.80 ms /    63 runs   (   11.77 ms per token,    84.93 tokens per second)
0.01.660.101 I llama_perf_context_print:       total time =     788.34 ms /    70 tokens
0.01.660.266 I ggml_metal_free: deallocating

real	0m1.679s
user	0m0.114s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.897 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.832 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.836 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.838 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.839 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.839 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.839 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.840 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.840 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.841 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.841 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.841 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.842 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.842 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.843 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.844 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.844 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.845 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.841 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.902 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.866 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.867 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.867 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.868 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.868 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.868 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.869 I llama_model_loader: - type  f32:  194 tensors
0.00.025.869 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.869 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.967 I llm_load_vocab: special tokens cache size = 25
0.00.053.110 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.112 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.113 I llm_load_print_meta: arch             = gptneox
0.00.053.113 I llm_load_print_meta: vocab type       = BPE
0.00.053.113 I llm_load_print_meta: n_vocab          = 50304
0.00.053.113 I llm_load_print_meta: n_merges         = 50009
0.00.053.114 I llm_load_print_meta: vocab_only       = 0
0.00.053.114 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.114 I llm_load_print_meta: n_embd           = 2048
0.00.053.114 I llm_load_print_meta: n_layer          = 24
0.00.053.117 I llm_load_print_meta: n_head           = 16
0.00.053.118 I llm_load_print_meta: n_head_kv        = 16
0.00.053.118 I llm_load_print_meta: n_rot            = 32
0.00.053.118 I llm_load_print_meta: n_swa            = 0
0.00.053.118 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.118 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.119 I llm_load_print_meta: n_gqa            = 1
0.00.053.120 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.121 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.121 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.122 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.123 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.123 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.123 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.124 I llm_load_print_meta: n_ff             = 8192
0.00.053.124 I llm_load_print_meta: n_expert         = 0
0.00.053.126 I llm_load_print_meta: n_expert_used    = 0
0.00.053.126 I llm_load_print_meta: causal attn      = 1
0.00.053.126 I llm_load_print_meta: pooling type     = 0
0.00.053.126 I llm_load_print_meta: rope type        = 2
0.00.053.126 I llm_load_print_meta: rope scaling     = linear
0.00.053.127 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.127 I llm_load_print_meta: freq_scale_train = 1
0.00.053.127 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.128 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.128 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.128 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.128 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.128 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.128 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.128 I llm_load_print_meta: model type       = 1.4B
0.00.053.129 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.129 I llm_load_print_meta: model params     = 1.41 B
0.00.053.130 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.130 I llm_load_print_meta: general.name     = 1.4B
0.00.053.130 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.131 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.131 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.131 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.131 I llm_load_print_meta: LF token         = 128 ''
0.00.053.132 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.132 I llm_load_print_meta: max token length = 1024
0.00.054.718 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.718 I llm_load_tensors: offloading output layer to GPU
0.00.054.719 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.728 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.729 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.568 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.569 I llama_new_context_with_model: n_ctx         = 128
0.00.055.569 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.569 I llama_new_context_with_model: n_batch       = 128
0.00.055.569 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.570 I llama_new_context_with_model: flash_attn    = 0
0.00.055.570 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.570 I llama_new_context_with_model: freq_scale    = 1
0.00.055.571 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.571 I ggml_metal_init: allocating
0.00.055.575 I ggml_metal_init: found device: Apple M4
0.00.055.577 I ggml_metal_init: picking default device: Apple M4
0.00.056.133 I ggml_metal_init: using embedded metal library
0.00.058.429 I ggml_metal_init: GPU name:   Apple M4
0.00.058.430 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.431 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.431 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.431 I ggml_metal_init: simdgroup reduction   = true
0.00.058.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.432 I ggml_metal_init: has bfloat            = true
0.00.058.432 I ggml_metal_init: use bfloat            = true
0.00.058.432 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.433 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.283 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.563 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.566 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.583 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.500 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.501 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.502 I llama_new_context_with_model: graph nodes  = 967
0.00.070.502 I llama_new_context_with_model: graph splits = 2
0.00.070.517 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.518 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.998 I 
0.00.672.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.051 I perplexity: tokenizing the input ..
0.00.679.644 I perplexity: tokenization took 7.591 ms
0.00.679.647 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.387 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.815.585 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.815.599 I llama_perf_context_print:        load time =     661.10 ms
0.00.815.600 I llama_perf_context_print: prompt eval time =     134.51 ms /   128 tokens (    1.05 ms per token,   951.62 tokens per second)
0.00.815.601 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.602 I llama_perf_context_print:       total time =     143.60 ms /   129 tokens
0.00.816.063 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.080s
sys	0m0.119s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.015.516 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.601 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.030.605 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.607 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.609 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.610 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.610 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.616 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.617 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.618 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.619 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.619 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.620 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.620 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.621 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.622 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.622 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.623 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.692 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.240 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.242 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.242 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.242 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.243 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.243 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.040.244 I llama_model_loader: - type  f32:  194 tensors
0.00.040.244 I llama_model_loader: - type q5_1:   97 tensors
0.00.040.244 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.671 I llm_load_vocab: special tokens cache size = 25
0.00.073.991 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.995 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.995 I llm_load_print_meta: arch             = gptneox
0.00.073.996 I llm_load_print_meta: vocab type       = BPE
0.00.073.996 I llm_load_print_meta: n_vocab          = 50304
0.00.073.996 I llm_load_print_meta: n_merges         = 50009
0.00.073.996 I llm_load_print_meta: vocab_only       = 0
0.00.073.996 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.997 I llm_load_print_meta: n_embd           = 2048
0.00.073.997 I llm_load_print_meta: n_layer          = 24
0.00.073.999 I llm_load_print_meta: n_head           = 16
0.00.074.000 I llm_load_print_meta: n_head_kv        = 16
0.00.074.000 I llm_load_print_meta: n_rot            = 32
0.00.074.001 I llm_load_print_meta: n_swa            = 0
0.00.074.001 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.001 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.002 I llm_load_print_meta: n_gqa            = 1
0.00.074.003 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.004 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.004 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.005 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.005 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.005 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.005 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.006 I llm_load_print_meta: n_ff             = 8192
0.00.074.006 I llm_load_print_meta: n_expert         = 0
0.00.074.006 I llm_load_print_meta: n_expert_used    = 0
0.00.074.006 I llm_load_print_meta: causal attn      = 1
0.00.074.007 I llm_load_print_meta: pooling type     = 0
0.00.074.007 I llm_load_print_meta: rope type        = 2
0.00.074.007 I llm_load_print_meta: rope scaling     = linear
0.00.074.008 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.008 I llm_load_print_meta: freq_scale_train = 1
0.00.074.008 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.011 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.011 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.011 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.011 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.011 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.011 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.012 I llm_load_print_meta: model type       = 1.4B
0.00.074.012 I llm_load_print_meta: model ftype      = Q5_1
0.00.074.013 I llm_load_print_meta: model params     = 1.41 B
0.00.074.013 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.074.013 I llm_load_print_meta: general.name     = 1.4B
0.00.074.014 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.014 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.014 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.014 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.015 I llm_load_print_meta: LF token         = 128 ''
0.00.074.015 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.015 I llm_load_print_meta: max token length = 1024
0.00.076.403 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.404 I llm_load_tensors: offloading output layer to GPU
0.00.076.404 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.415 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.076.416 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.077.495 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.496 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.496 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.496 I llama_new_context_with_model: n_batch       = 2048
0.00.077.497 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.497 I llama_new_context_with_model: flash_attn    = 0
0.00.077.497 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.497 I llama_new_context_with_model: freq_scale    = 1
0.00.077.498 I ggml_metal_init: allocating
0.00.077.501 I ggml_metal_init: found device: Apple M4
0.00.077.503 I ggml_metal_init: picking default device: Apple M4
0.00.078.175 I ggml_metal_init: using embedded metal library
0.00.081.103 I ggml_metal_init: GPU name:   Apple M4
0.00.081.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.106 I ggml_metal_init: simdgroup reduction   = true
0.00.081.106 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.107 I ggml_metal_init: has bfloat            = true
0.00.081.107 I ggml_metal_init: use bfloat            = true
0.00.081.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.942 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.114.917 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.924 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.943 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.926 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.927 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.927 I llama_new_context_with_model: graph nodes  = 967
0.00.115.928 I llama_new_context_with_model: graph splits = 2
0.00.115.939 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.116.074 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.116.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.904.484 I main: llama threadpool init, n_threads = 4
0.00.904.520 I 
0.00.904.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.904.547 I 
0.00.904.768 I sampler seed: 1234
0.00.904.772 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.904.808 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.904.810 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.904.810 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.754.719 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.754.719 I llama_perf_context_print:        load time =     888.96 ms
0.01.754.721 I llama_perf_context_print: prompt eval time =      42.24 ms /     7 tokens (    6.03 ms per token,   165.71 tokens per second)
0.01.754.721 I llama_perf_context_print:        eval time =     804.91 ms /    63 runs   (   12.78 ms per token,    78.27 tokens per second)
0.01.754.722 I llama_perf_context_print:       total time =     850.24 ms /    70 tokens
0.01.754.917 I ggml_metal_free: deallocating

real	0m1.773s
user	0m0.122s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.709 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.586 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.591 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.593 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.594 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.594 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.596 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.599 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.580 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.501 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.501 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.501 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.502 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.502 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.503 I llama_model_loader: - type  f32:  194 tensors
0.00.023.503 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.503 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.798 I llm_load_vocab: special tokens cache size = 25
0.00.049.743 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.746 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.746 I llm_load_print_meta: arch             = gptneox
0.00.049.747 I llm_load_print_meta: vocab type       = BPE
0.00.049.747 I llm_load_print_meta: n_vocab          = 50304
0.00.049.747 I llm_load_print_meta: n_merges         = 50009
0.00.049.747 I llm_load_print_meta: vocab_only       = 0
0.00.049.747 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.748 I llm_load_print_meta: n_embd           = 2048
0.00.049.748 I llm_load_print_meta: n_layer          = 24
0.00.049.750 I llm_load_print_meta: n_head           = 16
0.00.049.751 I llm_load_print_meta: n_head_kv        = 16
0.00.049.751 I llm_load_print_meta: n_rot            = 32
0.00.049.752 I llm_load_print_meta: n_swa            = 0
0.00.049.752 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.752 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.753 I llm_load_print_meta: n_gqa            = 1
0.00.049.754 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.754 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.755 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.755 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.755 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.755 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.756 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.756 I llm_load_print_meta: n_ff             = 8192
0.00.049.756 I llm_load_print_meta: n_expert         = 0
0.00.049.757 I llm_load_print_meta: n_expert_used    = 0
0.00.049.757 I llm_load_print_meta: causal attn      = 1
0.00.049.757 I llm_load_print_meta: pooling type     = 0
0.00.049.757 I llm_load_print_meta: rope type        = 2
0.00.049.760 I llm_load_print_meta: rope scaling     = linear
0.00.049.760 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.761 I llm_load_print_meta: freq_scale_train = 1
0.00.049.761 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.761 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.761 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.761 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.761 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.761 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.762 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.762 I llm_load_print_meta: model type       = 1.4B
0.00.049.762 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.763 I llm_load_print_meta: model params     = 1.41 B
0.00.049.763 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.763 I llm_load_print_meta: general.name     = 1.4B
0.00.049.764 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.764 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.764 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.764 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.765 I llm_load_print_meta: LF token         = 128 ''
0.00.049.765 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.765 I llm_load_print_meta: max token length = 1024
0.00.051.734 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.735 I llm_load_tensors: offloading output layer to GPU
0.00.051.735 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.745 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.746 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.641 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.642 I llama_new_context_with_model: n_ctx         = 128
0.00.052.642 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.642 I llama_new_context_with_model: n_batch       = 128
0.00.052.642 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.642 I llama_new_context_with_model: flash_attn    = 0
0.00.052.643 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.643 I llama_new_context_with_model: freq_scale    = 1
0.00.052.643 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.644 I ggml_metal_init: allocating
0.00.052.650 I ggml_metal_init: found device: Apple M4
0.00.052.652 I ggml_metal_init: picking default device: Apple M4
0.00.053.209 I ggml_metal_init: using embedded metal library
0.00.055.512 I ggml_metal_init: GPU name:   Apple M4
0.00.055.513 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.513 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.514 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.514 I ggml_metal_init: simdgroup reduction   = true
0.00.055.514 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.514 I ggml_metal_init: has bfloat            = true
0.00.055.514 I ggml_metal_init: use bfloat            = true
0.00.055.515 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.515 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.022 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.338 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.341 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.357 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.229 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.230 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.230 I llama_new_context_with_model: graph nodes  = 967
0.00.067.231 I llama_new_context_with_model: graph splits = 2
0.00.067.243 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.244 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.747 I 
0.00.712.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.802 I perplexity: tokenizing the input ..
0.00.720.606 I perplexity: tokenization took 7.802 ms
0.00.720.610 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.855.614 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.856.765 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.856.777 I llama_perf_context_print:        load time =     704.03 ms
0.00.856.778 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.72 tokens per second)
0.00.856.779 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.856.780 I llama_perf_context_print:       total time =     144.03 ms /   129 tokens
0.00.857.270 I ggml_metal_free: deallocating

real	0m0.873s
user	0m0.078s
sys	0m0.116s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.013.838 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.332 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.021.337 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.339 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.339 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.340 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.340 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.340 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.342 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.342 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.343 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.343 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.343 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.344 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.346 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.346 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.346 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.755 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.757 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.031.758 I llama_model_loader: - type  f32:  194 tensors
0.00.031.759 I llama_model_loader: - type q2_K:   49 tensors
0.00.031.759 I llama_model_loader: - type q3_K:   48 tensors
0.00.031.759 I llama_model_loader: - type q6_K:    1 tensors
0.00.060.352 I llm_load_vocab: special tokens cache size = 25
0.00.070.576 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.580 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.581 I llm_load_print_meta: arch             = gptneox
0.00.070.581 I llm_load_print_meta: vocab type       = BPE
0.00.070.581 I llm_load_print_meta: n_vocab          = 50304
0.00.070.582 I llm_load_print_meta: n_merges         = 50009
0.00.070.582 I llm_load_print_meta: vocab_only       = 0
0.00.070.582 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.582 I llm_load_print_meta: n_embd           = 2048
0.00.070.583 I llm_load_print_meta: n_layer          = 24
0.00.070.586 I llm_load_print_meta: n_head           = 16
0.00.070.587 I llm_load_print_meta: n_head_kv        = 16
0.00.070.587 I llm_load_print_meta: n_rot            = 32
0.00.070.587 I llm_load_print_meta: n_swa            = 0
0.00.070.587 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.588 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.589 I llm_load_print_meta: n_gqa            = 1
0.00.070.590 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.591 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.592 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.592 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.592 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.593 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.593 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.594 I llm_load_print_meta: n_ff             = 8192
0.00.070.594 I llm_load_print_meta: n_expert         = 0
0.00.070.594 I llm_load_print_meta: n_expert_used    = 0
0.00.070.595 I llm_load_print_meta: causal attn      = 1
0.00.070.595 I llm_load_print_meta: pooling type     = 0
0.00.070.595 I llm_load_print_meta: rope type        = 2
0.00.070.595 I llm_load_print_meta: rope scaling     = linear
0.00.070.596 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.596 I llm_load_print_meta: freq_scale_train = 1
0.00.070.597 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.597 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.597 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.597 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.597 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.597 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.601 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.601 I llm_load_print_meta: model type       = 1.4B
0.00.070.602 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.070.602 I llm_load_print_meta: model params     = 1.41 B
0.00.070.603 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.070.603 I llm_load_print_meta: general.name     = 1.4B
0.00.070.603 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.604 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.604 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.604 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.610 I llm_load_print_meta: LF token         = 128 ''
0.00.070.610 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.611 I llm_load_print_meta: max token length = 1024
0.00.073.316 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.318 I llm_load_tensors: offloading output layer to GPU
0.00.073.318 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.329 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.073.330 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.074.805 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.806 I llama_new_context_with_model: n_ctx         = 2048
0.00.074.807 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.074.807 I llama_new_context_with_model: n_batch       = 2048
0.00.074.807 I llama_new_context_with_model: n_ubatch      = 512
0.00.074.808 I llama_new_context_with_model: flash_attn    = 0
0.00.074.808 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.809 I llama_new_context_with_model: freq_scale    = 1
0.00.074.809 I ggml_metal_init: allocating
0.00.074.814 I ggml_metal_init: found device: Apple M4
0.00.074.817 I ggml_metal_init: picking default device: Apple M4
0.00.075.714 I ggml_metal_init: using embedded metal library
0.00.079.488 I ggml_metal_init: GPU name:   Apple M4
0.00.079.491 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.492 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.492 I ggml_metal_init: simdgroup reduction   = true
0.00.079.492 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.492 I ggml_metal_init: has bfloat            = true
0.00.079.493 I ggml_metal_init: use bfloat            = true
0.00.079.493 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.494 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.017 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.116.052 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.116.063 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.089 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.055 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.117.057 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.117.057 I llama_new_context_with_model: graph nodes  = 967
0.00.117.057 I llama_new_context_with_model: graph splits = 2
0.00.117.068 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.117.202 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.117.203 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.525.050 I main: llama threadpool init, n_threads = 4
0.00.525.100 I 
0.00.525.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.525.133 I 
0.00.525.379 I sampler seed: 1234
0.00.525.386 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.525.426 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.525.430 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.525.431 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.205.379 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57770.55 tokens per second)
0.01.205.381 I llama_perf_context_print:        load time =     511.20 ms
0.01.205.381 I llama_perf_context_print: prompt eval time =      35.80 ms /     7 tokens (    5.11 ms per token,   195.53 tokens per second)
0.01.205.382 I llama_perf_context_print:        eval time =     641.11 ms /    63 runs   (   10.18 ms per token,    98.27 tokens per second)
0.01.205.382 I llama_perf_context_print:       total time =     680.33 ms /    70 tokens
0.01.205.578 I ggml_metal_free: deallocating

real	0m1.236s
user	0m0.132s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.923 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.581 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.583 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.583 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.592 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.592 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.603 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.706 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.707 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.708 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.708 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.709 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.709 I llama_model_loader: - type  f32:  194 tensors
0.00.024.709 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.710 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.710 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.691 I llm_load_vocab: special tokens cache size = 25
0.00.051.598 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.601 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.601 I llm_load_print_meta: arch             = gptneox
0.00.051.601 I llm_load_print_meta: vocab type       = BPE
0.00.051.602 I llm_load_print_meta: n_vocab          = 50304
0.00.051.602 I llm_load_print_meta: n_merges         = 50009
0.00.051.602 I llm_load_print_meta: vocab_only       = 0
0.00.051.602 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.602 I llm_load_print_meta: n_embd           = 2048
0.00.051.603 I llm_load_print_meta: n_layer          = 24
0.00.051.605 I llm_load_print_meta: n_head           = 16
0.00.051.606 I llm_load_print_meta: n_head_kv        = 16
0.00.051.606 I llm_load_print_meta: n_rot            = 32
0.00.051.606 I llm_load_print_meta: n_swa            = 0
0.00.051.606 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.606 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.607 I llm_load_print_meta: n_gqa            = 1
0.00.051.608 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.609 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.609 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.610 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.610 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.610 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.610 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.611 I llm_load_print_meta: n_ff             = 8192
0.00.051.611 I llm_load_print_meta: n_expert         = 0
0.00.051.611 I llm_load_print_meta: n_expert_used    = 0
0.00.051.611 I llm_load_print_meta: causal attn      = 1
0.00.051.611 I llm_load_print_meta: pooling type     = 0
0.00.051.612 I llm_load_print_meta: rope type        = 2
0.00.051.612 I llm_load_print_meta: rope scaling     = linear
0.00.051.612 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.612 I llm_load_print_meta: freq_scale_train = 1
0.00.051.614 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.614 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.616 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.616 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.616 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.616 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.616 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.616 I llm_load_print_meta: model type       = 1.4B
0.00.051.617 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.617 I llm_load_print_meta: model params     = 1.41 B
0.00.051.618 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.622 I llm_load_print_meta: general.name     = 1.4B
0.00.051.622 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.623 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.623 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.624 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.625 I llm_load_print_meta: LF token         = 128 ''
0.00.051.625 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.625 I llm_load_print_meta: max token length = 1024
0.00.053.462 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.463 I llm_load_tensors: offloading output layer to GPU
0.00.053.463 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.473 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.474 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.377 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.378 I llama_new_context_with_model: n_ctx         = 128
0.00.054.378 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.378 I llama_new_context_with_model: n_batch       = 128
0.00.054.378 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.378 I llama_new_context_with_model: flash_attn    = 0
0.00.054.379 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.379 I llama_new_context_with_model: freq_scale    = 1
0.00.054.379 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.380 I ggml_metal_init: allocating
0.00.054.385 I ggml_metal_init: found device: Apple M4
0.00.054.387 I ggml_metal_init: picking default device: Apple M4
0.00.054.921 I ggml_metal_init: using embedded metal library
0.00.057.224 I ggml_metal_init: GPU name:   Apple M4
0.00.057.225 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.225 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.226 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.226 I ggml_metal_init: simdgroup reduction   = true
0.00.057.226 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.226 I ggml_metal_init: has bfloat            = true
0.00.057.226 I ggml_metal_init: use bfloat            = true
0.00.057.227 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.227 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.620 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.893 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.896 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.910 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.729 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.730 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.731 I llama_new_context_with_model: graph nodes  = 967
0.00.068.731 I llama_new_context_with_model: graph splits = 2
0.00.068.743 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.744 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.408.235 I 
0.00.408.285 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.408.305 I perplexity: tokenizing the input ..
0.00.415.971 I perplexity: tokenization took 7.665 ms
0.00.415.975 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.547.913 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.549.101 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.549.116 I llama_perf_context_print:        load time =     398.31 ms
0.00.549.118 I llama_perf_context_print: prompt eval time =     131.71 ms /   128 tokens (    1.03 ms per token,   971.82 tokens per second)
0.00.549.119 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.549.119 I llama_perf_context_print:       total time =     140.88 ms /   129 tokens
0.00.549.547 I ggml_metal_free: deallocating

real	0m0.564s
user	0m0.079s
sys	0m0.068s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.826 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.635 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.028.640 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.644 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.645 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.645 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.646 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.646 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.647 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.647 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.648 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.648 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.649 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.649 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.651 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.651 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.652 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.652 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.871 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.872 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.872 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.037.873 I llama_model_loader: - type  f32:  194 tensors
0.00.037.873 I llama_model_loader: - type q3_K:   25 tensors
0.00.037.874 I llama_model_loader: - type q4_K:   71 tensors
0.00.037.874 I llama_model_loader: - type q5_K:    1 tensors
0.00.037.874 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.754 I llm_load_vocab: special tokens cache size = 25
0.00.068.462 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.465 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.465 I llm_load_print_meta: arch             = gptneox
0.00.068.466 I llm_load_print_meta: vocab type       = BPE
0.00.068.466 I llm_load_print_meta: n_vocab          = 50304
0.00.068.466 I llm_load_print_meta: n_merges         = 50009
0.00.068.466 I llm_load_print_meta: vocab_only       = 0
0.00.068.466 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.466 I llm_load_print_meta: n_embd           = 2048
0.00.068.467 I llm_load_print_meta: n_layer          = 24
0.00.068.469 I llm_load_print_meta: n_head           = 16
0.00.068.470 I llm_load_print_meta: n_head_kv        = 16
0.00.068.470 I llm_load_print_meta: n_rot            = 32
0.00.068.470 I llm_load_print_meta: n_swa            = 0
0.00.068.471 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.471 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.473 I llm_load_print_meta: n_gqa            = 1
0.00.068.474 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.475 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.475 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.477 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.477 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.477 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.479 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.479 I llm_load_print_meta: n_ff             = 8192
0.00.068.479 I llm_load_print_meta: n_expert         = 0
0.00.068.479 I llm_load_print_meta: n_expert_used    = 0
0.00.068.480 I llm_load_print_meta: causal attn      = 1
0.00.068.480 I llm_load_print_meta: pooling type     = 0
0.00.068.480 I llm_load_print_meta: rope type        = 2
0.00.068.480 I llm_load_print_meta: rope scaling     = linear
0.00.068.480 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.481 I llm_load_print_meta: freq_scale_train = 1
0.00.068.481 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.481 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.481 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.481 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.481 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.481 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.482 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.482 I llm_load_print_meta: model type       = 1.4B
0.00.068.482 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.068.486 I llm_load_print_meta: model params     = 1.41 B
0.00.068.487 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.068.487 I llm_load_print_meta: general.name     = 1.4B
0.00.068.487 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.487 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.488 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.488 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.488 I llm_load_print_meta: LF token         = 128 ''
0.00.068.488 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.488 I llm_load_print_meta: max token length = 1024
0.00.070.481 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.482 I llm_load_tensors: offloading output layer to GPU
0.00.070.482 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.487 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.070.488 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.071.530 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.531 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.531 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.531 I llama_new_context_with_model: n_batch       = 2048
0.00.071.532 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.532 I llama_new_context_with_model: flash_attn    = 0
0.00.071.532 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.533 I llama_new_context_with_model: freq_scale    = 1
0.00.071.533 I ggml_metal_init: allocating
0.00.071.541 I ggml_metal_init: found device: Apple M4
0.00.071.544 I ggml_metal_init: picking default device: Apple M4
0.00.072.214 I ggml_metal_init: using embedded metal library
0.00.074.981 I ggml_metal_init: GPU name:   Apple M4
0.00.074.983 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.983 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.984 I ggml_metal_init: simdgroup reduction   = true
0.00.074.984 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.984 I ggml_metal_init: has bfloat            = true
0.00.074.984 I ggml_metal_init: use bfloat            = true
0.00.074.984 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.985 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.073 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.106.924 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.929 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.946 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.990 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.992 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.992 I llama_new_context_with_model: graph nodes  = 967
0.00.107.992 I llama_new_context_with_model: graph splits = 2
0.00.108.007 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.108.151 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.108.152 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.108 I main: llama threadpool init, n_threads = 4
0.00.601.155 I 
0.00.601.194 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.601.194 I 
0.00.601.427 I sampler seed: 1234
0.00.601.432 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.601.470 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.601.472 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.601.472 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.376.524 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.01.376.525 I llama_perf_context_print:        load time =     591.27 ms
0.01.376.525 I llama_perf_context_print: prompt eval time =      40.46 ms /     7 tokens (    5.78 ms per token,   173.02 tokens per second)
0.01.376.526 I llama_perf_context_print:        eval time =     731.57 ms /    63 runs   (   11.61 ms per token,    86.12 tokens per second)
0.01.376.527 I llama_perf_context_print:       total time =     775.42 ms /    70 tokens
0.01.376.716 I ggml_metal_free: deallocating

real	0m1.393s
user	0m0.115s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.647 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.571 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.580 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.581 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.581 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.589 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.589 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.519 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.377 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.378 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.379 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.379 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.379 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.380 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.380 I llama_model_loader: - type  f32:  194 tensors
0.00.023.381 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.381 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.381 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.381 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.741 I llm_load_vocab: special tokens cache size = 25
0.00.049.800 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.803 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.803 I llm_load_print_meta: arch             = gptneox
0.00.049.804 I llm_load_print_meta: vocab type       = BPE
0.00.049.804 I llm_load_print_meta: n_vocab          = 50304
0.00.049.804 I llm_load_print_meta: n_merges         = 50009
0.00.049.804 I llm_load_print_meta: vocab_only       = 0
0.00.049.805 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.805 I llm_load_print_meta: n_embd           = 2048
0.00.049.805 I llm_load_print_meta: n_layer          = 24
0.00.049.807 I llm_load_print_meta: n_head           = 16
0.00.049.808 I llm_load_print_meta: n_head_kv        = 16
0.00.049.808 I llm_load_print_meta: n_rot            = 32
0.00.049.809 I llm_load_print_meta: n_swa            = 0
0.00.049.809 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.809 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.812 I llm_load_print_meta: n_gqa            = 1
0.00.049.813 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.814 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.814 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.815 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.815 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.815 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.815 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.816 I llm_load_print_meta: n_ff             = 8192
0.00.049.816 I llm_load_print_meta: n_expert         = 0
0.00.049.816 I llm_load_print_meta: n_expert_used    = 0
0.00.049.816 I llm_load_print_meta: causal attn      = 1
0.00.049.817 I llm_load_print_meta: pooling type     = 0
0.00.049.817 I llm_load_print_meta: rope type        = 2
0.00.049.817 I llm_load_print_meta: rope scaling     = linear
0.00.049.817 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.818 I llm_load_print_meta: freq_scale_train = 1
0.00.049.818 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.818 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.818 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.818 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.819 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.819 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.819 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.819 I llm_load_print_meta: model type       = 1.4B
0.00.049.819 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.820 I llm_load_print_meta: model params     = 1.41 B
0.00.049.821 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.821 I llm_load_print_meta: general.name     = 1.4B
0.00.049.821 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.821 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.823 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.823 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.823 I llm_load_print_meta: LF token         = 128 ''
0.00.049.823 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.824 I llm_load_print_meta: max token length = 1024
0.00.051.627 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.628 I llm_load_tensors: offloading output layer to GPU
0.00.051.629 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.633 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.634 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.569 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.570 I llama_new_context_with_model: n_ctx         = 128
0.00.052.570 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.571 I llama_new_context_with_model: n_batch       = 128
0.00.052.571 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.571 I llama_new_context_with_model: flash_attn    = 0
0.00.052.571 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.572 I llama_new_context_with_model: freq_scale    = 1
0.00.052.572 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.572 I ggml_metal_init: allocating
0.00.052.575 I ggml_metal_init: found device: Apple M4
0.00.052.577 I ggml_metal_init: picking default device: Apple M4
0.00.053.135 I ggml_metal_init: using embedded metal library
0.00.055.452 I ggml_metal_init: GPU name:   Apple M4
0.00.055.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.454 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.454 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.455 I ggml_metal_init: simdgroup reduction   = true
0.00.055.455 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.455 I ggml_metal_init: has bfloat            = true
0.00.055.455 I ggml_metal_init: use bfloat            = true
0.00.055.455 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.171 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.436 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.438 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.451 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.402 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.403 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.404 I llama_new_context_with_model: graph nodes  = 967
0.00.067.404 I llama_new_context_with_model: graph splits = 2
0.00.067.412 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.414 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.490.808 I 
0.00.490.848 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.490.863 I perplexity: tokenizing the input ..
0.00.498.351 I perplexity: tokenization took 7.486 ms
0.00.498.354 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.630.605 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.631.765 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.631.785 I llama_perf_context_print:        load time =     482.15 ms
0.00.631.786 I llama_perf_context_print: prompt eval time =     132.01 ms /   128 tokens (    1.03 ms per token,   969.59 tokens per second)
0.00.631.787 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.631.787 I llama_perf_context_print:       total time =     140.98 ms /   129 tokens
0.00.632.298 I ggml_metal_free: deallocating

real	0m0.646s
user	0m0.078s
sys	0m0.090s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.013.003 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.156 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.027.161 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.167 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.167 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.168 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.168 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.168 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.169 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.171 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.172 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.173 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.173 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.174 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.176 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.176 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.190 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.200 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.201 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.201 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.202 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.202 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.202 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.036.203 I llama_model_loader: - type  f32:  194 tensors
0.00.036.203 I llama_model_loader: - type q4_K:   61 tensors
0.00.036.203 I llama_model_loader: - type q5_K:   24 tensors
0.00.036.204 I llama_model_loader: - type q6_K:   13 tensors
0.00.059.101 I llm_load_vocab: special tokens cache size = 25
0.00.065.349 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.352 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.352 I llm_load_print_meta: arch             = gptneox
0.00.065.353 I llm_load_print_meta: vocab type       = BPE
0.00.065.353 I llm_load_print_meta: n_vocab          = 50304
0.00.065.353 I llm_load_print_meta: n_merges         = 50009
0.00.065.353 I llm_load_print_meta: vocab_only       = 0
0.00.065.353 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.353 I llm_load_print_meta: n_embd           = 2048
0.00.065.354 I llm_load_print_meta: n_layer          = 24
0.00.065.356 I llm_load_print_meta: n_head           = 16
0.00.065.358 I llm_load_print_meta: n_head_kv        = 16
0.00.065.358 I llm_load_print_meta: n_rot            = 32
0.00.065.358 I llm_load_print_meta: n_swa            = 0
0.00.065.358 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.359 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.359 I llm_load_print_meta: n_gqa            = 1
0.00.065.360 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.360 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.361 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.361 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.361 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.362 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.362 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.362 I llm_load_print_meta: n_ff             = 8192
0.00.065.362 I llm_load_print_meta: n_expert         = 0
0.00.065.363 I llm_load_print_meta: n_expert_used    = 0
0.00.065.363 I llm_load_print_meta: causal attn      = 1
0.00.065.363 I llm_load_print_meta: pooling type     = 0
0.00.065.363 I llm_load_print_meta: rope type        = 2
0.00.065.363 I llm_load_print_meta: rope scaling     = linear
0.00.065.364 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.364 I llm_load_print_meta: freq_scale_train = 1
0.00.065.364 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.364 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.366 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.366 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.366 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.366 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.366 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.366 I llm_load_print_meta: model type       = 1.4B
0.00.065.367 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.065.367 I llm_load_print_meta: model params     = 1.41 B
0.00.065.368 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.065.368 I llm_load_print_meta: general.name     = 1.4B
0.00.065.368 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.368 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.368 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.368 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.372 I llm_load_print_meta: LF token         = 128 ''
0.00.065.372 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.372 I llm_load_print_meta: max token length = 1024
0.00.067.085 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.086 I llm_load_tensors: offloading output layer to GPU
0.00.067.086 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.096 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.067.097 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.067.975 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.976 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.976 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.976 I llama_new_context_with_model: n_batch       = 2048
0.00.067.977 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.977 I llama_new_context_with_model: flash_attn    = 0
0.00.067.977 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.978 I llama_new_context_with_model: freq_scale    = 1
0.00.067.978 I ggml_metal_init: allocating
0.00.067.981 I ggml_metal_init: found device: Apple M4
0.00.067.983 I ggml_metal_init: picking default device: Apple M4
0.00.068.580 I ggml_metal_init: using embedded metal library
0.00.070.996 I ggml_metal_init: GPU name:   Apple M4
0.00.070.998 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.998 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.999 I ggml_metal_init: simdgroup reduction   = true
0.00.070.999 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.999 I ggml_metal_init: has bfloat            = true
0.00.070.999 I ggml_metal_init: use bfloat            = true
0.00.070.999 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.274 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.105.049 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.057 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.078 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.251 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.253 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.253 I llama_new_context_with_model: graph nodes  = 967
0.00.106.254 I llama_new_context_with_model: graph splits = 2
0.00.106.270 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.431 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.576 I main: llama threadpool init, n_threads = 4
0.00.741.618 I 
0.00.741.652 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.656 I 
0.00.741.820 I sampler seed: 1234
0.00.741.824 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.838 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.840 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.840 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.522.574 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.01.522.574 I llama_perf_context_print:        load time =     728.57 ms
0.01.522.575 I llama_perf_context_print: prompt eval time =      47.17 ms /     7 tokens (    6.74 ms per token,   148.40 tokens per second)
0.01.522.578 I llama_perf_context_print:        eval time =     730.64 ms /    63 runs   (   11.60 ms per token,    86.23 tokens per second)
0.01.522.579 I llama_perf_context_print:       total time =     781.00 ms /    70 tokens
0.01.522.795 I ggml_metal_free: deallocating

real	0m1.541s
user	0m0.115s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.189 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.985 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.990 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.992 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.992 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.993 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.993 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.994 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.994 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.997 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.997 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.997 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.997 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.998 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.999 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.001 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.001 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.907 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.014 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.999 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.000 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.001 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.001 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.001 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.002 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.002 I llama_model_loader: - type  f32:  194 tensors
0.00.024.002 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.003 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.003 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.167 I llm_load_vocab: special tokens cache size = 25
0.00.051.340 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.343 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.343 I llm_load_print_meta: arch             = gptneox
0.00.051.343 I llm_load_print_meta: vocab type       = BPE
0.00.051.343 I llm_load_print_meta: n_vocab          = 50304
0.00.051.344 I llm_load_print_meta: n_merges         = 50009
0.00.051.344 I llm_load_print_meta: vocab_only       = 0
0.00.051.344 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.344 I llm_load_print_meta: n_embd           = 2048
0.00.051.344 I llm_load_print_meta: n_layer          = 24
0.00.051.347 I llm_load_print_meta: n_head           = 16
0.00.051.348 I llm_load_print_meta: n_head_kv        = 16
0.00.051.348 I llm_load_print_meta: n_rot            = 32
0.00.051.348 I llm_load_print_meta: n_swa            = 0
0.00.051.348 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.348 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.349 I llm_load_print_meta: n_gqa            = 1
0.00.051.350 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.351 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.351 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.352 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.352 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.352 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.352 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.353 I llm_load_print_meta: n_ff             = 8192
0.00.051.353 I llm_load_print_meta: n_expert         = 0
0.00.051.353 I llm_load_print_meta: n_expert_used    = 0
0.00.051.353 I llm_load_print_meta: causal attn      = 1
0.00.051.353 I llm_load_print_meta: pooling type     = 0
0.00.051.353 I llm_load_print_meta: rope type        = 2
0.00.051.354 I llm_load_print_meta: rope scaling     = linear
0.00.051.354 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.355 I llm_load_print_meta: freq_scale_train = 1
0.00.051.355 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.355 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.355 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.355 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.355 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.356 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.356 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.358 I llm_load_print_meta: model type       = 1.4B
0.00.051.358 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.358 I llm_load_print_meta: model params     = 1.41 B
0.00.051.359 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.359 I llm_load_print_meta: general.name     = 1.4B
0.00.051.359 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: LF token         = 128 ''
0.00.051.361 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.361 I llm_load_print_meta: max token length = 1024
0.00.053.456 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.457 I llm_load_tensors: offloading output layer to GPU
0.00.053.457 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.468 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.469 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.402 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.403 I llama_new_context_with_model: n_ctx         = 128
0.00.054.403 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.403 I llama_new_context_with_model: n_batch       = 128
0.00.054.404 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.404 I llama_new_context_with_model: flash_attn    = 0
0.00.054.404 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.404 I llama_new_context_with_model: freq_scale    = 1
0.00.054.405 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.405 I ggml_metal_init: allocating
0.00.054.408 I ggml_metal_init: found device: Apple M4
0.00.054.410 I ggml_metal_init: picking default device: Apple M4
0.00.055.009 I ggml_metal_init: using embedded metal library
0.00.058.599 I ggml_metal_init: GPU name:   Apple M4
0.00.058.600 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.601 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.601 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.601 I ggml_metal_init: simdgroup reduction   = true
0.00.058.601 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.601 I ggml_metal_init: has bfloat            = true
0.00.058.602 I ggml_metal_init: use bfloat            = true
0.00.058.602 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.603 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.707 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.070.026 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.028 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.041 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.909 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.910 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.910 I llama_new_context_with_model: graph nodes  = 967
0.00.070.910 I llama_new_context_with_model: graph splits = 2
0.00.070.923 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.923 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.160 I 
0.00.541.208 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.541.220 I perplexity: tokenizing the input ..
0.00.548.792 I perplexity: tokenization took 7.569 ms
0.00.548.796 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.682.781 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.684.269 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.684.283 I llama_perf_context_print:        load time =     531.97 ms
0.00.684.284 I llama_perf_context_print: prompt eval time =     133.75 ms /   128 tokens (    1.04 ms per token,   957.03 tokens per second)
0.00.684.284 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.684.285 I llama_perf_context_print:       total time =     143.12 ms /   129 tokens
0.00.684.613 I ggml_metal_free: deallocating

real	0m0.699s
user	0m0.079s
sys	0m0.093s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.802 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.156 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.161 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.163 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.164 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.164 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.165 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.165 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.166 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.173 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.173 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.139 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.205 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.111 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.111 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.112 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.112 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.112 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.113 I llama_model_loader: - type  f32:  194 tensors
0.00.025.113 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.113 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.110 I llm_load_vocab: special tokens cache size = 25
0.00.052.079 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.082 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.082 I llm_load_print_meta: arch             = gptneox
0.00.052.082 I llm_load_print_meta: vocab type       = BPE
0.00.052.083 I llm_load_print_meta: n_vocab          = 50304
0.00.052.083 I llm_load_print_meta: n_merges         = 50009
0.00.052.083 I llm_load_print_meta: vocab_only       = 0
0.00.052.083 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.083 I llm_load_print_meta: n_embd           = 2048
0.00.052.083 I llm_load_print_meta: n_layer          = 24
0.00.052.086 I llm_load_print_meta: n_head           = 16
0.00.052.087 I llm_load_print_meta: n_head_kv        = 16
0.00.052.087 I llm_load_print_meta: n_rot            = 32
0.00.052.087 I llm_load_print_meta: n_swa            = 0
0.00.052.087 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.088 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.088 I llm_load_print_meta: n_gqa            = 1
0.00.052.089 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.090 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.091 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.091 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.091 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.091 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.092 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.092 I llm_load_print_meta: n_ff             = 8192
0.00.052.092 I llm_load_print_meta: n_expert         = 0
0.00.052.092 I llm_load_print_meta: n_expert_used    = 0
0.00.052.094 I llm_load_print_meta: causal attn      = 1
0.00.052.096 I llm_load_print_meta: pooling type     = 0
0.00.052.096 I llm_load_print_meta: rope type        = 2
0.00.052.096 I llm_load_print_meta: rope scaling     = linear
0.00.052.096 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.097 I llm_load_print_meta: freq_scale_train = 1
0.00.052.097 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.097 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.097 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.097 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.098 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.098 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.098 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.098 I llm_load_print_meta: model type       = 1.4B
0.00.052.098 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.099 I llm_load_print_meta: model params     = 1.41 B
0.00.052.099 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.100 I llm_load_print_meta: general.name     = 1.4B
0.00.052.100 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.100 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.100 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.100 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.101 I llm_load_print_meta: LF token         = 128 ''
0.00.052.102 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.102 I llm_load_print_meta: max token length = 1024
0.00.054.186 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.187 I llm_load_tensors: offloading output layer to GPU
0.00.054.187 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.198 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.199 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.116 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.116 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.116 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.117 I llama_new_context_with_model: n_batch       = 2048
0.00.055.117 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.117 I llama_new_context_with_model: flash_attn    = 0
0.00.055.118 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.118 I llama_new_context_with_model: freq_scale    = 1
0.00.055.118 I ggml_metal_init: allocating
0.00.055.126 I ggml_metal_init: found device: Apple M4
0.00.055.129 I ggml_metal_init: picking default device: Apple M4
0.00.055.747 I ggml_metal_init: using embedded metal library
0.00.058.067 I ggml_metal_init: GPU name:   Apple M4
0.00.058.068 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.070 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.071 I ggml_metal_init: simdgroup reduction   = true
0.00.058.071 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.071 I ggml_metal_init: has bfloat            = true
0.00.058.071 I ggml_metal_init: use bfloat            = true
0.00.058.071 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.127 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.712 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.718 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.734 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.718 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.719 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.720 I llama_new_context_with_model: graph nodes  = 967
0.00.088.720 I llama_new_context_with_model: graph splits = 2
0.00.088.735 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.878 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.878 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.162 I main: llama threadpool init, n_threads = 4
0.00.685.198 I 
0.00.685.230 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.232 I 
0.00.685.461 I sampler seed: 1234
0.00.685.466 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.685.480 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.685.481 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.685.482 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.535.116 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.535.117 I llama_perf_context_print:        load time =     675.36 ms
0.01.535.118 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.36 ms per token,   135.79 tokens per second)
0.01.535.119 I llama_perf_context_print:        eval time =     795.05 ms /    63 runs   (   12.62 ms per token,    79.24 tokens per second)
0.01.535.120 I llama_perf_context_print:       total time =     849.96 ms /    70 tokens
0.01.535.312 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.111s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.773 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.706 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.712 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.714 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.714 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.715 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.715 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.715 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.716 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.716 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.717 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.717 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.718 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.718 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.720 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.724 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.724 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.724 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.625 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.743 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.753 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.755 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.755 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.756 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.756 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.756 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.757 I llama_model_loader: - type  f32:  194 tensors
0.00.023.758 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.758 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.738 I llm_load_vocab: special tokens cache size = 25
0.00.050.833 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.836 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.836 I llm_load_print_meta: arch             = gptneox
0.00.050.837 I llm_load_print_meta: vocab type       = BPE
0.00.050.837 I llm_load_print_meta: n_vocab          = 50304
0.00.050.837 I llm_load_print_meta: n_merges         = 50009
0.00.050.837 I llm_load_print_meta: vocab_only       = 0
0.00.050.838 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.838 I llm_load_print_meta: n_embd           = 2048
0.00.050.838 I llm_load_print_meta: n_layer          = 24
0.00.050.841 I llm_load_print_meta: n_head           = 16
0.00.050.844 I llm_load_print_meta: n_head_kv        = 16
0.00.050.845 I llm_load_print_meta: n_rot            = 32
0.00.050.845 I llm_load_print_meta: n_swa            = 0
0.00.050.845 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.845 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.846 I llm_load_print_meta: n_gqa            = 1
0.00.050.847 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.848 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.848 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.849 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.849 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.849 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.849 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.850 I llm_load_print_meta: n_ff             = 8192
0.00.050.850 I llm_load_print_meta: n_expert         = 0
0.00.050.850 I llm_load_print_meta: n_expert_used    = 0
0.00.050.850 I llm_load_print_meta: causal attn      = 1
0.00.050.851 I llm_load_print_meta: pooling type     = 0
0.00.050.851 I llm_load_print_meta: rope type        = 2
0.00.050.852 I llm_load_print_meta: rope scaling     = linear
0.00.050.852 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.853 I llm_load_print_meta: freq_scale_train = 1
0.00.050.853 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.853 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.853 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.853 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.853 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.853 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.854 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.854 I llm_load_print_meta: model type       = 1.4B
0.00.050.854 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.855 I llm_load_print_meta: model params     = 1.41 B
0.00.050.855 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.856 I llm_load_print_meta: general.name     = 1.4B
0.00.050.856 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.856 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.856 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.857 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.858 I llm_load_print_meta: LF token         = 128 ''
0.00.050.858 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.859 I llm_load_print_meta: max token length = 1024
0.00.052.944 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.945 I llm_load_tensors: offloading output layer to GPU
0.00.052.945 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.956 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.957 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.850 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.850 I llama_new_context_with_model: n_ctx         = 128
0.00.053.850 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.851 I llama_new_context_with_model: n_batch       = 128
0.00.053.851 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.851 I llama_new_context_with_model: flash_attn    = 0
0.00.053.851 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.852 I llama_new_context_with_model: freq_scale    = 1
0.00.053.852 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.852 I ggml_metal_init: allocating
0.00.053.856 I ggml_metal_init: found device: Apple M4
0.00.053.858 I ggml_metal_init: picking default device: Apple M4
0.00.054.449 I ggml_metal_init: using embedded metal library
0.00.056.792 I ggml_metal_init: GPU name:   Apple M4
0.00.056.793 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.793 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.794 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.794 I ggml_metal_init: simdgroup reduction   = true
0.00.056.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.794 I ggml_metal_init: has bfloat            = true
0.00.056.795 I ggml_metal_init: use bfloat            = true
0.00.056.795 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.796 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.851 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.111 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.114 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.137 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.068 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.069 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.070 I llama_new_context_with_model: graph nodes  = 967
0.00.069.070 I llama_new_context_with_model: graph splits = 2
0.00.069.083 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.083 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.712 I 
0.00.642.777 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.805 I perplexity: tokenizing the input ..
0.00.650.346 I perplexity: tokenization took 7.54 ms
0.00.650.353 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.187 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.792.463 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.792.476 I llama_perf_context_print:        load time =     633.93 ms
0.00.792.477 I llama_perf_context_print: prompt eval time =     140.61 ms /   128 tokens (    1.10 ms per token,   910.33 tokens per second)
0.00.792.478 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.478 I llama_perf_context_print:       total time =     149.77 ms /   129 tokens
0.00.792.853 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.080s
sys	0m0.117s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.355 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.969 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.974 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.975 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.976 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.976 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.977 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.977 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.978 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.979 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.979 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.980 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.980 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.981 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.984 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.984 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.985 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.822 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.850 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.655 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.656 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.657 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.657 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.657 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.658 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.658 I llama_model_loader: - type  f32:  194 tensors
0.00.024.659 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.118 I llm_load_vocab: special tokens cache size = 25
0.00.051.177 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.180 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.180 I llm_load_print_meta: arch             = gptneox
0.00.051.180 I llm_load_print_meta: vocab type       = BPE
0.00.051.181 I llm_load_print_meta: n_vocab          = 50304
0.00.051.181 I llm_load_print_meta: n_merges         = 50009
0.00.051.181 I llm_load_print_meta: vocab_only       = 0
0.00.051.181 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.181 I llm_load_print_meta: n_embd           = 2048
0.00.051.182 I llm_load_print_meta: n_layer          = 24
0.00.051.186 I llm_load_print_meta: n_head           = 16
0.00.051.187 I llm_load_print_meta: n_head_kv        = 16
0.00.051.188 I llm_load_print_meta: n_rot            = 32
0.00.051.188 I llm_load_print_meta: n_swa            = 0
0.00.051.189 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.189 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.190 I llm_load_print_meta: n_gqa            = 1
0.00.051.190 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.191 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.192 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.192 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.193 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.193 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.193 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.194 I llm_load_print_meta: n_ff             = 8192
0.00.051.194 I llm_load_print_meta: n_expert         = 0
0.00.051.196 I llm_load_print_meta: n_expert_used    = 0
0.00.051.196 I llm_load_print_meta: causal attn      = 1
0.00.051.197 I llm_load_print_meta: pooling type     = 0
0.00.051.198 I llm_load_print_meta: rope type        = 2
0.00.051.198 I llm_load_print_meta: rope scaling     = linear
0.00.051.202 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.203 I llm_load_print_meta: freq_scale_train = 1
0.00.051.203 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.203 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.203 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.203 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.203 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.205 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.205 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.205 I llm_load_print_meta: model type       = 1.4B
0.00.051.205 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.206 I llm_load_print_meta: model params     = 1.41 B
0.00.051.206 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.206 I llm_load_print_meta: general.name     = 1.4B
0.00.051.206 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.207 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.209 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.209 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.210 I llm_load_print_meta: LF token         = 128 ''
0.00.051.210 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.210 I llm_load_print_meta: max token length = 1024
0.00.053.034 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.035 I llm_load_tensors: offloading output layer to GPU
0.00.053.035 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.041 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.041 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.051 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.052 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.052 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.052 I llama_new_context_with_model: n_batch       = 2048
0.00.054.052 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.052 I llama_new_context_with_model: flash_attn    = 0
0.00.054.053 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.053 I llama_new_context_with_model: freq_scale    = 1
0.00.054.054 I ggml_metal_init: allocating
0.00.054.061 I ggml_metal_init: found device: Apple M4
0.00.054.063 I ggml_metal_init: picking default device: Apple M4
0.00.054.675 I ggml_metal_init: using embedded metal library
0.00.057.024 I ggml_metal_init: GPU name:   Apple M4
0.00.057.026 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.026 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.027 I ggml_metal_init: simdgroup reduction   = true
0.00.057.028 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.028 I ggml_metal_init: has bfloat            = true
0.00.057.028 I ggml_metal_init: use bfloat            = true
0.00.057.029 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.837 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.672 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.679 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.696 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.802 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.804 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.805 I llama_new_context_with_model: graph nodes  = 967
0.00.088.805 I llama_new_context_with_model: graph splits = 2
0.00.088.820 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.969 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.092 I main: llama threadpool init, n_threads = 4
0.00.758.159 I 
0.00.758.193 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.194 I 
0.00.758.425 I sampler seed: 1234
0.00.758.429 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.462 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.463 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.463 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.645.740 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54074.64 tokens per second)
0.01.645.740 I llama_perf_context_print:        load time =     748.73 ms
0.01.645.741 I llama_perf_context_print: prompt eval time =      54.49 ms /     7 tokens (    7.78 ms per token,   128.47 tokens per second)
0.01.645.742 I llama_perf_context_print:        eval time =     829.89 ms /    63 runs   (   13.17 ms per token,    75.91 tokens per second)
0.01.645.742 I llama_perf_context_print:       total time =     887.65 ms /    70 tokens
0.01.645.947 I ggml_metal_free: deallocating

real	0m1.664s
user	0m0.111s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4364 (57bb2c40) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.970 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.702 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.706 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.712 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.712 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.713 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.713 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.714 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.715 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.715 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.716 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.717 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.601 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.656 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.605 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.606 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.607 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.607 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.607 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.608 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.608 I llama_model_loader: - type  f32:  194 tensors
0.00.024.609 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.955 I llm_load_vocab: special tokens cache size = 25
0.00.050.913 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.917 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.917 I llm_load_print_meta: arch             = gptneox
0.00.050.917 I llm_load_print_meta: vocab type       = BPE
0.00.050.918 I llm_load_print_meta: n_vocab          = 50304
0.00.050.918 I llm_load_print_meta: n_merges         = 50009
0.00.050.918 I llm_load_print_meta: vocab_only       = 0
0.00.050.918 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.919 I llm_load_print_meta: n_embd           = 2048
0.00.050.919 I llm_load_print_meta: n_layer          = 24
0.00.050.921 I llm_load_print_meta: n_head           = 16
0.00.050.922 I llm_load_print_meta: n_head_kv        = 16
0.00.050.922 I llm_load_print_meta: n_rot            = 32
0.00.050.922 I llm_load_print_meta: n_swa            = 0
0.00.050.923 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.925 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.926 I llm_load_print_meta: n_gqa            = 1
0.00.050.927 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.927 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.928 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.928 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.928 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.929 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.929 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.929 I llm_load_print_meta: n_ff             = 8192
0.00.050.930 I llm_load_print_meta: n_expert         = 0
0.00.050.930 I llm_load_print_meta: n_expert_used    = 0
0.00.050.930 I llm_load_print_meta: causal attn      = 1
0.00.050.930 I llm_load_print_meta: pooling type     = 0
0.00.050.930 I llm_load_print_meta: rope type        = 2
0.00.050.930 I llm_load_print_meta: rope scaling     = linear
0.00.050.932 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.932 I llm_load_print_meta: freq_scale_train = 1
0.00.050.932 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.933 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.933 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.933 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.933 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.933 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.933 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.934 I llm_load_print_meta: model type       = 1.4B
0.00.050.934 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.934 I llm_load_print_meta: model params     = 1.41 B
0.00.050.935 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.936 I llm_load_print_meta: general.name     = 1.4B
0.00.050.939 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.939 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: LF token         = 128 ''
0.00.050.940 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.940 I llm_load_print_meta: max token length = 1024
0.00.053.010 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.011 I llm_load_tensors: offloading output layer to GPU
0.00.053.011 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.021 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.022 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.989 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.990 I llama_new_context_with_model: n_ctx         = 128
0.00.053.990 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.990 I llama_new_context_with_model: n_batch       = 128
0.00.053.990 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.991 I llama_new_context_with_model: flash_attn    = 0
0.00.053.991 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.991 I llama_new_context_with_model: freq_scale    = 1
0.00.053.992 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.992 I ggml_metal_init: allocating
0.00.053.998 I ggml_metal_init: found device: Apple M4
0.00.054.000 I ggml_metal_init: picking default device: Apple M4
0.00.054.576 I ggml_metal_init: using embedded metal library
0.00.056.884 I ggml_metal_init: GPU name:   Apple M4
0.00.056.885 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.886 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.886 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.886 I ggml_metal_init: simdgroup reduction   = true
0.00.056.886 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.887 I ggml_metal_init: has bfloat            = true
0.00.056.887 I ggml_metal_init: use bfloat            = true
0.00.056.887 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.888 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.482 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.757 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.759 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.773 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.680 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.681 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.681 I llama_new_context_with_model: graph nodes  = 967
0.00.068.682 I llama_new_context_with_model: graph splits = 2
0.00.068.696 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.696 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.139.974 I 
0.00.140.009 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.140.021 I perplexity: tokenizing the input ..
0.00.147.734 I perplexity: tokenization took 7.711 ms
0.00.147.739 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.287.611 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.288.765 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.288.788 I llama_perf_context_print:        load time =     130.00 ms
0.00.288.791 I llama_perf_context_print: prompt eval time =     139.63 ms /   128 tokens (    1.09 ms per token,   916.71 tokens per second)
0.00.288.792 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.288.792 I llama_perf_context_print:       total time =     148.81 ms /   129 tokens
0.00.289.372 I ggml_metal_free: deallocating

real	0m0.304s
user	0m0.079s
sys	0m0.042s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4364 (57bb2c40)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14cc0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14cc0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14cc0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14cc0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14cc0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14cc0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14cc0c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14cc0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14cc0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14cc0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14cc0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14cc0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14cc0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14cc0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14cc0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14cc101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14cc10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14cc11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14cc11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14cc11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14cc12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14cc12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14cc13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14cc13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14cc14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14cc14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14cc14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14cc15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14cc15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14cc16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14cc16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14cc168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14cc17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14cc176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14cc17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14cc17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14cc182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14cc18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14cc18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14cc19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14cc19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14cc199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14cc19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14cc1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14cc1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14cc1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14cc1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14cc1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14cc1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14cc1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14cc1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14cc1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14cc1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14cc1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14cc1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14cc1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14cc1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14cc1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14cc1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14cc20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14cc20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14cc208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14cc20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14cc21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14cc216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14cc21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14cc21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14cc22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14cc22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14cc22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14cc23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14cc23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14cc23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14cc240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14cc24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14cc24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14cc250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14cc25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14cc25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14cc260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14cc26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14cc26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14cc270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14cc27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14cc27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14cc280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14cc28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14cc28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14cc290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14cc295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14cc29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14cc2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14cc2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14cc2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14cc2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14cc2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14cc2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14cc1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14cc2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14cc2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14cc2cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14cc2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14cc2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14cc2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14cc2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14cc2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14cc2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14cc2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14cc2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14cc2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14cc301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14cc30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14cc30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14cc310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14cc31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14cc31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14cc31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14cc32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14cc32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14cc32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14cc33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14cc335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14cc33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14cc33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14cc343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14cc34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14cc34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14cc351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14cc35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14cc35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14cc35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14cc36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14cc368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14cc36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14cc37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14cc376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14cc37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14cc37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14cc38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14cc38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14cc38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14cc39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14cc39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14cc39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14cc3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14cc3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14cc3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14cc3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14cc3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14cc3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14cc3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14cc3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14cc3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14cc3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14cc3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14cc3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14cc3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14cc3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14cc3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14cc3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14cc3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14cc3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14cc3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14cc3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14cc3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14cc40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14cc40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14cc40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14cc40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14cc413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14cc41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14cc41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14cc421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14cc42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14cc42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14cc42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14cc43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14cc438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14cc43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14cc44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14cc446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14cc44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14cc45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14cc454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14cc45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14cc45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14cc46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14cc46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14cc46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14cc47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14cc47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14cc479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14cc47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14cc483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14cc488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14cc48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14cc49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14cc49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14cc49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14cc4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14cc4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14cc4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14cc4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14cc4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14cc4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14cc4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14cc4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14cc4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14cc4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14cc4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14cc4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14cc4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14cc4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14cc4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14cc4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14cc4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14cc50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14cc506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14cc50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14cc51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14cc51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14cc51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14cc52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14cc52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14cc52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14cc53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14cc53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14cc53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14cc54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14cc54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14cc54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14cc55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14cc55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14cc55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14cc560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14cc56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14cc56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14cc570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14cc57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14cc57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14cc580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14cc58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14cc58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14cc590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14cc59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14cc59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14cc5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14cc5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14cc5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14cc5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14cc5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14cc5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14cc5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14cc5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14cc5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14cc5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14cc5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14cc5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14cc5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14cc5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14cc5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14cc5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14cc5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14cc5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14cc60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14cc605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14cc60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14cc60f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14cc61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14cc618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14cc61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14cc62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14cc626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14cc62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14cc62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14cc63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14cc63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14cc63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14cc64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14cc64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14cc64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14cc65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14cc655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14cc65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14cc663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14cc66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14cc67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14cc674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14cc67cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14cc67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14cc685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.158.365 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.158.368 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ca05510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ca057d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ca05c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ca060b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ca06520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ca06990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ca06e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ca07270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ca076e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ca07b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ca07fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ca086e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ca09200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ca099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ca0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ca0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ca0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ca0b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ca0be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ca0c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ca0cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ca0d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ca0dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ca0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ca0e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ca0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ca0ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ca0f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ca0f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ca0fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ca10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ca10580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ca109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ca10cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ca11120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ca11590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ca11a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ca11e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ca122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ca12750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ca12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ca13030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ca134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ca13910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ca13d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ca141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ca14660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ca14ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ca14f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ca153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ca15820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ca15c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ca16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ca16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ca169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ca16e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ca173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ca178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ca17d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ca181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ca18610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ca18a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ca18ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ca19360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ca197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ca19c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ca1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ca1a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ca1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ca1ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ca1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ca1b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ca1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14ca1bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14ca1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14ca1c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14ca1cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14ca1d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14ca1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14ca1da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14ca1ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14ca1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14ca1e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14ca1ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14ca1f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14ca1f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14ca1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14ca1fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14ca20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14ca206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14ca20b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14ca20fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14ca21410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14ca21880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14ca21cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14ca22160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14ca225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14ca22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14ca22eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14ca23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14ca23790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14ca23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14ca24070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14ca244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14ca24950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14ca24dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14ca25230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14ca256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14ca25b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14ca25f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14ca263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14ca26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14ca26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ca27140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ca275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ca27a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ca27e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ca28300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ca28770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ca28be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ca29050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ca294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ca29930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ca29da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ca2a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ca2a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ca2aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ca2af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ca2b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ca2b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ca2bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ca2c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ca2c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ca2ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ca2ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ca2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ca2d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ca2dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ca2e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ca2e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ca2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ca2ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ca2f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ca2f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ca2fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ca2ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ca303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ca30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ca30c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ca31100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ca31570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ca319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ca31e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ca322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ca32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ca32ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ca33010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ca33480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ca338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ca33d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ca341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ca34640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ca34ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ca34f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ca35390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ca35800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ca35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ca360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ca36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ca369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ca36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ca372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ca37710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ca37b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ca37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ca38460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ca388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ca38d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ca391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ca39620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ca39a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ca39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ca3a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ca3a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ca3ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ca3b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ca3b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ca3b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ca3be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ca3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ca3c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ca3cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ca3cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ca3d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ca3d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ca3dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ca3e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ca3e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ca3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ca3eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ca3f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14ca3f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14ca3fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ca400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ca40510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14ca40980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ca40df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ca41380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ca417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ca41c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ca427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ca42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ca42d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ca431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ca43610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ca43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ca43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ca44360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ca447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ca44c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ca450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ca45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ca45990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ca45e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ca46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ca466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ca46b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ca46fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ca47430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ca478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ca47d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ca48180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ca485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ca48a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ca48ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ca49340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ca497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ca49c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ca4a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ca4a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ca4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ca4ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ca4b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ca4b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ca4bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ca4bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ca4c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ca4c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ca4ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ca4d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ca4d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ca4da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ca4deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ca4e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ca4e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ca4ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ca4f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ca4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ca4f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ca4fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ca50230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ca506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ca50b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ca50f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ca513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ca51860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ca51cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14ca52140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14ca525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ca52a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ca52e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ca53300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ca53770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ca53be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ca54050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ca544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ca54930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ca54da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ca55210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ca55680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ca55af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ca55f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ca563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ca56e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ca57560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ca57c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ca583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ca58660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14ca58ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ca590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ca596e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c904fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c905420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c905890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c905d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c906170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c9065e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c906a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c906ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c907330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c9077a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c907c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c908270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c908d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c909540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c909d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c90a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c90ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c90b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c90b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c90c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c90c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c90cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c90d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c90de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c90e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c90e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c90eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c90ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c90f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c90f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c90fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c9101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c910620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c9108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c910d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c9111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c911630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c911aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c911f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c912380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c9127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c912c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c9130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c913540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c9139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c913e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c914290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c914700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c914b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c914fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c915450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c9158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c915d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c9161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c916610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c916a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c916ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c9174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c917960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c917dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c918240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c9186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c918b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c918f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c919400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c919870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c919ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c91a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c91a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c91aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c91aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c91b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c91b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c91bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c91c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c91c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c91c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c91cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c91d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c91d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c91db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c91df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c91e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c91e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c91ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c91f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c91f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c91fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c91fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c9202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c920760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c920bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c921040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c9214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c921920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c921d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c922200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c922670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c922ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c922f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c9233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c923830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c923ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c924110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c924580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c9249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c924e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c9252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c925740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c925bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c926020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c926490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c926900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c926d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c9271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c927650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c927ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c927f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c9283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c928810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c928c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c9290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c929560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c9299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c929e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c92a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c92a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c92ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c92b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c92b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c92b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c92bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c92c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c92c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c92caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c92cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c92d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c92d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c92dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c92e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c92e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c92e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c92ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c92f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c92f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c92fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c92ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c930450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c9308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c930d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c9311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c931610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c931a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c931ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c932360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c9327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c932c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c9330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c933520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c933990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c933e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c934270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c9346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c934b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c934fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c935430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c9358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c935d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c936180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c9365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c936a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c936ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c937340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c9377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c937c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c938090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c938500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c938970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c938de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c939250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c9396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c939b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c939fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c93a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c93a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c93acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14cb04f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14cb053d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14cb05840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14cb05cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14cb06120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14cb06590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14cb06a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14cb06e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14cb072e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14cb07750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14cb07bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14cb08030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14cb084a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14cb08910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14cb08d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14cb091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14cb09660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14cb09ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14cb09f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14cb0a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14cb0a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14cb0ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14cb0b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14cb0b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14cb0c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14cb0c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14cb0c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14cb0caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14cb0cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14cb0d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14cb0d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14cb0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14cb0e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14cb0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14cb0ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14cb0ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14cb0f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14cb0f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14cb0fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14cb10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14cb104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14cb10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14cb10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14cb111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14cb11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14cb11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14cb11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14cb123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14cb12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14cb12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14cb13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14cb13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14cb139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14cb13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14cb142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14cb14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14cb14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14cb15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14cb15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14cb15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14cb160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14cb16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14cb16990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14cb16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14cb17270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14cb176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14cb17b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14cb17fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14cb18430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14cb188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14cb18d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14cb19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14cb195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14cb19a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14cb19ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14cb1a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14cb1a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14cb1ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14cb1b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14cb1b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14cb1b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14cb1bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14cb1c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14cb1c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14cb1cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14cb1cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14cb1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14cb1d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14cb1dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14cb1e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14cb1e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14cb1ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14cb1eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14cb1f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14cb1f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14cb1fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14cb20070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14cb20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14cb21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14cb21920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14cb22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14cb22300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14cb22770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14cb22d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14cb23380 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.815s
user	0m0.289s
sys	0m0.313s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4364 (57bb2c40)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13df0d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13df0d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13df0df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13df0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13df0ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13df0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13df0f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13df0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13df10120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13df10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13df10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13df11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13df11b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13df122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13df12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13df13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13df13940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13df14060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13df14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13df14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13df15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13df15d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13df164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13df16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13df17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13df17730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13df17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13df189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13df18ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13df191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13df19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13df19910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13df1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13df1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13df1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13df1ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13df1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13df1b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13df1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13df1c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13df1c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13df1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13df1cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13df1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13df1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13df1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13df1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13df1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13df1f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13df1f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13df1fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13df20380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13df20990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13df20fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13df21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13df21c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13df220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13df22390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13df229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13df23190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13df23450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13df238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13df23d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13df24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13df246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13df24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13df25010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13df254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13df25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13df25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13df26290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13df26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13df26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13df27120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13df27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13df27bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13df28110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13df28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13df28bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13df29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13df29650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13df29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13df2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13df2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13df2ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13df2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13df2b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13df2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13df2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13df2c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13df2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13df2d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13df2d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13df2db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13df2e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13df2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13df2eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13df1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13df2efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13df2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13df2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13df30210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13df30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13df30cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13df31200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13df31750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13df31ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13df321f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13df32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13df32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13df331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13df33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13df33c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13df34120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13df345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13df34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13df34f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13df353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13df35840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13df35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13df36180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13df36620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13df36ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13df36f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13df37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13df378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13df37d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13df381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13df38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13df38b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13df38fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13df39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13df39900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13df39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13df3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13df3a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13df3ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13df3b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13df3b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13df3b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13df3be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13df3c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13df3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13df3cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13df3d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13df3d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13df3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13df3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13df3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13df3e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13df3ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13df3f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13df3f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13df3fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13df3fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13df40360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13df40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13df40ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13df41140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13df415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13df41a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13df41f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13df423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13df42860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13df42d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13df431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13df43640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13df43ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13df43f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13df44420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13df448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13df44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13df45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13df456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13df45b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13df45fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13df46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13df46920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13df46dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13df47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13df47700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13df47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13df48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13df484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13df48980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13df48e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13df492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13df49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13df49c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13df4a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13df4a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13df4a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13df4ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13df4b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13df4b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13df4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13df4c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13df4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13df4cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13df4d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13df4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13df4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13df4e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13df4e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13df4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13df4f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13df4fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13df500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13df50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13df509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13df511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13df516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13df51c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13df52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13df526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13df52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13df53180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13df536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13df53c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13df54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13df546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13df54c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13df55160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13df556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13df55c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13df56150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13df566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13df56bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13df57140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13df57690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13df57be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13df58130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13df58680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13df58bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13df59120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13df59670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13df59bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13df5a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13df5a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13df5abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13df5b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13df5b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13df5bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13df5c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13df5c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13df5cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13df5d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13df5d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13df5db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13df5e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13df5e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13df5eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13df5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13df5f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13df5fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13df600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13df60600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13df60b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13df610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13df615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13df61b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13df62090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13df625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13df62b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13df63080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13df635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13df63b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13df63fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13df64460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13df64900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13df64da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13df65240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13df656e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13df65b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13df66020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13df664c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13df66960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13df66e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13df672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13df67740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13df67be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13df68080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13df685d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13df68cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13df69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13df69b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13df6a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13df6a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13df6ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13df6afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13df6b5d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.107 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.110 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f004ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f005150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f0055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f005a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f005ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f006310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f006780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f006bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f007060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f0074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f007940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f008020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f008b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f0092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f009b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f00a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f00a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f00b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f00b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f00bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f00c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f00cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f00d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f00dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f00e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f00e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f00e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f00ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f00f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f00f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f00fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f00ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f0103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f010690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f010b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f010f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f0113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f011850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f011cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f012130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f0125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f012a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f012e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f0132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f013760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f013bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f014040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f0144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f014920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f014d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f015200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f015670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f015ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f015f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f0163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f016830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f016da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f0172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f017710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f017b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f017ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f018460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f0188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f018d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f0191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f019620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f019a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f019f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f01a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f01a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f01ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f01b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f01b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f01b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f01be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f01c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f01c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f01cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f01cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f01d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f01d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f01dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f01e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f01e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f01ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f01eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f01f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f01f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f01fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f0200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f020510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f020980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f020df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f021260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f0216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f021b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f021fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f022420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f022890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f022d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f023170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f0235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f023a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f023ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f024330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f0247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f024c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f025080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f0254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f025960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f025dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f026240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f0266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f026b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f026f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f027400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f027870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f027ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f028150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f0285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f028a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f028ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f029310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f029780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f029bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f02a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f02a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f02a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f02adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f02b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f02b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f02bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f02bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f02c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f02c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f02ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f02d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f02d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f02da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f02de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f02e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f02e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f02ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f02f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f02f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f02f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f02fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f030200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f030670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f030ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f030f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f0313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f031830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f031ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f032110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f032580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f0329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f032e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f0332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f033740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f033bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f034020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f034490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f034900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f034d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f0351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f035650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f035ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f035f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f0363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f036810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f036c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f0370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f037560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f0379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f037e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f0382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f038720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f038b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f039000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f039470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f0398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f039d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f03a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f03a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f03aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f03af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f03b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f03b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f03bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f03c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f03c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f03c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f03ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f03d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f03d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f03db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f03dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f03e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f03e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f03ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f03f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f03f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f03fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f03fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f040360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f0407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f040d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f0411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f041640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f042190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f042450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f042710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f042b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f042ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f043460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f0438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f043d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f0441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f044620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f044a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f044f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f045370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f0457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f045c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f0460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f046530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f0469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f046e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f047280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f0476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f047b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f047fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f048440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f0488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f048d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f049190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f049600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f049a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f049ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f04a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f04a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f04ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f04b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f04b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f04b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f04bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f04c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f04c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f04cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f04cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f04d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f04d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f04dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f04e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f04e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f04ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f04eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f04f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f04f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f04fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f050080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f0504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f050960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f050dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f051240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f0516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f051b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f051f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f052400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f052870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f052ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f053150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f0535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f053a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f053ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f054310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f054780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f054bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f055060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f0554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f055940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f055db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f056820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f056f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f057660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f057d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f058040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f0584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f058ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f0590c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13df0e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13df0df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13df0ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13df0eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13df0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13df0f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13df0fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13df10090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13df0d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13df27b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13df27df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13df28260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13df28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13df292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13df29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13df2a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13df2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13df2af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13df2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13df2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13df2c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13df2cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13df2d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13df2dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13df2e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13df2e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13df2eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13df2eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13df2f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13df2f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13df2fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13df301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13df30620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13df308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13df30d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13df311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13df31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13df31aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13df31f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13df32380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13df327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13df32c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13df330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13df33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13df339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13df33e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13df34290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13df34700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13df34b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13df34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13df35450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13df358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13df35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13df361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13df36610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13df36a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13df36ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13df37360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13df377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13df37c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13df380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13df38520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13df38990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13df38e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13df39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13df396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13df39b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13df39fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13df3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13df3a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13df3ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13df3b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13df3b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13df3ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13df3bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13df3c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13df3c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13df3cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13df3d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13df3d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13df3d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13df3dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13df3e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13df3e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13df3eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13df3efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13df3f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13df3f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13df3fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13df40160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13df405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13df40a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13df40eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13df41320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13df41790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13df41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13df42070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13df424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13df42950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13df42dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13df43230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13df436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13df43b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13df43f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13df443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13df44860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13df44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13df45140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13df455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13df45a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13df45e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13df46300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13df46770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13df46be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13df47050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13df474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13df47930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13df47da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13df48210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13df48680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13df48af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13df48f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13df493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13df49840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13df49cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13df4a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13df4a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13df4aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13df4ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13df4b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13df4b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13df4bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13df4c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13df4c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13df4c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13df4cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13df4d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13df4d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13df4dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13df4df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13df4e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13df4e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13df4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13df4f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13df4f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13df4f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13df4fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13df502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13df50730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13df50ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13df51010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13df51480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13df518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13df51d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13df521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13df52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13df52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13df52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13df53390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13df53800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13df53c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13df540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13df54550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13df549c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13df54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13df552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13df55710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13df55b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13df55ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13df56460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13df568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13df56d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13df571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13df57620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13df57a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13df57f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13df58370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13df587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13df58c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13df590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13df59530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13df599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13df59e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13df5a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13df5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13df5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13df5afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13df5b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13df5b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13df5bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13df5c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13df5c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13df5ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13df5cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13df5d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13df5d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13df5dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13df5e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13df5e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13df5e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13df5edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13df5f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13df5f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13df5fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13df5ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13df60420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13df60890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13df60d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13df61170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13df615e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13df61d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13df621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13df62640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13df62ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13df62f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13df63390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13df63800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13df63c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13df640e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13df64550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13df649c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13df64e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13df652a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13df65710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13df65b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13df65ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13df66460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13df668d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13df66d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13df671b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13df67620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13df67a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13df67f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13df68370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13df687e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13df68c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13df690c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13df69530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13df699a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13df69e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13df6a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13df6a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13df6ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13df6afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13df6b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13df1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13df1ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13df1afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13df1b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13df1b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13df1bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13df1c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13df1c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13df1ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13df1cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13df1d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13df1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13df1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13df1e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13df1e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13df1e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13df1edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13df1f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13df1f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13df1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13df1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13df20400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13df20870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13df20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13df21150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13df215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13df21a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13df21ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13df22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13df22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13df22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13df23060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13df234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13df23940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13df23db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13df24220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13df24690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13df24b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13df251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13df258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13df25fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13df266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13df26b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13df26fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13df27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13df18ee0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.928s
user	0m0.244s
sys	0m0.144s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.31 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.59 sec*proc (2 tests)

Total Test time (real) =   0.60 sec
        0.60 real         0.16 user         0.05 sys
```
