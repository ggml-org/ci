### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.34 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.74 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.40 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.96 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.21 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.02 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.28 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed  177.32 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    1.06 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   25.83 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.33 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.24 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 219.61 sec*proc (27 tests)

Total Test time (real) = 219.62 sec

real	3m39.657s
user	7m30.167s
sys	0m6.034s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.19 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed   29.22 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.37 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   14.05 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.21 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.19 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.80 sec*proc (27 tests)

Total Test time (real) =  50.82 sec

real	0m50.827s
user	1m11.587s
sys	0m5.512s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.173 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.953 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.202 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.209 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.211 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.212 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.213 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.213 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.214 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.216 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.217 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.217 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.218 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.218 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.221 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.222 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.223 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.223 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.224 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.224 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.225 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.330 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.670 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.672 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.673 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.673 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.674 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.030.674 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.675 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.030.675 I llama_model_loader: - type  f32:  124 tensors
0.00.030.676 I llama_model_loader: - type  f16:   73 tensors
0.00.035.115 I llm_load_vocab: special tokens cache size = 5
0.00.037.349 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.037.353 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.037.353 I llm_load_print_meta: arch             = bert
0.00.037.354 I llm_load_print_meta: vocab type       = WPM
0.00.037.354 I llm_load_print_meta: n_vocab          = 30522
0.00.037.354 I llm_load_print_meta: n_merges         = 0
0.00.037.355 I llm_load_print_meta: vocab_only       = 0
0.00.037.355 I llm_load_print_meta: n_ctx_train      = 512
0.00.037.355 I llm_load_print_meta: n_embd           = 384
0.00.037.355 I llm_load_print_meta: n_layer          = 12
0.00.037.384 I llm_load_print_meta: n_head           = 12
0.00.037.387 I llm_load_print_meta: n_head_kv        = 12
0.00.037.387 I llm_load_print_meta: n_rot            = 32
0.00.037.388 I llm_load_print_meta: n_swa            = 0
0.00.037.388 I llm_load_print_meta: n_embd_head_k    = 32
0.00.037.388 I llm_load_print_meta: n_embd_head_v    = 32
0.00.037.389 I llm_load_print_meta: n_gqa            = 1
0.00.037.390 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.037.390 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.037.394 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.037.394 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.037.394 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.037.395 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.037.395 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.037.398 I llm_load_print_meta: n_ff             = 1536
0.00.037.398 I llm_load_print_meta: n_expert         = 0
0.00.037.398 I llm_load_print_meta: n_expert_used    = 0
0.00.037.399 I llm_load_print_meta: causal attn      = 0
0.00.037.399 I llm_load_print_meta: pooling type     = 2
0.00.037.399 I llm_load_print_meta: rope type        = 2
0.00.037.399 I llm_load_print_meta: rope scaling     = linear
0.00.037.400 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.037.400 I llm_load_print_meta: freq_scale_train = 1
0.00.037.400 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.037.401 I llm_load_print_meta: rope_finetuned   = unknown
0.00.037.401 I llm_load_print_meta: ssm_d_conv       = 0
0.00.037.401 I llm_load_print_meta: ssm_d_inner      = 0
0.00.037.407 I llm_load_print_meta: ssm_d_state      = 0
0.00.037.408 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.037.408 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.037.419 I llm_load_print_meta: model type       = 33M
0.00.037.420 I llm_load_print_meta: model ftype      = F16
0.00.037.421 I llm_load_print_meta: model params     = 33.21 M
0.00.037.422 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.037.425 I llm_load_print_meta: general.name     = Bge Small
0.00.037.425 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.037.426 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.037.426 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.037.426 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.037.426 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.037.427 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.037.427 I llm_load_print_meta: max token length = 21
0.00.039.513 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.039.513 I llm_load_tensors: offloading output layer to GPU
0.00.039.519 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.039.546 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.548 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.040.139 I llama_new_context_with_model: n_seq_max     = 1
0.00.040.140 I llama_new_context_with_model: n_ctx         = 512
0.00.040.140 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.040.141 I llama_new_context_with_model: n_batch       = 2048
0.00.040.141 I llama_new_context_with_model: n_ubatch      = 2048
0.00.040.141 I llama_new_context_with_model: flash_attn    = 0
0.00.040.142 I llama_new_context_with_model: freq_base     = 10000.0
0.00.040.142 I llama_new_context_with_model: freq_scale    = 1
0.00.040.143 I ggml_metal_init: allocating
0.00.040.147 I ggml_metal_init: found device: Apple M4
0.00.040.150 I ggml_metal_init: picking default device: Apple M4
0.00.041.039 I ggml_metal_init: using embedded metal library
0.00.045.341 I ggml_metal_init: GPU name:   Apple M4
0.00.045.343 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.344 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.344 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.345 I ggml_metal_init: simdgroup reduction   = true
0.00.045.345 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.345 I ggml_metal_init: has bfloat            = true
0.00.045.345 I ggml_metal_init: use bfloat            = true
0.00.045.346 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.347 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.275 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.058.277 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.058.278 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.058.994 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.058.995 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.058.996 I llama_new_context_with_model: graph nodes  = 429
0.00.058.996 I llama_new_context_with_model: graph splits = 2
0.00.059.017 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.483 I 
0.00.065.510 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.160 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.070.933 I llama_perf_context_print:        load time =      45.52 ms
0.00.070.934 I llama_perf_context_print: prompt eval time =       4.62 ms /     9 tokens (    0.51 ms per token,  1947.21 tokens per second)
0.00.070.935 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.070.935 I llama_perf_context_print:       total time =       5.45 ms /    10 tokens
0.00.071.075 I ggml_metal_free: deallocating

real	0m0.255s
user	0m0.050s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.445 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.467 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.470 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.472 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.472 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.476 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.476 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.477 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.479 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.479 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.479 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.480 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.480 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.482 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.482 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.483 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.483 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.483 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.484 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.484 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.911 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.543 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.544 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.544 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.544 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.545 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.545 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.545 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.546 I llama_model_loader: - type  f32:  124 tensors
0.00.014.546 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.951 I llm_load_vocab: special tokens cache size = 5
0.00.018.232 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.235 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.235 I llm_load_print_meta: arch             = bert
0.00.018.236 I llm_load_print_meta: vocab type       = WPM
0.00.018.236 I llm_load_print_meta: n_vocab          = 30522
0.00.018.236 I llm_load_print_meta: n_merges         = 0
0.00.018.236 I llm_load_print_meta: vocab_only       = 0
0.00.018.237 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.237 I llm_load_print_meta: n_embd           = 384
0.00.018.237 I llm_load_print_meta: n_layer          = 12
0.00.018.247 I llm_load_print_meta: n_head           = 12
0.00.018.247 I llm_load_print_meta: n_head_kv        = 12
0.00.018.247 I llm_load_print_meta: n_rot            = 32
0.00.018.248 I llm_load_print_meta: n_swa            = 0
0.00.018.248 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.248 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.248 I llm_load_print_meta: n_gqa            = 1
0.00.018.249 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.249 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.250 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.250 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.250 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.251 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.251 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.251 I llm_load_print_meta: n_ff             = 1536
0.00.018.252 I llm_load_print_meta: n_expert         = 0
0.00.018.252 I llm_load_print_meta: n_expert_used    = 0
0.00.018.252 I llm_load_print_meta: causal attn      = 0
0.00.018.252 I llm_load_print_meta: pooling type     = 2
0.00.018.252 I llm_load_print_meta: rope type        = 2
0.00.018.252 I llm_load_print_meta: rope scaling     = linear
0.00.018.253 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.255 I llm_load_print_meta: freq_scale_train = 1
0.00.018.255 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.255 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.256 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.256 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.256 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.256 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.256 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.261 I llm_load_print_meta: model type       = 33M
0.00.018.261 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.261 I llm_load_print_meta: model params     = 33.21 M
0.00.018.262 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.264 I llm_load_print_meta: general.name     = Bge Small
0.00.018.264 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.264 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.265 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.265 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.265 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.265 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.265 I llm_load_print_meta: max token length = 21
0.00.019.525 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.525 I llm_load_tensors: offloading output layer to GPU
0.00.019.525 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.533 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.534 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.886 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.887 I llama_new_context_with_model: n_ctx         = 512
0.00.019.887 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.887 I llama_new_context_with_model: n_batch       = 2048
0.00.019.887 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.887 I llama_new_context_with_model: flash_attn    = 0
0.00.019.888 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.888 I llama_new_context_with_model: freq_scale    = 1
0.00.019.888 I ggml_metal_init: allocating
0.00.019.892 I ggml_metal_init: found device: Apple M4
0.00.019.894 I ggml_metal_init: picking default device: Apple M4
0.00.020.492 I ggml_metal_init: using embedded metal library
0.00.022.979 I ggml_metal_init: GPU name:   Apple M4
0.00.022.982 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.982 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.982 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.983 I ggml_metal_init: simdgroup reduction   = true
0.00.022.983 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.983 I ggml_metal_init: has bfloat            = true
0.00.022.983 I ggml_metal_init: use bfloat            = true
0.00.022.984 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.985 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.682 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.684 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.686 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.267 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.269 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.269 I llama_new_context_with_model: graph nodes  = 429
0.00.034.269 I llama_new_context_with_model: graph splits = 2
0.00.034.282 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.780 I 
0.00.038.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.349 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.583 I llama_perf_context_print:        load time =      29.33 ms
0.00.043.585 I llama_perf_context_print: prompt eval time =       4.11 ms /     9 tokens (    0.46 ms per token,  2192.45 tokens per second)
0.00.043.585 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.586 I llama_perf_context_print:       total time =       4.80 ms /    10 tokens
0.00.043.777 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.119 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.323 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.621 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.628 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.030.629 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.634 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.030.634 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.030.636 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.030.638 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.030.638 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.030.639 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.030.640 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.030.640 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.030.645 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.646 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.647 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.030.653 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.653 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.038.044 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.040.307 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.045.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.045.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.045.060 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.045.061 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.045.061 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.045.061 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.045.062 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.045.062 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.045.062 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.045.063 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.045.063 I llama_model_loader: - type  f32:   41 tensors
0.00.045.064 I llama_model_loader: - type  f16:   29 tensors
0.00.062.667 W llm_load_vocab: empty token at index 5
0.00.067.177 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.068.465 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.068.490 I llm_load_vocab: special tokens cache size = 5
0.00.327.850 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.327.857 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.327.860 I llm_load_print_meta: arch             = jina-bert-v2
0.00.327.861 I llm_load_print_meta: vocab type       = BPE
0.00.327.862 I llm_load_print_meta: n_vocab          = 61056
0.00.327.865 I llm_load_print_meta: n_merges         = 39382
0.00.327.865 I llm_load_print_meta: vocab_only       = 0
0.00.327.865 I llm_load_print_meta: n_ctx_train      = 8192
0.00.327.865 I llm_load_print_meta: n_embd           = 384
0.00.327.866 I llm_load_print_meta: n_layer          = 4
0.00.327.902 I llm_load_print_meta: n_head           = 12
0.00.327.903 I llm_load_print_meta: n_head_kv        = 12
0.00.327.903 I llm_load_print_meta: n_rot            = 32
0.00.327.903 I llm_load_print_meta: n_swa            = 0
0.00.327.904 I llm_load_print_meta: n_embd_head_k    = 32
0.00.327.904 I llm_load_print_meta: n_embd_head_v    = 32
0.00.327.904 I llm_load_print_meta: n_gqa            = 1
0.00.327.905 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.327.905 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.327.906 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.327.907 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.327.908 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.327.909 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.327.910 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.327.911 I llm_load_print_meta: n_ff             = 1536
0.00.327.911 I llm_load_print_meta: n_expert         = 0
0.00.327.911 I llm_load_print_meta: n_expert_used    = 0
0.00.327.911 I llm_load_print_meta: causal attn      = 0
0.00.327.911 I llm_load_print_meta: pooling type     = -1
0.00.327.911 I llm_load_print_meta: rope type        = -1
0.00.327.912 I llm_load_print_meta: rope scaling     = linear
0.00.327.913 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.327.914 I llm_load_print_meta: freq_scale_train = 1
0.00.327.914 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.327.914 I llm_load_print_meta: rope_finetuned   = unknown
0.00.327.914 I llm_load_print_meta: ssm_d_conv       = 0
0.00.327.915 I llm_load_print_meta: ssm_d_inner      = 0
0.00.327.915 I llm_load_print_meta: ssm_d_state      = 0
0.00.327.915 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.327.915 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.327.936 I llm_load_print_meta: model type       = 33M
0.00.327.936 I llm_load_print_meta: model ftype      = F16
0.00.327.937 I llm_load_print_meta: model params     = 32.90 M
0.00.327.937 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.327.937 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.327.938 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.327.940 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.327.940 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.327.940 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.327.940 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.327.940 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.327.941 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.327.941 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.327.941 I llm_load_print_meta: max token length = 45
0.00.329.363 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.329.363 I llm_load_tensors: offloading output layer to GPU
0.00.329.363 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.329.391 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.329.392 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.330.457 I llama_new_context_with_model: n_seq_max     = 1
0.00.330.458 I llama_new_context_with_model: n_ctx         = 8192
0.00.330.458 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.330.459 I llama_new_context_with_model: n_batch       = 2048
0.00.330.459 I llama_new_context_with_model: n_ubatch      = 2048
0.00.330.459 I llama_new_context_with_model: flash_attn    = 0
0.00.330.460 I llama_new_context_with_model: freq_base     = 10000.0
0.00.330.460 I llama_new_context_with_model: freq_scale    = 1
0.00.330.461 I ggml_metal_init: allocating
0.00.330.464 I ggml_metal_init: found device: Apple M4
0.00.330.494 I ggml_metal_init: picking default device: Apple M4
0.00.331.615 I ggml_metal_init: using embedded metal library
0.00.334.564 I ggml_metal_init: GPU name:   Apple M4
0.00.334.566 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.334.566 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.334.567 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.334.567 I ggml_metal_init: simdgroup reduction   = true
0.00.334.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.334.567 I ggml_metal_init: has bfloat            = true
0.00.334.567 I ggml_metal_init: use bfloat            = true
0.00.334.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.334.568 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.346.586 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.346.588 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.346.589 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.347.174 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.347.175 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.347.175 I llama_new_context_with_model: graph nodes  = 154
0.00.347.176 I llama_new_context_with_model: graph splits = 2
0.00.347.194 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.957 I 
0.00.357.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.358.138 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.358.139 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.358.142 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.358.142 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.358.149 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.358.149 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.358.702 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.361.495 I llama_perf_context_print:        load time =     337.63 ms
0.00.361.496 I llama_perf_context_print: prompt eval time =       2.79 ms /    62 tokens (    0.04 ms per token, 22262.12 tokens per second)
0.00.361.497 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.361.497 I llama_perf_context_print:       total time =       3.54 ms /    63 tokens
0.00.361.679 I ggml_metal_free: deallocating

real	0m1.050s
user	0m0.337s
sys	0m0.041s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.109 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.217 I main: llama backend init
0.00.000.222 I main: load the model and apply lora adapter, if any
0.00.040.542 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.052.146 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.052.163 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.052.168 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.052.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.052.169 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.052.170 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.052.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.052.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.052.173 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.052.174 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.052.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.052.176 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.052.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.052.177 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.052.188 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.052.189 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.052.189 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.059.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.062.565 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.071.607 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.071.610 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.071.610 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.071.611 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.071.611 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.071.612 I llama_model_loader: - type  f32:  194 tensors
0.00.071.613 I llama_model_loader: - type  f16:   98 tensors
0.00.104.406 I llm_load_vocab: special tokens cache size = 25
0.00.111.462 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.111.465 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.111.465 I llm_load_print_meta: arch             = gptneox
0.00.111.465 I llm_load_print_meta: vocab type       = BPE
0.00.111.466 I llm_load_print_meta: n_vocab          = 50304
0.00.111.466 I llm_load_print_meta: n_merges         = 50009
0.00.111.466 I llm_load_print_meta: vocab_only       = 0
0.00.111.466 I llm_load_print_meta: n_ctx_train      = 2048
0.00.111.466 I llm_load_print_meta: n_embd           = 2048
0.00.111.466 I llm_load_print_meta: n_layer          = 24
0.00.111.489 I llm_load_print_meta: n_head           = 16
0.00.111.490 I llm_load_print_meta: n_head_kv        = 16
0.00.111.490 I llm_load_print_meta: n_rot            = 32
0.00.111.490 I llm_load_print_meta: n_swa            = 0
0.00.111.490 I llm_load_print_meta: n_embd_head_k    = 128
0.00.111.491 I llm_load_print_meta: n_embd_head_v    = 128
0.00.111.491 I llm_load_print_meta: n_gqa            = 1
0.00.111.492 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.111.492 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.111.493 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.111.493 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.111.495 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.111.495 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.111.496 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.111.496 I llm_load_print_meta: n_ff             = 8192
0.00.111.496 I llm_load_print_meta: n_expert         = 0
0.00.111.496 I llm_load_print_meta: n_expert_used    = 0
0.00.111.497 I llm_load_print_meta: causal attn      = 1
0.00.111.497 I llm_load_print_meta: pooling type     = 0
0.00.111.498 I llm_load_print_meta: rope type        = 2
0.00.111.498 I llm_load_print_meta: rope scaling     = linear
0.00.111.500 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.111.500 I llm_load_print_meta: freq_scale_train = 1
0.00.111.500 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.111.500 I llm_load_print_meta: rope_finetuned   = unknown
0.00.111.500 I llm_load_print_meta: ssm_d_conv       = 0
0.00.111.500 I llm_load_print_meta: ssm_d_inner      = 0
0.00.111.501 I llm_load_print_meta: ssm_d_state      = 0
0.00.111.501 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.111.501 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.111.511 I llm_load_print_meta: model type       = 1.4B
0.00.111.511 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.111.511 I llm_load_print_meta: model params     = 1.41 B
0.00.111.512 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.111.512 I llm_load_print_meta: general.name     = 1.4B
0.00.111.512 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.111.513 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.111.513 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.111.513 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.111.513 I llm_load_print_meta: LF token         = 128 ''
0.00.111.514 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.111.515 I llm_load_print_meta: max token length = 1024
0.00.114.121 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.114.122 I llm_load_tensors: offloading output layer to GPU
0.00.114.122 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.114.140 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.114.141 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.115.090 I llama_new_context_with_model: n_seq_max     = 1
0.00.115.091 I llama_new_context_with_model: n_ctx         = 2048
0.00.115.091 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.115.091 I llama_new_context_with_model: n_batch       = 2048
0.00.115.091 I llama_new_context_with_model: n_ubatch      = 512
0.00.115.092 I llama_new_context_with_model: flash_attn    = 0
0.00.115.092 I llama_new_context_with_model: freq_base     = 10000.0
0.00.115.092 I llama_new_context_with_model: freq_scale    = 1
0.00.115.093 I ggml_metal_init: allocating
0.00.115.100 I ggml_metal_init: found device: Apple M4
0.00.115.102 I ggml_metal_init: picking default device: Apple M4
0.00.115.769 I ggml_metal_init: using embedded metal library
0.00.125.244 I ggml_metal_init: GPU name:   Apple M4
0.00.125.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.125.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.125.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.125.247 I ggml_metal_init: simdgroup reduction   = true
0.00.125.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.125.248 I ggml_metal_init: has bfloat            = true
0.00.125.248 I ggml_metal_init: use bfloat            = true
0.00.125.248 I ggml_metal_init: hasUnifiedMemory      = true
0.00.125.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.174.482 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.174.488 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.174.510 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.175.477 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.175.479 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.175.479 I llama_new_context_with_model: graph nodes  = 967
0.00.175.479 I llama_new_context_with_model: graph splits = 2
0.00.175.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.252.000 I main: llama threadpool init, n_threads = 4
0.00.252.035 I 
0.00.252.068 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.252.069 I 
0.00.252.151 I sampler seed: 1234
0.00.252.155 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.252.179 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.252.180 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.252.181 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.102.694 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56528.66 tokens per second)
0.02.102.694 I llama_perf_context_print:        load time =     211.44 ms
0.02.102.695 I llama_perf_context_print: prompt eval time =      43.79 ms /     7 tokens (    6.26 ms per token,   159.86 tokens per second)
0.02.102.696 I llama_perf_context_print:        eval time =    1803.74 ms /    63 runs   (   28.63 ms per token,    34.93 tokens per second)
0.02.102.696 I llama_perf_context_print:       total time =    1850.70 ms /    70 tokens
0.02.102.887 I ggml_metal_free: deallocating

real	0m2.426s
user	0m0.146s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.446 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.427 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.900 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.906 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.907 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.908 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.908 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.909 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.910 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.911 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.911 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.912 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.912 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.912 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.913 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.913 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.915 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.916 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.916 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.583 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.592 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.292 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.049.293 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.294 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.295 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.295 I llama_model_loader: - type  f32:  194 tensors
0.00.049.296 I llama_model_loader: - type  f16:   98 tensors
0.00.077.484 I llm_load_vocab: special tokens cache size = 25
0.00.083.772 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.775 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.775 I llm_load_print_meta: arch             = gptneox
0.00.083.776 I llm_load_print_meta: vocab type       = BPE
0.00.083.776 I llm_load_print_meta: n_vocab          = 50304
0.00.083.776 I llm_load_print_meta: n_merges         = 50009
0.00.083.776 I llm_load_print_meta: vocab_only       = 0
0.00.083.776 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.777 I llm_load_print_meta: n_embd           = 2048
0.00.083.777 I llm_load_print_meta: n_layer          = 24
0.00.083.791 I llm_load_print_meta: n_head           = 16
0.00.083.792 I llm_load_print_meta: n_head_kv        = 16
0.00.083.792 I llm_load_print_meta: n_rot            = 32
0.00.083.792 I llm_load_print_meta: n_swa            = 0
0.00.083.792 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.792 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.793 I llm_load_print_meta: n_gqa            = 1
0.00.083.793 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.794 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.795 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.795 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.795 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.795 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.795 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.796 I llm_load_print_meta: n_ff             = 8192
0.00.083.796 I llm_load_print_meta: n_expert         = 0
0.00.083.796 I llm_load_print_meta: n_expert_used    = 0
0.00.083.796 I llm_load_print_meta: causal attn      = 1
0.00.083.797 I llm_load_print_meta: pooling type     = 0
0.00.083.797 I llm_load_print_meta: rope type        = 2
0.00.083.797 I llm_load_print_meta: rope scaling     = linear
0.00.083.797 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.798 I llm_load_print_meta: freq_scale_train = 1
0.00.083.798 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.798 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.800 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.800 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.800 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.800 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.800 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.810 I llm_load_print_meta: model type       = 1.4B
0.00.083.810 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.083.811 I llm_load_print_meta: model params     = 1.41 B
0.00.083.811 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.083.813 I llm_load_print_meta: general.name     = 1.4B
0.00.083.813 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.813 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.813 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.813 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.814 I llm_load_print_meta: LF token         = 128 ''
0.00.083.814 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.814 I llm_load_print_meta: max token length = 1024
0.00.086.395 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.086.396 I llm_load_tensors: offloading output layer to GPU
0.00.086.396 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.086.407 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.086.408 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.087.339 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.340 I llama_new_context_with_model: n_ctx         = 128
0.00.087.340 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.087.340 I llama_new_context_with_model: n_batch       = 128
0.00.087.340 I llama_new_context_with_model: n_ubatch      = 128
0.00.087.341 I llama_new_context_with_model: flash_attn    = 0
0.00.087.341 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.341 I llama_new_context_with_model: freq_scale    = 1
0.00.087.342 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.342 I ggml_metal_init: allocating
0.00.087.345 I ggml_metal_init: found device: Apple M4
0.00.087.347 I ggml_metal_init: picking default device: Apple M4
0.00.087.962 I ggml_metal_init: using embedded metal library
0.00.090.476 I ggml_metal_init: GPU name:   Apple M4
0.00.090.477 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.478 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.478 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.478 I ggml_metal_init: simdgroup reduction   = true
0.00.090.478 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.479 I ggml_metal_init: has bfloat            = true
0.00.090.479 I ggml_metal_init: use bfloat            = true
0.00.090.479 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.480 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.333 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.337 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.351 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.175 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.176 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.176 I llama_new_context_with_model: graph nodes  = 967
0.00.103.177 I llama_new_context_with_model: graph splits = 2
0.00.103.189 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.985.855 I 
0.00.985.953 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.985.985 I perplexity: tokenizing the input ..
0.00.998.602 I perplexity: tokenization took 12.616 ms
0.00.998.627 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.118.524 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.120.350 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.120.374 I llama_perf_context_print:        load time =     965.40 ms
0.01.120.375 I llama_perf_context_print: prompt eval time =     119.52 ms /   128 tokens (    0.93 ms per token,  1070.95 tokens per second)
0.01.120.376 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.120.377 I llama_perf_context_print:       total time =     134.53 ms /   129 tokens
0.01.121.175 I ggml_metal_free: deallocating

real	0m1.313s
user	0m0.121s
sys	0m0.195s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.740 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.199 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.204 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.205 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.206 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.206 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.206 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.207 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.209 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.212 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.213 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.139 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.277 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.260 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.260 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.261 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.262 I llama_model_loader: - type  f32:  194 tensors
0.00.027.263 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.078 I llm_load_vocab: special tokens cache size = 25
0.00.055.083 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.088 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.088 I llm_load_print_meta: arch             = gptneox
0.00.055.089 I llm_load_print_meta: vocab type       = BPE
0.00.055.089 I llm_load_print_meta: n_vocab          = 50304
0.00.055.089 I llm_load_print_meta: n_merges         = 50009
0.00.055.089 I llm_load_print_meta: vocab_only       = 0
0.00.055.090 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.090 I llm_load_print_meta: n_embd           = 2048
0.00.055.090 I llm_load_print_meta: n_layer          = 24
0.00.055.109 I llm_load_print_meta: n_head           = 16
0.00.055.110 I llm_load_print_meta: n_head_kv        = 16
0.00.055.111 I llm_load_print_meta: n_rot            = 32
0.00.055.111 I llm_load_print_meta: n_swa            = 0
0.00.055.111 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.111 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.111 I llm_load_print_meta: n_gqa            = 1
0.00.055.112 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.113 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.113 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.114 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.114 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.114 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.114 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.115 I llm_load_print_meta: n_ff             = 8192
0.00.055.117 I llm_load_print_meta: n_expert         = 0
0.00.055.117 I llm_load_print_meta: n_expert_used    = 0
0.00.055.117 I llm_load_print_meta: causal attn      = 1
0.00.055.117 I llm_load_print_meta: pooling type     = 0
0.00.055.117 I llm_load_print_meta: rope type        = 2
0.00.055.118 I llm_load_print_meta: rope scaling     = linear
0.00.055.118 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.118 I llm_load_print_meta: freq_scale_train = 1
0.00.055.118 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.119 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.119 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.119 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.119 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.119 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.119 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.130 I llm_load_print_meta: model type       = 1.4B
0.00.055.130 I llm_load_print_meta: model ftype      = Q8_0
0.00.055.131 I llm_load_print_meta: model params     = 1.41 B
0.00.055.131 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.055.131 I llm_load_print_meta: general.name     = 1.4B
0.00.055.131 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.132 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.132 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.132 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.132 I llm_load_print_meta: LF token         = 128 ''
0.00.055.133 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.133 I llm_load_print_meta: max token length = 1024
0.00.057.686 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.686 I llm_load_tensors: offloading output layer to GPU
0.00.057.686 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.698 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.057.699 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.058.718 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.719 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.719 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.719 I llama_new_context_with_model: n_batch       = 2048
0.00.058.720 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.720 I llama_new_context_with_model: flash_attn    = 0
0.00.058.720 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.720 I llama_new_context_with_model: freq_scale    = 1
0.00.058.721 I ggml_metal_init: allocating
0.00.058.725 I ggml_metal_init: found device: Apple M4
0.00.058.727 I ggml_metal_init: picking default device: Apple M4
0.00.059.483 I ggml_metal_init: using embedded metal library
0.00.062.009 I ggml_metal_init: GPU name:   Apple M4
0.00.062.011 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.011 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.012 I ggml_metal_init: simdgroup reduction   = true
0.00.062.012 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.013 I ggml_metal_init: has bfloat            = true
0.00.062.013 I ggml_metal_init: use bfloat            = true
0.00.062.013 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.178 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.200 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.234 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.429 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.432 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.432 I llama_new_context_with_model: graph nodes  = 967
0.00.098.432 I llama_new_context_with_model: graph splits = 2
0.00.098.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.129.629 I main: llama threadpool init, n_threads = 4
0.01.129.679 I 
0.01.129.722 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.129.722 I 
0.01.129.974 I sampler seed: 1234
0.01.129.978 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.130.013 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.130.016 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.130.016 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.213.037 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49374.13 tokens per second)
0.02.213.037 I llama_perf_context_print:        load time =    1119.88 ms
0.02.213.038 I llama_perf_context_print: prompt eval time =      39.61 ms /     7 tokens (    5.66 ms per token,   176.71 tokens per second)
0.02.213.039 I llama_perf_context_print:        eval time =    1040.64 ms /    63 runs   (   16.52 ms per token,    60.54 tokens per second)
0.02.213.041 I llama_perf_context_print:       total time =    1083.41 ms /    70 tokens
0.02.213.233 I ggml_metal_free: deallocating

real	0m2.232s
user	0m0.112s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.126 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.577 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.073 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.079 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.081 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.082 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.083 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.083 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.084 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.084 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.085 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.085 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.086 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.086 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.086 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.089 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.089 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.090 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.640 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.194 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.726 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.728 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.729 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.729 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.730 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.731 I llama_model_loader: - type  f32:  194 tensors
0.00.032.731 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.866 I llm_load_vocab: special tokens cache size = 25
0.00.064.127 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.129 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.130 I llm_load_print_meta: arch             = gptneox
0.00.064.130 I llm_load_print_meta: vocab type       = BPE
0.00.064.130 I llm_load_print_meta: n_vocab          = 50304
0.00.064.131 I llm_load_print_meta: n_merges         = 50009
0.00.064.131 I llm_load_print_meta: vocab_only       = 0
0.00.064.131 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.131 I llm_load_print_meta: n_embd           = 2048
0.00.064.131 I llm_load_print_meta: n_layer          = 24
0.00.064.147 I llm_load_print_meta: n_head           = 16
0.00.064.149 I llm_load_print_meta: n_head_kv        = 16
0.00.064.149 I llm_load_print_meta: n_rot            = 32
0.00.064.150 I llm_load_print_meta: n_swa            = 0
0.00.064.150 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.150 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.151 I llm_load_print_meta: n_gqa            = 1
0.00.064.151 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.152 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.152 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.153 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.153 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.153 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.153 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.154 I llm_load_print_meta: n_ff             = 8192
0.00.064.154 I llm_load_print_meta: n_expert         = 0
0.00.064.154 I llm_load_print_meta: n_expert_used    = 0
0.00.064.154 I llm_load_print_meta: causal attn      = 1
0.00.064.154 I llm_load_print_meta: pooling type     = 0
0.00.064.155 I llm_load_print_meta: rope type        = 2
0.00.064.155 I llm_load_print_meta: rope scaling     = linear
0.00.064.155 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.155 I llm_load_print_meta: freq_scale_train = 1
0.00.064.158 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.158 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.158 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.158 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.158 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.158 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.158 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.168 I llm_load_print_meta: model type       = 1.4B
0.00.064.168 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.169 I llm_load_print_meta: model params     = 1.41 B
0.00.064.169 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.169 I llm_load_print_meta: general.name     = 1.4B
0.00.064.169 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.170 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.170 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.170 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.170 I llm_load_print_meta: LF token         = 128 ''
0.00.064.171 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.171 I llm_load_print_meta: max token length = 1024
0.00.066.467 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.469 I llm_load_tensors: offloading output layer to GPU
0.00.066.469 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.480 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.481 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.415 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.416 I llama_new_context_with_model: n_ctx         = 128
0.00.067.416 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.416 I llama_new_context_with_model: n_batch       = 128
0.00.067.416 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.417 I llama_new_context_with_model: flash_attn    = 0
0.00.067.417 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.417 I llama_new_context_with_model: freq_scale    = 1
0.00.067.418 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.418 I ggml_metal_init: allocating
0.00.067.421 I ggml_metal_init: found device: Apple M4
0.00.067.423 I ggml_metal_init: picking default device: Apple M4
0.00.068.065 I ggml_metal_init: using embedded metal library
0.00.070.675 I ggml_metal_init: GPU name:   Apple M4
0.00.070.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.677 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.678 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.678 I ggml_metal_init: simdgroup reduction   = true
0.00.070.678 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.678 I ggml_metal_init: has bfloat            = true
0.00.070.678 I ggml_metal_init: use bfloat            = true
0.00.070.679 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.680 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.009 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.014 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.029 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.052 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.083.053 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.083.054 I llama_new_context_with_model: graph nodes  = 967
0.00.083.054 I llama_new_context_with_model: graph splits = 2
0.00.083.067 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.886.833 I 
0.00.886.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.886.868 I perplexity: tokenizing the input ..
0.00.894.687 I perplexity: tokenization took 7.817 ms
0.00.894.700 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.019.397 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.020.673 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.020.692 I llama_perf_context_print:        load time =     875.25 ms
0.01.020.693 I llama_perf_context_print: prompt eval time =     124.46 ms /   128 tokens (    0.97 ms per token,  1028.45 tokens per second)
0.01.020.694 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.020.694 I llama_perf_context_print:       total time =     133.86 ms /   129 tokens
0.01.021.054 I ggml_metal_free: deallocating

real	0m1.038s
user	0m0.091s
sys	0m0.142s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.666 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.677 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.677 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.678 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.678 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.680 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.680 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.680 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.681 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.681 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.681 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.682 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.685 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.686 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.562 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.431 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.433 I llama_model_loader: - type  f32:  194 tensors
0.00.026.434 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.434 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.963 I llm_load_vocab: special tokens cache size = 25
0.00.053.005 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.008 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.008 I llm_load_print_meta: arch             = gptneox
0.00.053.008 I llm_load_print_meta: vocab type       = BPE
0.00.053.009 I llm_load_print_meta: n_vocab          = 50304
0.00.053.009 I llm_load_print_meta: n_merges         = 50009
0.00.053.009 I llm_load_print_meta: vocab_only       = 0
0.00.053.009 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.009 I llm_load_print_meta: n_embd           = 2048
0.00.053.010 I llm_load_print_meta: n_layer          = 24
0.00.053.026 I llm_load_print_meta: n_head           = 16
0.00.053.028 I llm_load_print_meta: n_head_kv        = 16
0.00.053.028 I llm_load_print_meta: n_rot            = 32
0.00.053.028 I llm_load_print_meta: n_swa            = 0
0.00.053.028 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.028 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.029 I llm_load_print_meta: n_gqa            = 1
0.00.053.031 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.032 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.032 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.033 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.033 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.035 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.036 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.036 I llm_load_print_meta: n_ff             = 8192
0.00.053.036 I llm_load_print_meta: n_expert         = 0
0.00.053.036 I llm_load_print_meta: n_expert_used    = 0
0.00.053.037 I llm_load_print_meta: causal attn      = 1
0.00.053.037 I llm_load_print_meta: pooling type     = 0
0.00.053.037 I llm_load_print_meta: rope type        = 2
0.00.053.037 I llm_load_print_meta: rope scaling     = linear
0.00.053.038 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.043 I llm_load_print_meta: freq_scale_train = 1
0.00.053.043 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.043 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.044 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.044 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.044 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.044 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.044 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.055 I llm_load_print_meta: model type       = 1.4B
0.00.053.055 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.055 I llm_load_print_meta: model params     = 1.41 B
0.00.053.056 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.056 I llm_load_print_meta: general.name     = 1.4B
0.00.053.056 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.056 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.057 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.057 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.057 I llm_load_print_meta: LF token         = 128 ''
0.00.053.057 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.057 I llm_load_print_meta: max token length = 1024
0.00.054.954 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.954 I llm_load_tensors: offloading output layer to GPU
0.00.054.954 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.965 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.966 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.873 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.874 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.874 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.874 I llama_new_context_with_model: n_batch       = 2048
0.00.055.874 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.874 I llama_new_context_with_model: flash_attn    = 0
0.00.055.875 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.875 I llama_new_context_with_model: freq_scale    = 1
0.00.055.875 I ggml_metal_init: allocating
0.00.055.879 I ggml_metal_init: found device: Apple M4
0.00.055.880 I ggml_metal_init: picking default device: Apple M4
0.00.056.672 I ggml_metal_init: using embedded metal library
0.00.059.217 I ggml_metal_init: GPU name:   Apple M4
0.00.059.218 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.219 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.219 I ggml_metal_init: simdgroup reduction   = true
0.00.059.220 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.220 I ggml_metal_init: has bfloat            = true
0.00.059.220 I ggml_metal_init: use bfloat            = true
0.00.059.220 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.221 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.036 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.046 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.074 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.253 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.255 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.255 I llama_new_context_with_model: graph nodes  = 967
0.00.094.255 I llama_new_context_with_model: graph splits = 2
0.00.094.272 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.581 I main: llama threadpool init, n_threads = 4
0.00.670.621 I 
0.00.670.649 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.649 I 
0.00.670.875 I sampler seed: 1234
0.00.670.879 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.670.899 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.670.900 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.670.900 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.349.432 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.01.349.432 I llama_perf_context_print:        load time =     659.82 ms
0.01.349.433 I llama_perf_context_print: prompt eval time =      42.23 ms /     7 tokens (    6.03 ms per token,   165.77 tokens per second)
0.01.349.434 I llama_perf_context_print:        eval time =     633.21 ms /    63 runs   (   10.05 ms per token,    99.49 tokens per second)
0.01.349.434 I llama_perf_context_print:       total time =     678.85 ms /    70 tokens
0.01.349.613 I ggml_metal_free: deallocating

real	0m1.369s
user	0m0.111s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.191 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.130 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.134 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.136 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.142 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.143 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.143 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.144 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.144 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.146 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.147 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.147 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.147 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.148 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.148 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.149 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.150 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.150 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.015 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.805 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.805 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.806 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.806 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.807 I llama_model_loader: - type  f32:  194 tensors
0.00.024.807 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.807 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.770 I llm_load_vocab: special tokens cache size = 25
0.00.050.834 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.836 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.837 I llm_load_print_meta: arch             = gptneox
0.00.050.837 I llm_load_print_meta: vocab type       = BPE
0.00.050.837 I llm_load_print_meta: n_vocab          = 50304
0.00.050.838 I llm_load_print_meta: n_merges         = 50009
0.00.050.838 I llm_load_print_meta: vocab_only       = 0
0.00.050.838 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.838 I llm_load_print_meta: n_embd           = 2048
0.00.050.838 I llm_load_print_meta: n_layer          = 24
0.00.050.853 I llm_load_print_meta: n_head           = 16
0.00.050.854 I llm_load_print_meta: n_head_kv        = 16
0.00.050.855 I llm_load_print_meta: n_rot            = 32
0.00.050.855 I llm_load_print_meta: n_swa            = 0
0.00.050.855 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.855 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.856 I llm_load_print_meta: n_gqa            = 1
0.00.050.857 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.857 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.858 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.861 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.861 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.861 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.861 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.862 I llm_load_print_meta: n_ff             = 8192
0.00.050.862 I llm_load_print_meta: n_expert         = 0
0.00.050.862 I llm_load_print_meta: n_expert_used    = 0
0.00.050.862 I llm_load_print_meta: causal attn      = 1
0.00.050.862 I llm_load_print_meta: pooling type     = 0
0.00.050.862 I llm_load_print_meta: rope type        = 2
0.00.050.864 I llm_load_print_meta: rope scaling     = linear
0.00.050.864 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.864 I llm_load_print_meta: freq_scale_train = 1
0.00.050.864 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.864 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.865 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.865 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.866 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.866 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.866 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.875 I llm_load_print_meta: model type       = 1.4B
0.00.050.876 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.876 I llm_load_print_meta: model params     = 1.41 B
0.00.050.876 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.877 I llm_load_print_meta: general.name     = 1.4B
0.00.050.877 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.877 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.877 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.877 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.877 I llm_load_print_meta: LF token         = 128 ''
0.00.050.878 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.878 I llm_load_print_meta: max token length = 1024
0.00.052.788 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.788 I llm_load_tensors: offloading output layer to GPU
0.00.052.788 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.798 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.800 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.723 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.724 I llama_new_context_with_model: n_ctx         = 128
0.00.053.724 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.724 I llama_new_context_with_model: n_batch       = 128
0.00.053.724 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.724 I llama_new_context_with_model: flash_attn    = 0
0.00.053.725 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.725 I llama_new_context_with_model: freq_scale    = 1
0.00.053.725 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.726 I ggml_metal_init: allocating
0.00.053.729 I ggml_metal_init: found device: Apple M4
0.00.053.730 I ggml_metal_init: picking default device: Apple M4
0.00.054.296 I ggml_metal_init: using embedded metal library
0.00.056.585 I ggml_metal_init: GPU name:   Apple M4
0.00.056.586 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.586 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.587 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.587 I ggml_metal_init: simdgroup reduction   = true
0.00.056.587 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.587 I ggml_metal_init: has bfloat            = true
0.00.056.587 I ggml_metal_init: use bfloat            = true
0.00.056.588 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.365 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.367 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.380 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.286 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.287 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.287 I llama_new_context_with_model: graph nodes  = 967
0.00.068.287 I llama_new_context_with_model: graph splits = 2
0.00.068.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.485 I 
0.00.622.520 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.530 I perplexity: tokenizing the input ..
0.00.630.687 I perplexity: tokenization took 8.156 ms
0.00.630.698 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.753.468 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.754.640 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.754.666 I llama_perf_context_print:        load time =     612.29 ms
0.00.754.667 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.55 tokens per second)
0.00.754.671 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.671 I llama_perf_context_print:       total time =     132.18 ms /   129 tokens
0.00.755.166 I ggml_metal_free: deallocating

real	0m0.771s
user	0m0.078s
sys	0m0.098s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.333 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.358 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.362 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.368 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.369 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.369 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.371 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.372 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.372 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.373 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.373 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.373 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.374 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.378 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.378 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.380 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.380 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.380 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.333 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.202 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.203 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.204 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.204 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.204 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.205 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.205 I llama_model_loader: - type  f32:  194 tensors
0.00.025.206 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.206 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.115 I llm_load_vocab: special tokens cache size = 25
0.00.052.046 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.048 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.049 I llm_load_print_meta: arch             = gptneox
0.00.052.049 I llm_load_print_meta: vocab type       = BPE
0.00.052.049 I llm_load_print_meta: n_vocab          = 50304
0.00.052.049 I llm_load_print_meta: n_merges         = 50009
0.00.052.050 I llm_load_print_meta: vocab_only       = 0
0.00.052.050 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.050 I llm_load_print_meta: n_embd           = 2048
0.00.052.050 I llm_load_print_meta: n_layer          = 24
0.00.052.059 I llm_load_print_meta: n_head           = 16
0.00.052.060 I llm_load_print_meta: n_head_kv        = 16
0.00.052.060 I llm_load_print_meta: n_rot            = 32
0.00.052.060 I llm_load_print_meta: n_swa            = 0
0.00.052.061 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.063 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.064 I llm_load_print_meta: n_gqa            = 1
0.00.052.065 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.065 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.066 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.066 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.066 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.067 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.067 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.071 I llm_load_print_meta: n_ff             = 8192
0.00.052.071 I llm_load_print_meta: n_expert         = 0
0.00.052.072 I llm_load_print_meta: n_expert_used    = 0
0.00.052.072 I llm_load_print_meta: causal attn      = 1
0.00.052.072 I llm_load_print_meta: pooling type     = 0
0.00.052.072 I llm_load_print_meta: rope type        = 2
0.00.052.072 I llm_load_print_meta: rope scaling     = linear
0.00.052.072 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.073 I llm_load_print_meta: freq_scale_train = 1
0.00.052.073 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.073 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.073 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.073 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.074 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.074 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.075 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.079 I llm_load_print_meta: model type       = 1.4B
0.00.052.080 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.081 I llm_load_print_meta: model params     = 1.41 B
0.00.052.081 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.082 I llm_load_print_meta: general.name     = 1.4B
0.00.052.082 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.082 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.082 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.082 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.082 I llm_load_print_meta: LF token         = 128 ''
0.00.052.083 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.083 I llm_load_print_meta: max token length = 1024
0.00.053.797 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.797 I llm_load_tensors: offloading output layer to GPU
0.00.053.798 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.803 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.803 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.681 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.681 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.681 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.682 I llama_new_context_with_model: n_batch       = 2048
0.00.054.682 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.682 I llama_new_context_with_model: flash_attn    = 0
0.00.054.683 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.683 I llama_new_context_with_model: freq_scale    = 1
0.00.054.683 I ggml_metal_init: allocating
0.00.054.686 I ggml_metal_init: found device: Apple M4
0.00.054.688 I ggml_metal_init: picking default device: Apple M4
0.00.055.267 I ggml_metal_init: using embedded metal library
0.00.057.571 I ggml_metal_init: GPU name:   Apple M4
0.00.057.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.574 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.574 I ggml_metal_init: simdgroup reduction   = true
0.00.057.574 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.574 I ggml_metal_init: has bfloat            = true
0.00.057.574 I ggml_metal_init: use bfloat            = true
0.00.057.575 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.222 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.231 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.249 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.244 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.245 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.245 I llama_new_context_with_model: graph nodes  = 967
0.00.086.245 I llama_new_context_with_model: graph splits = 2
0.00.086.259 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.232 I main: llama threadpool init, n_threads = 4
0.00.690.273 I 
0.00.690.305 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.305 I 
0.00.690.536 I sampler seed: 1234
0.00.690.541 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.563 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.563 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.563 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.414.310 I llama_perf_sampler_print:    sampling time =       1.08 ms /    71 runs   (    0.02 ms per token, 65985.13 tokens per second)
0.01.414.311 I llama_perf_context_print:        load time =     680.89 ms
0.01.414.312 I llama_perf_context_print: prompt eval time =      39.61 ms /     7 tokens (    5.66 ms per token,   176.71 tokens per second)
0.01.414.312 I llama_perf_context_print:        eval time =     681.35 ms /    63 runs   (   10.82 ms per token,    92.46 tokens per second)
0.01.414.313 I llama_perf_context_print:       total time =     724.08 ms /    70 tokens
0.01.414.512 I ggml_metal_free: deallocating

real	0m1.430s
user	0m0.109s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.883 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.918 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.923 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.924 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.925 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.925 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.925 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.926 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.926 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.927 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.927 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.927 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.928 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.928 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.928 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.934 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.934 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.935 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.849 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.903 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.908 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.909 I llama_model_loader: - type  f32:  194 tensors
0.00.023.909 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.909 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.711 I llm_load_vocab: special tokens cache size = 25
0.00.050.668 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.671 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.671 I llm_load_print_meta: arch             = gptneox
0.00.050.672 I llm_load_print_meta: vocab type       = BPE
0.00.050.672 I llm_load_print_meta: n_vocab          = 50304
0.00.050.672 I llm_load_print_meta: n_merges         = 50009
0.00.050.672 I llm_load_print_meta: vocab_only       = 0
0.00.050.673 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.673 I llm_load_print_meta: n_embd           = 2048
0.00.050.673 I llm_load_print_meta: n_layer          = 24
0.00.050.687 I llm_load_print_meta: n_head           = 16
0.00.050.689 I llm_load_print_meta: n_head_kv        = 16
0.00.050.689 I llm_load_print_meta: n_rot            = 32
0.00.050.689 I llm_load_print_meta: n_swa            = 0
0.00.050.689 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.690 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.690 I llm_load_print_meta: n_gqa            = 1
0.00.050.691 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.692 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.692 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.693 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.693 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.693 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.694 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.694 I llm_load_print_meta: n_ff             = 8192
0.00.050.694 I llm_load_print_meta: n_expert         = 0
0.00.050.695 I llm_load_print_meta: n_expert_used    = 0
0.00.050.695 I llm_load_print_meta: causal attn      = 1
0.00.050.696 I llm_load_print_meta: pooling type     = 0
0.00.050.696 I llm_load_print_meta: rope type        = 2
0.00.050.696 I llm_load_print_meta: rope scaling     = linear
0.00.050.698 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.699 I llm_load_print_meta: freq_scale_train = 1
0.00.050.700 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.700 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.700 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.700 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.702 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.702 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.703 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.712 I llm_load_print_meta: model type       = 1.4B
0.00.050.713 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.713 I llm_load_print_meta: model params     = 1.41 B
0.00.050.714 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.714 I llm_load_print_meta: general.name     = 1.4B
0.00.050.714 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.714 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.714 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.714 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.715 I llm_load_print_meta: LF token         = 128 ''
0.00.050.715 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.715 I llm_load_print_meta: max token length = 1024
0.00.052.705 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.705 I llm_load_tensors: offloading output layer to GPU
0.00.052.706 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.717 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.718 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.648 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.649 I llama_new_context_with_model: n_ctx         = 128
0.00.053.650 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.650 I llama_new_context_with_model: n_batch       = 128
0.00.053.650 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.650 I llama_new_context_with_model: flash_attn    = 0
0.00.053.651 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.651 I llama_new_context_with_model: freq_scale    = 1
0.00.053.651 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.652 I ggml_metal_init: allocating
0.00.053.658 I ggml_metal_init: found device: Apple M4
0.00.053.660 I ggml_metal_init: picking default device: Apple M4
0.00.054.246 I ggml_metal_init: using embedded metal library
0.00.056.579 I ggml_metal_init: GPU name:   Apple M4
0.00.056.581 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.581 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.581 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.582 I ggml_metal_init: simdgroup reduction   = true
0.00.056.582 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.582 I ggml_metal_init: has bfloat            = true
0.00.056.582 I ggml_metal_init: use bfloat            = true
0.00.056.582 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.583 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.366 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.372 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.387 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.291 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.292 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.292 I llama_new_context_with_model: graph nodes  = 967
0.00.068.293 I llama_new_context_with_model: graph splits = 2
0.00.068.305 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.985 I 
0.00.645.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.024 I perplexity: tokenizing the input ..
0.00.652.833 I perplexity: tokenization took 7.808 ms
0.00.652.848 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.775.702 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.776.862 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.776.872 I llama_perf_context_print:        load time =     636.10 ms
0.00.776.873 I llama_perf_context_print: prompt eval time =     122.63 ms /   128 tokens (    0.96 ms per token,  1043.80 tokens per second)
0.00.776.874 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.875 I llama_perf_context_print:       total time =     131.89 ms /   129 tokens
0.00.777.282 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.078s
sys	0m0.104s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.012.404 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.139 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.143 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.149 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.149 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.150 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.151 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.151 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.151 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.152 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.152 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.153 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.154 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.155 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.066 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.106 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.939 I llama_model_loader: - type  f32:  194 tensors
0.00.027.939 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.940 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.076 I llm_load_vocab: special tokens cache size = 25
0.00.055.127 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.130 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.130 I llm_load_print_meta: arch             = gptneox
0.00.055.131 I llm_load_print_meta: vocab type       = BPE
0.00.055.131 I llm_load_print_meta: n_vocab          = 50304
0.00.055.131 I llm_load_print_meta: n_merges         = 50009
0.00.055.131 I llm_load_print_meta: vocab_only       = 0
0.00.055.132 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.132 I llm_load_print_meta: n_embd           = 2048
0.00.055.132 I llm_load_print_meta: n_layer          = 24
0.00.055.147 I llm_load_print_meta: n_head           = 16
0.00.055.148 I llm_load_print_meta: n_head_kv        = 16
0.00.055.148 I llm_load_print_meta: n_rot            = 32
0.00.055.148 I llm_load_print_meta: n_swa            = 0
0.00.055.148 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.149 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.151 I llm_load_print_meta: n_gqa            = 1
0.00.055.152 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.153 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.153 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.155 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.155 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.155 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.155 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.156 I llm_load_print_meta: n_ff             = 8192
0.00.055.156 I llm_load_print_meta: n_expert         = 0
0.00.055.156 I llm_load_print_meta: n_expert_used    = 0
0.00.055.157 I llm_load_print_meta: causal attn      = 1
0.00.055.158 I llm_load_print_meta: pooling type     = 0
0.00.055.158 I llm_load_print_meta: rope type        = 2
0.00.055.158 I llm_load_print_meta: rope scaling     = linear
0.00.055.158 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.159 I llm_load_print_meta: freq_scale_train = 1
0.00.055.159 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.159 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.159 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.159 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.159 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.159 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.159 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.169 I llm_load_print_meta: model type       = 1.4B
0.00.055.170 I llm_load_print_meta: model ftype      = Q5_0
0.00.055.170 I llm_load_print_meta: model params     = 1.41 B
0.00.055.170 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.055.171 I llm_load_print_meta: general.name     = 1.4B
0.00.055.173 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.173 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.173 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.173 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.174 I llm_load_print_meta: LF token         = 128 ''
0.00.055.174 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.174 I llm_load_print_meta: max token length = 1024
0.00.057.206 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.206 I llm_load_tensors: offloading output layer to GPU
0.00.057.206 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.217 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.057.218 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.058.201 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.202 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.202 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.202 I llama_new_context_with_model: n_batch       = 2048
0.00.058.202 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.202 I llama_new_context_with_model: flash_attn    = 0
0.00.058.203 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.203 I llama_new_context_with_model: freq_scale    = 1
0.00.058.204 I ggml_metal_init: allocating
0.00.058.210 I ggml_metal_init: found device: Apple M4
0.00.058.212 I ggml_metal_init: picking default device: Apple M4
0.00.058.809 I ggml_metal_init: using embedded metal library
0.00.061.135 I ggml_metal_init: GPU name:   Apple M4
0.00.061.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.137 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.138 I ggml_metal_init: simdgroup reduction   = true
0.00.061.138 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.138 I ggml_metal_init: has bfloat            = true
0.00.061.138 I ggml_metal_init: use bfloat            = true
0.00.061.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.139 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.736 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.745 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.761 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.827 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.828 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.828 I llama_new_context_with_model: graph nodes  = 967
0.00.092.829 I llama_new_context_with_model: graph splits = 2
0.00.092.842 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.683 I main: llama threadpool init, n_threads = 4
0.00.765.722 I 
0.00.765.753 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.756 I 
0.00.765.876 I sampler seed: 1234
0.00.765.881 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.765.914 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.765.915 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.765.915 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.557.574 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.01.557.575 I llama_perf_context_print:        load time =     753.27 ms
0.01.557.579 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.36 tokens per second)
0.01.557.580 I llama_perf_context_print:        eval time =     745.86 ms /    63 runs   (   11.84 ms per token,    84.47 tokens per second)
0.01.557.580 I llama_perf_context_print:       total time =     791.89 ms /    70 tokens
0.01.557.803 I ggml_metal_free: deallocating

real	0m1.577s
user	0m0.111s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.261 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.161 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.165 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.166 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.167 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.167 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.168 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.168 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.169 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.171 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.172 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.172 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.173 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.173 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.995 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.784 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.785 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.785 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.785 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.786 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.786 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.787 I llama_model_loader: - type  f32:  194 tensors
0.00.023.787 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.787 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.805 I llm_load_vocab: special tokens cache size = 25
0.00.049.829 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.832 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.832 I llm_load_print_meta: arch             = gptneox
0.00.049.833 I llm_load_print_meta: vocab type       = BPE
0.00.049.833 I llm_load_print_meta: n_vocab          = 50304
0.00.049.833 I llm_load_print_meta: n_merges         = 50009
0.00.049.833 I llm_load_print_meta: vocab_only       = 0
0.00.049.834 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.834 I llm_load_print_meta: n_embd           = 2048
0.00.049.834 I llm_load_print_meta: n_layer          = 24
0.00.049.848 I llm_load_print_meta: n_head           = 16
0.00.049.849 I llm_load_print_meta: n_head_kv        = 16
0.00.049.849 I llm_load_print_meta: n_rot            = 32
0.00.049.849 I llm_load_print_meta: n_swa            = 0
0.00.049.849 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.850 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.850 I llm_load_print_meta: n_gqa            = 1
0.00.049.851 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.852 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.854 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.855 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.855 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.855 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.856 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.857 I llm_load_print_meta: n_ff             = 8192
0.00.049.857 I llm_load_print_meta: n_expert         = 0
0.00.049.857 I llm_load_print_meta: n_expert_used    = 0
0.00.049.857 I llm_load_print_meta: causal attn      = 1
0.00.049.861 I llm_load_print_meta: pooling type     = 0
0.00.049.861 I llm_load_print_meta: rope type        = 2
0.00.049.861 I llm_load_print_meta: rope scaling     = linear
0.00.049.861 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.863 I llm_load_print_meta: freq_scale_train = 1
0.00.049.863 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.863 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.863 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.863 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.863 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.864 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.864 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.873 I llm_load_print_meta: model type       = 1.4B
0.00.049.874 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.874 I llm_load_print_meta: model params     = 1.41 B
0.00.049.875 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.875 I llm_load_print_meta: general.name     = 1.4B
0.00.049.875 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.875 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.875 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.875 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.876 I llm_load_print_meta: LF token         = 128 ''
0.00.049.876 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.876 I llm_load_print_meta: max token length = 1024
0.00.051.763 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.763 I llm_load_tensors: offloading output layer to GPU
0.00.051.763 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.773 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.774 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.691 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.692 I llama_new_context_with_model: n_ctx         = 128
0.00.052.692 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.693 I llama_new_context_with_model: n_batch       = 128
0.00.052.693 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.693 I llama_new_context_with_model: flash_attn    = 0
0.00.052.693 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.693 I llama_new_context_with_model: freq_scale    = 1
0.00.052.694 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.694 I ggml_metal_init: allocating
0.00.052.697 I ggml_metal_init: found device: Apple M4
0.00.052.699 I ggml_metal_init: picking default device: Apple M4
0.00.053.249 I ggml_metal_init: using embedded metal library
0.00.055.558 I ggml_metal_init: GPU name:   Apple M4
0.00.055.560 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.560 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.561 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.561 I ggml_metal_init: simdgroup reduction   = true
0.00.055.561 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.561 I ggml_metal_init: has bfloat            = true
0.00.055.561 I ggml_metal_init: use bfloat            = true
0.00.055.562 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.562 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.309 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.313 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.326 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.187 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.188 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.189 I llama_new_context_with_model: graph nodes  = 967
0.00.067.189 I llama_new_context_with_model: graph splits = 2
0.00.067.201 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.735 I 
0.00.703.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.781 I perplexity: tokenizing the input ..
0.00.711.536 I perplexity: tokenization took 7.754 ms
0.00.711.547 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.846.403 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.847.568 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.847.585 I llama_perf_context_print:        load time =     694.47 ms
0.00.847.586 I llama_perf_context_print: prompt eval time =     134.63 ms /   128 tokens (    1.05 ms per token,   950.77 tokens per second)
0.00.847.587 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.588 I llama_perf_context_print:       total time =     143.85 ms /   129 tokens
0.00.848.066 I ggml_metal_free: deallocating

real	0m0.865s
user	0m0.077s
sys	0m0.122s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.746 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.648 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.653 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.660 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.661 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.662 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.662 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.663 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.663 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.663 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.666 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.569 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.626 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.570 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.572 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.572 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.572 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.573 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.573 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.574 I llama_model_loader: - type  f32:  194 tensors
0.00.023.574 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.574 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.745 I llm_load_vocab: special tokens cache size = 25
0.00.049.836 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.839 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.839 I llm_load_print_meta: arch             = gptneox
0.00.049.839 I llm_load_print_meta: vocab type       = BPE
0.00.049.840 I llm_load_print_meta: n_vocab          = 50304
0.00.049.840 I llm_load_print_meta: n_merges         = 50009
0.00.049.840 I llm_load_print_meta: vocab_only       = 0
0.00.049.840 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.840 I llm_load_print_meta: n_embd           = 2048
0.00.049.840 I llm_load_print_meta: n_layer          = 24
0.00.049.856 I llm_load_print_meta: n_head           = 16
0.00.049.857 I llm_load_print_meta: n_head_kv        = 16
0.00.049.857 I llm_load_print_meta: n_rot            = 32
0.00.049.857 I llm_load_print_meta: n_swa            = 0
0.00.049.857 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.857 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.858 I llm_load_print_meta: n_gqa            = 1
0.00.049.859 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.860 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.860 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.860 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.861 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.862 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.862 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.864 I llm_load_print_meta: n_ff             = 8192
0.00.049.865 I llm_load_print_meta: n_expert         = 0
0.00.049.865 I llm_load_print_meta: n_expert_used    = 0
0.00.049.866 I llm_load_print_meta: causal attn      = 1
0.00.049.866 I llm_load_print_meta: pooling type     = 0
0.00.049.866 I llm_load_print_meta: rope type        = 2
0.00.049.866 I llm_load_print_meta: rope scaling     = linear
0.00.049.867 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.867 I llm_load_print_meta: freq_scale_train = 1
0.00.049.867 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.867 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.868 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.868 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.868 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.868 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.868 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.877 I llm_load_print_meta: model type       = 1.4B
0.00.049.878 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.878 I llm_load_print_meta: model params     = 1.41 B
0.00.049.879 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.879 I llm_load_print_meta: general.name     = 1.4B
0.00.049.879 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.879 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.879 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.879 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.880 I llm_load_print_meta: LF token         = 128 ''
0.00.049.880 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.880 I llm_load_print_meta: max token length = 1024
0.00.051.820 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.821 I llm_load_tensors: offloading output layer to GPU
0.00.051.821 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.832 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.833 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.770 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.771 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.771 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.771 I llama_new_context_with_model: n_batch       = 2048
0.00.052.771 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.771 I llama_new_context_with_model: flash_attn    = 0
0.00.052.772 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.772 I llama_new_context_with_model: freq_scale    = 1
0.00.052.772 I ggml_metal_init: allocating
0.00.052.776 I ggml_metal_init: found device: Apple M4
0.00.052.778 I ggml_metal_init: picking default device: Apple M4
0.00.053.380 I ggml_metal_init: using embedded metal library
0.00.055.739 I ggml_metal_init: GPU name:   Apple M4
0.00.055.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.741 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.742 I ggml_metal_init: simdgroup reduction   = true
0.00.055.742 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.742 I ggml_metal_init: has bfloat            = true
0.00.055.742 I ggml_metal_init: use bfloat            = true
0.00.055.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.007 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.014 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.036 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.005 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.007 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.007 I llama_new_context_with_model: graph nodes  = 967
0.00.086.008 I llama_new_context_with_model: graph splits = 2
0.00.086.022 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.436 I main: llama threadpool init, n_threads = 4
0.00.713.477 I 
0.00.713.528 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.529 I 
0.00.713.755 I sampler seed: 1234
0.00.713.759 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.713.770 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.713.771 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.713.771 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.551.186 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.01.551.187 I llama_perf_context_print:        load time =     704.68 ms
0.01.551.187 I llama_perf_context_print: prompt eval time =      42.27 ms /     7 tokens (    6.04 ms per token,   165.61 tokens per second)
0.01.551.188 I llama_perf_context_print:        eval time =     792.17 ms /    63 runs   (   12.57 ms per token,    79.53 tokens per second)
0.01.551.188 I llama_perf_context_print:       total time =     837.75 ms /    70 tokens
0.01.551.352 I ggml_metal_free: deallocating

real	0m1.569s
user	0m0.111s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.820 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.469 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.477 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.478 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.478 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.478 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.479 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.479 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.480 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.480 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.481 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.481 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.481 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.482 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.392 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.299 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.300 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.301 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.301 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.301 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.302 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.302 I llama_model_loader: - type  f32:  194 tensors
0.00.023.302 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.303 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.070 I llm_load_vocab: special tokens cache size = 25
0.00.050.140 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.142 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.143 I llm_load_print_meta: arch             = gptneox
0.00.050.143 I llm_load_print_meta: vocab type       = BPE
0.00.050.143 I llm_load_print_meta: n_vocab          = 50304
0.00.050.143 I llm_load_print_meta: n_merges         = 50009
0.00.050.144 I llm_load_print_meta: vocab_only       = 0
0.00.050.144 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.144 I llm_load_print_meta: n_embd           = 2048
0.00.050.144 I llm_load_print_meta: n_layer          = 24
0.00.050.159 I llm_load_print_meta: n_head           = 16
0.00.050.160 I llm_load_print_meta: n_head_kv        = 16
0.00.050.160 I llm_load_print_meta: n_rot            = 32
0.00.050.160 I llm_load_print_meta: n_swa            = 0
0.00.050.163 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.163 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.164 I llm_load_print_meta: n_gqa            = 1
0.00.050.164 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.165 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.166 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.166 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.166 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.166 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.166 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.167 I llm_load_print_meta: n_ff             = 8192
0.00.050.168 I llm_load_print_meta: n_expert         = 0
0.00.050.168 I llm_load_print_meta: n_expert_used    = 0
0.00.050.168 I llm_load_print_meta: causal attn      = 1
0.00.050.169 I llm_load_print_meta: pooling type     = 0
0.00.050.169 I llm_load_print_meta: rope type        = 2
0.00.050.170 I llm_load_print_meta: rope scaling     = linear
0.00.050.170 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.170 I llm_load_print_meta: freq_scale_train = 1
0.00.050.170 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.171 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.171 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.171 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.171 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.171 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.171 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.181 I llm_load_print_meta: model type       = 1.4B
0.00.050.181 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.182 I llm_load_print_meta: model params     = 1.41 B
0.00.050.183 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.183 I llm_load_print_meta: general.name     = 1.4B
0.00.050.183 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.183 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.183 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.184 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.184 I llm_load_print_meta: LF token         = 128 ''
0.00.050.184 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.184 I llm_load_print_meta: max token length = 1024
0.00.052.150 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.151 I llm_load_tensors: offloading output layer to GPU
0.00.052.151 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.161 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.163 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.055 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.055 I llama_new_context_with_model: n_ctx         = 128
0.00.053.056 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.056 I llama_new_context_with_model: n_batch       = 128
0.00.053.056 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.056 I llama_new_context_with_model: flash_attn    = 0
0.00.053.057 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.057 I llama_new_context_with_model: freq_scale    = 1
0.00.053.057 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.058 I ggml_metal_init: allocating
0.00.053.061 I ggml_metal_init: found device: Apple M4
0.00.053.063 I ggml_metal_init: picking default device: Apple M4
0.00.053.636 I ggml_metal_init: using embedded metal library
0.00.055.924 I ggml_metal_init: GPU name:   Apple M4
0.00.055.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.926 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.926 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.927 I ggml_metal_init: simdgroup reduction   = true
0.00.055.927 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.927 I ggml_metal_init: has bfloat            = true
0.00.055.927 I ggml_metal_init: use bfloat            = true
0.00.055.927 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.795 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.798 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.813 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.713 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.714 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.714 I llama_new_context_with_model: graph nodes  = 967
0.00.067.714 I llama_new_context_with_model: graph splits = 2
0.00.067.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.933 I 
0.00.672.964 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.973 I perplexity: tokenizing the input ..
0.00.681.236 I perplexity: tokenization took 8.262 ms
0.00.681.247 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.815.600 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.816.760 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.816.769 I llama_perf_context_print:        load time =     664.11 ms
0.00.816.771 I llama_perf_context_print: prompt eval time =     134.13 ms /   128 tokens (    1.05 ms per token,   954.32 tokens per second)
0.00.816.772 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.816.772 I llama_perf_context_print:       total time =     143.84 ms /   129 tokens
0.00.817.274 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.079s
sys	0m0.126s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.699 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.150 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.158 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.163 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.163 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.163 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.962 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.056 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.909 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.910 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.910 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.911 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.911 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.911 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.912 I llama_model_loader: - type  f32:  194 tensors
0.00.023.912 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.912 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.913 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.063 I llm_load_vocab: special tokens cache size = 25
0.00.050.041 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.044 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.044 I llm_load_print_meta: arch             = gptneox
0.00.050.045 I llm_load_print_meta: vocab type       = BPE
0.00.050.045 I llm_load_print_meta: n_vocab          = 50304
0.00.050.046 I llm_load_print_meta: n_merges         = 50009
0.00.050.046 I llm_load_print_meta: vocab_only       = 0
0.00.050.046 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.046 I llm_load_print_meta: n_embd           = 2048
0.00.050.046 I llm_load_print_meta: n_layer          = 24
0.00.050.061 I llm_load_print_meta: n_head           = 16
0.00.050.063 I llm_load_print_meta: n_head_kv        = 16
0.00.050.063 I llm_load_print_meta: n_rot            = 32
0.00.050.063 I llm_load_print_meta: n_swa            = 0
0.00.050.063 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.063 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.064 I llm_load_print_meta: n_gqa            = 1
0.00.050.065 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.065 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.066 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.066 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.066 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.067 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.067 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.067 I llm_load_print_meta: n_ff             = 8192
0.00.050.067 I llm_load_print_meta: n_expert         = 0
0.00.050.068 I llm_load_print_meta: n_expert_used    = 0
0.00.050.068 I llm_load_print_meta: causal attn      = 1
0.00.050.070 I llm_load_print_meta: pooling type     = 0
0.00.050.070 I llm_load_print_meta: rope type        = 2
0.00.050.070 I llm_load_print_meta: rope scaling     = linear
0.00.050.071 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.071 I llm_load_print_meta: freq_scale_train = 1
0.00.050.071 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.071 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.071 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.071 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.072 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.072 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.072 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.081 I llm_load_print_meta: model type       = 1.4B
0.00.050.081 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.082 I llm_load_print_meta: model params     = 1.41 B
0.00.050.082 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.082 I llm_load_print_meta: general.name     = 1.4B
0.00.050.083 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.083 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.083 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.083 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.084 I llm_load_print_meta: LF token         = 128 ''
0.00.050.084 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.084 I llm_load_print_meta: max token length = 1024
0.00.051.918 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.918 I llm_load_tensors: offloading output layer to GPU
0.00.051.919 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.929 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.930 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.885 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.886 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.886 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.886 I llama_new_context_with_model: n_batch       = 2048
0.00.052.886 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.886 I llama_new_context_with_model: flash_attn    = 0
0.00.052.887 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.887 I llama_new_context_with_model: freq_scale    = 1
0.00.052.887 I ggml_metal_init: allocating
0.00.052.890 I ggml_metal_init: found device: Apple M4
0.00.052.893 I ggml_metal_init: picking default device: Apple M4
0.00.053.468 I ggml_metal_init: using embedded metal library
0.00.055.755 I ggml_metal_init: GPU name:   Apple M4
0.00.055.756 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.757 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.757 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.757 I ggml_metal_init: simdgroup reduction   = true
0.00.055.758 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.758 I ggml_metal_init: has bfloat            = true
0.00.055.758 I ggml_metal_init: use bfloat            = true
0.00.055.758 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.759 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.717 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.723 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.743 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.774 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.776 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.776 I llama_new_context_with_model: graph nodes  = 967
0.00.085.776 I llama_new_context_with_model: graph splits = 2
0.00.085.791 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.442.554 I main: llama threadpool init, n_threads = 4
0.00.442.593 I 
0.00.442.630 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.442.631 I 
0.00.442.863 I sampler seed: 1234
0.00.442.869 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.442.911 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.442.913 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.442.913 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.121.345 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.121.346 I llama_perf_context_print:        load time =     432.85 ms
0.01.121.347 I llama_perf_context_print: prompt eval time =      35.94 ms /     7 tokens (    5.13 ms per token,   194.78 tokens per second)
0.01.121.347 I llama_perf_context_print:        eval time =     639.51 ms /    63 runs   (   10.15 ms per token,    98.51 tokens per second)
0.01.121.348 I llama_perf_context_print:       total time =     678.80 ms /    70 tokens
0.01.121.569 I ggml_metal_free: deallocating

real	0m1.141s
user	0m0.109s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.858 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.362 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.367 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.369 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.369 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.370 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.370 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.370 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.371 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.371 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.372 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.372 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.375 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.375 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.375 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.377 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.942 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.944 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.944 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.944 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.944 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.945 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.945 I llama_model_loader: - type  f32:  194 tensors
0.00.023.946 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.946 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.946 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.882 I llm_load_vocab: special tokens cache size = 25
0.00.049.860 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.862 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.863 I llm_load_print_meta: arch             = gptneox
0.00.049.863 I llm_load_print_meta: vocab type       = BPE
0.00.049.864 I llm_load_print_meta: n_vocab          = 50304
0.00.049.864 I llm_load_print_meta: n_merges         = 50009
0.00.049.864 I llm_load_print_meta: vocab_only       = 0
0.00.049.864 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.864 I llm_load_print_meta: n_embd           = 2048
0.00.049.864 I llm_load_print_meta: n_layer          = 24
0.00.049.880 I llm_load_print_meta: n_head           = 16
0.00.049.882 I llm_load_print_meta: n_head_kv        = 16
0.00.049.882 I llm_load_print_meta: n_rot            = 32
0.00.049.882 I llm_load_print_meta: n_swa            = 0
0.00.049.882 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.882 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.883 I llm_load_print_meta: n_gqa            = 1
0.00.049.884 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.884 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.886 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.886 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.887 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.887 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.887 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.887 I llm_load_print_meta: n_ff             = 8192
0.00.049.888 I llm_load_print_meta: n_expert         = 0
0.00.049.888 I llm_load_print_meta: n_expert_used    = 0
0.00.049.888 I llm_load_print_meta: causal attn      = 1
0.00.049.888 I llm_load_print_meta: pooling type     = 0
0.00.049.888 I llm_load_print_meta: rope type        = 2
0.00.049.888 I llm_load_print_meta: rope scaling     = linear
0.00.049.889 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.889 I llm_load_print_meta: freq_scale_train = 1
0.00.049.890 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.890 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.890 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.890 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.890 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.890 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.890 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.900 I llm_load_print_meta: model type       = 1.4B
0.00.049.900 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.901 I llm_load_print_meta: model params     = 1.41 B
0.00.049.901 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.901 I llm_load_print_meta: general.name     = 1.4B
0.00.049.901 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.902 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.902 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.902 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.902 I llm_load_print_meta: LF token         = 128 ''
0.00.049.902 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.903 I llm_load_print_meta: max token length = 1024
0.00.051.742 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.742 I llm_load_tensors: offloading output layer to GPU
0.00.051.742 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.752 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.754 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.681 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.682 I llama_new_context_with_model: n_ctx         = 128
0.00.052.682 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.682 I llama_new_context_with_model: n_batch       = 128
0.00.052.683 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.683 I llama_new_context_with_model: flash_attn    = 0
0.00.052.683 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.683 I llama_new_context_with_model: freq_scale    = 1
0.00.052.684 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.684 I ggml_metal_init: allocating
0.00.052.688 I ggml_metal_init: found device: Apple M4
0.00.052.689 I ggml_metal_init: picking default device: Apple M4
0.00.053.242 I ggml_metal_init: using embedded metal library
0.00.055.524 I ggml_metal_init: GPU name:   Apple M4
0.00.055.525 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.525 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.526 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.526 I ggml_metal_init: simdgroup reduction   = true
0.00.055.526 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.526 I ggml_metal_init: has bfloat            = true
0.00.055.527 I ggml_metal_init: use bfloat            = true
0.00.055.527 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.527 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.201 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.203 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.216 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.126 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.127 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.127 I llama_new_context_with_model: graph nodes  = 967
0.00.067.127 I llama_new_context_with_model: graph splits = 2
0.00.067.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.371.014 I 
0.00.371.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.371.064 I perplexity: tokenizing the input ..
0.00.379.212 I perplexity: tokenization took 8.147 ms
0.00.379.223 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.512.074 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.513.346 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.513.359 I llama_perf_context_print:        load time =     361.15 ms
0.00.513.360 I llama_perf_context_print: prompt eval time =     132.63 ms /   128 tokens (    1.04 ms per token,   965.12 tokens per second)
0.00.513.361 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.513.361 I llama_perf_context_print:       total time =     142.35 ms /   129 tokens
0.00.513.784 I ggml_metal_free: deallocating

real	0m0.530s
user	0m0.077s
sys	0m0.069s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.010.301 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.721 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.728 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.729 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.729 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.730 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.730 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.731 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.732 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.732 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.732 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.733 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.733 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.733 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.737 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.738 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.645 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.734 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.545 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.546 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.547 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.547 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.547 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.548 I llama_model_loader: - type  f32:  194 tensors
0.00.024.548 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.548 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.549 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.549 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.811 I llm_load_vocab: special tokens cache size = 25
0.00.050.762 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.765 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.765 I llm_load_print_meta: arch             = gptneox
0.00.050.766 I llm_load_print_meta: vocab type       = BPE
0.00.050.766 I llm_load_print_meta: n_vocab          = 50304
0.00.050.766 I llm_load_print_meta: n_merges         = 50009
0.00.050.766 I llm_load_print_meta: vocab_only       = 0
0.00.050.767 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.767 I llm_load_print_meta: n_embd           = 2048
0.00.050.767 I llm_load_print_meta: n_layer          = 24
0.00.050.782 I llm_load_print_meta: n_head           = 16
0.00.050.783 I llm_load_print_meta: n_head_kv        = 16
0.00.050.783 I llm_load_print_meta: n_rot            = 32
0.00.050.783 I llm_load_print_meta: n_swa            = 0
0.00.050.783 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.784 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.784 I llm_load_print_meta: n_gqa            = 1
0.00.050.785 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.787 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.788 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.788 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.788 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.788 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.788 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.789 I llm_load_print_meta: n_ff             = 8192
0.00.050.789 I llm_load_print_meta: n_expert         = 0
0.00.050.789 I llm_load_print_meta: n_expert_used    = 0
0.00.050.789 I llm_load_print_meta: causal attn      = 1
0.00.050.790 I llm_load_print_meta: pooling type     = 0
0.00.050.790 I llm_load_print_meta: rope type        = 2
0.00.050.790 I llm_load_print_meta: rope scaling     = linear
0.00.050.790 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.790 I llm_load_print_meta: freq_scale_train = 1
0.00.050.791 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.791 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.791 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.791 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.791 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.793 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.793 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.802 I llm_load_print_meta: model type       = 1.4B
0.00.050.803 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.803 I llm_load_print_meta: model params     = 1.41 B
0.00.050.803 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.804 I llm_load_print_meta: general.name     = 1.4B
0.00.050.804 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.804 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.804 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.804 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.805 I llm_load_print_meta: LF token         = 128 ''
0.00.050.805 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.805 I llm_load_print_meta: max token length = 1024
0.00.052.719 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.719 I llm_load_tensors: offloading output layer to GPU
0.00.052.719 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.730 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.731 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.050 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.051 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.051 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.052 I llama_new_context_with_model: n_batch       = 2048
0.00.054.052 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.052 I llama_new_context_with_model: flash_attn    = 0
0.00.054.052 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.053 I llama_new_context_with_model: freq_scale    = 1
0.00.054.053 I ggml_metal_init: allocating
0.00.054.059 I ggml_metal_init: found device: Apple M4
0.00.054.062 I ggml_metal_init: picking default device: Apple M4
0.00.054.620 I ggml_metal_init: using embedded metal library
0.00.056.991 I ggml_metal_init: GPU name:   Apple M4
0.00.056.992 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.993 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.993 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.993 I ggml_metal_init: simdgroup reduction   = true
0.00.056.993 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.993 I ggml_metal_init: has bfloat            = true
0.00.056.993 I ggml_metal_init: use bfloat            = true
0.00.056.994 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.994 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.536 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.541 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.559 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.587 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.588 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.588 I llama_new_context_with_model: graph nodes  = 967
0.00.086.588 I llama_new_context_with_model: graph splits = 2
0.00.086.602 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.533.938 I main: llama threadpool init, n_threads = 4
0.00.533.981 I 
0.00.534.039 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.534.040 I 
0.00.534.277 I sampler seed: 1234
0.00.534.283 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.534.324 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.534.336 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.534.336 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.279.344 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.279.345 I llama_perf_context_print:        load time =     523.63 ms
0.01.279.346 I llama_perf_context_print: prompt eval time =      40.45 ms /     7 tokens (    5.78 ms per token,   173.04 tokens per second)
0.01.279.347 I llama_perf_context_print:        eval time =     701.52 ms /    63 runs   (   11.14 ms per token,    89.80 tokens per second)
0.01.279.347 I llama_perf_context_print:       total time =     745.41 ms /    70 tokens
0.01.279.543 I ggml_metal_free: deallocating

real	0m1.296s
user	0m0.109s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.076 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.716 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.248 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.253 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.255 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.255 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.255 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.256 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.256 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.259 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.260 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.260 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.260 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.261 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.261 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.261 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.263 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.263 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.263 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.129 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.180 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.174 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.174 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.175 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.175 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.175 I llama_model_loader: - type  f32:  194 tensors
0.00.023.176 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.176 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.176 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.176 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.077 I llm_load_vocab: special tokens cache size = 25
0.00.048.983 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.985 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.986 I llm_load_print_meta: arch             = gptneox
0.00.048.986 I llm_load_print_meta: vocab type       = BPE
0.00.048.986 I llm_load_print_meta: n_vocab          = 50304
0.00.048.987 I llm_load_print_meta: n_merges         = 50009
0.00.048.987 I llm_load_print_meta: vocab_only       = 0
0.00.048.987 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.987 I llm_load_print_meta: n_embd           = 2048
0.00.048.987 I llm_load_print_meta: n_layer          = 24
0.00.049.002 I llm_load_print_meta: n_head           = 16
0.00.049.003 I llm_load_print_meta: n_head_kv        = 16
0.00.049.003 I llm_load_print_meta: n_rot            = 32
0.00.049.003 I llm_load_print_meta: n_swa            = 0
0.00.049.003 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.003 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.004 I llm_load_print_meta: n_gqa            = 1
0.00.049.009 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.010 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.011 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.011 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.012 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.013 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.013 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.014 I llm_load_print_meta: n_ff             = 8192
0.00.049.014 I llm_load_print_meta: n_expert         = 0
0.00.049.014 I llm_load_print_meta: n_expert_used    = 0
0.00.049.014 I llm_load_print_meta: causal attn      = 1
0.00.049.015 I llm_load_print_meta: pooling type     = 0
0.00.049.016 I llm_load_print_meta: rope type        = 2
0.00.049.017 I llm_load_print_meta: rope scaling     = linear
0.00.049.018 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.018 I llm_load_print_meta: freq_scale_train = 1
0.00.049.018 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.018 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.019 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.019 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.019 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.019 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.019 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.029 I llm_load_print_meta: model type       = 1.4B
0.00.049.029 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.030 I llm_load_print_meta: model params     = 1.41 B
0.00.049.030 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.030 I llm_load_print_meta: general.name     = 1.4B
0.00.049.031 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.031 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.031 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.031 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.031 I llm_load_print_meta: LF token         = 128 ''
0.00.049.032 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.032 I llm_load_print_meta: max token length = 1024
0.00.050.982 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.982 I llm_load_tensors: offloading output layer to GPU
0.00.050.983 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.994 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.995 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.928 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.929 I llama_new_context_with_model: n_ctx         = 128
0.00.051.929 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.929 I llama_new_context_with_model: n_batch       = 128
0.00.051.929 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.929 I llama_new_context_with_model: flash_attn    = 0
0.00.051.930 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.930 I llama_new_context_with_model: freq_scale    = 1
0.00.051.930 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.931 I ggml_metal_init: allocating
0.00.051.934 I ggml_metal_init: found device: Apple M4
0.00.051.936 I ggml_metal_init: picking default device: Apple M4
0.00.052.516 I ggml_metal_init: using embedded metal library
0.00.054.807 I ggml_metal_init: GPU name:   Apple M4
0.00.054.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.811 I ggml_metal_init: simdgroup reduction   = true
0.00.054.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.811 I ggml_metal_init: has bfloat            = true
0.00.054.811 I ggml_metal_init: use bfloat            = true
0.00.054.812 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.812 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.557 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.559 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.573 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.507 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.508 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.508 I llama_new_context_with_model: graph nodes  = 967
0.00.066.509 I llama_new_context_with_model: graph splits = 2
0.00.066.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.099 I 
0.00.487.128 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.136 I perplexity: tokenizing the input ..
0.00.495.305 I perplexity: tokenization took 8.167 ms
0.00.495.315 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.627.779 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.628.950 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.628.973 I llama_perf_context_print:        load time =     478.38 ms
0.00.628.974 I llama_perf_context_print: prompt eval time =     132.24 ms /   128 tokens (    1.03 ms per token,   967.97 tokens per second)
0.00.628.975 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.628.975 I llama_perf_context_print:       total time =     141.87 ms /   129 tokens
0.00.629.485 I ggml_metal_free: deallocating

real	0m0.643s
user	0m0.078s
sys	0m0.089s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.030 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.435 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.440 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.442 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.442 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.443 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.443 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.443 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.444 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.447 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.447 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.447 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.448 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.449 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.449 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.450 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.344 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.423 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.183 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.184 I llama_model_loader: - type  f32:  194 tensors
0.00.023.184 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.184 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.185 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.384 I llm_load_vocab: special tokens cache size = 25
0.00.049.364 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.366 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.367 I llm_load_print_meta: arch             = gptneox
0.00.049.367 I llm_load_print_meta: vocab type       = BPE
0.00.049.368 I llm_load_print_meta: n_vocab          = 50304
0.00.049.368 I llm_load_print_meta: n_merges         = 50009
0.00.049.368 I llm_load_print_meta: vocab_only       = 0
0.00.049.368 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.368 I llm_load_print_meta: n_embd           = 2048
0.00.049.368 I llm_load_print_meta: n_layer          = 24
0.00.049.383 I llm_load_print_meta: n_head           = 16
0.00.049.383 I llm_load_print_meta: n_head_kv        = 16
0.00.049.384 I llm_load_print_meta: n_rot            = 32
0.00.049.384 I llm_load_print_meta: n_swa            = 0
0.00.049.384 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.387 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.387 I llm_load_print_meta: n_gqa            = 1
0.00.049.388 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.389 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.389 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.390 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.390 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.390 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.390 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.392 I llm_load_print_meta: n_ff             = 8192
0.00.049.392 I llm_load_print_meta: n_expert         = 0
0.00.049.392 I llm_load_print_meta: n_expert_used    = 0
0.00.049.392 I llm_load_print_meta: causal attn      = 1
0.00.049.393 I llm_load_print_meta: pooling type     = 0
0.00.049.393 I llm_load_print_meta: rope type        = 2
0.00.049.393 I llm_load_print_meta: rope scaling     = linear
0.00.049.394 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.394 I llm_load_print_meta: freq_scale_train = 1
0.00.049.395 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.395 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.395 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.395 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.395 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.395 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.396 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.405 I llm_load_print_meta: model type       = 1.4B
0.00.049.405 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.406 I llm_load_print_meta: model params     = 1.41 B
0.00.049.406 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.406 I llm_load_print_meta: general.name     = 1.4B
0.00.049.407 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.407 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.408 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.408 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.408 I llm_load_print_meta: LF token         = 128 ''
0.00.049.408 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.409 I llm_load_print_meta: max token length = 1024
0.00.051.332 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.332 I llm_load_tensors: offloading output layer to GPU
0.00.051.332 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.343 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.344 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.214 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.215 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.215 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.215 I llama_new_context_with_model: n_batch       = 2048
0.00.052.215 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.216 I llama_new_context_with_model: flash_attn    = 0
0.00.052.216 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.216 I llama_new_context_with_model: freq_scale    = 1
0.00.052.217 I ggml_metal_init: allocating
0.00.052.220 I ggml_metal_init: found device: Apple M4
0.00.052.222 I ggml_metal_init: picking default device: Apple M4
0.00.052.801 I ggml_metal_init: using embedded metal library
0.00.055.099 I ggml_metal_init: GPU name:   Apple M4
0.00.055.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.101 I ggml_metal_init: simdgroup reduction   = true
0.00.055.101 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.101 I ggml_metal_init: has bfloat            = true
0.00.055.102 I ggml_metal_init: use bfloat            = true
0.00.055.102 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.122 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.128 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.148 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.153 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.154 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.154 I llama_new_context_with_model: graph nodes  = 967
0.00.085.154 I llama_new_context_with_model: graph splits = 2
0.00.085.168 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.564 I main: llama threadpool init, n_threads = 4
0.00.630.602 I 
0.00.630.635 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.635 I 
0.00.630.889 I sampler seed: 1234
0.00.630.895 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.630.906 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.630.908 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.630.908 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.389.028 I llama_perf_sampler_print:    sampling time =       1.56 ms /    71 runs   (    0.02 ms per token, 45512.82 tokens per second)
0.01.389.029 I llama_perf_context_print:        load time =     621.53 ms
0.01.389.030 I llama_perf_context_print: prompt eval time =      49.61 ms /     7 tokens (    7.09 ms per token,   141.11 tokens per second)
0.01.389.031 I llama_perf_context_print:        eval time =     705.91 ms /    63 runs   (   11.20 ms per token,    89.25 tokens per second)
0.01.389.031 I llama_perf_context_print:       total time =     758.47 ms /    70 tokens
0.01.389.241 I ggml_metal_free: deallocating

real	0m1.408s
user	0m0.109s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.312 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.815 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.820 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.825 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.825 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.827 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.827 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.828 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.828 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.829 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.829 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.833 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.833 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.834 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.835 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.836 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.836 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.718 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.735 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.486 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.487 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.487 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.488 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.488 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.488 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.489 I llama_model_loader: - type  f32:  194 tensors
0.00.023.489 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.489 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.489 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.359 I llm_load_vocab: special tokens cache size = 25
0.00.050.328 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.331 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.331 I llm_load_print_meta: arch             = gptneox
0.00.050.332 I llm_load_print_meta: vocab type       = BPE
0.00.050.332 I llm_load_print_meta: n_vocab          = 50304
0.00.050.332 I llm_load_print_meta: n_merges         = 50009
0.00.050.332 I llm_load_print_meta: vocab_only       = 0
0.00.050.332 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.332 I llm_load_print_meta: n_embd           = 2048
0.00.050.333 I llm_load_print_meta: n_layer          = 24
0.00.050.347 I llm_load_print_meta: n_head           = 16
0.00.050.349 I llm_load_print_meta: n_head_kv        = 16
0.00.050.349 I llm_load_print_meta: n_rot            = 32
0.00.050.349 I llm_load_print_meta: n_swa            = 0
0.00.050.349 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.349 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.350 I llm_load_print_meta: n_gqa            = 1
0.00.050.351 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.351 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.354 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.354 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.355 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.355 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.355 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.356 I llm_load_print_meta: n_ff             = 8192
0.00.050.356 I llm_load_print_meta: n_expert         = 0
0.00.050.356 I llm_load_print_meta: n_expert_used    = 0
0.00.050.356 I llm_load_print_meta: causal attn      = 1
0.00.050.357 I llm_load_print_meta: pooling type     = 0
0.00.050.357 I llm_load_print_meta: rope type        = 2
0.00.050.357 I llm_load_print_meta: rope scaling     = linear
0.00.050.357 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.357 I llm_load_print_meta: freq_scale_train = 1
0.00.050.358 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.358 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.358 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.359 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.359 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.359 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.359 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.369 I llm_load_print_meta: model type       = 1.4B
0.00.050.369 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.369 I llm_load_print_meta: model params     = 1.41 B
0.00.050.370 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.370 I llm_load_print_meta: general.name     = 1.4B
0.00.050.370 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.370 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.371 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.371 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.371 I llm_load_print_meta: LF token         = 128 ''
0.00.050.371 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.371 I llm_load_print_meta: max token length = 1024
0.00.052.354 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.354 I llm_load_tensors: offloading output layer to GPU
0.00.052.354 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.364 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.366 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.270 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.271 I llama_new_context_with_model: n_ctx         = 128
0.00.053.271 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.271 I llama_new_context_with_model: n_batch       = 128
0.00.053.271 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.271 I llama_new_context_with_model: flash_attn    = 0
0.00.053.272 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.272 I llama_new_context_with_model: freq_scale    = 1
0.00.053.272 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.273 I ggml_metal_init: allocating
0.00.053.279 I ggml_metal_init: found device: Apple M4
0.00.053.281 I ggml_metal_init: picking default device: Apple M4
0.00.053.818 I ggml_metal_init: using embedded metal library
0.00.056.109 I ggml_metal_init: GPU name:   Apple M4
0.00.056.110 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.111 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.111 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.111 I ggml_metal_init: simdgroup reduction   = true
0.00.056.112 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.112 I ggml_metal_init: has bfloat            = true
0.00.056.112 I ggml_metal_init: use bfloat            = true
0.00.056.112 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.113 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.622 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.629 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.644 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.487 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.487 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.488 I llama_new_context_with_model: graph nodes  = 967
0.00.067.488 I llama_new_context_with_model: graph splits = 2
0.00.067.500 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.286 I 
0.00.575.316 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.575.323 I perplexity: tokenizing the input ..
0.00.583.922 I perplexity: tokenization took 8.596 ms
0.00.583.935 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.717.446 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.718.838 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.718.853 I llama_perf_context_print:        load time =     565.97 ms
0.00.718.854 I llama_perf_context_print: prompt eval time =     133.27 ms /   128 tokens (    1.04 ms per token,   960.43 tokens per second)
0.00.718.855 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.718.855 I llama_perf_context_print:       total time =     143.57 ms /   129 tokens
0.00.719.234 I ggml_metal_free: deallocating

real	0m0.736s
user	0m0.079s
sys	0m0.108s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.009.205 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.026.135 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.137 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.137 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.137 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.137 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.138 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.140 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.140 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.140 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.144 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.144 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.148 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.148 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.267 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.268 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.268 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.269 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.269 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.035.270 I llama_model_loader: - type  f32:  194 tensors
0.00.035.270 I llama_model_loader: - type q5_K:   61 tensors
0.00.035.270 I llama_model_loader: - type q6_K:   37 tensors
0.00.057.650 I llm_load_vocab: special tokens cache size = 25
0.00.063.804 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.807 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.807 I llm_load_print_meta: arch             = gptneox
0.00.063.808 I llm_load_print_meta: vocab type       = BPE
0.00.063.808 I llm_load_print_meta: n_vocab          = 50304
0.00.063.808 I llm_load_print_meta: n_merges         = 50009
0.00.063.808 I llm_load_print_meta: vocab_only       = 0
0.00.063.808 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.808 I llm_load_print_meta: n_embd           = 2048
0.00.063.808 I llm_load_print_meta: n_layer          = 24
0.00.063.818 I llm_load_print_meta: n_head           = 16
0.00.063.818 I llm_load_print_meta: n_head_kv        = 16
0.00.063.819 I llm_load_print_meta: n_rot            = 32
0.00.063.819 I llm_load_print_meta: n_swa            = 0
0.00.063.819 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.819 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.820 I llm_load_print_meta: n_gqa            = 1
0.00.063.823 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.823 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.824 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.824 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.824 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.824 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.824 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.828 I llm_load_print_meta: n_ff             = 8192
0.00.063.829 I llm_load_print_meta: n_expert         = 0
0.00.063.829 I llm_load_print_meta: n_expert_used    = 0
0.00.063.830 I llm_load_print_meta: causal attn      = 1
0.00.063.831 I llm_load_print_meta: pooling type     = 0
0.00.063.831 I llm_load_print_meta: rope type        = 2
0.00.063.832 I llm_load_print_meta: rope scaling     = linear
0.00.063.832 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.832 I llm_load_print_meta: freq_scale_train = 1
0.00.063.832 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.832 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.833 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.833 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.834 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.834 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.834 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.838 I llm_load_print_meta: model type       = 1.4B
0.00.063.839 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.063.839 I llm_load_print_meta: model params     = 1.41 B
0.00.063.840 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.063.840 I llm_load_print_meta: general.name     = 1.4B
0.00.063.840 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.841 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.842 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.842 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.842 I llm_load_print_meta: LF token         = 128 ''
0.00.063.842 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.842 I llm_load_print_meta: max token length = 1024
0.00.065.685 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.685 I llm_load_tensors: offloading output layer to GPU
0.00.065.686 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.691 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.065.692 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.066.646 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.647 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.647 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.647 I llama_new_context_with_model: n_batch       = 2048
0.00.066.647 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.647 I llama_new_context_with_model: flash_attn    = 0
0.00.066.648 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.648 I llama_new_context_with_model: freq_scale    = 1
0.00.066.649 I ggml_metal_init: allocating
0.00.066.656 I ggml_metal_init: found device: Apple M4
0.00.066.660 I ggml_metal_init: picking default device: Apple M4
0.00.067.287 I ggml_metal_init: using embedded metal library
0.00.069.617 I ggml_metal_init: GPU name:   Apple M4
0.00.069.619 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.621 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.621 I ggml_metal_init: simdgroup reduction   = true
0.00.069.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.621 I ggml_metal_init: has bfloat            = true
0.00.069.622 I ggml_metal_init: use bfloat            = true
0.00.069.622 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.623 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.057 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.066 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.084 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.150 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.152 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.152 I llama_new_context_with_model: graph nodes  = 967
0.00.102.153 I llama_new_context_with_model: graph splits = 2
0.00.102.167 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.826.358 I main: llama threadpool init, n_threads = 4
0.00.826.396 I 
0.00.826.429 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.826.430 I 
0.00.826.670 I sampler seed: 1234
0.00.826.674 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.826.712 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.826.713 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.826.713 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.684.455 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.01.684.456 I llama_perf_context_print:        load time =     817.15 ms
0.01.684.457 I llama_perf_context_print: prompt eval time =      55.54 ms /     7 tokens (    7.93 ms per token,   126.03 tokens per second)
0.01.684.458 I llama_perf_context_print:        eval time =     799.26 ms /    63 runs   (   12.69 ms per token,    78.82 tokens per second)
0.01.684.458 I llama_perf_context_print:       total time =     858.10 ms /    70 tokens
0.01.684.653 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.112s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.811 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.600 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.606 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.607 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.608 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.608 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.609 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.609 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.610 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.610 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.611 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.611 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.611 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.612 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.612 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.615 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.616 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.616 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.785 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.787 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.788 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.789 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.790 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.790 I llama_model_loader: - type  f32:  194 tensors
0.00.023.791 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.791 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.416 I llm_load_vocab: special tokens cache size = 25
0.00.051.593 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.597 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.598 I llm_load_print_meta: arch             = gptneox
0.00.051.598 I llm_load_print_meta: vocab type       = BPE
0.00.051.598 I llm_load_print_meta: n_vocab          = 50304
0.00.051.598 I llm_load_print_meta: n_merges         = 50009
0.00.051.599 I llm_load_print_meta: vocab_only       = 0
0.00.051.600 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.600 I llm_load_print_meta: n_embd           = 2048
0.00.051.601 I llm_load_print_meta: n_layer          = 24
0.00.051.617 I llm_load_print_meta: n_head           = 16
0.00.051.619 I llm_load_print_meta: n_head_kv        = 16
0.00.051.619 I llm_load_print_meta: n_rot            = 32
0.00.051.619 I llm_load_print_meta: n_swa            = 0
0.00.051.619 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.619 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.620 I llm_load_print_meta: n_gqa            = 1
0.00.051.621 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.621 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.622 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.622 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.626 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.626 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.627 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.629 I llm_load_print_meta: n_ff             = 8192
0.00.051.631 I llm_load_print_meta: n_expert         = 0
0.00.051.631 I llm_load_print_meta: n_expert_used    = 0
0.00.051.631 I llm_load_print_meta: causal attn      = 1
0.00.051.631 I llm_load_print_meta: pooling type     = 0
0.00.051.631 I llm_load_print_meta: rope type        = 2
0.00.051.632 I llm_load_print_meta: rope scaling     = linear
0.00.051.632 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.632 I llm_load_print_meta: freq_scale_train = 1
0.00.051.632 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.632 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.633 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.633 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.633 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.633 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.633 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.643 I llm_load_print_meta: model type       = 1.4B
0.00.051.644 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.644 I llm_load_print_meta: model params     = 1.41 B
0.00.051.644 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.645 I llm_load_print_meta: general.name     = 1.4B
0.00.051.645 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.645 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.645 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.645 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.646 I llm_load_print_meta: LF token         = 128 ''
0.00.051.646 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.646 I llm_load_print_meta: max token length = 1024
0.00.053.560 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.560 I llm_load_tensors: offloading output layer to GPU
0.00.053.560 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.572 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.573 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.495 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.496 I llama_new_context_with_model: n_ctx         = 128
0.00.054.496 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.496 I llama_new_context_with_model: n_batch       = 128
0.00.054.496 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.496 I llama_new_context_with_model: flash_attn    = 0
0.00.054.497 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.497 I llama_new_context_with_model: freq_scale    = 1
0.00.054.497 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.498 I ggml_metal_init: allocating
0.00.054.502 I ggml_metal_init: found device: Apple M4
0.00.054.507 I ggml_metal_init: picking default device: Apple M4
0.00.055.152 I ggml_metal_init: using embedded metal library
0.00.057.540 I ggml_metal_init: GPU name:   Apple M4
0.00.057.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.542 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.542 I ggml_metal_init: simdgroup reduction   = true
0.00.057.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.543 I ggml_metal_init: has bfloat            = true
0.00.057.543 I ggml_metal_init: use bfloat            = true
0.00.057.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.107 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.114 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.132 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.101 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.102 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.102 I llama_new_context_with_model: graph nodes  = 967
0.00.069.103 I llama_new_context_with_model: graph splits = 2
0.00.069.116 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.634.838 I 
0.00.634.878 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.634.889 I perplexity: tokenizing the input ..
0.00.643.145 I perplexity: tokenization took 8.255 ms
0.00.643.155 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.215 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.785.464 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.785.482 I llama_perf_context_print:        load time =     626.02 ms
0.00.785.483 I llama_perf_context_print: prompt eval time =     140.80 ms /   128 tokens (    1.10 ms per token,   909.10 tokens per second)
0.00.785.484 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.484 I llama_perf_context_print:       total time =     150.64 ms /   129 tokens
0.00.785.917 I ggml_metal_free: deallocating

real	0m0.800s
user	0m0.080s
sys	0m0.115s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.015.404 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.975 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.029.981 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.988 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.989 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.990 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.990 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.991 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.991 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.992 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.992 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.993 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.993 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.994 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.994 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.996 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.996 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.997 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.049 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.762 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.764 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.764 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.765 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.765 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.766 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.043.766 I llama_model_loader: - type  f32:  194 tensors
0.00.043.767 I llama_model_loader: - type q6_K:   98 tensors
0.00.083.258 I llm_load_vocab: special tokens cache size = 25
0.00.092.392 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.395 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.396 I llm_load_print_meta: arch             = gptneox
0.00.092.396 I llm_load_print_meta: vocab type       = BPE
0.00.092.397 I llm_load_print_meta: n_vocab          = 50304
0.00.092.397 I llm_load_print_meta: n_merges         = 50009
0.00.092.397 I llm_load_print_meta: vocab_only       = 0
0.00.092.397 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.397 I llm_load_print_meta: n_embd           = 2048
0.00.092.398 I llm_load_print_meta: n_layer          = 24
0.00.092.407 I llm_load_print_meta: n_head           = 16
0.00.092.408 I llm_load_print_meta: n_head_kv        = 16
0.00.092.408 I llm_load_print_meta: n_rot            = 32
0.00.092.409 I llm_load_print_meta: n_swa            = 0
0.00.092.409 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.412 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.413 I llm_load_print_meta: n_gqa            = 1
0.00.092.414 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.414 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.415 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.415 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.415 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.416 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.416 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.417 I llm_load_print_meta: n_ff             = 8192
0.00.092.421 I llm_load_print_meta: n_expert         = 0
0.00.092.421 I llm_load_print_meta: n_expert_used    = 0
0.00.092.421 I llm_load_print_meta: causal attn      = 1
0.00.092.421 I llm_load_print_meta: pooling type     = 0
0.00.092.421 I llm_load_print_meta: rope type        = 2
0.00.092.422 I llm_load_print_meta: rope scaling     = linear
0.00.092.422 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.423 I llm_load_print_meta: freq_scale_train = 1
0.00.092.423 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.423 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.423 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.423 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.424 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.424 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.425 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.430 I llm_load_print_meta: model type       = 1.4B
0.00.092.430 I llm_load_print_meta: model ftype      = Q6_K
0.00.092.431 I llm_load_print_meta: model params     = 1.41 B
0.00.092.431 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.092.431 I llm_load_print_meta: general.name     = 1.4B
0.00.092.432 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.432 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.432 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.432 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.434 I llm_load_print_meta: LF token         = 128 ''
0.00.092.434 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.435 I llm_load_print_meta: max token length = 1024
0.00.094.945 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.945 I llm_load_tensors: offloading output layer to GPU
0.00.094.945 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.956 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.094.957 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.096.113 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.114 I llama_new_context_with_model: n_ctx         = 2048
0.00.096.114 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.096.114 I llama_new_context_with_model: n_batch       = 2048
0.00.096.115 I llama_new_context_with_model: n_ubatch      = 512
0.00.096.115 I llama_new_context_with_model: flash_attn    = 0
0.00.096.115 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.116 I llama_new_context_with_model: freq_scale    = 1
0.00.096.116 I ggml_metal_init: allocating
0.00.096.119 I ggml_metal_init: found device: Apple M4
0.00.096.122 I ggml_metal_init: picking default device: Apple M4
0.00.096.865 I ggml_metal_init: using embedded metal library
0.00.100.052 I ggml_metal_init: GPU name:   Apple M4
0.00.100.054 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.054 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.055 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.055 I ggml_metal_init: simdgroup reduction   = true
0.00.100.055 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.055 I ggml_metal_init: has bfloat            = true
0.00.100.056 I ggml_metal_init: use bfloat            = true
0.00.100.056 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.057 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.130.530 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.130.538 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.130.559 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.131.504 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.131.505 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.131.505 I llama_new_context_with_model: graph nodes  = 967
0.00.131.506 I llama_new_context_with_model: graph splits = 2
0.00.131.521 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.833.423 I main: llama threadpool init, n_threads = 4
0.00.833.508 I 
0.00.833.575 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.833.577 I 
0.00.834.084 I sampler seed: 1234
0.00.834.097 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.834.120 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.834.122 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.834.122 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.719.444 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.01.719.445 I llama_perf_context_print:        load time =     818.00 ms
0.01.719.446 I llama_perf_context_print: prompt eval time =      55.09 ms /     7 tokens (    7.87 ms per token,   127.05 tokens per second)
0.01.719.447 I llama_perf_context_print:        eval time =     827.23 ms /    63 runs   (   13.13 ms per token,    76.16 tokens per second)
0.01.719.447 I llama_perf_context_print:       total time =     886.03 ms /    70 tokens
0.01.719.658 I ggml_metal_free: deallocating

real	0m1.764s
user	0m0.150s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4317 (adffa6ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.387 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.915 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.919 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.920 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.920 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.921 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.927 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.928 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.928 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.929 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.929 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.929 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.930 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.930 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.930 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.932 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.932 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.933 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.628 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.626 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.383 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.385 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.385 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.386 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.386 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.387 I llama_model_loader: - type  f32:  194 tensors
0.00.027.387 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.303 I llm_load_vocab: special tokens cache size = 25
0.00.053.198 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.201 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.201 I llm_load_print_meta: arch             = gptneox
0.00.053.201 I llm_load_print_meta: vocab type       = BPE
0.00.053.202 I llm_load_print_meta: n_vocab          = 50304
0.00.053.202 I llm_load_print_meta: n_merges         = 50009
0.00.053.202 I llm_load_print_meta: vocab_only       = 0
0.00.053.202 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.202 I llm_load_print_meta: n_embd           = 2048
0.00.053.202 I llm_load_print_meta: n_layer          = 24
0.00.053.211 I llm_load_print_meta: n_head           = 16
0.00.053.212 I llm_load_print_meta: n_head_kv        = 16
0.00.053.212 I llm_load_print_meta: n_rot            = 32
0.00.053.212 I llm_load_print_meta: n_swa            = 0
0.00.053.213 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.213 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.213 I llm_load_print_meta: n_gqa            = 1
0.00.053.214 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.215 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.215 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.216 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.216 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.216 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.216 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.217 I llm_load_print_meta: n_ff             = 8192
0.00.053.217 I llm_load_print_meta: n_expert         = 0
0.00.053.217 I llm_load_print_meta: n_expert_used    = 0
0.00.053.217 I llm_load_print_meta: causal attn      = 1
0.00.053.217 I llm_load_print_meta: pooling type     = 0
0.00.053.218 I llm_load_print_meta: rope type        = 2
0.00.053.218 I llm_load_print_meta: rope scaling     = linear
0.00.053.218 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.219 I llm_load_print_meta: freq_scale_train = 1
0.00.053.219 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.219 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.219 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.219 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.219 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.220 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.221 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.225 I llm_load_print_meta: model type       = 1.4B
0.00.053.225 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.226 I llm_load_print_meta: model params     = 1.41 B
0.00.053.226 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.227 I llm_load_print_meta: general.name     = 1.4B
0.00.053.227 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.227 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.227 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.228 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.228 I llm_load_print_meta: LF token         = 128 ''
0.00.053.228 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.228 I llm_load_print_meta: max token length = 1024
0.00.054.799 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.799 I llm_load_tensors: offloading output layer to GPU
0.00.054.799 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.805 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.806 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.733 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.733 I llama_new_context_with_model: n_ctx         = 128
0.00.055.734 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.734 I llama_new_context_with_model: n_batch       = 128
0.00.055.734 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.734 I llama_new_context_with_model: flash_attn    = 0
0.00.055.735 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.735 I llama_new_context_with_model: freq_scale    = 1
0.00.055.735 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.736 I ggml_metal_init: allocating
0.00.055.739 I ggml_metal_init: found device: Apple M4
0.00.055.741 I ggml_metal_init: picking default device: Apple M4
0.00.056.288 I ggml_metal_init: using embedded metal library
0.00.058.818 I ggml_metal_init: GPU name:   Apple M4
0.00.058.820 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.820 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.820 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.821 I ggml_metal_init: simdgroup reduction   = true
0.00.058.821 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.821 I ggml_metal_init: has bfloat            = true
0.00.058.821 I ggml_metal_init: use bfloat            = true
0.00.058.821 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.822 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.438 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.442 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.455 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.292 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.293 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.294 I llama_new_context_with_model: graph nodes  = 967
0.00.070.294 I llama_new_context_with_model: graph splits = 2
0.00.070.301 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.275.021 I 
0.00.275.058 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.275.065 I perplexity: tokenizing the input ..
0.00.283.250 I perplexity: tokenization took 8.182 ms
0.00.283.267 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.423.536 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.424.715 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.424.731 I llama_perf_context_print:        load time =     261.63 ms
0.00.424.734 I llama_perf_context_print: prompt eval time =     140.01 ms /   128 tokens (    1.09 ms per token,   914.22 tokens per second)
0.00.424.735 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.424.735 I llama_perf_context_print:       total time =     149.71 ms /   129 tokens
0.00.425.147 I ggml_metal_free: deallocating

real	0m0.440s
user	0m0.077s
sys	0m0.059s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4317 (adffa6ff)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11960a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11960a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11960af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11960b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11960bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11960c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11960c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11960cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11960d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11960d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11960db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11960e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11960eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11960f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11960fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x119610270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x119610990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1196110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1196117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x119611fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1196126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x119612de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x119613500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x119613da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1196144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x119614780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x119614d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x119615a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x119615f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x119616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1196166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x119616960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1196171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x119617730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1196179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119617e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119618330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1196187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119618c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119619110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1196195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119619a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119619ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11961a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11961a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11961ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11961b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11961bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11961c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11961c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11961cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11961d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11961d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11961dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11961e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11961ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11961f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11961f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11961f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1196201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1196204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x119620940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119620de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x119621280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119621720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x119621bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x119622060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x119622500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1196229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x119622e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1196232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x119623780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x119623c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x119624170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1196246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x119624c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x119625160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1196256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x119625c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x119626150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1196266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x119626bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x119627140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x119627690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x119627be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119628130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x119628680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x119628bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x119629120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x119629670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x119629bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11962a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11962a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11962abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11962b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11962b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11962bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11961b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11962c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11962c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11962cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11962d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11962d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11962dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11962e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11962e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11962ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11962f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11962f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11962fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x119630230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x119630780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x119630cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x119631170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x119631610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x119631ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x119631f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1196323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x119632890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x119632d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1196331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x119633670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x119633b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x119633fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x119634450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1196348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x119634d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x119635230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1196356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x119635b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x119636010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1196364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x119636950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x119636df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x119637290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x119637730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x119637bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x119638070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119638510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1196389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x119638e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1196392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x119639790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119639c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11963a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11963a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11963aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11963aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11963b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11963b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11963bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11963c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11963c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11963ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11963cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11963d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11963d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11963dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11963e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11963e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11963ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11963ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11963f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11963f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11963fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1196401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x119640690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x119640b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x119640fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x119641470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x119641910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x119641db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x119642250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1196426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x119642b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x119643030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1196434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x119643970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x119643e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1196442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x119644750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x119644bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x119645090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x119645530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1196459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x119645e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x119646310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1196467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x119646c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1196470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x119647590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x119647a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x119647ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x119648420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x119648970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x119648ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x119649410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1196496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x119649ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11964a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11964a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11964b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11964b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11964b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11964be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11964c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11964cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11964d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11964d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11964da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11964e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11964e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11964ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11964f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11964f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11964fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1196501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x119650720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x119650c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1196511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x119651710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x119651c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1196521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x119652700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x119652c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1196531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1196536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x119653c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119654190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1196546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x119654c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x119655180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1196556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x119655c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x119656170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1196566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x119656c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119657160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1196576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x119657c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x119658150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1196586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x119658bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x119659140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x119659690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x119659be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11965a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11965a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11965abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11965b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11965b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11965bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11965c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11965c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11965cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11965d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11965d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11965dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11965e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11965e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11965eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11965f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11965f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11965fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1196600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x119660620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x119660b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x119661010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1196614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x119661950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x119661df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x119662290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x119662730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x119662bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x119663070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x119663510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1196639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x119663e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1196642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x119664790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x119664c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1196650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x119665620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119665d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x119666460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x119666b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1196672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119667560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x119667d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119668010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x119668620 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.146.029 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x109704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x109705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1097056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x109705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x109705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x109706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x109706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x109706ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x109707150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1097075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x109707a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x109708120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x109708c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1097093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x109709c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10970a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10970aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10970b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10970b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10970bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10970c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10970cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10970d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10970dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10970e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10970e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10970e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10970ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10970f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10970f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10970fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10970ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x109710430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1097106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x109710b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x109710fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x109711440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1097118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11960a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1196251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119625610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119625a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119625ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119626360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1196267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119626c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1196270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119627520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x119627990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119627e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x119628270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1196286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x119628b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x119628fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119629430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1196298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119629d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11962a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11962a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11962aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11962aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11962b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11962b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11962bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11962c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11962c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11962c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11962cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11962d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11962d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11962db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11962dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11962e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11962e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11962ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11962f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11962f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11962fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11962feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x119630320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x119630790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x119630c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x119631070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1196314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119631950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x119631dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x119632230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1196326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x119632b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x119632f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1196333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x119633860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x119633cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119634140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1196345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x119634a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x119634e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119635300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x119635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x119635be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x119636050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1196364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x119636930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x119636da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x119637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x119637680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x119637af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x119637f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1196383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x119638840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x119638cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x119639120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x119639590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x119639a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x119639e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11963a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11963a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11963abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11963b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11963b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11963b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11963bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11963c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11963c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11963cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11963cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11963d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11963d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11963dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11963e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11963e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11963e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11963ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11963f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11963f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11963fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x119640010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119640480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1196408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x119640d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1196411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x119641640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119641ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x119641f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x119642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x119642800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x119642c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1196430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119643550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1196439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x119643e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1196442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x119644710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x119644b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x119644ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119645460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1196458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x119645d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1196461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x119646620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x119646a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x119646f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x119647370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1196477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x119647c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1196480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x119648530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1196489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x119648e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x119649280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1196496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x119649b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x119649fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11964a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11964a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11964ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11964b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11964b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11964ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11964bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11964c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11964c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11964cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11964d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11964d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11964d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11964ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11964e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11964e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11964eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11964efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11964f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11964f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11964fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x119650170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1196505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x119650a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x119650ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x119651330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1196517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x119651c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x119652080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1196524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x119652960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x119652dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x119653240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1196536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119653b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x119653f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x119654880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x119654cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x119655160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1196555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x119655a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x119655eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x119656320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x119656790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x119656c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x119657070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1196574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x119657950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x119657dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x119658230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1196586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x119658b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x119658f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1196593f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119659860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x119659cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11965a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11965a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11965aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11965ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11965b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11965b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11965bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11965c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11965c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11965c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11965cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11965d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11965d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11965daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11965df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11965e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11965e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11965ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11965f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11965f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11965fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11965fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1196602e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x119660750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x119660bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x119661030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1196614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x119661910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x119661d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1196621f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x119662660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x119662ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x119662f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1196633b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x119663820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x119663c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x119664100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x119664570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1196649e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x119664e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1196652c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x119665730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x119665ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x119666010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x119666480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1196668f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x119666d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1196671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x119667640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x119667ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x119667f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x119668390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11960b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11960ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11960a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1196179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x119617ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119618350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1196187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119618c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1196190a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11960af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11960a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x119624b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x119624e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1196252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x119625720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x119625b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x119626000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x119626470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1196268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x119626d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x119627330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x119627c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1196283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x119628b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x119629270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x119629960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11962a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11962a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11962b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11962b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11962bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11962c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11962cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11962d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11962d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11962dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11962e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11962e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11962e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11962ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11962f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11962f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11962f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11962fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119630290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119630700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119630b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119630fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119631450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1196318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119631d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1196321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119632610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119632a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x119632ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119633360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1196337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119633c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1196340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119634520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x119634990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119634e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x119635270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1196356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119635b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119635fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119636430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1196368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x119636d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x119637180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1196375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119637a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x119637ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119638340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1196387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x119638c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x119639090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x119639500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x119639970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x119639de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11963a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11963a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11963ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11963afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11963b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11963b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11963bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11963c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11963c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11963ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11963ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11963d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11963d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11963dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11963e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11963e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11963e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11963edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11963f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11963f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11963fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11963ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1196403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119640860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x119640cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x119641140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1196415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119641a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x119641e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x119642300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x119642770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x119642be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x119643050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1196434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x119643930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x119643da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x119644210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x119644680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x119644af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x119644f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1196453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x119645840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x119645cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x119646120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x119646590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x119646a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x119646e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1196472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x119647750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x119647bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x119648030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1196484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x119648910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x119648d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1196491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x119649660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x119649ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x119649f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11964a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11964a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11964ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11964b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11964b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11964b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11964be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11964c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11964c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11964cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11964d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11964d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11964d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11964dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11964e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11964e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11964eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11964ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11964f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11964f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11964fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1196500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x119650550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1196509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x119650e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1196512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x119651710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119651b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x119651ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x119652460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1196528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x119652d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1196531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x119653620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x119653a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x119653f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x119654370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1196547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x119654c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1196550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x119655530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1196559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x119655e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x119656280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1196566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x119656b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x119656fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x119657440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1196578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x119657d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x119658190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x119658600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x119658a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x119658ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x119659350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1196597c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x119659c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11965a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11965a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11965a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11965adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11965b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11965b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11965bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11965bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11965c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11965c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11965cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11965d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11965d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11965da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11965dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11965e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11965e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11965ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11965f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11965f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11965f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11965fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119660240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1196606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x119660e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1196612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x119661710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x119661b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x119661ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x119662460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1196628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x119662d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1196631b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x119663620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x119663a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x119663f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x119664370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1196647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x119664c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1196650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x119665530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1196659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119665e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x119666280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1196666f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x119666b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x119666fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x119667440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1196678b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x119667d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x119668190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119668600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11960b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x119617720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x119617b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x119618000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x119618470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1196188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x119618d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1196191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x119619630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x119619aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x119619f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11961a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11961a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11961ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11961b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11961b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11961b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11961be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11961c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11961c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11961cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11961cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11961d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11961d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11961dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11961e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11961e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11961ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11961eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11961f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11961f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11961fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1196200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x119620520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x119620990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x119620e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x119621270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1196216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x119621b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x119621fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x119622430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1196228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x119622d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x119623180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1196235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119623e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x119624540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x119616360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x119616a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119616ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11960d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11960daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11960df10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.798s
user	0m0.292s
sys	0m0.299s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4317 (adffa6ff)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14f7102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14f7109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14f710f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14f711530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14f711ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14f712090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14f712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14f712bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14f7131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14f7136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14f713ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14f7140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14f714bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14f715370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14f715b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14f7162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14f7169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14f7170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14f717800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14f717fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14f7186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14f718e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14f719530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14f719dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14f71a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14f71a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14f71adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14f71ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14f71bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14f71c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14f71c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14f71c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14f71d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14f71d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14f71da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14f71dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14f71e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14f71e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14f71eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14f71f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14f71f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14f71fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14f71ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14f7203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14f720680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14f720c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14f7212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14f721bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14f7221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14f7227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14f722df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14f723400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14f723a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14f724020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14f724810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14f724cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14f725150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14f725410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14f725a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14f726210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14f7264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14f726970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14f726e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14f7272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14f727750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14f727bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14f728090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14f728530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14f7289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14f728e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14f729310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14f7297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14f729c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14f72a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14f72a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14f72ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14f72b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14f72b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14f72bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14f72c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14f72c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14f72cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14f72d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14f72d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14f72dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14f72e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14f72e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14f72ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14f72f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14f72f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14f72fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14f730140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14f730690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14f730be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14f731130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14f731680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14f731bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14f7218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14f732040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14f7327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14f732d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14f733290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14f7337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14f733d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14f734280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14f7347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14f734d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14f735270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14f7357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14f735d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14f736260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14f7367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14f736d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14f7371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14f737640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14f737ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14f737f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14f738420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14f7388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14f738d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14f739200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14f7396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14f739b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14f739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14f73a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14f73a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14f73adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14f73b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14f73b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14f73bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14f73c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14f73c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14f73c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14f73ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14f73d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14f73d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14f73dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14f73e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14f73e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14f73e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14f73ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14f73f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14f73f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14f73fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14f740100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14f7405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14f740a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14f740ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14f741380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14f741820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14f741cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14f742160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14f742600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14f742aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14f742f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14f7433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14f743880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14f743d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14f7441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14f744660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14f744b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14f744fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14f745440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14f7458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14f745d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14f746220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14f7466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14f746b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14f747000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14f7474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14f747940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14f747de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14f748280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14f748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14f748bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14f749060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14f749500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14f7499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14f749e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14f74a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14f74a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14f74ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14f74b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14f74b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14f74ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14f74bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14f74c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14f74c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14f74cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14f74d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14f74d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14f74da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14f74df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14f74e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14f74e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14f74eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14f74f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14f74f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14f74fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14f750320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14f750930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14f751120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14f7515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14f751880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14f751e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14f7524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14f752c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14f753130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14f7535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14f753a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14f754220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14f754770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14f754cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14f755210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14f755760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14f755cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14f756200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14f756750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14f756ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14f7571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14f757740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14f757c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14f7581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14f758730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14f758c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14f7591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14f759720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14f759c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14f75a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14f75a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14f75ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14f75b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14f75b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14f75bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14f75c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14f75c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14f75cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14f75d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14f75d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14f75dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14f75e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14f75e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14f75ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14f75f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14f75f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14f75fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14f760160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14f7606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14f760c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14f761150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14f7616a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14f761bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14f762140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14f762690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14f762be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14f763130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14f763680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14f763bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14f764120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14f764670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14f764bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14f765110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14f765660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14f765bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14f766100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14f766650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14f766ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14f767040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14f7674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14f767980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14f767e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14f7682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14f768760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14f768c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14f7690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14f769540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14f7699e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14f769e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14f76a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14f76a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14f76ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14f76b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14f76b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14f76bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14f76c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14f76cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14f76d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14f76d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14f76dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14f76e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14f76e650 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.730 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13fe04d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13fe051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13fe05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13fe05aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13fe05f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13fe06380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13fe067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13fe06c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13fe070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13fe07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13fe079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13fe080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13fe08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13fe09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13fe09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13fe0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13fe0a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13fe0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13fe0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13fe0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13fe0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13fe0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13fe0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13fe0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13fe0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13fe0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13fe0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13fe0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13fe0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13fe0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13fe0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13fe0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13fe103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13fe10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13fe10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13fe10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13fe113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13fe11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13fe11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13fe12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13fe12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13fe129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13fe12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13fe132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13fe13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13fe13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13fe14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13fe14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13fe14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13fe14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13fe151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13fe15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13fe15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13fe15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13fe163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13fe16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13fe16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13fe17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13fe176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13fe17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13fe17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13fe18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13fe188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13fe18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13fe19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13fe19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13fe19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13fe19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13fe1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13fe1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13fe1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13fe1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13fe1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13fe1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13fe1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13fe1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13fe1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13fe1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13fe1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13fe1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13fe1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13fe1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13fe1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13fe1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13fe1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13fe1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13fe1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13fe1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13fe1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13fe20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13fe204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13fe20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13fe20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13fe21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13fe216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13fe21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13fe21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13fe22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13fe22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13fe22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13fe23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13fe235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13fe23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13fe23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13fe24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13fe24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13fe24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13fe25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13fe254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13fe25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13fe25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13fe26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13fe26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13fe26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13fe26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13fe273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13fe27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13fe27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13fe28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13fe285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13fe28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13fe28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13fe292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13fe29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13fe29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13fe2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13fe2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13fe2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13fe2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13fe2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13fe2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13fe2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13fe2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13fe2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13fe2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13fe2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13fe2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13fe2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13fe2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13fe2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13fe2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13fe2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13fe2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13fe2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13fe2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13fe2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13fe2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13fe301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13fe30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13fe30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13fe30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13fe313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13fe31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13fe31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13fe320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13fe32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13fe329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13fe32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13fe332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13fe33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13fe33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13fe34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13fe34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13fe348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13fe34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13fe351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13fe35630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13fe35aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13fe35f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13fe36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13fe367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13fe36c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13fe370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13fe37540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13fe379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13fe37e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13fe38290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13fe38700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13fe38b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13fe38fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13fe39450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13fe398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13fe39d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13fe3a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13fe3a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13fe3aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13fe3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13fe3b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13fe3b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13fe3bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13fe3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13fe3c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13fe3c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13fe3ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13fe3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13fe3d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13fe3db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13fe3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13fe3e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13fe3e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13fe3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13fe3f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13fe3f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13fe3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13fe3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13fe40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13fe407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13fe40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13fe411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13fe41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13fe42170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13fe42430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13fe426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13fe42b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13fe42fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13fe43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13fe438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13fe43d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13fe44190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13fe44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13fe44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13fe44ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13fe45350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13fe457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13fe45c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13fe460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13fe46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13fe46980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13fe46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13fe47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13fe476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13fe47b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13fe47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13fe48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13fe48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13fe48d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13fe49170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13fe495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13fe49a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13fe49ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13fe4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13fe4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13fe4ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13fe4b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13fe4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13fe4b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13fe4bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13fe4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13fe4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13fe4cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13fe4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13fe4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13fe4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13fe4dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13fe4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13fe4e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13fe4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13fe4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13fe4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13fe4f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13fe4fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13fe50060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13fe504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13fe50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13fe50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13fe51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13fe51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13fe51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13fe51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13fe523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13fe52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13fe52cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13fe53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13fe535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13fe53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13fe53e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13fe542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13fe54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13fe54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13fe55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13fe554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13fe55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13fe55d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13fe56800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13fe56f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13fe57640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13fe57d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13fe58020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13fe58490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13fe58a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13fe590a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14f72ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14f72b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14f72b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14f72bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14f72bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14f72c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14f72c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14f72cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14f72d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14f72d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14f72d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14f72dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14f72e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14f72f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14f72f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14f72fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14f7305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14f730cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14f7313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14f731d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14f732420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14f732b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14f733200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14f7338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14f733fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14f734450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14f7348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14f734d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14f7351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14f735610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14f735a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14f735ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14f736360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14f736620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14f736a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14f736f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14f737370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14f7377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14f737c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14f7380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14f738530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14f7389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14f738e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14f739280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14f7396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14f739b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14f739fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14f73a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14f73a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14f73ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14f73b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14f73b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14f73ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14f73bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14f73c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14f73c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14f73cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14f73d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14f73d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14f73d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14f73ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14f73e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14f73e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14f73eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14f73efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14f73f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14f73f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14f73fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14f740170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14f7405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14f740a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14f740ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14f741330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14f7417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14f741c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14f742080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14f7424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14f742960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14f742dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14f743240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14f7436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14f743b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14f743f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14f744400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14f744870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14f744ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14f745150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14f7455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14f745a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14f745ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14f746310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14f746780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14f746bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14f747060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14f7474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14f747940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14f747db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14f748220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14f748690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14f748b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14f748f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14f7493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14f749850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14f749cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14f74a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14f74a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14f74aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14f74ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14f74b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14f74b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14f74bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14f74c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14f74c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14f74c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14f74cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14f74d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14f74d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14f74dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14f74df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14f74e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14f74e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14f74eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14f74f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14f74f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14f74f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14f74fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14f7502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14f750740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14f750bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14f751020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14f751490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14f751900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14f751d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14f7521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14f752650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14f752ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14f752f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14f7533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14f753810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14f753c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14f7540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14f754560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14f7549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14f754e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14f7552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14f755720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14f755b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14f756000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14f756470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14f7568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14f756d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14f7571c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14f757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14f757aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14f757f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14f758380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14f7587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14f758c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14f7590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14f759540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14f7599b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14f759e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14f75a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14f75a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14f75ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14f75afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14f75b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14f75b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14f75bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14f75c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14f75c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14f75ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14f75cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14f75d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14f75d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14f75dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14f75e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14f75e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14f75e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14f75ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14f75f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14f75f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14f75fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14f75ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14f760430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14f7608a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14f760d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150804230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1508046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x150804b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150804f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1508053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150805860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x150805cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150806140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1508065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150806a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150806e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150807300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150807770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x150807be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x150808050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1508084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150808930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x150808da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x150809210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x150809680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150809c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15080a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15080a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15080b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15080b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15080b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15080ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15080be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15080c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15080c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15080cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15080d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15080d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15080d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15080dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15080e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15080e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15080eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15080ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15080f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15080f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15080fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x150810120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150810590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150810a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x150810e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1508112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x150811750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150811bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150812030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1508124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150812910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150812d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1508131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x150813660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x150813ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x150813f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1508148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x150814b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x150814fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x150815450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1508158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150815d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1508161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x150816610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x150816a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150816ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x150817360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1508177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x150817c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1508180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x150818520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150818990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x150818e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x150819270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1508196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150819b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x150819fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15081a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15081a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15081ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15081b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15081b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15081ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15081bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15081c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15081c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15081cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15081d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15081d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15081d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15081dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15081e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15081e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15081eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15081efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15081fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x150820130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150820850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x150820f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x150821230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1508216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x150821ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1508222b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.943s
user	0m0.243s
sys	0m0.150s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.53 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.11 sec*proc (2 tests)

Total Test time (real) =   1.12 sec
        1.14 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.24 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
