### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.37 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.78 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.02 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.43 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.16 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.66 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  179.53 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.90 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.84 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.22 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 222.44 sec*proc (27 tests)

Total Test time (real) = 222.45 sec

real	3m42.482s
user	7m38.193s
sys	0m6.299s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.13 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.32 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.08 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.20 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.94 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.18 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.28 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.38 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.05 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.20 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.12 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.88 sec*proc (27 tests)

Total Test time (real) =  50.89 sec

real	0m50.900s
user	1m10.130s
sys	0m6.002s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.122 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.414 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.693 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.704 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.705 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.706 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.707 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.708 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.709 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.709 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.710 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.711 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.714 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.714 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.715 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.716 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.716 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.717 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.718 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.027.758 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.080 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.083 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.084 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.084 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.085 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.029.085 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.086 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.029.087 I llama_model_loader: - type  f32:  124 tensors
0.00.029.087 I llama_model_loader: - type  f16:   73 tensors
0.00.033.900 I llm_load_vocab: special tokens cache size = 5
0.00.036.132 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.036.136 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.036.137 I llm_load_print_meta: arch             = bert
0.00.036.137 I llm_load_print_meta: vocab type       = WPM
0.00.036.137 I llm_load_print_meta: n_vocab          = 30522
0.00.036.138 I llm_load_print_meta: n_merges         = 0
0.00.036.138 I llm_load_print_meta: vocab_only       = 0
0.00.036.138 I llm_load_print_meta: n_ctx_train      = 512
0.00.036.138 I llm_load_print_meta: n_embd           = 384
0.00.036.139 I llm_load_print_meta: n_layer          = 12
0.00.036.167 I llm_load_print_meta: n_head           = 12
0.00.036.169 I llm_load_print_meta: n_head_kv        = 12
0.00.036.169 I llm_load_print_meta: n_rot            = 32
0.00.036.169 I llm_load_print_meta: n_swa            = 0
0.00.036.170 I llm_load_print_meta: n_embd_head_k    = 32
0.00.036.170 I llm_load_print_meta: n_embd_head_v    = 32
0.00.036.171 I llm_load_print_meta: n_gqa            = 1
0.00.036.172 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.036.172 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.036.173 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.036.174 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.036.174 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.036.174 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.036.174 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.036.175 I llm_load_print_meta: n_ff             = 1536
0.00.036.176 I llm_load_print_meta: n_expert         = 0
0.00.036.176 I llm_load_print_meta: n_expert_used    = 0
0.00.036.176 I llm_load_print_meta: causal attn      = 0
0.00.036.177 I llm_load_print_meta: pooling type     = 2
0.00.036.177 I llm_load_print_meta: rope type        = 2
0.00.036.177 I llm_load_print_meta: rope scaling     = linear
0.00.036.178 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.036.178 I llm_load_print_meta: freq_scale_train = 1
0.00.036.179 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.036.179 I llm_load_print_meta: rope_finetuned   = unknown
0.00.036.179 I llm_load_print_meta: ssm_d_conv       = 0
0.00.036.179 I llm_load_print_meta: ssm_d_inner      = 0
0.00.036.179 I llm_load_print_meta: ssm_d_state      = 0
0.00.036.180 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.036.182 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.036.194 I llm_load_print_meta: model type       = 33M
0.00.036.194 I llm_load_print_meta: model ftype      = F16
0.00.036.195 I llm_load_print_meta: model params     = 33.21 M
0.00.036.196 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.036.196 I llm_load_print_meta: general.name     = Bge Small
0.00.036.196 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.036.197 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.036.197 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.036.197 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.036.198 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.036.198 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.036.198 I llm_load_print_meta: max token length = 21
0.00.038.296 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.038.302 I llm_load_tensors: offloading output layer to GPU
0.00.038.302 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.038.331 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.038.332 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.038.917 I llama_new_context_with_model: n_seq_max     = 1
0.00.038.918 I llama_new_context_with_model: n_ctx         = 512
0.00.038.919 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.038.919 I llama_new_context_with_model: n_batch       = 2048
0.00.038.919 I llama_new_context_with_model: n_ubatch      = 2048
0.00.038.920 I llama_new_context_with_model: flash_attn    = 0
0.00.038.920 I llama_new_context_with_model: freq_base     = 10000.0
0.00.038.920 I llama_new_context_with_model: freq_scale    = 1
0.00.038.921 I ggml_metal_init: allocating
0.00.038.925 I ggml_metal_init: found device: Apple M4
0.00.038.928 I ggml_metal_init: picking default device: Apple M4
0.00.039.834 I ggml_metal_init: using embedded metal library
0.00.044.257 I ggml_metal_init: GPU name:   Apple M4
0.00.044.260 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.044.260 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.044.261 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.044.261 I ggml_metal_init: simdgroup reduction   = true
0.00.044.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.044.262 I ggml_metal_init: has bfloat            = true
0.00.044.262 I ggml_metal_init: use bfloat            = true
0.00.044.263 I ggml_metal_init: hasUnifiedMemory      = true
0.00.044.263 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.057.627 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.057.629 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.057.630 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.058.412 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.058.413 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.058.414 I llama_new_context_with_model: graph nodes  = 429
0.00.058.414 I llama_new_context_with_model: graph splits = 2
0.00.058.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.093 I 
0.00.065.123 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.065.785 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.070.538 I llama_perf_context_print:        load time =      46.67 ms
0.00.070.539 I llama_perf_context_print: prompt eval time =       4.59 ms /     9 tokens (    0.51 ms per token,  1961.64 tokens per second)
0.00.070.540 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.070.541 I llama_perf_context_print:       total time =       5.44 ms /    10 tokens
0.00.070.677 I ggml_metal_free: deallocating

real	0m0.249s
user	0m0.050s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.440 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.651 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.654 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.656 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.658 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.658 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.658 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.659 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.660 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.660 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.660 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.661 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.661 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.663 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.663 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.664 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.664 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.664 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.664 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.665 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.322 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.015 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.017 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.017 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.017 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.018 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.018 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.018 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.019 I llama_model_loader: - type  f32:  124 tensors
0.00.015.019 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.659 I llm_load_vocab: special tokens cache size = 5
0.00.018.942 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.944 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.945 I llm_load_print_meta: arch             = bert
0.00.018.945 I llm_load_print_meta: vocab type       = WPM
0.00.018.945 I llm_load_print_meta: n_vocab          = 30522
0.00.018.945 I llm_load_print_meta: n_merges         = 0
0.00.018.945 I llm_load_print_meta: vocab_only       = 0
0.00.018.946 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.946 I llm_load_print_meta: n_embd           = 384
0.00.018.946 I llm_load_print_meta: n_layer          = 12
0.00.018.955 I llm_load_print_meta: n_head           = 12
0.00.018.955 I llm_load_print_meta: n_head_kv        = 12
0.00.018.955 I llm_load_print_meta: n_rot            = 32
0.00.018.956 I llm_load_print_meta: n_swa            = 0
0.00.018.956 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.956 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.956 I llm_load_print_meta: n_gqa            = 1
0.00.018.957 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.957 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.958 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.958 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.958 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.958 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.959 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.959 I llm_load_print_meta: n_ff             = 1536
0.00.018.959 I llm_load_print_meta: n_expert         = 0
0.00.018.959 I llm_load_print_meta: n_expert_used    = 0
0.00.018.961 I llm_load_print_meta: causal attn      = 0
0.00.018.961 I llm_load_print_meta: pooling type     = 2
0.00.018.961 I llm_load_print_meta: rope type        = 2
0.00.018.966 I llm_load_print_meta: rope scaling     = linear
0.00.018.966 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.966 I llm_load_print_meta: freq_scale_train = 1
0.00.018.966 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.967 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.967 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.967 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.967 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.967 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.969 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.973 I llm_load_print_meta: model type       = 33M
0.00.018.974 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.974 I llm_load_print_meta: model params     = 33.21 M
0.00.018.974 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.976 I llm_load_print_meta: general.name     = Bge Small
0.00.018.976 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.976 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.977 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.977 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.977 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.977 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.977 I llm_load_print_meta: max token length = 21
0.00.020.279 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.279 I llm_load_tensors: offloading output layer to GPU
0.00.020.280 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.287 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.287 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.640 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.641 I llama_new_context_with_model: n_ctx         = 512
0.00.020.641 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.642 I llama_new_context_with_model: n_batch       = 2048
0.00.020.642 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.642 I llama_new_context_with_model: flash_attn    = 0
0.00.020.642 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.643 I llama_new_context_with_model: freq_scale    = 1
0.00.020.643 I ggml_metal_init: allocating
0.00.020.646 I ggml_metal_init: found device: Apple M4
0.00.020.648 I ggml_metal_init: picking default device: Apple M4
0.00.021.268 I ggml_metal_init: using embedded metal library
0.00.023.850 I ggml_metal_init: GPU name:   Apple M4
0.00.023.852 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.852 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.852 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.854 I ggml_metal_init: simdgroup reduction   = true
0.00.023.854 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.854 I ggml_metal_init: has bfloat            = true
0.00.023.854 I ggml_metal_init: use bfloat            = true
0.00.023.855 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.857 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.554 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.558 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.560 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.161 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.162 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.162 I llama_new_context_with_model: graph nodes  = 429
0.00.035.163 I llama_new_context_with_model: graph splits = 2
0.00.035.175 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.485 I 
0.00.040.511 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.052 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.617 I llama_perf_context_print:        load time =      31.04 ms
0.00.045.618 I llama_perf_context_print: prompt eval time =       4.41 ms /     9 tokens (    0.49 ms per token,  2041.28 tokens per second)
0.00.045.618 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.619 I llama_perf_context_print:       total time =       5.13 ms /    10 tokens
0.00.045.810 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.139 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.955 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.500 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.505 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.507 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.508 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.509 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.510 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.511 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.512 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.513 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.513 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.514 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.516 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.519 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.519 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.520 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.521 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.522 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.988 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.110 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.111 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.111 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.111 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.112 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.112 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.112 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.113 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.113 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.113 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.114 I llama_model_loader: - type  f32:   41 tensors
0.00.048.114 I llama_model_loader: - type  f16:   29 tensors
0.00.065.798 W llm_load_vocab: empty token at index 5
0.00.070.401 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.071.605 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.630 I llm_load_vocab: special tokens cache size = 5
0.00.331.519 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.331.552 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.331.558 I llm_load_print_meta: arch             = jina-bert-v2
0.00.331.559 I llm_load_print_meta: vocab type       = BPE
0.00.331.559 I llm_load_print_meta: n_vocab          = 61056
0.00.331.560 I llm_load_print_meta: n_merges         = 39382
0.00.331.560 I llm_load_print_meta: vocab_only       = 0
0.00.331.561 I llm_load_print_meta: n_ctx_train      = 8192
0.00.331.561 I llm_load_print_meta: n_embd           = 384
0.00.331.562 I llm_load_print_meta: n_layer          = 4
0.00.331.602 I llm_load_print_meta: n_head           = 12
0.00.331.603 I llm_load_print_meta: n_head_kv        = 12
0.00.331.603 I llm_load_print_meta: n_rot            = 32
0.00.331.603 I llm_load_print_meta: n_swa            = 0
0.00.331.603 I llm_load_print_meta: n_embd_head_k    = 32
0.00.331.604 I llm_load_print_meta: n_embd_head_v    = 32
0.00.331.609 I llm_load_print_meta: n_gqa            = 1
0.00.331.611 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.331.611 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.331.620 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.331.621 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.331.621 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.331.621 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.331.621 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.331.622 I llm_load_print_meta: n_ff             = 1536
0.00.331.622 I llm_load_print_meta: n_expert         = 0
0.00.331.622 I llm_load_print_meta: n_expert_used    = 0
0.00.331.622 I llm_load_print_meta: causal attn      = 0
0.00.331.622 I llm_load_print_meta: pooling type     = -1
0.00.331.623 I llm_load_print_meta: rope type        = -1
0.00.331.623 I llm_load_print_meta: rope scaling     = linear
0.00.331.623 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.331.624 I llm_load_print_meta: freq_scale_train = 1
0.00.331.626 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.331.626 I llm_load_print_meta: rope_finetuned   = unknown
0.00.331.626 I llm_load_print_meta: ssm_d_conv       = 0
0.00.331.626 I llm_load_print_meta: ssm_d_inner      = 0
0.00.331.627 I llm_load_print_meta: ssm_d_state      = 0
0.00.331.627 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.331.627 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.331.652 I llm_load_print_meta: model type       = 33M
0.00.331.652 I llm_load_print_meta: model ftype      = F16
0.00.331.653 I llm_load_print_meta: model params     = 32.90 M
0.00.331.653 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.331.657 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.331.657 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.331.658 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.331.658 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.331.658 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.331.658 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.331.658 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.331.659 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.331.659 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.331.659 I llm_load_print_meta: max token length = 45
0.00.333.149 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.333.150 I llm_load_tensors: offloading output layer to GPU
0.00.333.150 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.333.178 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.333.179 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.334.186 I llama_new_context_with_model: n_seq_max     = 1
0.00.334.187 I llama_new_context_with_model: n_ctx         = 8192
0.00.334.187 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.334.187 I llama_new_context_with_model: n_batch       = 2048
0.00.334.188 I llama_new_context_with_model: n_ubatch      = 2048
0.00.334.188 I llama_new_context_with_model: flash_attn    = 0
0.00.334.188 I llama_new_context_with_model: freq_base     = 10000.0
0.00.334.188 I llama_new_context_with_model: freq_scale    = 1
0.00.334.189 I ggml_metal_init: allocating
0.00.334.192 I ggml_metal_init: found device: Apple M4
0.00.334.194 I ggml_metal_init: picking default device: Apple M4
0.00.335.393 I ggml_metal_init: using embedded metal library
0.00.338.101 I ggml_metal_init: GPU name:   Apple M4
0.00.338.104 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.338.105 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.338.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.338.105 I ggml_metal_init: simdgroup reduction   = true
0.00.338.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.338.105 I ggml_metal_init: has bfloat            = true
0.00.338.106 I ggml_metal_init: use bfloat            = true
0.00.338.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.338.107 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.350.205 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.350.207 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.350.211 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.350.862 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.863 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.863 I llama_new_context_with_model: graph nodes  = 154
0.00.350.863 I llama_new_context_with_model: graph splits = 2
0.00.350.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.364.822 I 
0.00.364.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.124 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.365.125 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.365.133 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.365.133 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.365.135 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.365.136 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.365.680 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.369.310 I llama_perf_context_print:        load time =     341.86 ms
0.00.369.311 I llama_perf_context_print: prompt eval time =       3.62 ms /    62 tokens (    0.06 ms per token, 17127.07 tokens per second)
0.00.369.312 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.369.312 I llama_perf_context_print:       total time =       4.49 ms /    63 tokens
0.00.369.534 I ggml_metal_free: deallocating

real	0m1.051s
user	0m0.338s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.111 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.244 I main: llama backend init
0.00.000.250 I main: load the model and apply lora adapter, if any
0.00.087.928 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.098.914 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.098.926 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.098.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.098.931 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.098.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.098.932 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.098.932 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.098.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.098.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.098.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.098.937 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.098.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.098.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.098.939 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.098.953 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.098.954 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.098.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.105.790 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.108.003 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.114.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.114.986 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.114.987 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.114.988 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.114.989 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.114.990 I llama_model_loader: - type  f32:  194 tensors
0.00.114.991 I llama_model_loader: - type  f16:   98 tensors
0.00.155.971 I llm_load_vocab: special tokens cache size = 25
0.00.163.890 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.163.894 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.163.894 I llm_load_print_meta: arch             = gptneox
0.00.163.895 I llm_load_print_meta: vocab type       = BPE
0.00.163.895 I llm_load_print_meta: n_vocab          = 50304
0.00.163.895 I llm_load_print_meta: n_merges         = 50009
0.00.163.895 I llm_load_print_meta: vocab_only       = 0
0.00.163.896 I llm_load_print_meta: n_ctx_train      = 2048
0.00.163.896 I llm_load_print_meta: n_embd           = 2048
0.00.163.896 I llm_load_print_meta: n_layer          = 24
0.00.163.920 I llm_load_print_meta: n_head           = 16
0.00.163.922 I llm_load_print_meta: n_head_kv        = 16
0.00.163.922 I llm_load_print_meta: n_rot            = 32
0.00.163.922 I llm_load_print_meta: n_swa            = 0
0.00.163.922 I llm_load_print_meta: n_embd_head_k    = 128
0.00.163.922 I llm_load_print_meta: n_embd_head_v    = 128
0.00.163.923 I llm_load_print_meta: n_gqa            = 1
0.00.163.924 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.163.924 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.163.925 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.163.926 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.163.926 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.163.926 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.163.926 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.163.927 I llm_load_print_meta: n_ff             = 8192
0.00.163.927 I llm_load_print_meta: n_expert         = 0
0.00.163.927 I llm_load_print_meta: n_expert_used    = 0
0.00.163.927 I llm_load_print_meta: causal attn      = 1
0.00.163.927 I llm_load_print_meta: pooling type     = 0
0.00.163.928 I llm_load_print_meta: rope type        = 2
0.00.163.930 I llm_load_print_meta: rope scaling     = linear
0.00.163.930 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.163.931 I llm_load_print_meta: freq_scale_train = 1
0.00.163.931 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.163.931 I llm_load_print_meta: rope_finetuned   = unknown
0.00.163.931 I llm_load_print_meta: ssm_d_conv       = 0
0.00.163.932 I llm_load_print_meta: ssm_d_inner      = 0
0.00.163.932 I llm_load_print_meta: ssm_d_state      = 0
0.00.163.932 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.163.932 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.163.942 I llm_load_print_meta: model type       = 1.4B
0.00.163.942 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.163.942 I llm_load_print_meta: model params     = 1.41 B
0.00.163.943 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.163.943 I llm_load_print_meta: general.name     = 1.4B
0.00.163.944 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.163.944 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.163.944 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.163.944 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.163.944 I llm_load_print_meta: LF token         = 128 ''
0.00.163.945 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.163.945 I llm_load_print_meta: max token length = 1024
0.00.166.654 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.166.654 I llm_load_tensors: offloading output layer to GPU
0.00.166.654 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.166.673 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.166.674 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.167.680 I llama_new_context_with_model: n_seq_max     = 1
0.00.167.681 I llama_new_context_with_model: n_ctx         = 2048
0.00.167.681 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.167.681 I llama_new_context_with_model: n_batch       = 2048
0.00.167.682 I llama_new_context_with_model: n_ubatch      = 512
0.00.167.682 I llama_new_context_with_model: flash_attn    = 0
0.00.167.682 I llama_new_context_with_model: freq_base     = 10000.0
0.00.167.683 I llama_new_context_with_model: freq_scale    = 1
0.00.167.683 I ggml_metal_init: allocating
0.00.167.686 I ggml_metal_init: found device: Apple M4
0.00.167.689 I ggml_metal_init: picking default device: Apple M4
0.00.168.401 I ggml_metal_init: using embedded metal library
0.00.178.097 I ggml_metal_init: GPU name:   Apple M4
0.00.178.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.178.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.178.100 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.178.100 I ggml_metal_init: simdgroup reduction   = true
0.00.178.100 I ggml_metal_init: simdgroup matrix mul. = true
0.00.178.100 I ggml_metal_init: has bfloat            = true
0.00.178.101 I ggml_metal_init: use bfloat            = true
0.00.178.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.178.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.225.644 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.225.652 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.225.672 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.226.722 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.226.725 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.226.725 I llama_new_context_with_model: graph nodes  = 967
0.00.226.725 I llama_new_context_with_model: graph splits = 2
0.00.226.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.301.201 I main: llama threadpool init, n_threads = 4
0.00.301.235 I 
0.00.301.269 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.301.271 I 
0.00.301.353 I sampler seed: 1234
0.00.301.358 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.301.382 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.301.383 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.301.383 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.152.139 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55468.75 tokens per second)
0.02.152.140 I llama_perf_context_print:        load time =     213.26 ms
0.02.152.141 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.03 tokens per second)
0.02.152.141 I llama_perf_context_print:        eval time =    1804.02 ms /    63 runs   (   28.64 ms per token,    34.92 tokens per second)
0.02.152.142 I llama_perf_context_print:       total time =    1850.94 ms /    70 tokens
0.02.152.326 I ggml_metal_free: deallocating

real	0m2.456s
user	0m0.153s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.625 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.155 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.489 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.495 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.497 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.498 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.498 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.501 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.503 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.504 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.515 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.516 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.517 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.036 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.013 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.801 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.052.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.804 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.804 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.805 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.806 I llama_model_loader: - type  f32:  194 tensors
0.00.052.806 I llama_model_loader: - type  f16:   98 tensors
0.00.081.810 I llm_load_vocab: special tokens cache size = 25
0.00.088.420 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.423 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.423 I llm_load_print_meta: arch             = gptneox
0.00.088.424 I llm_load_print_meta: vocab type       = BPE
0.00.088.424 I llm_load_print_meta: n_vocab          = 50304
0.00.088.424 I llm_load_print_meta: n_merges         = 50009
0.00.088.424 I llm_load_print_meta: vocab_only       = 0
0.00.088.424 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.425 I llm_load_print_meta: n_embd           = 2048
0.00.088.425 I llm_load_print_meta: n_layer          = 24
0.00.088.439 I llm_load_print_meta: n_head           = 16
0.00.088.440 I llm_load_print_meta: n_head_kv        = 16
0.00.088.440 I llm_load_print_meta: n_rot            = 32
0.00.088.440 I llm_load_print_meta: n_swa            = 0
0.00.088.441 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.441 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.441 I llm_load_print_meta: n_gqa            = 1
0.00.088.442 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.443 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.443 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.443 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.444 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.444 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.444 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.444 I llm_load_print_meta: n_ff             = 8192
0.00.088.445 I llm_load_print_meta: n_expert         = 0
0.00.088.445 I llm_load_print_meta: n_expert_used    = 0
0.00.088.446 I llm_load_print_meta: causal attn      = 1
0.00.088.446 I llm_load_print_meta: pooling type     = 0
0.00.088.446 I llm_load_print_meta: rope type        = 2
0.00.088.446 I llm_load_print_meta: rope scaling     = linear
0.00.088.446 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.447 I llm_load_print_meta: freq_scale_train = 1
0.00.088.447 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.447 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.448 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.448 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.448 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.448 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.448 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.458 I llm_load_print_meta: model type       = 1.4B
0.00.088.458 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.458 I llm_load_print_meta: model params     = 1.41 B
0.00.088.459 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.459 I llm_load_print_meta: general.name     = 1.4B
0.00.088.459 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.460 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.460 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.460 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.460 I llm_load_print_meta: LF token         = 128 ''
0.00.088.460 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.460 I llm_load_print_meta: max token length = 1024
0.00.090.931 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.931 I llm_load_tensors: offloading output layer to GPU
0.00.090.931 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.942 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.943 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.091.866 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.867 I llama_new_context_with_model: n_ctx         = 128
0.00.091.867 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.867 I llama_new_context_with_model: n_batch       = 128
0.00.091.867 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.867 I llama_new_context_with_model: flash_attn    = 0
0.00.091.868 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.868 I llama_new_context_with_model: freq_scale    = 1
0.00.091.868 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.869 I ggml_metal_init: allocating
0.00.091.876 I ggml_metal_init: found device: Apple M4
0.00.091.879 I ggml_metal_init: picking default device: Apple M4
0.00.092.502 I ggml_metal_init: using embedded metal library
0.00.095.057 I ggml_metal_init: GPU name:   Apple M4
0.00.095.059 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.059 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.060 I ggml_metal_init: simdgroup reduction   = true
0.00.095.060 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.060 I ggml_metal_init: has bfloat            = true
0.00.095.060 I ggml_metal_init: use bfloat            = true
0.00.095.060 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.061 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.073 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.076 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.090 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.977 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.978 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.978 I llama_new_context_with_model: graph nodes  = 967
0.00.107.978 I llama_new_context_with_model: graph splits = 2
0.00.107.990 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.919.540 I 
0.00.919.598 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.919.639 I perplexity: tokenizing the input ..
0.00.931.963 I perplexity: tokenization took 12.322 ms
0.00.931.994 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.053.218 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.054.888 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.054.925 I llama_perf_context_print:        load time =     896.37 ms
0.01.054.927 I llama_perf_context_print: prompt eval time =     120.84 ms /   128 tokens (    0.94 ms per token,  1059.29 tokens per second)
0.01.054.928 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.054.929 I llama_perf_context_print:       total time =     135.39 ms /   129 tokens
0.01.055.649 I ggml_metal_free: deallocating

real	0m1.242s
user	0m0.121s
sys	0m0.201s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.338 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.153 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.163 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.163 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.163 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.164 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.165 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.165 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.165 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.166 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.166 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.166 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.168 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.169 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.237 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.349 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.403 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.404 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.405 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.405 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.406 I llama_model_loader: - type  f32:  194 tensors
0.00.036.407 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.403 I llm_load_vocab: special tokens cache size = 25
0.00.068.113 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.118 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.118 I llm_load_print_meta: arch             = gptneox
0.00.068.118 I llm_load_print_meta: vocab type       = BPE
0.00.068.119 I llm_load_print_meta: n_vocab          = 50304
0.00.068.120 I llm_load_print_meta: n_merges         = 50009
0.00.068.121 I llm_load_print_meta: vocab_only       = 0
0.00.068.121 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.121 I llm_load_print_meta: n_embd           = 2048
0.00.068.121 I llm_load_print_meta: n_layer          = 24
0.00.068.140 I llm_load_print_meta: n_head           = 16
0.00.068.151 I llm_load_print_meta: n_head_kv        = 16
0.00.068.152 I llm_load_print_meta: n_rot            = 32
0.00.068.153 I llm_load_print_meta: n_swa            = 0
0.00.068.153 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.153 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.154 I llm_load_print_meta: n_gqa            = 1
0.00.068.155 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.155 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.156 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.157 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.157 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.157 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.157 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.158 I llm_load_print_meta: n_ff             = 8192
0.00.068.158 I llm_load_print_meta: n_expert         = 0
0.00.068.159 I llm_load_print_meta: n_expert_used    = 0
0.00.068.159 I llm_load_print_meta: causal attn      = 1
0.00.068.159 I llm_load_print_meta: pooling type     = 0
0.00.068.160 I llm_load_print_meta: rope type        = 2
0.00.068.161 I llm_load_print_meta: rope scaling     = linear
0.00.068.161 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.161 I llm_load_print_meta: freq_scale_train = 1
0.00.068.162 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.162 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.162 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.162 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.162 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.162 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.165 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.172 I llm_load_print_meta: model type       = 1.4B
0.00.068.172 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.173 I llm_load_print_meta: model params     = 1.41 B
0.00.068.173 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.175 I llm_load_print_meta: general.name     = 1.4B
0.00.068.175 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.175 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.175 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.175 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.176 I llm_load_print_meta: LF token         = 128 ''
0.00.068.176 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.176 I llm_load_print_meta: max token length = 1024
0.00.070.502 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.503 I llm_load_tensors: offloading output layer to GPU
0.00.070.503 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.509 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.511 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.578 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.579 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.579 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.579 I llama_new_context_with_model: n_batch       = 2048
0.00.071.579 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.580 I llama_new_context_with_model: flash_attn    = 0
0.00.071.580 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.580 I llama_new_context_with_model: freq_scale    = 1
0.00.071.581 I ggml_metal_init: allocating
0.00.071.585 I ggml_metal_init: found device: Apple M4
0.00.071.588 I ggml_metal_init: picking default device: Apple M4
0.00.072.354 I ggml_metal_init: using embedded metal library
0.00.075.258 I ggml_metal_init: GPU name:   Apple M4
0.00.075.260 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.260 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.261 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.261 I ggml_metal_init: simdgroup reduction   = true
0.00.075.261 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.262 I ggml_metal_init: has bfloat            = true
0.00.075.262 I ggml_metal_init: use bfloat            = true
0.00.075.262 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.263 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.556 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.111.566 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.111.597 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.112.753 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.112.754 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.112.755 I llama_new_context_with_model: graph nodes  = 967
0.00.112.755 I llama_new_context_with_model: graph splits = 2
0.00.112.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.464.428 I main: llama threadpool init, n_threads = 4
0.01.464.511 I 
0.01.464.584 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.464.586 I 
0.01.465.066 I sampler seed: 1234
0.01.465.072 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.465.099 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.465.101 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.465.101 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.555.844 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.02.555.846 I llama_perf_context_print:        load time =    1455.09 ms
0.02.555.847 I llama_perf_context_print: prompt eval time =      39.84 ms /     7 tokens (    5.69 ms per token,   175.72 tokens per second)
0.02.555.847 I llama_perf_context_print:        eval time =    1048.12 ms /    63 runs   (   16.64 ms per token,    60.11 tokens per second)
0.02.555.848 I llama_perf_context_print:       total time =    1091.42 ms /    70 tokens
0.02.556.075 I ggml_metal_free: deallocating

real	0m2.575s
user	0m0.125s
sys	0m0.248s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.130 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.634 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.157 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.163 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.165 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.165 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.165 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.166 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.166 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.167 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.172 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.172 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.172 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.802 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.277 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.859 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.861 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.863 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.864 I llama_model_loader: - type  f32:  194 tensors
0.00.032.864 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.657 I llm_load_vocab: special tokens cache size = 25
0.00.064.109 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.112 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.113 I llm_load_print_meta: arch             = gptneox
0.00.064.113 I llm_load_print_meta: vocab type       = BPE
0.00.064.113 I llm_load_print_meta: n_vocab          = 50304
0.00.064.113 I llm_load_print_meta: n_merges         = 50009
0.00.064.113 I llm_load_print_meta: vocab_only       = 0
0.00.064.114 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.114 I llm_load_print_meta: n_embd           = 2048
0.00.064.114 I llm_load_print_meta: n_layer          = 24
0.00.064.130 I llm_load_print_meta: n_head           = 16
0.00.064.132 I llm_load_print_meta: n_head_kv        = 16
0.00.064.132 I llm_load_print_meta: n_rot            = 32
0.00.064.132 I llm_load_print_meta: n_swa            = 0
0.00.064.132 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.132 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.133 I llm_load_print_meta: n_gqa            = 1
0.00.064.134 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.134 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.135 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.135 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.140 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.140 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.140 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.141 I llm_load_print_meta: n_ff             = 8192
0.00.064.141 I llm_load_print_meta: n_expert         = 0
0.00.064.141 I llm_load_print_meta: n_expert_used    = 0
0.00.064.141 I llm_load_print_meta: causal attn      = 1
0.00.064.142 I llm_load_print_meta: pooling type     = 0
0.00.064.142 I llm_load_print_meta: rope type        = 2
0.00.064.142 I llm_load_print_meta: rope scaling     = linear
0.00.064.142 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.143 I llm_load_print_meta: freq_scale_train = 1
0.00.064.143 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.143 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.143 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.143 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.143 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.144 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.144 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.155 I llm_load_print_meta: model type       = 1.4B
0.00.064.155 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.157 I llm_load_print_meta: model params     = 1.41 B
0.00.064.158 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.158 I llm_load_print_meta: general.name     = 1.4B
0.00.064.158 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.159 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.159 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.159 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.160 I llm_load_print_meta: LF token         = 128 ''
0.00.064.160 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.160 I llm_load_print_meta: max token length = 1024
0.00.066.624 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.624 I llm_load_tensors: offloading output layer to GPU
0.00.066.625 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.636 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.637 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.557 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.557 I llama_new_context_with_model: n_ctx         = 128
0.00.067.558 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.558 I llama_new_context_with_model: n_batch       = 128
0.00.067.558 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.558 I llama_new_context_with_model: flash_attn    = 0
0.00.067.559 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.559 I llama_new_context_with_model: freq_scale    = 1
0.00.067.559 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.560 I ggml_metal_init: allocating
0.00.067.565 I ggml_metal_init: found device: Apple M4
0.00.067.567 I ggml_metal_init: picking default device: Apple M4
0.00.068.204 I ggml_metal_init: using embedded metal library
0.00.070.667 I ggml_metal_init: GPU name:   Apple M4
0.00.070.669 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.670 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.670 I ggml_metal_init: simdgroup reduction   = true
0.00.070.670 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.670 I ggml_metal_init: has bfloat            = true
0.00.070.671 I ggml_metal_init: use bfloat            = true
0.00.070.671 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.128 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.081.131 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.148 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.002 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.082.003 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.082.004 I llama_new_context_with_model: graph nodes  = 967
0.00.082.004 I llama_new_context_with_model: graph splits = 2
0.00.082.017 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.963.351 I 
0.00.963.428 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.963.447 I perplexity: tokenizing the input ..
0.00.981.485 I perplexity: tokenization took 18.035 ms
0.00.981.505 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.121.755 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.123.369 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.123.394 I llama_perf_context_print:        load time =     951.71 ms
0.01.123.395 I llama_perf_context_print: prompt eval time =     139.36 ms /   128 tokens (    1.09 ms per token,   918.49 tokens per second)
0.01.123.396 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.123.397 I llama_perf_context_print:       total time =     160.05 ms /   129 tokens
0.01.123.957 I ggml_metal_free: deallocating

real	0m1.143s
user	0m0.107s
sys	0m0.166s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.770 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.567 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.571 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.573 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.574 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.574 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.575 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.576 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.576 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.577 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.577 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.577 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.579 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.580 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.580 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.571 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.648 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.649 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.650 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.651 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.651 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.652 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.652 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.653 I llama_model_loader: - type  f32:  194 tensors
0.00.026.653 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.654 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.134 I llm_load_vocab: special tokens cache size = 25
0.00.054.131 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.134 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.135 I llm_load_print_meta: arch             = gptneox
0.00.054.135 I llm_load_print_meta: vocab type       = BPE
0.00.054.135 I llm_load_print_meta: n_vocab          = 50304
0.00.054.136 I llm_load_print_meta: n_merges         = 50009
0.00.054.136 I llm_load_print_meta: vocab_only       = 0
0.00.054.136 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.136 I llm_load_print_meta: n_embd           = 2048
0.00.054.136 I llm_load_print_meta: n_layer          = 24
0.00.054.155 I llm_load_print_meta: n_head           = 16
0.00.054.156 I llm_load_print_meta: n_head_kv        = 16
0.00.054.156 I llm_load_print_meta: n_rot            = 32
0.00.054.156 I llm_load_print_meta: n_swa            = 0
0.00.054.157 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.157 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.157 I llm_load_print_meta: n_gqa            = 1
0.00.054.158 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.158 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.159 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.159 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.160 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.160 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.160 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.161 I llm_load_print_meta: n_ff             = 8192
0.00.054.161 I llm_load_print_meta: n_expert         = 0
0.00.054.161 I llm_load_print_meta: n_expert_used    = 0
0.00.054.161 I llm_load_print_meta: causal attn      = 1
0.00.054.161 I llm_load_print_meta: pooling type     = 0
0.00.054.161 I llm_load_print_meta: rope type        = 2
0.00.054.161 I llm_load_print_meta: rope scaling     = linear
0.00.054.162 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.162 I llm_load_print_meta: freq_scale_train = 1
0.00.054.162 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.162 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.162 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.162 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.163 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.164 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.165 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.175 I llm_load_print_meta: model type       = 1.4B
0.00.054.175 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.177 I llm_load_print_meta: model params     = 1.41 B
0.00.054.178 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.178 I llm_load_print_meta: general.name     = 1.4B
0.00.054.178 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.179 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.180 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.180 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.180 I llm_load_print_meta: LF token         = 128 ''
0.00.054.180 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.180 I llm_load_print_meta: max token length = 1024
0.00.056.493 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.494 I llm_load_tensors: offloading output layer to GPU
0.00.056.494 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.506 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.507 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.528 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.529 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.529 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.529 I llama_new_context_with_model: n_batch       = 2048
0.00.057.529 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.529 I llama_new_context_with_model: flash_attn    = 0
0.00.057.530 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.530 I llama_new_context_with_model: freq_scale    = 1
0.00.057.531 I ggml_metal_init: allocating
0.00.057.538 I ggml_metal_init: found device: Apple M4
0.00.057.541 I ggml_metal_init: picking default device: Apple M4
0.00.058.311 I ggml_metal_init: using embedded metal library
0.00.060.833 I ggml_metal_init: GPU name:   Apple M4
0.00.060.835 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.835 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.835 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.836 I ggml_metal_init: simdgroup reduction   = true
0.00.060.836 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.836 I ggml_metal_init: has bfloat            = true
0.00.060.836 I ggml_metal_init: use bfloat            = true
0.00.060.836 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.837 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.767 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.776 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.800 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.981 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.984 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.984 I llama_new_context_with_model: graph nodes  = 967
0.00.095.984 I llama_new_context_with_model: graph splits = 2
0.00.096.001 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.062 I main: llama threadpool init, n_threads = 4
0.00.711.100 I 
0.00.711.131 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.131 I 
0.00.711.376 I sampler seed: 1234
0.00.711.381 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.393 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.393 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.393 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.389.913 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60169.49 tokens per second)
0.01.389.913 I llama_perf_context_print:        load time =     700.29 ms
0.01.389.914 I llama_perf_context_print: prompt eval time =      39.77 ms /     7 tokens (    5.68 ms per token,   176.02 tokens per second)
0.01.389.914 I llama_perf_context_print:        eval time =     635.78 ms /    63 runs   (   10.09 ms per token,    99.09 tokens per second)
0.01.389.916 I llama_perf_context_print:       total time =     678.85 ms /    70 tokens
0.01.390.123 I ggml_metal_free: deallocating

real	0m1.408s
user	0m0.111s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.438 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.294 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.299 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.301 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.302 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.302 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.302 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.303 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.304 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.304 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.304 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.305 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.305 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.305 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.306 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.308 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.308 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.308 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.408 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.839 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.412 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.413 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.413 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.414 I llama_model_loader: - type  f32:  194 tensors
0.00.030.414 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.414 I llama_model_loader: - type q6_K:    1 tensors
0.00.053.310 I llm_load_vocab: special tokens cache size = 25
0.00.059.230 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.233 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.233 I llm_load_print_meta: arch             = gptneox
0.00.059.234 I llm_load_print_meta: vocab type       = BPE
0.00.059.234 I llm_load_print_meta: n_vocab          = 50304
0.00.059.234 I llm_load_print_meta: n_merges         = 50009
0.00.059.234 I llm_load_print_meta: vocab_only       = 0
0.00.059.234 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.234 I llm_load_print_meta: n_embd           = 2048
0.00.059.235 I llm_load_print_meta: n_layer          = 24
0.00.059.250 I llm_load_print_meta: n_head           = 16
0.00.059.251 I llm_load_print_meta: n_head_kv        = 16
0.00.059.251 I llm_load_print_meta: n_rot            = 32
0.00.059.251 I llm_load_print_meta: n_swa            = 0
0.00.059.252 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.252 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.253 I llm_load_print_meta: n_gqa            = 1
0.00.059.253 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.254 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.255 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.255 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.255 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.255 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.255 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.256 I llm_load_print_meta: n_ff             = 8192
0.00.059.256 I llm_load_print_meta: n_expert         = 0
0.00.059.256 I llm_load_print_meta: n_expert_used    = 0
0.00.059.257 I llm_load_print_meta: causal attn      = 1
0.00.059.257 I llm_load_print_meta: pooling type     = 0
0.00.059.257 I llm_load_print_meta: rope type        = 2
0.00.059.257 I llm_load_print_meta: rope scaling     = linear
0.00.059.257 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.260 I llm_load_print_meta: freq_scale_train = 1
0.00.059.260 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.260 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.260 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.260 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.260 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.261 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.261 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.270 I llm_load_print_meta: model type       = 1.4B
0.00.059.270 I llm_load_print_meta: model ftype      = Q4_0
0.00.059.271 I llm_load_print_meta: model params     = 1.41 B
0.00.059.271 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.059.271 I llm_load_print_meta: general.name     = 1.4B
0.00.059.273 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.273 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.273 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.273 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.274 I llm_load_print_meta: LF token         = 128 ''
0.00.059.274 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.274 I llm_load_print_meta: max token length = 1024
0.00.061.185 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.185 I llm_load_tensors: offloading output layer to GPU
0.00.061.185 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.196 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.061.197 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.062.120 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.120 I llama_new_context_with_model: n_ctx         = 128
0.00.062.121 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.121 I llama_new_context_with_model: n_batch       = 128
0.00.062.121 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.121 I llama_new_context_with_model: flash_attn    = 0
0.00.062.121 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.122 I llama_new_context_with_model: freq_scale    = 1
0.00.062.122 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.123 I ggml_metal_init: allocating
0.00.062.126 I ggml_metal_init: found device: Apple M4
0.00.062.128 I ggml_metal_init: picking default device: Apple M4
0.00.062.691 I ggml_metal_init: using embedded metal library
0.00.065.021 I ggml_metal_init: GPU name:   Apple M4
0.00.065.022 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.022 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.022 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.023 I ggml_metal_init: simdgroup reduction   = true
0.00.065.023 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.023 I ggml_metal_init: has bfloat            = true
0.00.065.023 I ggml_metal_init: use bfloat            = true
0.00.065.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.024 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.000 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.002 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.015 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.076.939 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.076.940 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.076.940 I llama_new_context_with_model: graph nodes  = 967
0.00.076.940 I llama_new_context_with_model: graph splits = 2
0.00.076.953 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.870 I 
0.00.650.929 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.943 I perplexity: tokenizing the input ..
0.00.658.789 I perplexity: tokenization took 7.845 ms
0.00.658.800 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.781.394 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.782.554 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.782.573 I llama_perf_context_print:        load time =     639.42 ms
0.00.782.574 I llama_perf_context_print: prompt eval time =     122.37 ms /   128 tokens (    0.96 ms per token,  1046.02 tokens per second)
0.00.782.578 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.579 I llama_perf_context_print:       total time =     131.71 ms /   129 tokens
0.00.783.043 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.087s
sys	0m0.118s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.012.458 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.079 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.083 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.085 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.085 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.087 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.088 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.088 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.089 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.089 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.089 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.090 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.090 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.090 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.091 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.094 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.094 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.094 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.962 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.862 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.863 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.864 I llama_model_loader: - type  f32:  194 tensors
0.00.027.864 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.864 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.834 I llm_load_vocab: special tokens cache size = 25
0.00.054.834 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.836 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.837 I llm_load_print_meta: arch             = gptneox
0.00.054.837 I llm_load_print_meta: vocab type       = BPE
0.00.054.837 I llm_load_print_meta: n_vocab          = 50304
0.00.054.837 I llm_load_print_meta: n_merges         = 50009
0.00.054.838 I llm_load_print_meta: vocab_only       = 0
0.00.054.838 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.838 I llm_load_print_meta: n_embd           = 2048
0.00.054.838 I llm_load_print_meta: n_layer          = 24
0.00.054.852 I llm_load_print_meta: n_head           = 16
0.00.054.855 I llm_load_print_meta: n_head_kv        = 16
0.00.054.856 I llm_load_print_meta: n_rot            = 32
0.00.054.856 I llm_load_print_meta: n_swa            = 0
0.00.054.856 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.856 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.857 I llm_load_print_meta: n_gqa            = 1
0.00.054.858 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.859 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.860 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.860 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.860 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.861 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.861 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.861 I llm_load_print_meta: n_ff             = 8192
0.00.054.862 I llm_load_print_meta: n_expert         = 0
0.00.054.862 I llm_load_print_meta: n_expert_used    = 0
0.00.054.863 I llm_load_print_meta: causal attn      = 1
0.00.054.865 I llm_load_print_meta: pooling type     = 0
0.00.054.865 I llm_load_print_meta: rope type        = 2
0.00.054.865 I llm_load_print_meta: rope scaling     = linear
0.00.054.865 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.865 I llm_load_print_meta: freq_scale_train = 1
0.00.054.866 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.866 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.866 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.866 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.866 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.866 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.866 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.876 I llm_load_print_meta: model type       = 1.4B
0.00.054.876 I llm_load_print_meta: model ftype      = Q4_1
0.00.054.876 I llm_load_print_meta: model params     = 1.41 B
0.00.054.877 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.054.877 I llm_load_print_meta: general.name     = 1.4B
0.00.054.877 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.878 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.878 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.878 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.878 I llm_load_print_meta: LF token         = 128 ''
0.00.054.878 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.879 I llm_load_print_meta: max token length = 1024
0.00.056.873 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.873 I llm_load_tensors: offloading output layer to GPU
0.00.056.873 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.884 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.056.885 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.057.808 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.809 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.809 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.809 I llama_new_context_with_model: n_batch       = 2048
0.00.057.810 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.810 I llama_new_context_with_model: flash_attn    = 0
0.00.057.810 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.811 I llama_new_context_with_model: freq_scale    = 1
0.00.057.811 I ggml_metal_init: allocating
0.00.057.818 I ggml_metal_init: found device: Apple M4
0.00.057.820 I ggml_metal_init: picking default device: Apple M4
0.00.058.412 I ggml_metal_init: using embedded metal library
0.00.060.708 I ggml_metal_init: GPU name:   Apple M4
0.00.060.710 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.710 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.710 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.711 I ggml_metal_init: simdgroup reduction   = true
0.00.060.711 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.711 I ggml_metal_init: has bfloat            = true
0.00.060.712 I ggml_metal_init: use bfloat            = true
0.00.060.713 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.713 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.734 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.740 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.758 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.800 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.802 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.802 I llama_new_context_with_model: graph nodes  = 967
0.00.089.803 I llama_new_context_with_model: graph splits = 2
0.00.089.817 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.759 I main: llama threadpool init, n_threads = 4
0.00.741.799 I 
0.00.741.829 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.831 I 
0.00.742.073 I sampler seed: 1234
0.00.742.077 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.103 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.104 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.104 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.469.363 I llama_perf_sampler_print:    sampling time =       1.04 ms /    71 runs   (    0.01 ms per token, 68334.94 tokens per second)
0.01.469.363 I llama_perf_context_print:        load time =     729.30 ms
0.01.469.364 I llama_perf_context_print: prompt eval time =      43.55 ms /     7 tokens (    6.22 ms per token,   160.74 tokens per second)
0.01.469.365 I llama_perf_context_print:        eval time =     680.96 ms /    63 runs   (   10.81 ms per token,    92.52 tokens per second)
0.01.469.365 I llama_perf_context_print:       total time =     727.61 ms /    70 tokens
0.01.469.564 I ggml_metal_free: deallocating

real	0m1.488s
user	0m0.110s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.736 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.735 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.736 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.737 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.737 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.738 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.741 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.741 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.739 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.776 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.771 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.771 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.771 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.772 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.772 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.772 I llama_model_loader: - type  f32:  194 tensors
0.00.023.773 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.773 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.659 I llm_load_vocab: special tokens cache size = 25
0.00.050.510 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.513 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.514 I llm_load_print_meta: arch             = gptneox
0.00.050.514 I llm_load_print_meta: vocab type       = BPE
0.00.050.514 I llm_load_print_meta: n_vocab          = 50304
0.00.050.514 I llm_load_print_meta: n_merges         = 50009
0.00.050.514 I llm_load_print_meta: vocab_only       = 0
0.00.050.515 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.515 I llm_load_print_meta: n_embd           = 2048
0.00.050.515 I llm_load_print_meta: n_layer          = 24
0.00.050.530 I llm_load_print_meta: n_head           = 16
0.00.050.531 I llm_load_print_meta: n_head_kv        = 16
0.00.050.531 I llm_load_print_meta: n_rot            = 32
0.00.050.531 I llm_load_print_meta: n_swa            = 0
0.00.050.532 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.532 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.533 I llm_load_print_meta: n_gqa            = 1
0.00.050.533 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.535 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.536 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.540 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.540 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.540 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.540 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.541 I llm_load_print_meta: n_ff             = 8192
0.00.050.541 I llm_load_print_meta: n_expert         = 0
0.00.050.541 I llm_load_print_meta: n_expert_used    = 0
0.00.050.541 I llm_load_print_meta: causal attn      = 1
0.00.050.541 I llm_load_print_meta: pooling type     = 0
0.00.050.541 I llm_load_print_meta: rope type        = 2
0.00.050.542 I llm_load_print_meta: rope scaling     = linear
0.00.050.542 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.542 I llm_load_print_meta: freq_scale_train = 1
0.00.050.543 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.543 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.543 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.543 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.543 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.543 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.543 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.553 I llm_load_print_meta: model type       = 1.4B
0.00.050.553 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.554 I llm_load_print_meta: model params     = 1.41 B
0.00.050.554 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.554 I llm_load_print_meta: general.name     = 1.4B
0.00.050.555 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.555 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.555 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.555 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.555 I llm_load_print_meta: LF token         = 128 ''
0.00.050.556 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.558 I llm_load_print_meta: max token length = 1024
0.00.052.492 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.492 I llm_load_tensors: offloading output layer to GPU
0.00.052.492 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.502 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.504 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.407 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.408 I llama_new_context_with_model: n_ctx         = 128
0.00.053.408 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.408 I llama_new_context_with_model: n_batch       = 128
0.00.053.409 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.409 I llama_new_context_with_model: flash_attn    = 0
0.00.053.409 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.410 I llama_new_context_with_model: freq_scale    = 1
0.00.053.410 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.410 I ggml_metal_init: allocating
0.00.053.413 I ggml_metal_init: found device: Apple M4
0.00.053.415 I ggml_metal_init: picking default device: Apple M4
0.00.053.973 I ggml_metal_init: using embedded metal library
0.00.056.294 I ggml_metal_init: GPU name:   Apple M4
0.00.056.295 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.296 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.296 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.296 I ggml_metal_init: simdgroup reduction   = true
0.00.056.297 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.297 I ggml_metal_init: has bfloat            = true
0.00.056.297 I ggml_metal_init: use bfloat            = true
0.00.056.297 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.298 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.294 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.297 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.311 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.251 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.252 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.253 I llama_new_context_with_model: graph nodes  = 967
0.00.068.253 I llama_new_context_with_model: graph splits = 2
0.00.068.265 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.560 I 
0.00.686.612 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.623 I perplexity: tokenizing the input ..
0.00.694.940 I perplexity: tokenization took 8.314 ms
0.00.694.953 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.685 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.818.842 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.818.856 I llama_perf_context_print:        load time =     677.82 ms
0.00.818.857 I llama_perf_context_print: prompt eval time =     122.51 ms /   128 tokens (    0.96 ms per token,  1044.85 tokens per second)
0.00.818.857 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.858 I llama_perf_context_print:       total time =     132.30 ms /   129 tokens
0.00.819.143 I ggml_metal_free: deallocating

real	0m0.833s
user	0m0.079s
sys	0m0.111s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.779 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.807 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.811 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.813 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.818 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.824 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.826 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.827 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.804 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.690 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.690 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.690 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.691 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.691 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.692 I llama_model_loader: - type  f32:  194 tensors
0.00.024.692 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.692 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.568 I llm_load_vocab: special tokens cache size = 25
0.00.051.526 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.529 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.529 I llm_load_print_meta: arch             = gptneox
0.00.051.530 I llm_load_print_meta: vocab type       = BPE
0.00.051.530 I llm_load_print_meta: n_vocab          = 50304
0.00.051.530 I llm_load_print_meta: n_merges         = 50009
0.00.051.530 I llm_load_print_meta: vocab_only       = 0
0.00.051.530 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.530 I llm_load_print_meta: n_embd           = 2048
0.00.051.531 I llm_load_print_meta: n_layer          = 24
0.00.051.546 I llm_load_print_meta: n_head           = 16
0.00.051.547 I llm_load_print_meta: n_head_kv        = 16
0.00.051.547 I llm_load_print_meta: n_rot            = 32
0.00.051.547 I llm_load_print_meta: n_swa            = 0
0.00.051.547 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.548 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.548 I llm_load_print_meta: n_gqa            = 1
0.00.051.549 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.551 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.552 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.552 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.552 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.552 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.553 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.553 I llm_load_print_meta: n_ff             = 8192
0.00.051.553 I llm_load_print_meta: n_expert         = 0
0.00.051.553 I llm_load_print_meta: n_expert_used    = 0
0.00.051.554 I llm_load_print_meta: causal attn      = 1
0.00.051.554 I llm_load_print_meta: pooling type     = 0
0.00.051.554 I llm_load_print_meta: rope type        = 2
0.00.051.554 I llm_load_print_meta: rope scaling     = linear
0.00.051.555 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.556 I llm_load_print_meta: freq_scale_train = 1
0.00.051.556 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.556 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.556 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.556 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.556 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.556 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.556 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.566 I llm_load_print_meta: model type       = 1.4B
0.00.051.566 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.567 I llm_load_print_meta: model params     = 1.41 B
0.00.051.567 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.567 I llm_load_print_meta: general.name     = 1.4B
0.00.051.567 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.568 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.568 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.568 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.568 I llm_load_print_meta: LF token         = 128 ''
0.00.051.569 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.569 I llm_load_print_meta: max token length = 1024
0.00.053.570 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.570 I llm_load_tensors: offloading output layer to GPU
0.00.053.571 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.581 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.582 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.491 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.492 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.492 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.492 I llama_new_context_with_model: n_batch       = 2048
0.00.054.492 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.492 I llama_new_context_with_model: flash_attn    = 0
0.00.054.493 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.493 I llama_new_context_with_model: freq_scale    = 1
0.00.054.494 I ggml_metal_init: allocating
0.00.054.499 I ggml_metal_init: found device: Apple M4
0.00.054.502 I ggml_metal_init: picking default device: Apple M4
0.00.055.089 I ggml_metal_init: using embedded metal library
0.00.057.478 I ggml_metal_init: GPU name:   Apple M4
0.00.057.479 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.480 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.480 I ggml_metal_init: simdgroup reduction   = true
0.00.057.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.481 I ggml_metal_init: has bfloat            = true
0.00.057.481 I ggml_metal_init: use bfloat            = true
0.00.057.481 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.482 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.080 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.086 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.107 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.144 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.145 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.146 I llama_new_context_with_model: graph nodes  = 967
0.00.088.146 I llama_new_context_with_model: graph splits = 2
0.00.088.160 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.042 I main: llama threadpool init, n_threads = 4
0.00.766.084 I 
0.00.766.116 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.119 I 
0.00.766.353 I sampler seed: 1234
0.00.766.357 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.369 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.371 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.371 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.557.921 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48431.11 tokens per second)
0.01.557.922 I llama_perf_context_print:        load time =     757.26 ms
0.01.557.922 I llama_perf_context_print: prompt eval time =      42.85 ms /     7 tokens (    6.12 ms per token,   163.37 tokens per second)
0.01.557.925 I llama_perf_context_print:        eval time =     746.16 ms /    63 runs   (   11.84 ms per token,    84.43 tokens per second)
0.01.557.927 I llama_perf_context_print:       total time =     791.88 ms /    70 tokens
0.01.558.132 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.109s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.392 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.105 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.109 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.110 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.111 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.111 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.111 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.111 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.112 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.112 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.112 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.113 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.113 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.113 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.114 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.117 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.117 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.118 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.887 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.902 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.706 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.708 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.708 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.708 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.709 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.709 I llama_model_loader: - type  f32:  194 tensors
0.00.023.709 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.710 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.671 I llm_load_vocab: special tokens cache size = 25
0.00.049.491 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.493 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.493 I llm_load_print_meta: arch             = gptneox
0.00.049.494 I llm_load_print_meta: vocab type       = BPE
0.00.049.494 I llm_load_print_meta: n_vocab          = 50304
0.00.049.494 I llm_load_print_meta: n_merges         = 50009
0.00.049.494 I llm_load_print_meta: vocab_only       = 0
0.00.049.494 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.495 I llm_load_print_meta: n_embd           = 2048
0.00.049.495 I llm_load_print_meta: n_layer          = 24
0.00.049.509 I llm_load_print_meta: n_head           = 16
0.00.049.510 I llm_load_print_meta: n_head_kv        = 16
0.00.049.510 I llm_load_print_meta: n_rot            = 32
0.00.049.511 I llm_load_print_meta: n_swa            = 0
0.00.049.511 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.511 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.512 I llm_load_print_meta: n_gqa            = 1
0.00.049.513 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.513 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.514 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.514 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.514 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.515 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.515 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.515 I llm_load_print_meta: n_ff             = 8192
0.00.049.516 I llm_load_print_meta: n_expert         = 0
0.00.049.516 I llm_load_print_meta: n_expert_used    = 0
0.00.049.516 I llm_load_print_meta: causal attn      = 1
0.00.049.516 I llm_load_print_meta: pooling type     = 0
0.00.049.516 I llm_load_print_meta: rope type        = 2
0.00.049.516 I llm_load_print_meta: rope scaling     = linear
0.00.049.517 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.517 I llm_load_print_meta: freq_scale_train = 1
0.00.049.517 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.517 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.517 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.518 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.518 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.518 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.518 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.527 I llm_load_print_meta: model type       = 1.4B
0.00.049.527 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.528 I llm_load_print_meta: model params     = 1.41 B
0.00.049.528 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.529 I llm_load_print_meta: general.name     = 1.4B
0.00.049.529 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.529 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.529 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.529 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.530 I llm_load_print_meta: LF token         = 128 ''
0.00.049.530 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.530 I llm_load_print_meta: max token length = 1024
0.00.051.476 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.476 I llm_load_tensors: offloading output layer to GPU
0.00.051.477 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.487 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.488 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.393 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.394 I llama_new_context_with_model: n_ctx         = 128
0.00.052.394 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.394 I llama_new_context_with_model: n_batch       = 128
0.00.052.394 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.395 I llama_new_context_with_model: flash_attn    = 0
0.00.052.395 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.395 I llama_new_context_with_model: freq_scale    = 1
0.00.052.396 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.396 I ggml_metal_init: allocating
0.00.052.403 I ggml_metal_init: found device: Apple M4
0.00.052.405 I ggml_metal_init: picking default device: Apple M4
0.00.052.986 I ggml_metal_init: using embedded metal library
0.00.055.323 I ggml_metal_init: GPU name:   Apple M4
0.00.055.324 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.325 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.325 I ggml_metal_init: simdgroup reduction   = true
0.00.055.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.326 I ggml_metal_init: has bfloat            = true
0.00.055.326 I ggml_metal_init: use bfloat            = true
0.00.055.326 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.058 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.061 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.074 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.990 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.991 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.992 I llama_new_context_with_model: graph nodes  = 967
0.00.066.992 I llama_new_context_with_model: graph splits = 2
0.00.067.004 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.361 I 
0.00.708.408 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.422 I perplexity: tokenizing the input ..
0.00.716.641 I perplexity: tokenization took 8.218 ms
0.00.716.656 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.894 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.853.155 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.853.166 I llama_perf_context_print:        load time =     698.96 ms
0.00.853.167 I llama_perf_context_print: prompt eval time =     134.99 ms /   128 tokens (    1.05 ms per token,   948.20 tokens per second)
0.00.853.168 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.853.168 I llama_perf_context_print:       total time =     144.81 ms /   129 tokens
0.00.853.484 I ggml_metal_free: deallocating

real	0m0.869s
user	0m0.077s
sys	0m0.125s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.826 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.391 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.402 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.403 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.403 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.405 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.405 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.406 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.407 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.412 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.413 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.413 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.359 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.370 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.371 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.371 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.372 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.372 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.372 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.373 I llama_model_loader: - type  f32:  194 tensors
0.00.025.373 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.374 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.344 I llm_load_vocab: special tokens cache size = 25
0.00.052.233 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.236 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.236 I llm_load_print_meta: arch             = gptneox
0.00.052.236 I llm_load_print_meta: vocab type       = BPE
0.00.052.237 I llm_load_print_meta: n_vocab          = 50304
0.00.052.237 I llm_load_print_meta: n_merges         = 50009
0.00.052.237 I llm_load_print_meta: vocab_only       = 0
0.00.052.237 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.237 I llm_load_print_meta: n_embd           = 2048
0.00.052.237 I llm_load_print_meta: n_layer          = 24
0.00.052.252 I llm_load_print_meta: n_head           = 16
0.00.052.253 I llm_load_print_meta: n_head_kv        = 16
0.00.052.254 I llm_load_print_meta: n_rot            = 32
0.00.052.254 I llm_load_print_meta: n_swa            = 0
0.00.052.254 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.254 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.255 I llm_load_print_meta: n_gqa            = 1
0.00.052.256 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.256 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.257 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.257 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.258 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.258 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.258 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.259 I llm_load_print_meta: n_ff             = 8192
0.00.052.259 I llm_load_print_meta: n_expert         = 0
0.00.052.259 I llm_load_print_meta: n_expert_used    = 0
0.00.052.259 I llm_load_print_meta: causal attn      = 1
0.00.052.259 I llm_load_print_meta: pooling type     = 0
0.00.052.260 I llm_load_print_meta: rope type        = 2
0.00.052.260 I llm_load_print_meta: rope scaling     = linear
0.00.052.260 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.260 I llm_load_print_meta: freq_scale_train = 1
0.00.052.260 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.261 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.261 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.261 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.261 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.261 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.261 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.271 I llm_load_print_meta: model type       = 1.4B
0.00.052.271 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.271 I llm_load_print_meta: model params     = 1.41 B
0.00.052.272 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.272 I llm_load_print_meta: general.name     = 1.4B
0.00.052.272 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.272 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.273 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.273 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.273 I llm_load_print_meta: LF token         = 128 ''
0.00.052.273 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.273 I llm_load_print_meta: max token length = 1024
0.00.054.299 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.300 I llm_load_tensors: offloading output layer to GPU
0.00.054.300 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.310 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.312 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.202 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.203 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.203 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.203 I llama_new_context_with_model: n_batch       = 2048
0.00.055.203 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.203 I llama_new_context_with_model: flash_attn    = 0
0.00.055.204 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.204 I llama_new_context_with_model: freq_scale    = 1
0.00.055.204 I ggml_metal_init: allocating
0.00.055.207 I ggml_metal_init: found device: Apple M4
0.00.055.210 I ggml_metal_init: picking default device: Apple M4
0.00.055.811 I ggml_metal_init: using embedded metal library
0.00.058.137 I ggml_metal_init: GPU name:   Apple M4
0.00.058.139 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.139 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.139 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.140 I ggml_metal_init: simdgroup reduction   = true
0.00.058.140 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.140 I ggml_metal_init: has bfloat            = true
0.00.058.140 I ggml_metal_init: use bfloat            = true
0.00.058.141 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.141 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.242 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.248 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.268 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.276 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.277 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.278 I llama_new_context_with_model: graph nodes  = 967
0.00.088.278 I llama_new_context_with_model: graph splits = 2
0.00.088.292 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.282 I main: llama threadpool init, n_threads = 4
0.00.713.322 I 
0.00.713.354 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.356 I 
0.00.713.584 I sampler seed: 1234
0.00.713.588 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.713.599 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.713.599 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.713.599 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.552.005 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.552.006 I llama_perf_context_print:        load time =     703.45 ms
0.01.552.006 I llama_perf_context_print: prompt eval time =      42.27 ms /     7 tokens (    6.04 ms per token,   165.59 tokens per second)
0.01.552.007 I llama_perf_context_print:        eval time =     793.07 ms /    63 runs   (   12.59 ms per token,    79.44 tokens per second)
0.01.552.007 I llama_perf_context_print:       total time =     838.73 ms /    70 tokens
0.01.552.227 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.766 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.686 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.691 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.697 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.698 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.698 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.699 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.700 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.700 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.702 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.702 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.704 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.625 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.659 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.558 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.560 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.560 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.560 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.561 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.561 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.561 I llama_model_loader: - type  f32:  194 tensors
0.00.023.562 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.562 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.351 I llm_load_vocab: special tokens cache size = 25
0.00.050.430 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.433 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.433 I llm_load_print_meta: arch             = gptneox
0.00.050.434 I llm_load_print_meta: vocab type       = BPE
0.00.050.434 I llm_load_print_meta: n_vocab          = 50304
0.00.050.434 I llm_load_print_meta: n_merges         = 50009
0.00.050.434 I llm_load_print_meta: vocab_only       = 0
0.00.050.434 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.435 I llm_load_print_meta: n_embd           = 2048
0.00.050.435 I llm_load_print_meta: n_layer          = 24
0.00.050.449 I llm_load_print_meta: n_head           = 16
0.00.050.452 I llm_load_print_meta: n_head_kv        = 16
0.00.050.452 I llm_load_print_meta: n_rot            = 32
0.00.050.453 I llm_load_print_meta: n_swa            = 0
0.00.050.453 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.453 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.453 I llm_load_print_meta: n_gqa            = 1
0.00.050.458 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.459 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.459 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.460 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.460 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.460 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.460 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.461 I llm_load_print_meta: n_ff             = 8192
0.00.050.461 I llm_load_print_meta: n_expert         = 0
0.00.050.461 I llm_load_print_meta: n_expert_used    = 0
0.00.050.461 I llm_load_print_meta: causal attn      = 1
0.00.050.461 I llm_load_print_meta: pooling type     = 0
0.00.050.461 I llm_load_print_meta: rope type        = 2
0.00.050.462 I llm_load_print_meta: rope scaling     = linear
0.00.050.462 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.463 I llm_load_print_meta: freq_scale_train = 1
0.00.050.463 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.463 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.464 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.464 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.464 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.464 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.465 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.474 I llm_load_print_meta: model type       = 1.4B
0.00.050.474 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.475 I llm_load_print_meta: model params     = 1.41 B
0.00.050.475 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.475 I llm_load_print_meta: general.name     = 1.4B
0.00.050.476 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.476 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.476 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.476 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.476 I llm_load_print_meta: LF token         = 128 ''
0.00.050.477 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.477 I llm_load_print_meta: max token length = 1024
0.00.052.421 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.422 I llm_load_tensors: offloading output layer to GPU
0.00.052.422 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.432 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.434 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.325 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.326 I llama_new_context_with_model: n_ctx         = 128
0.00.053.326 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.326 I llama_new_context_with_model: n_batch       = 128
0.00.053.326 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.326 I llama_new_context_with_model: flash_attn    = 0
0.00.053.327 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.327 I llama_new_context_with_model: freq_scale    = 1
0.00.053.327 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.328 I ggml_metal_init: allocating
0.00.053.331 I ggml_metal_init: found device: Apple M4
0.00.053.333 I ggml_metal_init: picking default device: Apple M4
0.00.053.903 I ggml_metal_init: using embedded metal library
0.00.056.205 I ggml_metal_init: GPU name:   Apple M4
0.00.056.206 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.207 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.207 I ggml_metal_init: simdgroup reduction   = true
0.00.056.207 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.207 I ggml_metal_init: has bfloat            = true
0.00.056.207 I ggml_metal_init: use bfloat            = true
0.00.056.208 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.057 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.060 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.072 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.012 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.013 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.014 I llama_new_context_with_model: graph nodes  = 967
0.00.068.014 I llama_new_context_with_model: graph splits = 2
0.00.068.026 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.369 I 
0.00.642.462 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.480 I perplexity: tokenizing the input ..
0.00.650.424 I perplexity: tokenization took 7.943 ms
0.00.650.435 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.491 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.785.970 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.785.986 I llama_perf_context_print:        load time =     633.59 ms
0.00.785.988 I llama_perf_context_print: prompt eval time =     133.81 ms /   128 tokens (    1.05 ms per token,   956.62 tokens per second)
0.00.785.988 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.989 I llama_perf_context_print:       total time =     143.62 ms /   129 tokens
0.00.786.332 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.078s
sys	0m0.112s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.752 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.174 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.176 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.177 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.179 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.179 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.182 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.182 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.994 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.994 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.995 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.995 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.996 I llama_model_loader: - type  f32:  194 tensors
0.00.023.996 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.996 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.997 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.800 I llm_load_vocab: special tokens cache size = 25
0.00.050.765 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.768 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.768 I llm_load_print_meta: arch             = gptneox
0.00.050.769 I llm_load_print_meta: vocab type       = BPE
0.00.050.769 I llm_load_print_meta: n_vocab          = 50304
0.00.050.769 I llm_load_print_meta: n_merges         = 50009
0.00.050.769 I llm_load_print_meta: vocab_only       = 0
0.00.050.770 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.770 I llm_load_print_meta: n_embd           = 2048
0.00.050.770 I llm_load_print_meta: n_layer          = 24
0.00.050.784 I llm_load_print_meta: n_head           = 16
0.00.050.786 I llm_load_print_meta: n_head_kv        = 16
0.00.050.786 I llm_load_print_meta: n_rot            = 32
0.00.050.786 I llm_load_print_meta: n_swa            = 0
0.00.050.786 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.786 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.787 I llm_load_print_meta: n_gqa            = 1
0.00.050.788 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.788 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.789 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.789 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.790 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.790 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.790 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.791 I llm_load_print_meta: n_ff             = 8192
0.00.050.791 I llm_load_print_meta: n_expert         = 0
0.00.050.791 I llm_load_print_meta: n_expert_used    = 0
0.00.050.791 I llm_load_print_meta: causal attn      = 1
0.00.050.791 I llm_load_print_meta: pooling type     = 0
0.00.050.791 I llm_load_print_meta: rope type        = 2
0.00.050.793 I llm_load_print_meta: rope scaling     = linear
0.00.050.793 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.793 I llm_load_print_meta: freq_scale_train = 1
0.00.050.793 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.793 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.794 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.794 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.794 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.794 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.794 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.804 I llm_load_print_meta: model type       = 1.4B
0.00.050.804 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.804 I llm_load_print_meta: model params     = 1.41 B
0.00.050.805 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.805 I llm_load_print_meta: general.name     = 1.4B
0.00.050.806 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.806 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.806 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.806 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.806 I llm_load_print_meta: LF token         = 128 ''
0.00.050.807 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.807 I llm_load_print_meta: max token length = 1024
0.00.052.673 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.673 I llm_load_tensors: offloading output layer to GPU
0.00.052.674 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.684 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.685 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.601 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.602 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.602 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.602 I llama_new_context_with_model: n_batch       = 2048
0.00.053.603 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.603 I llama_new_context_with_model: flash_attn    = 0
0.00.053.603 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.604 I llama_new_context_with_model: freq_scale    = 1
0.00.053.604 I ggml_metal_init: allocating
0.00.053.610 I ggml_metal_init: found device: Apple M4
0.00.053.612 I ggml_metal_init: picking default device: Apple M4
0.00.054.175 I ggml_metal_init: using embedded metal library
0.00.056.489 I ggml_metal_init: GPU name:   Apple M4
0.00.056.490 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.491 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.491 I ggml_metal_init: simdgroup reduction   = true
0.00.056.491 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.491 I ggml_metal_init: has bfloat            = true
0.00.056.492 I ggml_metal_init: use bfloat            = true
0.00.056.492 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.493 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.274 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.280 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.297 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.304 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.306 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.306 I llama_new_context_with_model: graph nodes  = 967
0.00.087.306 I llama_new_context_with_model: graph splits = 2
0.00.087.320 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.444.142 I main: llama threadpool init, n_threads = 4
0.00.444.186 I 
0.00.444.215 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.444.216 I 
0.00.444.465 I sampler seed: 1234
0.00.444.470 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.444.481 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.444.482 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.444.482 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.123.516 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59865.09 tokens per second)
0.01.123.517 I llama_perf_context_print:        load time =     434.38 ms
0.01.123.517 I llama_perf_context_print: prompt eval time =      35.79 ms /     7 tokens (    5.11 ms per token,   195.60 tokens per second)
0.01.123.518 I llama_perf_context_print:        eval time =     640.21 ms /    63 runs   (   10.16 ms per token,    98.40 tokens per second)
0.01.123.519 I llama_perf_context_print:       total time =     679.38 ms /    70 tokens
0.01.123.707 I ggml_metal_free: deallocating

real	0m1.142s
user	0m0.110s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.005 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.801 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.806 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.808 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.809 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.810 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.810 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.811 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.812 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.814 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.815 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.815 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.802 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.843 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.901 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.902 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.902 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.903 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.903 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.903 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.904 I llama_model_loader: - type  f32:  194 tensors
0.00.025.904 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.904 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.905 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.147 I llm_load_vocab: special tokens cache size = 25
0.00.053.233 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.237 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.237 I llm_load_print_meta: arch             = gptneox
0.00.053.238 I llm_load_print_meta: vocab type       = BPE
0.00.053.238 I llm_load_print_meta: n_vocab          = 50304
0.00.053.238 I llm_load_print_meta: n_merges         = 50009
0.00.053.238 I llm_load_print_meta: vocab_only       = 0
0.00.053.238 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.239 I llm_load_print_meta: n_embd           = 2048
0.00.053.244 I llm_load_print_meta: n_layer          = 24
0.00.053.256 I llm_load_print_meta: n_head           = 16
0.00.053.256 I llm_load_print_meta: n_head_kv        = 16
0.00.053.256 I llm_load_print_meta: n_rot            = 32
0.00.053.256 I llm_load_print_meta: n_swa            = 0
0.00.053.257 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.257 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.257 I llm_load_print_meta: n_gqa            = 1
0.00.053.258 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.259 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.259 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.259 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.260 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.260 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.260 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.261 I llm_load_print_meta: n_ff             = 8192
0.00.053.261 I llm_load_print_meta: n_expert         = 0
0.00.053.261 I llm_load_print_meta: n_expert_used    = 0
0.00.053.261 I llm_load_print_meta: causal attn      = 1
0.00.053.261 I llm_load_print_meta: pooling type     = 0
0.00.053.261 I llm_load_print_meta: rope type        = 2
0.00.053.262 I llm_load_print_meta: rope scaling     = linear
0.00.053.262 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.262 I llm_load_print_meta: freq_scale_train = 1
0.00.053.262 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.263 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.263 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.263 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.263 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.264 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.264 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.269 I llm_load_print_meta: model type       = 1.4B
0.00.053.269 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.053.269 I llm_load_print_meta: model params     = 1.41 B
0.00.053.270 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.270 I llm_load_print_meta: general.name     = 1.4B
0.00.053.270 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.270 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.270 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.270 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.271 I llm_load_print_meta: LF token         = 128 ''
0.00.053.271 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.271 I llm_load_print_meta: max token length = 1024
0.00.054.924 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.925 I llm_load_tensors: offloading output layer to GPU
0.00.054.925 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.931 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.932 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.907 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.908 I llama_new_context_with_model: n_ctx         = 128
0.00.055.908 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.908 I llama_new_context_with_model: n_batch       = 128
0.00.055.908 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.908 I llama_new_context_with_model: flash_attn    = 0
0.00.055.909 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.909 I llama_new_context_with_model: freq_scale    = 1
0.00.055.909 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.910 I ggml_metal_init: allocating
0.00.055.920 I ggml_metal_init: found device: Apple M4
0.00.055.923 I ggml_metal_init: picking default device: Apple M4
0.00.056.537 I ggml_metal_init: using embedded metal library
0.00.058.934 I ggml_metal_init: GPU name:   Apple M4
0.00.058.936 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.937 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.937 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.937 I ggml_metal_init: simdgroup reduction   = true
0.00.058.938 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.938 I ggml_metal_init: has bfloat            = true
0.00.058.938 I ggml_metal_init: use bfloat            = true
0.00.058.938 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.375 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.382 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.401 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.216 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.217 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.217 I llama_new_context_with_model: graph nodes  = 967
0.00.070.217 I llama_new_context_with_model: graph splits = 2
0.00.070.225 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.382.328 I 
0.00.382.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.382.373 I perplexity: tokenizing the input ..
0.00.390.153 I perplexity: tokenization took 7.778 ms
0.00.390.165 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.522.982 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.524.228 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.524.245 I llama_perf_context_print:        load time =     371.32 ms
0.00.524.246 I llama_perf_context_print: prompt eval time =     132.59 ms /   128 tokens (    1.04 ms per token,   965.37 tokens per second)
0.00.524.247 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.524.247 I llama_perf_context_print:       total time =     141.92 ms /   129 tokens
0.00.524.653 I ggml_metal_free: deallocating

real	0m0.540s
user	0m0.078s
sys	0m0.069s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.010.906 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.202 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.215 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.215 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.216 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.217 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.217 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.218 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.218 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.218 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.219 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.220 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.221 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.221 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.237 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.274 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.270 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.272 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.272 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.272 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.272 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.273 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.273 I llama_model_loader: - type  f32:  194 tensors
0.00.026.274 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.274 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.274 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.274 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.533 I llm_load_vocab: special tokens cache size = 25
0.00.052.450 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.453 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.453 I llm_load_print_meta: arch             = gptneox
0.00.052.453 I llm_load_print_meta: vocab type       = BPE
0.00.052.454 I llm_load_print_meta: n_vocab          = 50304
0.00.052.454 I llm_load_print_meta: n_merges         = 50009
0.00.052.454 I llm_load_print_meta: vocab_only       = 0
0.00.052.454 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.454 I llm_load_print_meta: n_embd           = 2048
0.00.052.454 I llm_load_print_meta: n_layer          = 24
0.00.052.469 I llm_load_print_meta: n_head           = 16
0.00.052.469 I llm_load_print_meta: n_head_kv        = 16
0.00.052.469 I llm_load_print_meta: n_rot            = 32
0.00.052.470 I llm_load_print_meta: n_swa            = 0
0.00.052.470 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.470 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.471 I llm_load_print_meta: n_gqa            = 1
0.00.052.472 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.472 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.473 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.473 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.473 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.474 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.474 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.476 I llm_load_print_meta: n_ff             = 8192
0.00.052.477 I llm_load_print_meta: n_expert         = 0
0.00.052.477 I llm_load_print_meta: n_expert_used    = 0
0.00.052.477 I llm_load_print_meta: causal attn      = 1
0.00.052.477 I llm_load_print_meta: pooling type     = 0
0.00.052.477 I llm_load_print_meta: rope type        = 2
0.00.052.477 I llm_load_print_meta: rope scaling     = linear
0.00.052.479 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.479 I llm_load_print_meta: freq_scale_train = 1
0.00.052.479 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.479 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.479 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.479 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.480 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.480 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.480 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.490 I llm_load_print_meta: model type       = 1.4B
0.00.052.490 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.490 I llm_load_print_meta: model params     = 1.41 B
0.00.052.491 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.491 I llm_load_print_meta: general.name     = 1.4B
0.00.052.491 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.491 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.492 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.492 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.492 I llm_load_print_meta: LF token         = 128 ''
0.00.052.492 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.492 I llm_load_print_meta: max token length = 1024
0.00.054.444 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.444 I llm_load_tensors: offloading output layer to GPU
0.00.054.444 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.455 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.457 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.392 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.393 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.394 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.394 I llama_new_context_with_model: n_batch       = 2048
0.00.055.394 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.394 I llama_new_context_with_model: flash_attn    = 0
0.00.055.394 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.395 I llama_new_context_with_model: freq_scale    = 1
0.00.055.395 I ggml_metal_init: allocating
0.00.055.398 I ggml_metal_init: found device: Apple M4
0.00.055.400 I ggml_metal_init: picking default device: Apple M4
0.00.056.000 I ggml_metal_init: using embedded metal library
0.00.058.283 I ggml_metal_init: GPU name:   Apple M4
0.00.058.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.285 I ggml_metal_init: simdgroup reduction   = true
0.00.058.285 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.285 I ggml_metal_init: has bfloat            = true
0.00.058.285 I ggml_metal_init: use bfloat            = true
0.00.058.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.286 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.700 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.704 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.722 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.678 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.679 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.680 I llama_new_context_with_model: graph nodes  = 967
0.00.087.680 I llama_new_context_with_model: graph splits = 2
0.00.087.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.544.967 I main: llama threadpool init, n_threads = 4
0.00.545.017 I 
0.00.545.063 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.545.064 I 
0.00.545.287 I sampler seed: 1234
0.00.545.291 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.545.311 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.545.311 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.545.311 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.294.964 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.01.294.965 I llama_perf_context_print:        load time =     534.06 ms
0.01.294.966 I llama_perf_context_print: prompt eval time =      44.37 ms /     7 tokens (    6.34 ms per token,   157.77 tokens per second)
0.01.294.967 I llama_perf_context_print:        eval time =     702.50 ms /    63 runs   (   11.15 ms per token,    89.68 tokens per second)
0.01.294.967 I llama_perf_context_print:       total time =     750.00 ms /    70 tokens
0.01.295.211 I ggml_metal_free: deallocating

real	0m1.313s
user	0m0.110s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.006 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.571 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.583 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.585 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.589 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.589 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.349 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.161 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.162 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.162 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.163 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.163 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.163 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.164 I llama_model_loader: - type  f32:  194 tensors
0.00.027.164 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.164 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.164 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.164 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.116 I llm_load_vocab: special tokens cache size = 25
0.00.052.955 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.958 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.959 I llm_load_print_meta: arch             = gptneox
0.00.052.959 I llm_load_print_meta: vocab type       = BPE
0.00.052.959 I llm_load_print_meta: n_vocab          = 50304
0.00.052.959 I llm_load_print_meta: n_merges         = 50009
0.00.052.959 I llm_load_print_meta: vocab_only       = 0
0.00.052.960 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.960 I llm_load_print_meta: n_embd           = 2048
0.00.052.960 I llm_load_print_meta: n_layer          = 24
0.00.052.975 I llm_load_print_meta: n_head           = 16
0.00.052.976 I llm_load_print_meta: n_head_kv        = 16
0.00.052.976 I llm_load_print_meta: n_rot            = 32
0.00.052.976 I llm_load_print_meta: n_swa            = 0
0.00.052.977 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.977 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.977 I llm_load_print_meta: n_gqa            = 1
0.00.052.978 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.979 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.979 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.980 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.980 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.980 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.980 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.981 I llm_load_print_meta: n_ff             = 8192
0.00.052.981 I llm_load_print_meta: n_expert         = 0
0.00.052.981 I llm_load_print_meta: n_expert_used    = 0
0.00.052.981 I llm_load_print_meta: causal attn      = 1
0.00.052.981 I llm_load_print_meta: pooling type     = 0
0.00.052.984 I llm_load_print_meta: rope type        = 2
0.00.052.985 I llm_load_print_meta: rope scaling     = linear
0.00.052.985 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.985 I llm_load_print_meta: freq_scale_train = 1
0.00.052.985 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.986 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.986 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.986 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.986 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.986 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.986 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.996 I llm_load_print_meta: model type       = 1.4B
0.00.052.996 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.996 I llm_load_print_meta: model params     = 1.41 B
0.00.052.997 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.997 I llm_load_print_meta: general.name     = 1.4B
0.00.052.997 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.997 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.997 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.998 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.998 I llm_load_print_meta: LF token         = 128 ''
0.00.052.998 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.998 I llm_load_print_meta: max token length = 1024
0.00.054.897 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.897 I llm_load_tensors: offloading output layer to GPU
0.00.054.898 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.908 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.910 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.846 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.846 I llama_new_context_with_model: n_ctx         = 128
0.00.055.847 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.847 I llama_new_context_with_model: n_batch       = 128
0.00.055.847 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.847 I llama_new_context_with_model: flash_attn    = 0
0.00.055.848 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.848 I llama_new_context_with_model: freq_scale    = 1
0.00.055.848 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.849 I ggml_metal_init: allocating
0.00.055.852 I ggml_metal_init: found device: Apple M4
0.00.055.854 I ggml_metal_init: picking default device: Apple M4
0.00.056.424 I ggml_metal_init: using embedded metal library
0.00.058.717 I ggml_metal_init: GPU name:   Apple M4
0.00.058.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.719 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.721 I ggml_metal_init: simdgroup reduction   = true
0.00.058.721 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.721 I ggml_metal_init: has bfloat            = true
0.00.058.721 I ggml_metal_init: use bfloat            = true
0.00.058.722 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.386 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.388 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.402 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.319 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.320 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.320 I llama_new_context_with_model: graph nodes  = 967
0.00.070.321 I llama_new_context_with_model: graph splits = 2
0.00.070.333 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.574.160 I 
0.00.574.230 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.574.249 I perplexity: tokenizing the input ..
0.00.582.485 I perplexity: tokenization took 8.234 ms
0.00.582.495 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.714.765 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.715.954 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.715.978 I llama_perf_context_print:        load time =     565.15 ms
0.00.715.979 I llama_perf_context_print: prompt eval time =     132.04 ms /   128 tokens (    1.03 ms per token,   969.38 tokens per second)
0.00.715.980 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.715.981 I llama_perf_context_print:       total time =     141.82 ms /   129 tokens
0.00.716.498 I ggml_metal_free: deallocating

real	0m0.731s
user	0m0.078s
sys	0m0.094s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.179 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.563 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.442 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.273 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.274 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.274 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.274 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.275 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.275 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.276 I llama_model_loader: - type  f32:  194 tensors
0.00.024.276 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.276 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.276 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.568 I llm_load_vocab: special tokens cache size = 25
0.00.050.397 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.400 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.401 I llm_load_print_meta: arch             = gptneox
0.00.050.401 I llm_load_print_meta: vocab type       = BPE
0.00.050.401 I llm_load_print_meta: n_vocab          = 50304
0.00.050.401 I llm_load_print_meta: n_merges         = 50009
0.00.050.402 I llm_load_print_meta: vocab_only       = 0
0.00.050.402 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.402 I llm_load_print_meta: n_embd           = 2048
0.00.050.402 I llm_load_print_meta: n_layer          = 24
0.00.050.417 I llm_load_print_meta: n_head           = 16
0.00.050.418 I llm_load_print_meta: n_head_kv        = 16
0.00.050.419 I llm_load_print_meta: n_rot            = 32
0.00.050.419 I llm_load_print_meta: n_swa            = 0
0.00.050.419 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.419 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.420 I llm_load_print_meta: n_gqa            = 1
0.00.050.421 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.423 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.425 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.425 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.425 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.425 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.425 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.426 I llm_load_print_meta: n_ff             = 8192
0.00.050.426 I llm_load_print_meta: n_expert         = 0
0.00.050.427 I llm_load_print_meta: n_expert_used    = 0
0.00.050.427 I llm_load_print_meta: causal attn      = 1
0.00.050.427 I llm_load_print_meta: pooling type     = 0
0.00.050.427 I llm_load_print_meta: rope type        = 2
0.00.050.427 I llm_load_print_meta: rope scaling     = linear
0.00.050.428 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.428 I llm_load_print_meta: freq_scale_train = 1
0.00.050.428 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.428 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.429 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.429 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.429 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.429 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.429 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.438 I llm_load_print_meta: model type       = 1.4B
0.00.050.439 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.439 I llm_load_print_meta: model params     = 1.41 B
0.00.050.440 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.440 I llm_load_print_meta: general.name     = 1.4B
0.00.050.440 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.440 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.440 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.440 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.442 I llm_load_print_meta: LF token         = 128 ''
0.00.050.442 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.443 I llm_load_print_meta: max token length = 1024
0.00.052.445 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.445 I llm_load_tensors: offloading output layer to GPU
0.00.052.445 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.456 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.457 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.355 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.356 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.356 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.356 I llama_new_context_with_model: n_batch       = 2048
0.00.053.356 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.357 I llama_new_context_with_model: flash_attn    = 0
0.00.053.357 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.357 I llama_new_context_with_model: freq_scale    = 1
0.00.053.358 I ggml_metal_init: allocating
0.00.053.362 I ggml_metal_init: found device: Apple M4
0.00.053.365 I ggml_metal_init: picking default device: Apple M4
0.00.053.974 I ggml_metal_init: using embedded metal library
0.00.056.292 I ggml_metal_init: GPU name:   Apple M4
0.00.056.293 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.293 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.294 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.294 I ggml_metal_init: simdgroup reduction   = true
0.00.056.294 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.294 I ggml_metal_init: has bfloat            = true
0.00.056.295 I ggml_metal_init: use bfloat            = true
0.00.056.295 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.295 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.426 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.438 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.456 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.557 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.559 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.559 I llama_new_context_with_model: graph nodes  = 967
0.00.086.560 I llama_new_context_with_model: graph splits = 2
0.00.086.573 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.940 I main: llama threadpool init, n_threads = 4
0.00.627.980 I 
0.00.628.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.010 I 
0.00.628.248 I sampler seed: 1234
0.00.628.253 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.628.294 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.628.296 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.628.296 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.392.388 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.392.389 I llama_perf_context_print:        load time =     618.76 ms
0.01.392.390 I llama_perf_context_print: prompt eval time =      51.06 ms /     7 tokens (    7.29 ms per token,   137.08 tokens per second)
0.01.392.391 I llama_perf_context_print:        eval time =     709.88 ms /    63 runs   (   11.27 ms per token,    88.75 tokens per second)
0.01.392.391 I llama_perf_context_print:       total time =     764.45 ms /    70 tokens
0.01.392.574 I ggml_metal_free: deallocating

real	0m1.411s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.616 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.182 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.026.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.190 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.190 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.190 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.191 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.192 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.192 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.193 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.193 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.193 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.194 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.195 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.196 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.196 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.552 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.706 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.052 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.053 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.054 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.054 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.055 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.055 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.036.055 I llama_model_loader: - type  f32:  194 tensors
0.00.036.056 I llama_model_loader: - type q4_K:   61 tensors
0.00.036.056 I llama_model_loader: - type q5_K:   24 tensors
0.00.036.056 I llama_model_loader: - type q6_K:   13 tensors
0.00.064.489 I llm_load_vocab: special tokens cache size = 25
0.00.073.699 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.703 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.703 I llm_load_print_meta: arch             = gptneox
0.00.073.704 I llm_load_print_meta: vocab type       = BPE
0.00.073.704 I llm_load_print_meta: n_vocab          = 50304
0.00.073.704 I llm_load_print_meta: n_merges         = 50009
0.00.073.704 I llm_load_print_meta: vocab_only       = 0
0.00.073.705 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.705 I llm_load_print_meta: n_embd           = 2048
0.00.073.705 I llm_load_print_meta: n_layer          = 24
0.00.073.720 I llm_load_print_meta: n_head           = 16
0.00.073.721 I llm_load_print_meta: n_head_kv        = 16
0.00.073.721 I llm_load_print_meta: n_rot            = 32
0.00.073.721 I llm_load_print_meta: n_swa            = 0
0.00.073.721 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.725 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.725 I llm_load_print_meta: n_gqa            = 1
0.00.073.726 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.727 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.728 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.728 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.728 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.729 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.730 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.731 I llm_load_print_meta: n_ff             = 8192
0.00.073.731 I llm_load_print_meta: n_expert         = 0
0.00.073.732 I llm_load_print_meta: n_expert_used    = 0
0.00.073.732 I llm_load_print_meta: causal attn      = 1
0.00.073.732 I llm_load_print_meta: pooling type     = 0
0.00.073.733 I llm_load_print_meta: rope type        = 2
0.00.073.733 I llm_load_print_meta: rope scaling     = linear
0.00.073.734 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.734 I llm_load_print_meta: freq_scale_train = 1
0.00.073.734 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.734 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.735 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.735 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.737 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.737 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.737 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.747 I llm_load_print_meta: model type       = 1.4B
0.00.073.748 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.073.748 I llm_load_print_meta: model params     = 1.41 B
0.00.073.749 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.073.749 I llm_load_print_meta: general.name     = 1.4B
0.00.073.751 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.751 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.751 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.752 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.752 I llm_load_print_meta: LF token         = 128 ''
0.00.073.752 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.753 I llm_load_print_meta: max token length = 1024
0.00.076.363 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.363 I llm_load_tensors: offloading output layer to GPU
0.00.076.364 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.375 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.076.377 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.077.702 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.703 I llama_new_context_with_model: n_ctx         = 128
0.00.077.703 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.077.704 I llama_new_context_with_model: n_batch       = 128
0.00.077.704 I llama_new_context_with_model: n_ubatch      = 128
0.00.077.704 I llama_new_context_with_model: flash_attn    = 0
0.00.077.705 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.705 I llama_new_context_with_model: freq_scale    = 1
0.00.077.706 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.077.706 I ggml_metal_init: allocating
0.00.077.713 I ggml_metal_init: found device: Apple M4
0.00.077.720 I ggml_metal_init: picking default device: Apple M4
0.00.078.475 I ggml_metal_init: using embedded metal library
0.00.081.859 I ggml_metal_init: GPU name:   Apple M4
0.00.081.861 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.862 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.862 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.863 I ggml_metal_init: simdgroup reduction   = true
0.00.081.863 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.863 I ggml_metal_init: has bfloat            = true
0.00.081.863 I ggml_metal_init: use bfloat            = true
0.00.081.864 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.864 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.181 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.094.185 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.094.199 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.203 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.095.205 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.095.205 I llama_new_context_with_model: graph nodes  = 967
0.00.095.205 I llama_new_context_with_model: graph splits = 2
0.00.095.212 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.932 I 
0.00.679.007 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.023 I perplexity: tokenizing the input ..
0.00.690.075 I perplexity: tokenization took 11.051 ms
0.00.690.091 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.551 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.825.792 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.825.809 I llama_perf_context_print:        load time =     662.31 ms
0.00.825.810 I llama_perf_context_print: prompt eval time =     134.24 ms /   128 tokens (    1.05 ms per token,   953.52 tokens per second)
0.00.825.811 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.811 I llama_perf_context_print:       total time =     146.88 ms /   129 tokens
0.00.826.270 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.098s
sys	0m0.116s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.730 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.236 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.243 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.243 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.248 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.250 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.251 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.252 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.252 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.256 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.256 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.259 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.259 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.260 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.099 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.100 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.101 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.101 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.101 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.102 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.102 I llama_model_loader: - type  f32:  194 tensors
0.00.024.103 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.103 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.054 I llm_load_vocab: special tokens cache size = 25
0.00.051.047 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.050 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.051 I llm_load_print_meta: arch             = gptneox
0.00.051.051 I llm_load_print_meta: vocab type       = BPE
0.00.051.051 I llm_load_print_meta: n_vocab          = 50304
0.00.051.051 I llm_load_print_meta: n_merges         = 50009
0.00.051.052 I llm_load_print_meta: vocab_only       = 0
0.00.051.052 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.052 I llm_load_print_meta: n_embd           = 2048
0.00.051.052 I llm_load_print_meta: n_layer          = 24
0.00.051.067 I llm_load_print_meta: n_head           = 16
0.00.051.068 I llm_load_print_meta: n_head_kv        = 16
0.00.051.069 I llm_load_print_meta: n_rot            = 32
0.00.051.069 I llm_load_print_meta: n_swa            = 0
0.00.051.069 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.069 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.070 I llm_load_print_meta: n_gqa            = 1
0.00.051.071 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.071 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.072 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.072 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.072 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.072 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.073 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.073 I llm_load_print_meta: n_ff             = 8192
0.00.051.073 I llm_load_print_meta: n_expert         = 0
0.00.051.074 I llm_load_print_meta: n_expert_used    = 0
0.00.051.075 I llm_load_print_meta: causal attn      = 1
0.00.051.077 I llm_load_print_meta: pooling type     = 0
0.00.051.077 I llm_load_print_meta: rope type        = 2
0.00.051.077 I llm_load_print_meta: rope scaling     = linear
0.00.051.077 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.078 I llm_load_print_meta: freq_scale_train = 1
0.00.051.079 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.079 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.079 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.079 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.079 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.079 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.080 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.089 I llm_load_print_meta: model type       = 1.4B
0.00.051.090 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.090 I llm_load_print_meta: model params     = 1.41 B
0.00.051.091 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.091 I llm_load_print_meta: general.name     = 1.4B
0.00.051.091 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.091 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.091 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.092 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.092 I llm_load_print_meta: LF token         = 128 ''
0.00.051.092 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.092 I llm_load_print_meta: max token length = 1024
0.00.053.122 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.122 I llm_load_tensors: offloading output layer to GPU
0.00.053.122 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.133 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.134 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.116 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.117 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.117 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.117 I llama_new_context_with_model: n_batch       = 2048
0.00.054.117 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.117 I llama_new_context_with_model: flash_attn    = 0
0.00.054.118 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.118 I llama_new_context_with_model: freq_scale    = 1
0.00.054.119 I ggml_metal_init: allocating
0.00.054.124 I ggml_metal_init: found device: Apple M4
0.00.054.126 I ggml_metal_init: picking default device: Apple M4
0.00.054.723 I ggml_metal_init: using embedded metal library
0.00.057.084 I ggml_metal_init: GPU name:   Apple M4
0.00.057.085 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.087 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.088 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.088 I ggml_metal_init: simdgroup reduction   = true
0.00.057.088 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.088 I ggml_metal_init: has bfloat            = true
0.00.057.088 I ggml_metal_init: use bfloat            = true
0.00.057.089 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.089 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.115 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.121 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.140 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.150 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.151 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.151 I llama_new_context_with_model: graph nodes  = 967
0.00.086.152 I llama_new_context_with_model: graph splits = 2
0.00.086.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.141 I main: llama threadpool init, n_threads = 4
0.00.696.180 I 
0.00.696.222 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.223 I 
0.00.696.459 I sampler seed: 1234
0.00.696.464 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.696.496 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.696.497 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.696.497 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.547.386 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60477.00 tokens per second)
0.01.547.386 I llama_perf_context_print:        load time =     687.41 ms
0.01.547.387 I llama_perf_context_print: prompt eval time =      51.61 ms /     7 tokens (    7.37 ms per token,   135.63 tokens per second)
0.01.547.389 I llama_perf_context_print:        eval time =     796.34 ms /    63 runs   (   12.64 ms per token,    79.11 tokens per second)
0.01.547.389 I llama_perf_context_print:       total time =     851.25 ms /    70 tokens
0.01.547.594 I ggml_metal_free: deallocating

real	0m1.564s
user	0m0.109s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.233 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.125 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.020.130 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.134 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.134 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.135 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.136 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.136 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.138 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.138 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.051 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.087 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.109 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.110 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.029.111 I llama_model_loader: - type  f32:  194 tensors
0.00.029.111 I llama_model_loader: - type q5_K:   61 tensors
0.00.029.112 I llama_model_loader: - type q6_K:   37 tensors
0.00.050.474 I llm_load_vocab: special tokens cache size = 25
0.00.056.457 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.459 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.459 I llm_load_print_meta: arch             = gptneox
0.00.056.460 I llm_load_print_meta: vocab type       = BPE
0.00.056.460 I llm_load_print_meta: n_vocab          = 50304
0.00.056.460 I llm_load_print_meta: n_merges         = 50009
0.00.056.460 I llm_load_print_meta: vocab_only       = 0
0.00.056.461 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.461 I llm_load_print_meta: n_embd           = 2048
0.00.056.461 I llm_load_print_meta: n_layer          = 24
0.00.056.475 I llm_load_print_meta: n_head           = 16
0.00.056.475 I llm_load_print_meta: n_head_kv        = 16
0.00.056.476 I llm_load_print_meta: n_rot            = 32
0.00.056.476 I llm_load_print_meta: n_swa            = 0
0.00.056.478 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.478 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.479 I llm_load_print_meta: n_gqa            = 1
0.00.056.480 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.480 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.481 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.481 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.481 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.482 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.482 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.482 I llm_load_print_meta: n_ff             = 8192
0.00.056.484 I llm_load_print_meta: n_expert         = 0
0.00.056.484 I llm_load_print_meta: n_expert_used    = 0
0.00.056.484 I llm_load_print_meta: causal attn      = 1
0.00.056.484 I llm_load_print_meta: pooling type     = 0
0.00.056.484 I llm_load_print_meta: rope type        = 2
0.00.056.485 I llm_load_print_meta: rope scaling     = linear
0.00.056.485 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.485 I llm_load_print_meta: freq_scale_train = 1
0.00.056.486 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.486 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.486 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.486 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.486 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.486 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.486 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.495 I llm_load_print_meta: model type       = 1.4B
0.00.056.496 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.056.496 I llm_load_print_meta: model params     = 1.41 B
0.00.056.496 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.056.496 I llm_load_print_meta: general.name     = 1.4B
0.00.056.497 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.498 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.498 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.498 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.498 I llm_load_print_meta: LF token         = 128 ''
0.00.056.499 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.499 I llm_load_print_meta: max token length = 1024
0.00.058.082 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.082 I llm_load_tensors: offloading output layer to GPU
0.00.058.082 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.092 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.058.093 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.058.977 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.978 I llama_new_context_with_model: n_ctx         = 128
0.00.058.979 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.058.979 I llama_new_context_with_model: n_batch       = 128
0.00.058.979 I llama_new_context_with_model: n_ubatch      = 128
0.00.058.979 I llama_new_context_with_model: flash_attn    = 0
0.00.058.979 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.980 I llama_new_context_with_model: freq_scale    = 1
0.00.058.980 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.058.980 I ggml_metal_init: allocating
0.00.058.984 I ggml_metal_init: found device: Apple M4
0.00.058.986 I ggml_metal_init: picking default device: Apple M4
0.00.059.541 I ggml_metal_init: using embedded metal library
0.00.061.883 I ggml_metal_init: GPU name:   Apple M4
0.00.061.884 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.885 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.885 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.885 I ggml_metal_init: simdgroup reduction   = true
0.00.061.885 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.886 I ggml_metal_init: has bfloat            = true
0.00.061.886 I ggml_metal_init: use bfloat            = true
0.00.061.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.887 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.861 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.864 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.886 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.073.766 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.073.767 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.073.767 I llama_new_context_with_model: graph nodes  = 967
0.00.073.767 I llama_new_context_with_model: graph splits = 2
0.00.073.780 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.872.160 I 
0.00.872.428 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.872.459 I perplexity: tokenizing the input ..
0.00.889.113 I perplexity: tokenization took 16.65 ms
0.00.889.138 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.034.075 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.01.037.003 I Final estimate: PPL = 10.2433 +/- 3.24778

0.01.037.039 I llama_perf_context_print:        load time =     859.91 ms
0.01.037.040 I llama_perf_context_print: prompt eval time =     144.54 ms /   128 tokens (    1.13 ms per token,   885.59 tokens per second)
0.01.037.042 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.037.043 I llama_perf_context_print:       total time =     164.89 ms /   129 tokens
0.01.038.441 I ggml_metal_free: deallocating

real	0m1.071s
user	0m0.109s
sys	0m0.142s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.724 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.015 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.020 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.027 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.027 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.028 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.028 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.028 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.029 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.029 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.030 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.030 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.030 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.031 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.031 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.032 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.033 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.033 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.085 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.208 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.208 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.209 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.209 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.210 I llama_model_loader: - type  f32:  194 tensors
0.00.026.210 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.405 I llm_load_vocab: special tokens cache size = 25
0.00.053.360 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.363 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.363 I llm_load_print_meta: arch             = gptneox
0.00.053.363 I llm_load_print_meta: vocab type       = BPE
0.00.053.364 I llm_load_print_meta: n_vocab          = 50304
0.00.053.364 I llm_load_print_meta: n_merges         = 50009
0.00.053.364 I llm_load_print_meta: vocab_only       = 0
0.00.053.364 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.364 I llm_load_print_meta: n_embd           = 2048
0.00.053.365 I llm_load_print_meta: n_layer          = 24
0.00.053.379 I llm_load_print_meta: n_head           = 16
0.00.053.380 I llm_load_print_meta: n_head_kv        = 16
0.00.053.380 I llm_load_print_meta: n_rot            = 32
0.00.053.380 I llm_load_print_meta: n_swa            = 0
0.00.053.380 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.383 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.384 I llm_load_print_meta: n_gqa            = 1
0.00.053.385 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.385 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.386 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.386 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.387 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.387 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.387 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.388 I llm_load_print_meta: n_ff             = 8192
0.00.053.388 I llm_load_print_meta: n_expert         = 0
0.00.053.388 I llm_load_print_meta: n_expert_used    = 0
0.00.053.388 I llm_load_print_meta: causal attn      = 1
0.00.053.388 I llm_load_print_meta: pooling type     = 0
0.00.053.389 I llm_load_print_meta: rope type        = 2
0.00.053.389 I llm_load_print_meta: rope scaling     = linear
0.00.053.389 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.390 I llm_load_print_meta: freq_scale_train = 1
0.00.053.390 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.390 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.390 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.390 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.390 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.390 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.391 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.400 I llm_load_print_meta: model type       = 1.4B
0.00.053.401 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.401 I llm_load_print_meta: model params     = 1.41 B
0.00.053.401 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.402 I llm_load_print_meta: general.name     = 1.4B
0.00.053.402 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.402 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.402 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.402 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.403 I llm_load_print_meta: LF token         = 128 ''
0.00.053.404 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.404 I llm_load_print_meta: max token length = 1024
0.00.055.481 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.481 I llm_load_tensors: offloading output layer to GPU
0.00.055.481 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.492 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.055.493 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.056.426 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.427 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.427 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.427 I llama_new_context_with_model: n_batch       = 2048
0.00.056.427 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.428 I llama_new_context_with_model: flash_attn    = 0
0.00.056.428 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.428 I llama_new_context_with_model: freq_scale    = 1
0.00.056.429 I ggml_metal_init: allocating
0.00.056.435 I ggml_metal_init: found device: Apple M4
0.00.056.437 I ggml_metal_init: picking default device: Apple M4
0.00.056.999 I ggml_metal_init: using embedded metal library
0.00.059.302 I ggml_metal_init: GPU name:   Apple M4
0.00.059.303 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.303 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.304 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.304 I ggml_metal_init: simdgroup reduction   = true
0.00.059.304 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.304 I ggml_metal_init: has bfloat            = true
0.00.059.304 I ggml_metal_init: use bfloat            = true
0.00.059.305 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.305 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.498 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.503 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.521 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.591 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.592 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.593 I llama_new_context_with_model: graph nodes  = 967
0.00.089.593 I llama_new_context_with_model: graph splits = 2
0.00.089.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.809 I main: llama threadpool init, n_threads = 4
0.00.754.844 I 
0.00.754.888 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.889 I 
0.00.755.107 I sampler seed: 1234
0.00.755.111 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.755.160 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.755.177 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.755.177 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.646.501 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 53992.40 tokens per second)
0.01.646.502 I llama_perf_context_print:        load time =     745.08 ms
0.01.646.503 I llama_perf_context_print: prompt eval time =      58.20 ms /     7 tokens (    8.31 ms per token,   120.28 tokens per second)
0.01.646.504 I llama_perf_context_print:        eval time =     830.49 ms /    63 runs   (   13.18 ms per token,    75.86 tokens per second)
0.01.646.504 I llama_perf_context_print:       total time =     891.69 ms /    70 tokens
0.01.646.715 I ggml_metal_free: deallocating

real	0m1.665s
user	0m0.110s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.217 I build: 4283 (ada8855f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.490 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.094 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.032.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.104 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.104 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.105 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.106 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.108 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.108 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.109 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.109 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.110 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.110 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.114 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.117 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.117 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.118 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.840 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.488 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.393 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.395 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.396 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.396 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.397 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.397 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.046.398 I llama_model_loader: - type  f32:  194 tensors
0.00.046.398 I llama_model_loader: - type q6_K:   98 tensors
0.00.072.217 I llm_load_vocab: special tokens cache size = 25
0.00.078.312 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.314 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.315 I llm_load_print_meta: arch             = gptneox
0.00.078.315 I llm_load_print_meta: vocab type       = BPE
0.00.078.315 I llm_load_print_meta: n_vocab          = 50304
0.00.078.315 I llm_load_print_meta: n_merges         = 50009
0.00.078.316 I llm_load_print_meta: vocab_only       = 0
0.00.078.316 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.316 I llm_load_print_meta: n_embd           = 2048
0.00.078.316 I llm_load_print_meta: n_layer          = 24
0.00.078.325 I llm_load_print_meta: n_head           = 16
0.00.078.328 I llm_load_print_meta: n_head_kv        = 16
0.00.078.328 I llm_load_print_meta: n_rot            = 32
0.00.078.328 I llm_load_print_meta: n_swa            = 0
0.00.078.329 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.329 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.329 I llm_load_print_meta: n_gqa            = 1
0.00.078.330 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.331 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.331 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.332 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.336 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.336 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.336 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.337 I llm_load_print_meta: n_ff             = 8192
0.00.078.337 I llm_load_print_meta: n_expert         = 0
0.00.078.337 I llm_load_print_meta: n_expert_used    = 0
0.00.078.338 I llm_load_print_meta: causal attn      = 1
0.00.078.338 I llm_load_print_meta: pooling type     = 0
0.00.078.338 I llm_load_print_meta: rope type        = 2
0.00.078.338 I llm_load_print_meta: rope scaling     = linear
0.00.078.338 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.339 I llm_load_print_meta: freq_scale_train = 1
0.00.078.339 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.339 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.339 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.340 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.340 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.340 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.341 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.346 I llm_load_print_meta: model type       = 1.4B
0.00.078.346 I llm_load_print_meta: model ftype      = Q6_K
0.00.078.346 I llm_load_print_meta: model params     = 1.41 B
0.00.078.347 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.078.347 I llm_load_print_meta: general.name     = 1.4B
0.00.078.347 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.347 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.347 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.347 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.348 I llm_load_print_meta: LF token         = 128 ''
0.00.078.348 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.348 I llm_load_print_meta: max token length = 1024
0.00.080.198 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.198 I llm_load_tensors: offloading output layer to GPU
0.00.080.198 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.204 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.080.205 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.081.784 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.785 I llama_new_context_with_model: n_ctx         = 128
0.00.081.785 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.081.785 I llama_new_context_with_model: n_batch       = 128
0.00.081.785 I llama_new_context_with_model: n_ubatch      = 128
0.00.081.786 I llama_new_context_with_model: flash_attn    = 0
0.00.081.786 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.786 I llama_new_context_with_model: freq_scale    = 1
0.00.081.787 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.081.792 I ggml_metal_init: allocating
0.00.081.816 I ggml_metal_init: found device: Apple M4
0.00.081.819 I ggml_metal_init: picking default device: Apple M4
0.00.082.459 I ggml_metal_init: using embedded metal library
0.00.085.023 I ggml_metal_init: GPU name:   Apple M4
0.00.085.026 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.027 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.027 I ggml_metal_init: simdgroup reduction   = true
0.00.085.027 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.027 I ggml_metal_init: has bfloat            = true
0.00.085.027 I ggml_metal_init: use bfloat            = true
0.00.085.028 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.816 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.094.822 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.094.836 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.711 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.095.712 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.095.712 I llama_new_context_with_model: graph nodes  = 967
0.00.095.713 I llama_new_context_with_model: graph splits = 2
0.00.095.720 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.210.272 I 
0.00.210.303 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.210.312 I perplexity: tokenizing the input ..
0.00.219.003 I perplexity: tokenization took 8.69 ms
0.00.219.014 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.359.295 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.360.482 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.360.501 I llama_perf_context_print:        load time =     189.78 ms
0.00.360.502 I llama_perf_context_print: prompt eval time =     140.04 ms /   128 tokens (    1.09 ms per token,   914.01 tokens per second)
0.00.360.505 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.360.505 I llama_perf_context_print:       total time =     150.23 ms /   129 tokens
0.00.360.894 I ggml_metal_free: deallocating

real	0m0.395s
user	0m0.101s
sys	0m0.050s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4283 (ada8855f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14570a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14570a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14570aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14570b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14570ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14570bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14570c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14570cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14570d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14570d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14570daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14570dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14570eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14570f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14570fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1457101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145711030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145711750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145711f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145712d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145713480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145713d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145714440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145714700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145714d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145715980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145715ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145716180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1457168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145717170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1457176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145717970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145717e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1457182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145718750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145718bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145719090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145719530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1457199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145719e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14571a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14571a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14571abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14571b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14571bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14571c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14571c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14571cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14571d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14571d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14571df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14571e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14571ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14571f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14571f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14571f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145720160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145720420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1457208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145720d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145721200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1457216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145721b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145721fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145722920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145722dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145723260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145723700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145723ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1457240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145724640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145724b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1457250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145725630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145725b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1457260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145726620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145726b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1457270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145727610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145727b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1457280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145728600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145728b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1457290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1457295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145729b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14572a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14572a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14572ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14572b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14572b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14572bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14571b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14572bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14572c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14572cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14572d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14572d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14572dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14572e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14572e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14572ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14572f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14572f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14572fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1457301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145730700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145730c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1457310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145731590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145731a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145731ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145732370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145732810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145732cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145733150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1457335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145733a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145733f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1457343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145734870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145734d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1457351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145735650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145735af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145735f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145736430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1457368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145736d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145737210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1457376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145737b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145737ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145738490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145738930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145738dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145739270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145739710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145739bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14573a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14573a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14573a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14573ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14573b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14573b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14573bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14573c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14573c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14573c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14573ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14573d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14573d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14573dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14573e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14573e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14573ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14573eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14573f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14573f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14573fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145740170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145740610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145740ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145740f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1457413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145741890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145741d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1457421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145742670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145742b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145742fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145743450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1457438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145743d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145744230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1457446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145744b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145745010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1457454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145745950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145745df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145746290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145746730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145746bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145747070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145747510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1457479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145747e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1457483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1457488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145748e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145749390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145749650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14574a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14574a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14574b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14574b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14574b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14574bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14574c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14574cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14574d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14574d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14574d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14574e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14574e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14574ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14574f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14574f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14574fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145750150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1457506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145751140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145751690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145751be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145752130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145752680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145752bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145753120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145753670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145753bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145754110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145754660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145754bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145755100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145755650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145755ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1457560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145756640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145756b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1457570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145757b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1457580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145758620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145758b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1457590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145759610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145759b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14575a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14575a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14575ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14575b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14575b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14575bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14575c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14575c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14575cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14575d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14575d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14575db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14575e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14575e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14575eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14575f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14575f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14575fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145760050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1457605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145760af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145760f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145761430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1457618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145761d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145762210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1457626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145762b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145762ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145763490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145763930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145763dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145764270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145764710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145764bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145765050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1457655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145765cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1457663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145766b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145767220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1457674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145767cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145767f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1457685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.143.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x135304b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135304f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135305400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135305870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x135305ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135306150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1353065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x135306a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135306ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135307310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135307780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135307e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135308990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135309140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x135309950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13530a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13530a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13530aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13530b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13530bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13530c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13530cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13530d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13530d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13530e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13530e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13530e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13530ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13530ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13530f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13530f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13530fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x135310180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x135310440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1353108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135310d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135311190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135311600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135311a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135311ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135312350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1353127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135312c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1353130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135313510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x135313980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135313df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135314260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1353146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135314b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135314fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135315420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x135315890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135315d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135316170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1353165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135316b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135317050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1353174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135317930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x135317da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135318210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x135318680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135318af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x135318f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1353193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135319840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x135319cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13531a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13531a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13531aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13531ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13531b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13531b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13531bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13531c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13531c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13531c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13531cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13531d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13531d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13531dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13531df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13531e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13531e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13531ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13531f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13531f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13531f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13531fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1353202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135320730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x135320ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135321010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x135321480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1353218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x135321d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1353221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135322640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x135322ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135322f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135323390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135323800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135323c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1353240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135324550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1353249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135324e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1353252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x135325710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135325b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135325ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135326460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1353268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135326d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1353271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x135327620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135327a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135327f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135328370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1353287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135328c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1353290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x135329530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1353299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x135329e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13532a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13532a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13532ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13532afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13532b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13532b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13532bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13532c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13532c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13532ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13532cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13532d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13532d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13532dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13532e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13532e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13532e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13532edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13532f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13532f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13532fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13532ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x135330420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135330890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135330d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x135331170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1353315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135331a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x135331ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135332330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1353327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x135332c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135333080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1353334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135333960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135333dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135334240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1353346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x135334b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135334f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x135335400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135335870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135335ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135336150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1353365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135336a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x135336ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x135337310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135337780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x135337bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135338060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1353384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x135338940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x135338db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x135339220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x135339690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x135339b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x135339f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13533a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13533a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13533acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13533b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13533b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13533ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13533be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13533c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13533c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13533cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13533d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13533d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13533d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13533dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13533e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13533e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13533eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13533ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13533f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13533f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13533fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x135340110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135340580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x135340b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135340f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1353413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135341f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135342200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1353424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x135342930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x135342da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135343210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135343680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x135343af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135343f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1353443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135344840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135344cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135345120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135345590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135345a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135345e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1353462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135346750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135346bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135347030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1353474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x135347910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x135347d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1353481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135348660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135348ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135348f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1353493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x135349820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x135349c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13534a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13534a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13534a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13534ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13534b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13534b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13534bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13534c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13534c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13534c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13534cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13534d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13534d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13534dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13534df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13534e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13534e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13534ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13534f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13534f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13534f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13534fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1353502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135350710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135350b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135350ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135351460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1353518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135351d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1353521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135352620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135352a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135352f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135353370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1353537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135353c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1353540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135354530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1353549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135354e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x135355280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1353556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135355b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1353565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135356cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135357410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x135357b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135357df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135358260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135358860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x135358e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x135304ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135304f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1353053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135305830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x135305ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135306110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x135306580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1353069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135306e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1353072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135307740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135307d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135308610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135308d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x135309570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135309c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13530a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13530aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13530b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13530bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13530c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13530c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13530cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13530d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13530dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13530e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13530e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13530eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13530ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13530f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13530f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13530fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1353100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1353103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x135310810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135310c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1353110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135311560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1353119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135311e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1353122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135312720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135312b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135313000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135313470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1353138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135313d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1353141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135314630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135314aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135314f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135315380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1353157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135315c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1353160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135316540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1353169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135316e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135317290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135317700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x135317b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135317fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x135318450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1353188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x135318d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1353191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135319610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x135319a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x135319ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13531a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13531a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13531ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13531b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13531b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13531b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13531be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13531c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13531c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13531cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13531cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13531d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13531d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13531dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13531e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13531e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13531ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13531eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13531f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13531f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13531fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x135320090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135320500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x135320970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135320de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x135321250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1353216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x135321b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x135321fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135322410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x135322880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135322cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135323160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1353235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135323a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x135323eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135324320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x135324790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135324c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x135325070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1353254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135325950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135325dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135326230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1353266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135326b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135326f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1353273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135327860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135327cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135328140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1353285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135328a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x135328e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x135329300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x135329770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x135329be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13532a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13532a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13532a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13532ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13532b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13532b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13532baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13532bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13532c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13532c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13532ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13532d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13532d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13532da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13532de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13532e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13532e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13532ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13532f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13532f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13532f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13532fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1353301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135330660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135330ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x135330f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1353313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135331820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x135331c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135332100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x135332570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1353329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135332e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1353332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135333730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135333ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135334010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x135334480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1353348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135334d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1353351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135335640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135335ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135335f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x135336390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135336800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x135336c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1353370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135337550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1353379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135337e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1353382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x135338710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x135338b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x135338ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x135339460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1353398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x135339d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13533a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13533a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13533aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13533af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13533b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13533b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13533bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13533c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13533c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13533c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13533ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13533d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13533d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13533db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13533dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13533e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13533e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13533ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13533f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13533f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13533fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13533fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135340350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1353407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135340c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1353410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135341820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135341c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x135342100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x135342570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1353429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135342e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1353432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x135343730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135343ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135344010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135344480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1353448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135344d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1353451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135345640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135345ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135345f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135346390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135346800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135346c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1353470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x135347550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1353479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135347e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1353482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135348710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135348b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x135348ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x135349460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1353498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x135349d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13534a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13534a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13534aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13534af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13534b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13534b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13534bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13534c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13534c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13534c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13534ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13534d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13534d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13534db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13534dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13534e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13534e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13534ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13534f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13534f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13534fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13534fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135350350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1353507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135350c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1353510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135351510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135351980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135351df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135352260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1353526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135352b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135352fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135353420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135353890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135353d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135354170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1353545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135354a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x135354ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x135355330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1353557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135356000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1353566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135356de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1353574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135357940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135357db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135358220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x135358690 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.805s
user	0m0.289s
sys	0m0.320s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4283 (ada8855f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11d6104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11d610bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11d611180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11d611730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11d611ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11d612290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11d612840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11d612df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11d6133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11d6138a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11d613da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11d6142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11d614dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11d615570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11d615d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11d6164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11d616bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11d6172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11d617a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11d6181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11d6188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11d619010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11d619730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11d619fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11d61a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11d61a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11d61afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11d61bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11d61c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11d61c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11d61c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11d61cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11d61d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11d61d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11d61dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11d61e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11d61e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11d61ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11d61eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11d61f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11d61f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11d61fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11d620120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11d6205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11d620880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11d620e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11d6214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11d621dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11d6223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11d6229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11d622ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11d623600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11d623c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11d624220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11d624a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11d624eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11d625350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11d625610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11d625c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11d626410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11d6266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11d626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11d627010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11d6274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11d627950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11d627df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d628290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d628730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d628bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d629070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d629510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d6299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d629e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d62a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d62a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d62ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d62b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d62b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d62be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d62c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d62c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d62ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d62d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d62d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d62de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d62e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d62e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d62ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d62f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d62f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d62fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d630340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d630890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d630de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d631330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d631880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d631dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d621ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d632240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d6329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d632f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d633490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d6339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d634480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d6349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d634f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d635470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d6359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d635f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d636460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d6369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d636f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d6373a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d637840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d637ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d638180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d638620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d638ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d638f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d639400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d6398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d639d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d63a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d63a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d63ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d63afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d63b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d63b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d63bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d63c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d63c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d63cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d63d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d63d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d63d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d63de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d63e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d63e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d63ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d63f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d63f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d63f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d63fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d640300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d6407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d640c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d6410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d641580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d641a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d641ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d642360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d642800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d642ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d643140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d6435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d643a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d643f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d6443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d644860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d644d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d6451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d645640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d645ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d645f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d646420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d6468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d646d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d647200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d6476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d647b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d647fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d648480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d648920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d648dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d649260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d649700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d649ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d64a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d64a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d64a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d64ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d64b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d64b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d64bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d64c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d64c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d64c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d64ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d64d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d64d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d64dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d64e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d64e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d64eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d64f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d64f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d64f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d64ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d650520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d651320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d6517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d651a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d652090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d6526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d652e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d653330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d6537d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d653c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d654420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d654970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d654ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d655410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d655960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d655eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d656400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d656950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d656ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d6573f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d657940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d657e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d6583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d658930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d658e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d6593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d659920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d659e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d65a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d65a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d65ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d65b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d65b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d65be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d65c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d65c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d65ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d65d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d65d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d65de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d65e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d65e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d65ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d65f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d65f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d65fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d660360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d6608b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d660e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d661350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d6618a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d661df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d662340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d662890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d662de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d663330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d663880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d663dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d664320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d664870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d664dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d665310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d665860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d665db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d666300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d666850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d666da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d667240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d6676e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d667b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d668020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d6684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d668960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d668e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d6692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d669740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d669be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d66a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d66a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d66a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d66ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d66b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d66b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d66bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d66c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d66cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d66d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d66d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d66df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d66e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d66e850 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.693 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x119f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x119f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x119f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x119f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x119f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x119f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x119f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x119f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x119f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x119f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x119f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x119f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x119f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x119f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x119f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x119f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x119f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x119f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x119f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x119f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x119f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x119f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x119f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x119f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x119f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x119f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x119f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x119f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x119f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x119f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x119f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x119f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x119f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x119f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x119f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x119f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x119f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x119f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x119f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x119f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x119f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x119f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x119f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x119f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x119f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x119f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x119f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x119f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x119f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x119f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x119f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x119f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x119f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x119f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x119f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x119f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x119f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x119f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x119f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x119f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x119f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x119f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x119f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x119f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x119f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x119f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x119f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x119f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x119f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x119f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x119f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x119f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x119f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x119f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x119f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x119f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x119f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x119f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x119f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x119f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x119f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x119f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x119f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x119f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x119f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x119f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x119f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x119f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x119f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x119f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x119f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x119f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x119f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x119f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x119f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x119f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x119f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x119f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x119f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x119f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x119f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x119f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x119f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x119f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x119f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x119f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x119f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x119f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x119f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x119f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x119f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x119f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x119f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x119f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x119f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x119f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x119f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x119f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x119f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x119f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x119f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x119f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x119f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x119f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x119f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x119f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x119f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x119f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x119f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x119f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x119f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x119f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x119f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x119f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x119f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x119f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x119f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x119f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x119f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x119f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x119f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x119f35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x119f35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x119f35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x119f36150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x119f365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x119f36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x119f36ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x119f37310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x119f37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x119f37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x119f38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x119f384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x119f38940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x119f38db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x119f39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x119f39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x119f39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x119f39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x119f3a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x119f3a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x119f3acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x119f3b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x119f3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x119f3ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x119f3be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x119f3c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x119f3c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x119f3cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x119f3d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x119f3d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x119f3d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x119f3dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x119f3e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x119f3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x119f3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x119f3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x119f3f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x119f3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x119f3fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x119f40110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x119f40580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x119f40b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119f40f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x119f413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x119f41f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x119f42200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x119f424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x119f42930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x119f42da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x119f43210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x119f43680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x119f43af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x119f43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x119f443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x119f44840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x119f44cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x119f45120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x119f45590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x119f45a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x119f45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x119f462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x119f46750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119f46bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x119f47030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x119f474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x119f47910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x119f47d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x119f481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x119f48660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x119f48ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x119f48f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119f493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x119f49820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x119f49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x119f4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x119f4a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x119f4a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x119f4ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x119f4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x119f4b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x119f4bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x119f4c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x119f4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x119f4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x119f4cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x119f4d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x119f4d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x119f4dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x119f4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x119f4e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x119f4e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x119f4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x119f4f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x119f4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x119f4f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x119f4fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x119f502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x119f50710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x119f50b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x119f50ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x119f51460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x119f518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x119f51d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x119f521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x119f52620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x119f52a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x119f52f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x119f53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x119f537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x119f53c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x119f540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x119f54530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x119f549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x119f54e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x119f55280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x119f556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x119f55b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119f565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x119f56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x119f57410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x119f57b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119f57df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x119f58260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119f58860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x119f58e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11d62af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11d62b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11d62b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11d62bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11d62c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11d62c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11d62ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11d62ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11d62d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11d62d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11d62dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11d62e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11d62ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11d62f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11d62f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11d6300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11d6307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11d630ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11d6315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11d631f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11d632620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11d632d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11d633400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11d633af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11d6341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11d634650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11d634ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11d634f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11d6353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11d635810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11d635c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11d6360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11d636560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11d636820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11d636c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11d637100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11d637570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11d6379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11d637e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11d6382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11d638730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11d638ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11d639010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11d639480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11d6398f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11d639d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11d63a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11d63a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11d63aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11d63af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11d63b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11d63b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11d63bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11d63c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11d63c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11d63c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11d63ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11d63d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11d63d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11d63db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11d63dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11d63e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11d63e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11d63ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11d63f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11d63f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d63fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d63ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d640370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d6407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d640c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d6410c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d641530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d6419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d641e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d642280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d6426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d642b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d642fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d643440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d6438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d643d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d644190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d644600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d644a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d644ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d645350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d6457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d645c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d6460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d646510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d646980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d646df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d647260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d6476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d647b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d647fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d648420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d648890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d648d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d649170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d6495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d649a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d649ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d64a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d64a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d64ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d64b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d64b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d64b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d64bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d64c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d64c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d64cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d64cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d64d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d64d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d64dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d64e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d64e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d64ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d64eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d64f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d64f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d64fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d650060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d6504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d650940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d650db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d651220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d651690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d651b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d651f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d6523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d652850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d652cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d653130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d6535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d653a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d653e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d6542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d654760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d654bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d655040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d6554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d655920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d655d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d656200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d656670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d656ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d656f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d6573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d657830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d657ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d658110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d658580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d6589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d658e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d6592d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d659740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d659bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d65a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d65a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d65a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d65ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d65b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d65b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d65bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d65bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d65c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d65c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d65cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d65d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d65d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d65d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d65de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d65e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d65e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d65eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d65f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d65f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d65f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d65fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d6601c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d660630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d660aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d660f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d661380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d6617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d661c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d6620d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d662540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d6629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d662e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d663290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d663700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d663b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d663fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d664450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d6648c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d664d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d6651a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d665610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d665a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d665ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d666360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d6667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d666c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d6670b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d667520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d667ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d668110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d668580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d6689f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d668e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d6692d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d669740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d669bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d66a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d66a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d66a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d66ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d66b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d66b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d66bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d66bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d66c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d66c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d66cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d66d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d66d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d66d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d66de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d66e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d66e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d611570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d610fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d60faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d610670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d61db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d61df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d61e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d61e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d61ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d61f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d61f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d61fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d61fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d6202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d620760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d620bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d621040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d6214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d621920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d621d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d622200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d622670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d622ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d622f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d6233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d623830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d623ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d624110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d624580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d6249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d624e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d6252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d625740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d625bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d626020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d626490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d626900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d626d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d6271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d627650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d627ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d627f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d6283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d628810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d628c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d6290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d629560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d6299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d62a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d62a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d61c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d61cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d61d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d613860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d613cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d614140 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.931s
user	0m0.244s
sys	0m0.144s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.56 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.14 user         0.04 sys
```
