### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.35 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.02 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.23 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.26 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.13 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.19 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.19 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.19 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  177.91 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.90 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.77 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.32 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.20 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 220.02 sec*proc (27 tests)

Total Test time (real) = 220.03 sec

real	3m40.073s
user	7m36.910s
sys	0m5.976s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.13 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.20 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.91 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.18 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.18 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.38 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.04 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.22 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.15 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.76 sec*proc (27 tests)

Total Test time (real) =  50.77 sec

real	0m50.775s
user	1m11.481s
sys	0m5.314s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.101 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.733 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.949 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.957 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.960 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.961 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.961 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.962 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.963 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.979 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.980 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.981 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.981 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.982 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.985 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.994 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.995 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.996 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.996 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.997 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.998 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.027.761 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.028.911 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.913 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.028.914 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.028.914 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.028.915 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.028.915 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.028.916 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.028.916 I llama_model_loader: - type  f32:  124 tensors
0.00.028.917 I llama_model_loader: - type  f16:   73 tensors
0.00.033.453 I llm_load_vocab: special tokens cache size = 5
0.00.035.748 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.035.752 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.035.752 I llm_load_print_meta: arch             = bert
0.00.035.753 I llm_load_print_meta: vocab type       = WPM
0.00.035.753 I llm_load_print_meta: n_vocab          = 30522
0.00.035.753 I llm_load_print_meta: n_merges         = 0
0.00.035.754 I llm_load_print_meta: vocab_only       = 0
0.00.035.754 I llm_load_print_meta: n_ctx_train      = 512
0.00.035.754 I llm_load_print_meta: n_embd           = 384
0.00.035.754 I llm_load_print_meta: n_layer          = 12
0.00.035.757 I llm_load_print_meta: n_head           = 12
0.00.035.758 I llm_load_print_meta: n_head_kv        = 12
0.00.035.759 I llm_load_print_meta: n_rot            = 32
0.00.035.759 I llm_load_print_meta: n_swa            = 0
0.00.035.759 I llm_load_print_meta: n_embd_head_k    = 32
0.00.035.759 I llm_load_print_meta: n_embd_head_v    = 32
0.00.035.760 I llm_load_print_meta: n_gqa            = 1
0.00.035.763 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.035.764 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.035.765 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.035.766 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.035.766 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.035.766 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.035.766 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.035.767 I llm_load_print_meta: n_ff             = 1536
0.00.035.768 I llm_load_print_meta: n_expert         = 0
0.00.035.768 I llm_load_print_meta: n_expert_used    = 0
0.00.035.768 I llm_load_print_meta: causal attn      = 0
0.00.035.770 I llm_load_print_meta: pooling type     = 2
0.00.035.771 I llm_load_print_meta: rope type        = 2
0.00.035.771 I llm_load_print_meta: rope scaling     = linear
0.00.035.772 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.035.772 I llm_load_print_meta: freq_scale_train = 1
0.00.035.772 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.035.773 I llm_load_print_meta: rope_finetuned   = unknown
0.00.035.773 I llm_load_print_meta: ssm_d_conv       = 0
0.00.035.773 I llm_load_print_meta: ssm_d_inner      = 0
0.00.035.773 I llm_load_print_meta: ssm_d_state      = 0
0.00.035.773 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.035.775 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.035.789 I llm_load_print_meta: model type       = 33M
0.00.035.789 I llm_load_print_meta: model ftype      = F16
0.00.035.790 I llm_load_print_meta: model params     = 33.21 M
0.00.035.791 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.035.791 I llm_load_print_meta: general.name     = Bge Small
0.00.035.792 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.035.792 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.035.792 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.035.792 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.035.793 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.035.793 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.035.794 I llm_load_print_meta: max token length = 21
0.00.037.814 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.037.815 I llm_load_tensors: offloading output layer to GPU
0.00.037.815 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.037.842 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.844 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.038.452 I llama_new_context_with_model: n_seq_max     = 1
0.00.038.454 I llama_new_context_with_model: n_ctx         = 512
0.00.038.454 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.038.455 I llama_new_context_with_model: n_batch       = 2048
0.00.038.455 I llama_new_context_with_model: n_ubatch      = 2048
0.00.038.455 I llama_new_context_with_model: flash_attn    = 0
0.00.038.456 I llama_new_context_with_model: freq_base     = 10000.0
0.00.038.456 I llama_new_context_with_model: freq_scale    = 1
0.00.038.457 I ggml_metal_init: allocating
0.00.038.469 I ggml_metal_init: found device: Apple M4
0.00.038.476 I ggml_metal_init: picking default device: Apple M4
0.00.039.368 I ggml_metal_init: using embedded metal library
0.00.043.502 I ggml_metal_init: GPU name:   Apple M4
0.00.043.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.043.506 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.043.506 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.043.507 I ggml_metal_init: simdgroup reduction   = true
0.00.043.507 I ggml_metal_init: simdgroup matrix mul. = true
0.00.043.507 I ggml_metal_init: has bfloat            = true
0.00.043.507 I ggml_metal_init: use bfloat            = true
0.00.043.508 I ggml_metal_init: hasUnifiedMemory      = true
0.00.043.509 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.056.766 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.056.768 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.056.769 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.057.559 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.057.561 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.057.561 I llama_new_context_with_model: graph nodes  = 429
0.00.057.561 I llama_new_context_with_model: graph splits = 2
0.00.057.583 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.064.190 I 
0.00.064.218 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.064.886 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.069.692 I llama_perf_context_print:        load time =      45.45 ms
0.00.069.694 I llama_perf_context_print: prompt eval time =       4.64 ms /     9 tokens (    0.52 ms per token,  1939.66 tokens per second)
0.00.069.695 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.069.696 I llama_perf_context_print:       total time =       5.50 ms /    10 tokens
0.00.069.828 I ggml_metal_free: deallocating

real	0m0.261s
user	0m0.050s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.034 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.324 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.469 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.472 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.473 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.475 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.476 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.476 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.483 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.483 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.484 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.484 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.484 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.486 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.487 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.487 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.487 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.488 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.489 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.489 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.026 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.742 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.743 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.743 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.744 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.744 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.744 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.745 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.745 I llama_model_loader: - type  f32:  124 tensors
0.00.014.745 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.300 I llm_load_vocab: special tokens cache size = 5
0.00.018.605 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.608 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.608 I llm_load_print_meta: arch             = bert
0.00.018.608 I llm_load_print_meta: vocab type       = WPM
0.00.018.608 I llm_load_print_meta: n_vocab          = 30522
0.00.018.609 I llm_load_print_meta: n_merges         = 0
0.00.018.609 I llm_load_print_meta: vocab_only       = 0
0.00.018.609 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.609 I llm_load_print_meta: n_embd           = 384
0.00.018.609 I llm_load_print_meta: n_layer          = 12
0.00.018.611 I llm_load_print_meta: n_head           = 12
0.00.018.612 I llm_load_print_meta: n_head_kv        = 12
0.00.018.612 I llm_load_print_meta: n_rot            = 32
0.00.018.612 I llm_load_print_meta: n_swa            = 0
0.00.018.613 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.613 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.613 I llm_load_print_meta: n_gqa            = 1
0.00.018.614 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.614 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.615 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.616 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.616 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.616 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.616 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.617 I llm_load_print_meta: n_ff             = 1536
0.00.018.617 I llm_load_print_meta: n_expert         = 0
0.00.018.617 I llm_load_print_meta: n_expert_used    = 0
0.00.018.617 I llm_load_print_meta: causal attn      = 0
0.00.018.617 I llm_load_print_meta: pooling type     = 2
0.00.018.617 I llm_load_print_meta: rope type        = 2
0.00.018.618 I llm_load_print_meta: rope scaling     = linear
0.00.018.618 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.618 I llm_load_print_meta: freq_scale_train = 1
0.00.018.618 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.618 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.619 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.619 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.620 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.620 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.620 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.626 I llm_load_print_meta: model type       = 33M
0.00.018.626 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.627 I llm_load_print_meta: model params     = 33.21 M
0.00.018.627 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.627 I llm_load_print_meta: general.name     = Bge Small
0.00.018.628 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.628 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.628 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.628 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.629 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.629 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.629 I llm_load_print_meta: max token length = 21
0.00.019.966 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.966 I llm_load_tensors: offloading output layer to GPU
0.00.019.967 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.975 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.976 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.352 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.353 I llama_new_context_with_model: n_ctx         = 512
0.00.020.353 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.353 I llama_new_context_with_model: n_batch       = 2048
0.00.020.353 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.353 I llama_new_context_with_model: flash_attn    = 0
0.00.020.354 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.354 I llama_new_context_with_model: freq_scale    = 1
0.00.020.354 I ggml_metal_init: allocating
0.00.020.361 I ggml_metal_init: found device: Apple M4
0.00.020.363 I ggml_metal_init: picking default device: Apple M4
0.00.020.932 I ggml_metal_init: using embedded metal library
0.00.023.453 I ggml_metal_init: GPU name:   Apple M4
0.00.023.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.455 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.455 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.456 I ggml_metal_init: simdgroup reduction   = true
0.00.023.456 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.456 I ggml_metal_init: has bfloat            = true
0.00.023.456 I ggml_metal_init: use bfloat            = true
0.00.023.456 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.457 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.187 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.189 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.190 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.793 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.795 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.795 I llama_new_context_with_model: graph nodes  = 429
0.00.034.795 I llama_new_context_with_model: graph splits = 2
0.00.034.808 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.275 I 
0.00.039.299 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.039.817 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.258 I llama_perf_context_print:        load time =      29.95 ms
0.00.044.259 I llama_perf_context_print: prompt eval time =       4.30 ms /     9 tokens (    0.48 ms per token,  2092.05 tokens per second)
0.00.044.259 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.260 I llama_perf_context_print:       total time =       4.98 ms /    10 tokens
0.00.044.405 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.199 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.702 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.591 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.031.595 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.598 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.031.599 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.599 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.031.600 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.031.601 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.031.627 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.031.629 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.031.630 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.031.630 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.031.631 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.031.634 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.031.635 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.031.643 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.031.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.039.248 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.041.411 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.750 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.045.752 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.752 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.045.753 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.045.753 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.045.753 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.045.754 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.045.754 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.045.754 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.045.755 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.045.755 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.045.755 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.045.756 I llama_model_loader: - type  f32:   41 tensors
0.00.045.756 I llama_model_loader: - type  f16:   29 tensors
0.00.063.523 W llm_load_vocab: empty token at index 5
0.00.067.940 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.069.188 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.069.233 I llm_load_vocab: special tokens cache size = 5
0.00.332.491 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.332.495 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.332.496 I llm_load_print_meta: arch             = jina-bert-v2
0.00.332.496 I llm_load_print_meta: vocab type       = BPE
0.00.332.497 I llm_load_print_meta: n_vocab          = 61056
0.00.332.497 I llm_load_print_meta: n_merges         = 39382
0.00.332.497 I llm_load_print_meta: vocab_only       = 0
0.00.332.497 I llm_load_print_meta: n_ctx_train      = 8192
0.00.332.497 I llm_load_print_meta: n_embd           = 384
0.00.332.498 I llm_load_print_meta: n_layer          = 4
0.00.332.502 I llm_load_print_meta: n_head           = 12
0.00.332.502 I llm_load_print_meta: n_head_kv        = 12
0.00.332.503 I llm_load_print_meta: n_rot            = 32
0.00.332.506 I llm_load_print_meta: n_swa            = 0
0.00.332.506 I llm_load_print_meta: n_embd_head_k    = 32
0.00.332.506 I llm_load_print_meta: n_embd_head_v    = 32
0.00.332.507 I llm_load_print_meta: n_gqa            = 1
0.00.332.508 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.332.512 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.332.513 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.332.513 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.332.514 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.332.514 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.332.515 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.332.515 I llm_load_print_meta: n_ff             = 1536
0.00.332.515 I llm_load_print_meta: n_expert         = 0
0.00.332.515 I llm_load_print_meta: n_expert_used    = 0
0.00.332.516 I llm_load_print_meta: causal attn      = 0
0.00.332.516 I llm_load_print_meta: pooling type     = -1
0.00.332.516 I llm_load_print_meta: rope type        = -1
0.00.332.516 I llm_load_print_meta: rope scaling     = linear
0.00.332.516 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.332.517 I llm_load_print_meta: freq_scale_train = 1
0.00.332.517 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.332.517 I llm_load_print_meta: rope_finetuned   = unknown
0.00.332.517 I llm_load_print_meta: ssm_d_conv       = 0
0.00.332.518 I llm_load_print_meta: ssm_d_inner      = 0
0.00.332.518 I llm_load_print_meta: ssm_d_state      = 0
0.00.332.518 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.332.518 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.332.540 I llm_load_print_meta: model type       = 33M
0.00.332.541 I llm_load_print_meta: model ftype      = F16
0.00.332.541 I llm_load_print_meta: model params     = 32.90 M
0.00.332.542 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.332.542 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.332.542 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.332.543 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.332.543 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.332.543 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.332.543 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.332.543 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.332.544 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.332.544 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.332.544 I llm_load_print_meta: max token length = 45
0.00.333.396 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.333.396 I llm_load_tensors: offloading output layer to GPU
0.00.333.396 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.333.420 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.333.421 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.334.135 I llama_new_context_with_model: n_seq_max     = 1
0.00.334.136 I llama_new_context_with_model: n_ctx         = 8192
0.00.334.136 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.334.137 I llama_new_context_with_model: n_batch       = 2048
0.00.334.137 I llama_new_context_with_model: n_ubatch      = 2048
0.00.334.137 I llama_new_context_with_model: flash_attn    = 0
0.00.334.137 I llama_new_context_with_model: freq_base     = 10000.0
0.00.334.138 I llama_new_context_with_model: freq_scale    = 1
0.00.334.138 I ggml_metal_init: allocating
0.00.334.146 I ggml_metal_init: found device: Apple M4
0.00.334.149 I ggml_metal_init: picking default device: Apple M4
0.00.334.893 I ggml_metal_init: using embedded metal library
0.00.337.605 I ggml_metal_init: GPU name:   Apple M4
0.00.337.607 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.608 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.608 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.608 I ggml_metal_init: simdgroup reduction   = true
0.00.337.608 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.608 I ggml_metal_init: has bfloat            = true
0.00.337.609 I ggml_metal_init: use bfloat            = true
0.00.337.609 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.610 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.569 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.349.571 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.349.573 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.350.128 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.129 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.130 I llama_new_context_with_model: graph nodes  = 154
0.00.350.130 I llama_new_context_with_model: graph splits = 2
0.00.350.147 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.280 I 
0.00.361.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.361.453 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.454 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.456 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.457 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.459 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.459 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.992 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.663 I llama_perf_context_print:        load time =     339.57 ms
0.00.365.664 I llama_perf_context_print: prompt eval time =       3.66 ms /    62 tokens (    0.06 ms per token, 16949.15 tokens per second)
0.00.365.665 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.665 I llama_perf_context_print:       total time =       4.38 ms /    63 tokens
0.00.365.903 I ggml_metal_free: deallocating

real	0m1.082s
user	0m0.340s
sys	0m0.043s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.107 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.238 I main: llama backend init
0.00.000.245 I main: load the model and apply lora adapter, if any
0.00.027.585 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.521 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.539 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.543 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.544 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.545 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.546 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.546 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.567 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.568 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.568 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.569 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.570 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.571 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.119 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.958 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.057.961 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.961 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.962 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.963 I llama_model_loader: - type  f32:  194 tensors
0.00.057.964 I llama_model_loader: - type  f16:   98 tensors
0.00.087.820 I llm_load_vocab: special tokens cache size = 25
0.00.094.417 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.419 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.420 I llm_load_print_meta: arch             = gptneox
0.00.094.420 I llm_load_print_meta: vocab type       = BPE
0.00.094.420 I llm_load_print_meta: n_vocab          = 50304
0.00.094.420 I llm_load_print_meta: n_merges         = 50009
0.00.094.421 I llm_load_print_meta: vocab_only       = 0
0.00.094.421 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.421 I llm_load_print_meta: n_embd           = 2048
0.00.094.421 I llm_load_print_meta: n_layer          = 24
0.00.094.424 I llm_load_print_meta: n_head           = 16
0.00.094.425 I llm_load_print_meta: n_head_kv        = 16
0.00.094.425 I llm_load_print_meta: n_rot            = 32
0.00.094.425 I llm_load_print_meta: n_swa            = 0
0.00.094.425 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.426 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.426 I llm_load_print_meta: n_gqa            = 1
0.00.094.428 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.429 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.429 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.430 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.430 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.430 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.430 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.431 I llm_load_print_meta: n_ff             = 8192
0.00.094.431 I llm_load_print_meta: n_expert         = 0
0.00.094.431 I llm_load_print_meta: n_expert_used    = 0
0.00.094.431 I llm_load_print_meta: causal attn      = 1
0.00.094.432 I llm_load_print_meta: pooling type     = 0
0.00.094.432 I llm_load_print_meta: rope type        = 2
0.00.094.432 I llm_load_print_meta: rope scaling     = linear
0.00.094.432 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.432 I llm_load_print_meta: freq_scale_train = 1
0.00.094.433 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.433 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.433 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.433 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.433 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.433 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.433 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.445 I llm_load_print_meta: model type       = 1.4B
0.00.094.446 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.094.446 I llm_load_print_meta: model params     = 1.41 B
0.00.094.448 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.094.449 I llm_load_print_meta: general.name     = 1.4B
0.00.094.449 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.449 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.449 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.449 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.449 I llm_load_print_meta: LF token         = 128 ''
0.00.094.450 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.450 I llm_load_print_meta: max token length = 1024
0.00.096.982 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.982 I llm_load_tensors: offloading output layer to GPU
0.00.096.983 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.001 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.002 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.097.955 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.956 I llama_new_context_with_model: n_ctx         = 2048
0.00.097.956 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.097.956 I llama_new_context_with_model: n_batch       = 2048
0.00.097.956 I llama_new_context_with_model: n_ubatch      = 512
0.00.097.956 I llama_new_context_with_model: flash_attn    = 0
0.00.097.957 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.957 I llama_new_context_with_model: freq_scale    = 1
0.00.097.958 I ggml_metal_init: allocating
0.00.097.967 I ggml_metal_init: found device: Apple M4
0.00.097.969 I ggml_metal_init: picking default device: Apple M4
0.00.098.602 I ggml_metal_init: using embedded metal library
0.00.107.016 I ggml_metal_init: GPU name:   Apple M4
0.00.107.017 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.107.018 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.107.018 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.107.019 I ggml_metal_init: simdgroup reduction   = true
0.00.107.019 I ggml_metal_init: simdgroup matrix mul. = true
0.00.107.019 I ggml_metal_init: has bfloat            = true
0.00.107.019 I ggml_metal_init: use bfloat            = true
0.00.107.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.107.020 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.148.772 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.148.778 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.148.797 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.149.705 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.149.706 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.149.706 I llama_new_context_with_model: graph nodes  = 967
0.00.149.707 I llama_new_context_with_model: graph splits = 2
0.00.149.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.220.295 I main: llama threadpool init, n_threads = 4
0.00.220.327 I 
0.00.220.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.220.366 I 
0.00.220.457 I sampler seed: 1234
0.00.220.462 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.220.512 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.220.514 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.220.515 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.074.807 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.02.074.807 I llama_perf_context_print:        load time =     192.70 ms
0.02.074.808 I llama_perf_context_print: prompt eval time =      43.85 ms /     7 tokens (    6.26 ms per token,   159.63 tokens per second)
0.02.074.809 I llama_perf_context_print:        eval time =    1807.43 ms /    63 runs   (   28.69 ms per token,    34.86 tokens per second)
0.02.074.812 I llama_perf_context_print:       total time =    1854.51 ms /    70 tokens
0.02.075.000 I ggml_metal_free: deallocating

real	0m2.360s
user	0m0.141s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.482 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.549 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.007 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.024 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.028 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.031 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.051 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.052 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.052 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.053 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.054 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.054 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.055 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.061 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.571 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.052.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.576 I llama_model_loader: - type  f32:  194 tensors
0.00.052.577 I llama_model_loader: - type  f16:   98 tensors
0.00.084.885 I llm_load_vocab: special tokens cache size = 25
0.00.092.093 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.096 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.096 I llm_load_print_meta: arch             = gptneox
0.00.092.096 I llm_load_print_meta: vocab type       = BPE
0.00.092.097 I llm_load_print_meta: n_vocab          = 50304
0.00.092.097 I llm_load_print_meta: n_merges         = 50009
0.00.092.097 I llm_load_print_meta: vocab_only       = 0
0.00.092.097 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.097 I llm_load_print_meta: n_embd           = 2048
0.00.092.097 I llm_load_print_meta: n_layer          = 24
0.00.092.101 I llm_load_print_meta: n_head           = 16
0.00.092.101 I llm_load_print_meta: n_head_kv        = 16
0.00.092.101 I llm_load_print_meta: n_rot            = 32
0.00.092.102 I llm_load_print_meta: n_swa            = 0
0.00.092.102 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.102 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.102 I llm_load_print_meta: n_gqa            = 1
0.00.092.103 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.104 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.104 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.104 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.105 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.107 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.107 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.108 I llm_load_print_meta: n_ff             = 8192
0.00.092.108 I llm_load_print_meta: n_expert         = 0
0.00.092.108 I llm_load_print_meta: n_expert_used    = 0
0.00.092.108 I llm_load_print_meta: causal attn      = 1
0.00.092.108 I llm_load_print_meta: pooling type     = 0
0.00.092.108 I llm_load_print_meta: rope type        = 2
0.00.092.109 I llm_load_print_meta: rope scaling     = linear
0.00.092.109 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.109 I llm_load_print_meta: freq_scale_train = 1
0.00.092.109 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.110 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.110 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.110 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.110 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.110 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.110 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.122 I llm_load_print_meta: model type       = 1.4B
0.00.092.123 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.123 I llm_load_print_meta: model params     = 1.41 B
0.00.092.124 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.125 I llm_load_print_meta: general.name     = 1.4B
0.00.092.125 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.125 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.125 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.126 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.126 I llm_load_print_meta: LF token         = 128 ''
0.00.092.126 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.127 I llm_load_print_meta: max token length = 1024
0.00.094.711 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.711 I llm_load_tensors: offloading output layer to GPU
0.00.094.711 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.722 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.723 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.717 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.718 I llama_new_context_with_model: n_ctx         = 128
0.00.095.718 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.718 I llama_new_context_with_model: n_batch       = 128
0.00.095.719 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.719 I llama_new_context_with_model: flash_attn    = 0
0.00.095.719 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.720 I llama_new_context_with_model: freq_scale    = 1
0.00.095.720 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.721 I ggml_metal_init: allocating
0.00.095.731 I ggml_metal_init: found device: Apple M4
0.00.095.734 I ggml_metal_init: picking default device: Apple M4
0.00.096.322 I ggml_metal_init: using embedded metal library
0.00.098.888 I ggml_metal_init: GPU name:   Apple M4
0.00.098.890 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.890 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.890 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.891 I ggml_metal_init: simdgroup reduction   = true
0.00.098.891 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.891 I ggml_metal_init: has bfloat            = true
0.00.098.891 I ggml_metal_init: use bfloat            = true
0.00.098.892 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.892 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.464 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.468 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.483 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.331 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.332 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.332 I llama_new_context_with_model: graph nodes  = 967
0.00.110.333 I llama_new_context_with_model: graph splits = 2
0.00.110.345 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.146.792 I 
0.01.146.873 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.146.925 I perplexity: tokenizing the input ..
0.01.160.225 I perplexity: tokenization took 13.297 ms
0.01.160.255 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.280.231 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.282.314 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.282.337 I llama_perf_context_print:        load time =    1124.22 ms
0.01.282.338 I llama_perf_context_print: prompt eval time =     119.58 ms /   128 tokens (    0.93 ms per token,  1070.38 tokens per second)
0.01.282.340 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.282.340 I llama_perf_context_print:       total time =     135.55 ms /   129 tokens
0.01.283.105 I ggml_metal_free: deallocating

real	0m1.471s
user	0m0.127s
sys	0m0.218s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.714 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.128 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.134 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.136 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.139 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.139 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.140 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.140 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.149 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.150 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.152 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.152 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.153 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.301 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.604 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.606 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.606 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.607 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.607 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.607 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.608 I llama_model_loader: - type  f32:  194 tensors
0.00.039.609 I llama_model_loader: - type q8_0:   98 tensors
0.00.064.675 I llm_load_vocab: special tokens cache size = 25
0.00.072.940 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.945 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.945 I llm_load_print_meta: arch             = gptneox
0.00.072.946 I llm_load_print_meta: vocab type       = BPE
0.00.072.946 I llm_load_print_meta: n_vocab          = 50304
0.00.072.946 I llm_load_print_meta: n_merges         = 50009
0.00.072.952 I llm_load_print_meta: vocab_only       = 0
0.00.072.954 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.954 I llm_load_print_meta: n_embd           = 2048
0.00.072.954 I llm_load_print_meta: n_layer          = 24
0.00.072.959 I llm_load_print_meta: n_head           = 16
0.00.072.960 I llm_load_print_meta: n_head_kv        = 16
0.00.072.961 I llm_load_print_meta: n_rot            = 32
0.00.072.961 I llm_load_print_meta: n_swa            = 0
0.00.072.961 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.961 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.962 I llm_load_print_meta: n_gqa            = 1
0.00.072.963 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.963 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.964 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.964 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.966 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.966 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.967 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.967 I llm_load_print_meta: n_ff             = 8192
0.00.072.968 I llm_load_print_meta: n_expert         = 0
0.00.072.970 I llm_load_print_meta: n_expert_used    = 0
0.00.072.970 I llm_load_print_meta: causal attn      = 1
0.00.072.970 I llm_load_print_meta: pooling type     = 0
0.00.072.970 I llm_load_print_meta: rope type        = 2
0.00.072.970 I llm_load_print_meta: rope scaling     = linear
0.00.072.971 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.971 I llm_load_print_meta: freq_scale_train = 1
0.00.072.971 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.971 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.972 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.973 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.973 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.973 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.973 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.986 I llm_load_print_meta: model type       = 1.4B
0.00.072.987 I llm_load_print_meta: model ftype      = Q8_0
0.00.072.988 I llm_load_print_meta: model params     = 1.41 B
0.00.072.989 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.072.989 I llm_load_print_meta: general.name     = 1.4B
0.00.072.989 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.990 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.992 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.992 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.992 I llm_load_print_meta: LF token         = 128 ''
0.00.072.993 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.993 I llm_load_print_meta: max token length = 1024
0.00.075.883 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.883 I llm_load_tensors: offloading output layer to GPU
0.00.075.884 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.895 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.075.897 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.077.096 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.097 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.098 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.098 I llama_new_context_with_model: n_batch       = 2048
0.00.077.098 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.098 I llama_new_context_with_model: flash_attn    = 0
0.00.077.099 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.099 I llama_new_context_with_model: freq_scale    = 1
0.00.077.100 I ggml_metal_init: allocating
0.00.077.104 I ggml_metal_init: found device: Apple M4
0.00.077.106 I ggml_metal_init: picking default device: Apple M4
0.00.077.858 I ggml_metal_init: using embedded metal library
0.00.081.053 I ggml_metal_init: GPU name:   Apple M4
0.00.081.055 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.055 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.056 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.056 I ggml_metal_init: simdgroup reduction   = true
0.00.081.056 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.056 I ggml_metal_init: has bfloat            = true
0.00.081.056 I ggml_metal_init: use bfloat            = true
0.00.081.057 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.057 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.119.633 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.651 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.683 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.758 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.120.759 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.120.760 I llama_new_context_with_model: graph nodes  = 967
0.00.120.760 I llama_new_context_with_model: graph splits = 2
0.00.120.775 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.350.170 I main: llama threadpool init, n_threads = 4
0.01.350.208 I 
0.01.350.236 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.350.237 I 
0.01.350.473 I sampler seed: 1234
0.01.350.478 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.350.509 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.350.511 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.350.511 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.440.959 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.02.440.959 I llama_perf_context_print:        load time =    1340.45 ms
0.02.440.964 I llama_perf_context_print: prompt eval time =      42.36 ms /     7 tokens (    6.05 ms per token,   165.25 tokens per second)
0.02.440.965 I llama_perf_context_print:        eval time =    1044.91 ms /    63 runs   (   16.59 ms per token,    60.29 tokens per second)
0.02.440.965 I llama_perf_context_print:       total time =    1090.79 ms /    70 tokens
0.02.441.148 I ggml_metal_free: deallocating

real	0m2.457s
user	0m0.123s
sys	0m0.237s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.116 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.378 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.875 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.880 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.883 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.883 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.884 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.884 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.884 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.892 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.893 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.894 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.895 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.898 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.900 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.900 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.900 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.462 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.245 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.247 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.248 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.248 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.248 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.249 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.250 I llama_model_loader: - type  f32:  194 tensors
0.00.032.250 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.924 I llm_load_vocab: special tokens cache size = 25
0.00.062.885 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.888 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.888 I llm_load_print_meta: arch             = gptneox
0.00.062.889 I llm_load_print_meta: vocab type       = BPE
0.00.062.889 I llm_load_print_meta: n_vocab          = 50304
0.00.062.889 I llm_load_print_meta: n_merges         = 50009
0.00.062.889 I llm_load_print_meta: vocab_only       = 0
0.00.062.889 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.890 I llm_load_print_meta: n_embd           = 2048
0.00.062.890 I llm_load_print_meta: n_layer          = 24
0.00.062.894 I llm_load_print_meta: n_head           = 16
0.00.062.895 I llm_load_print_meta: n_head_kv        = 16
0.00.062.895 I llm_load_print_meta: n_rot            = 32
0.00.062.895 I llm_load_print_meta: n_swa            = 0
0.00.062.895 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.896 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.896 I llm_load_print_meta: n_gqa            = 1
0.00.062.897 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.898 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.898 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.899 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.899 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.899 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.899 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.900 I llm_load_print_meta: n_ff             = 8192
0.00.062.900 I llm_load_print_meta: n_expert         = 0
0.00.062.903 I llm_load_print_meta: n_expert_used    = 0
0.00.062.903 I llm_load_print_meta: causal attn      = 1
0.00.062.903 I llm_load_print_meta: pooling type     = 0
0.00.062.903 I llm_load_print_meta: rope type        = 2
0.00.062.903 I llm_load_print_meta: rope scaling     = linear
0.00.062.904 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.904 I llm_load_print_meta: freq_scale_train = 1
0.00.062.904 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.904 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.905 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.905 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.905 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.905 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.906 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.918 I llm_load_print_meta: model type       = 1.4B
0.00.062.919 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.919 I llm_load_print_meta: model params     = 1.41 B
0.00.062.919 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.920 I llm_load_print_meta: general.name     = 1.4B
0.00.062.920 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.920 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.921 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.921 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.922 I llm_load_print_meta: LF token         = 128 ''
0.00.062.922 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.922 I llm_load_print_meta: max token length = 1024
0.00.065.202 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.202 I llm_load_tensors: offloading output layer to GPU
0.00.065.202 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.213 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.214 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.195 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.196 I llama_new_context_with_model: n_ctx         = 128
0.00.066.196 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.196 I llama_new_context_with_model: n_batch       = 128
0.00.066.196 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.197 I llama_new_context_with_model: flash_attn    = 0
0.00.066.197 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.197 I llama_new_context_with_model: freq_scale    = 1
0.00.066.198 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.198 I ggml_metal_init: allocating
0.00.066.208 I ggml_metal_init: found device: Apple M4
0.00.066.211 I ggml_metal_init: picking default device: Apple M4
0.00.066.831 I ggml_metal_init: using embedded metal library
0.00.069.426 I ggml_metal_init: GPU name:   Apple M4
0.00.069.428 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.429 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.429 I ggml_metal_init: simdgroup reduction   = true
0.00.069.429 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.429 I ggml_metal_init: has bfloat            = true
0.00.069.430 I ggml_metal_init: use bfloat            = true
0.00.069.430 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.431 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.510 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.512 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.529 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.516 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.517 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.517 I llama_new_context_with_model: graph nodes  = 967
0.00.081.518 I llama_new_context_with_model: graph splits = 2
0.00.081.530 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.949.629 I 
0.00.949.655 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.949.689 I perplexity: tokenizing the input ..
0.00.957.351 I perplexity: tokenization took 7.661 ms
0.00.957.361 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.081.771 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.083.120 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.083.139 I llama_perf_context_print:        load time =     938.25 ms
0.01.083.141 I llama_perf_context_print: prompt eval time =     124.17 ms /   128 tokens (    0.97 ms per token,  1030.80 tokens per second)
0.01.083.142 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.083.143 I llama_perf_context_print:       total time =     133.51 ms /   129 tokens
0.01.083.616 I ggml_metal_free: deallocating

real	0m1.100s
user	0m0.092s
sys	0m0.169s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.610 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.092 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.106 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.106 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.107 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.107 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.121 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.122 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.122 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.123 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.124 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.127 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.127 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.128 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.115 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.455 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.518 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.520 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.520 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.521 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.521 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.521 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.522 I llama_model_loader: - type  f32:  194 tensors
0.00.030.522 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.523 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.822 I llm_load_vocab: special tokens cache size = 25
0.00.073.838 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.842 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.843 I llm_load_print_meta: arch             = gptneox
0.00.073.843 I llm_load_print_meta: vocab type       = BPE
0.00.073.843 I llm_load_print_meta: n_vocab          = 50304
0.00.073.844 I llm_load_print_meta: n_merges         = 50009
0.00.073.844 I llm_load_print_meta: vocab_only       = 0
0.00.073.844 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.844 I llm_load_print_meta: n_embd           = 2048
0.00.073.846 I llm_load_print_meta: n_layer          = 24
0.00.073.850 I llm_load_print_meta: n_head           = 16
0.00.073.850 I llm_load_print_meta: n_head_kv        = 16
0.00.073.851 I llm_load_print_meta: n_rot            = 32
0.00.073.851 I llm_load_print_meta: n_swa            = 0
0.00.073.851 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.851 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.854 I llm_load_print_meta: n_gqa            = 1
0.00.073.855 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.856 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.857 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.868 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.869 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.871 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.871 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.877 I llm_load_print_meta: n_ff             = 8192
0.00.073.877 I llm_load_print_meta: n_expert         = 0
0.00.073.878 I llm_load_print_meta: n_expert_used    = 0
0.00.073.878 I llm_load_print_meta: causal attn      = 1
0.00.073.878 I llm_load_print_meta: pooling type     = 0
0.00.073.880 I llm_load_print_meta: rope type        = 2
0.00.073.880 I llm_load_print_meta: rope scaling     = linear
0.00.073.880 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.881 I llm_load_print_meta: freq_scale_train = 1
0.00.073.881 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.881 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.882 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.882 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.882 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.882 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.882 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.896 I llm_load_print_meta: model type       = 1.4B
0.00.073.897 I llm_load_print_meta: model ftype      = Q4_0
0.00.073.897 I llm_load_print_meta: model params     = 1.41 B
0.00.073.898 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.073.898 I llm_load_print_meta: general.name     = 1.4B
0.00.073.898 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.899 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.899 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.899 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.900 I llm_load_print_meta: LF token         = 128 ''
0.00.073.900 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.900 I llm_load_print_meta: max token length = 1024
0.00.076.644 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.644 I llm_load_tensors: offloading output layer to GPU
0.00.076.645 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.657 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.076.659 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.078.076 I llama_new_context_with_model: n_seq_max     = 1
0.00.078.077 I llama_new_context_with_model: n_ctx         = 2048
0.00.078.078 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.078.078 I llama_new_context_with_model: n_batch       = 2048
0.00.078.078 I llama_new_context_with_model: n_ubatch      = 512
0.00.078.079 I llama_new_context_with_model: flash_attn    = 0
0.00.078.079 I llama_new_context_with_model: freq_base     = 10000.0
0.00.078.079 I llama_new_context_with_model: freq_scale    = 1
0.00.078.080 I ggml_metal_init: allocating
0.00.078.084 I ggml_metal_init: found device: Apple M4
0.00.078.086 I ggml_metal_init: picking default device: Apple M4
0.00.078.980 I ggml_metal_init: using embedded metal library
0.00.082.351 I ggml_metal_init: GPU name:   Apple M4
0.00.082.353 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.354 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.354 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.354 I ggml_metal_init: simdgroup reduction   = true
0.00.082.355 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.355 I ggml_metal_init: has bfloat            = true
0.00.082.355 I ggml_metal_init: use bfloat            = true
0.00.082.355 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.356 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.116.493 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.116.501 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.524 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.636 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.117.637 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.117.638 I llama_new_context_with_model: graph nodes  = 967
0.00.117.638 I llama_new_context_with_model: graph splits = 2
0.00.117.654 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.984 I main: llama threadpool init, n_threads = 4
0.00.776.039 I 
0.00.776.093 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.776.095 I 
0.00.776.443 I sampler seed: 1234
0.00.776.448 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.776.516 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.776.522 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.776.522 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.468.959 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.01.468.960 I llama_perf_context_print:        load time =     764.37 ms
0.01.468.961 I llama_perf_context_print: prompt eval time =      47.59 ms /     7 tokens (    6.80 ms per token,   147.10 tokens per second)
0.01.468.961 I llama_perf_context_print:        eval time =     641.77 ms /    63 runs   (   10.19 ms per token,    98.17 tokens per second)
0.01.468.962 I llama_perf_context_print:       total time =     692.98 ms /    70 tokens
0.01.469.147 I ggml_metal_free: deallocating

real	0m1.511s
user	0m0.137s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.167 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.647 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.651 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.653 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.654 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.654 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.655 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.655 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.662 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.663 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.663 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.665 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.665 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.665 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.402 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.105 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.106 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.107 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.107 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.107 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.107 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.108 I llama_model_loader: - type  f32:  194 tensors
0.00.024.108 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.108 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.087 I llm_load_vocab: special tokens cache size = 25
0.00.050.030 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.033 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.033 I llm_load_print_meta: arch             = gptneox
0.00.050.034 I llm_load_print_meta: vocab type       = BPE
0.00.050.034 I llm_load_print_meta: n_vocab          = 50304
0.00.050.034 I llm_load_print_meta: n_merges         = 50009
0.00.050.034 I llm_load_print_meta: vocab_only       = 0
0.00.050.034 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.035 I llm_load_print_meta: n_embd           = 2048
0.00.050.035 I llm_load_print_meta: n_layer          = 24
0.00.050.037 I llm_load_print_meta: n_head           = 16
0.00.050.038 I llm_load_print_meta: n_head_kv        = 16
0.00.050.038 I llm_load_print_meta: n_rot            = 32
0.00.050.038 I llm_load_print_meta: n_swa            = 0
0.00.050.039 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.039 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.040 I llm_load_print_meta: n_gqa            = 1
0.00.050.040 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.041 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.042 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.042 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.042 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.042 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.043 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.043 I llm_load_print_meta: n_ff             = 8192
0.00.050.043 I llm_load_print_meta: n_expert         = 0
0.00.050.043 I llm_load_print_meta: n_expert_used    = 0
0.00.050.044 I llm_load_print_meta: causal attn      = 1
0.00.050.044 I llm_load_print_meta: pooling type     = 0
0.00.050.044 I llm_load_print_meta: rope type        = 2
0.00.050.044 I llm_load_print_meta: rope scaling     = linear
0.00.050.045 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.045 I llm_load_print_meta: freq_scale_train = 1
0.00.050.045 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.045 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.046 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.046 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.046 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.046 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.046 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.057 I llm_load_print_meta: model type       = 1.4B
0.00.050.057 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.058 I llm_load_print_meta: model params     = 1.41 B
0.00.050.059 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.059 I llm_load_print_meta: general.name     = 1.4B
0.00.050.059 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.059 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.059 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.061 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.062 I llm_load_print_meta: LF token         = 128 ''
0.00.050.062 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.062 I llm_load_print_meta: max token length = 1024
0.00.051.603 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.603 I llm_load_tensors: offloading output layer to GPU
0.00.051.603 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.613 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.614 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.444 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.445 I llama_new_context_with_model: n_ctx         = 128
0.00.052.445 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.445 I llama_new_context_with_model: n_batch       = 128
0.00.052.445 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.445 I llama_new_context_with_model: flash_attn    = 0
0.00.052.446 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.446 I llama_new_context_with_model: freq_scale    = 1
0.00.052.446 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.447 I ggml_metal_init: allocating
0.00.052.450 I ggml_metal_init: found device: Apple M4
0.00.052.452 I ggml_metal_init: picking default device: Apple M4
0.00.052.993 I ggml_metal_init: using embedded metal library
0.00.055.247 I ggml_metal_init: GPU name:   Apple M4
0.00.055.248 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.249 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.249 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.249 I ggml_metal_init: simdgroup reduction   = true
0.00.055.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.250 I ggml_metal_init: has bfloat            = true
0.00.055.250 I ggml_metal_init: use bfloat            = true
0.00.055.250 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.251 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.063 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.065 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.077 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.946 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.947 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.947 I llama_new_context_with_model: graph nodes  = 967
0.00.066.947 I llama_new_context_with_model: graph splits = 2
0.00.066.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.305 I 
0.00.635.367 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.635.401 I perplexity: tokenizing the input ..
0.00.643.095 I perplexity: tokenization took 7.693 ms
0.00.643.106 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.765.469 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.766.761 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.766.778 I llama_perf_context_print:        load time =     625.13 ms
0.00.766.779 I llama_perf_context_print: prompt eval time =     122.14 ms /   128 tokens (    0.95 ms per token,  1047.99 tokens per second)
0.00.766.780 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.766.780 I llama_perf_context_print:       total time =     131.48 ms /   129 tokens
0.00.767.230 I ggml_metal_free: deallocating

real	0m0.783s
user	0m0.078s
sys	0m0.107s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.677 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.758 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.761 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.762 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.763 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.763 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.766 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.767 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.773 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.774 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.774 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.774 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.776 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.777 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.780 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.780 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.780 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.477 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.357 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.358 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.358 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.358 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.359 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.359 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.359 I llama_model_loader: - type  f32:  194 tensors
0.00.031.360 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.360 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.081 I llm_load_vocab: special tokens cache size = 25
0.00.058.024 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.027 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.027 I llm_load_print_meta: arch             = gptneox
0.00.058.028 I llm_load_print_meta: vocab type       = BPE
0.00.058.028 I llm_load_print_meta: n_vocab          = 50304
0.00.058.028 I llm_load_print_meta: n_merges         = 50009
0.00.058.028 I llm_load_print_meta: vocab_only       = 0
0.00.058.028 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.029 I llm_load_print_meta: n_embd           = 2048
0.00.058.029 I llm_load_print_meta: n_layer          = 24
0.00.058.032 I llm_load_print_meta: n_head           = 16
0.00.058.033 I llm_load_print_meta: n_head_kv        = 16
0.00.058.033 I llm_load_print_meta: n_rot            = 32
0.00.058.033 I llm_load_print_meta: n_swa            = 0
0.00.058.033 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.037 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.038 I llm_load_print_meta: n_gqa            = 1
0.00.058.039 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.039 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.042 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.043 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.043 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.043 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.043 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.044 I llm_load_print_meta: n_ff             = 8192
0.00.058.044 I llm_load_print_meta: n_expert         = 0
0.00.058.044 I llm_load_print_meta: n_expert_used    = 0
0.00.058.045 I llm_load_print_meta: causal attn      = 1
0.00.058.046 I llm_load_print_meta: pooling type     = 0
0.00.058.046 I llm_load_print_meta: rope type        = 2
0.00.058.046 I llm_load_print_meta: rope scaling     = linear
0.00.058.047 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.047 I llm_load_print_meta: freq_scale_train = 1
0.00.058.047 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.047 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.047 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.048 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.048 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.048 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.048 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.059 I llm_load_print_meta: model type       = 1.4B
0.00.058.059 I llm_load_print_meta: model ftype      = Q4_1
0.00.058.060 I llm_load_print_meta: model params     = 1.41 B
0.00.058.060 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.058.060 I llm_load_print_meta: general.name     = 1.4B
0.00.058.061 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.061 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.061 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.063 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.063 I llm_load_print_meta: LF token         = 128 ''
0.00.058.063 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.063 I llm_load_print_meta: max token length = 1024
0.00.059.635 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.635 I llm_load_tensors: offloading output layer to GPU
0.00.059.635 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.645 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.059.646 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.060.476 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.477 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.477 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.477 I llama_new_context_with_model: n_batch       = 2048
0.00.060.478 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.478 I llama_new_context_with_model: flash_attn    = 0
0.00.060.478 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.478 I llama_new_context_with_model: freq_scale    = 1
0.00.060.479 I ggml_metal_init: allocating
0.00.060.482 I ggml_metal_init: found device: Apple M4
0.00.060.486 I ggml_metal_init: picking default device: Apple M4
0.00.061.034 I ggml_metal_init: using embedded metal library
0.00.063.306 I ggml_metal_init: GPU name:   Apple M4
0.00.063.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.308 I ggml_metal_init: simdgroup reduction   = true
0.00.063.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.308 I ggml_metal_init: has bfloat            = true
0.00.063.310 I ggml_metal_init: use bfloat            = true
0.00.063.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.311 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.787 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.793 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.811 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.958 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.960 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.960 I llama_new_context_with_model: graph nodes  = 967
0.00.093.960 I llama_new_context_with_model: graph splits = 2
0.00.093.974 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.957.888 I main: llama threadpool init, n_threads = 4
0.00.957.923 I 
0.00.957.950 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.957.951 I 
0.00.958.188 I sampler seed: 1234
0.00.958.193 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.958.204 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.958.205 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.958.205 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.692.073 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65137.61 tokens per second)
0.01.692.074 I llama_perf_context_print:        load time =     949.21 ms
0.01.692.074 I llama_perf_context_print: prompt eval time =      43.95 ms /     7 tokens (    6.28 ms per token,   159.26 tokens per second)
0.01.692.075 I llama_perf_context_print:        eval time =     687.05 ms /    63 runs   (   10.91 ms per token,    91.70 tokens per second)
0.01.692.076 I llama_perf_context_print:       total time =     734.19 ms /    70 tokens
0.01.692.262 I ggml_metal_free: deallocating

real	0m1.706s
user	0m0.109s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.738 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.383 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.391 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.391 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.392 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.392 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.399 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.400 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.400 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.400 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.401 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.401 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.401 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.403 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.403 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.167 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.168 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.169 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.169 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.169 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.170 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.170 I llama_model_loader: - type  f32:  194 tensors
0.00.023.171 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.171 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.183 I llm_load_vocab: special tokens cache size = 25
0.00.050.032 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.035 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.035 I llm_load_print_meta: arch             = gptneox
0.00.050.035 I llm_load_print_meta: vocab type       = BPE
0.00.050.035 I llm_load_print_meta: n_vocab          = 50304
0.00.050.036 I llm_load_print_meta: n_merges         = 50009
0.00.050.036 I llm_load_print_meta: vocab_only       = 0
0.00.050.036 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.036 I llm_load_print_meta: n_embd           = 2048
0.00.050.036 I llm_load_print_meta: n_layer          = 24
0.00.050.039 I llm_load_print_meta: n_head           = 16
0.00.050.040 I llm_load_print_meta: n_head_kv        = 16
0.00.050.040 I llm_load_print_meta: n_rot            = 32
0.00.050.040 I llm_load_print_meta: n_swa            = 0
0.00.050.040 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.041 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.042 I llm_load_print_meta: n_gqa            = 1
0.00.050.042 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.043 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.044 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.044 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.044 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.044 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.045 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.047 I llm_load_print_meta: n_ff             = 8192
0.00.050.047 I llm_load_print_meta: n_expert         = 0
0.00.050.047 I llm_load_print_meta: n_expert_used    = 0
0.00.050.048 I llm_load_print_meta: causal attn      = 1
0.00.050.048 I llm_load_print_meta: pooling type     = 0
0.00.050.048 I llm_load_print_meta: rope type        = 2
0.00.050.048 I llm_load_print_meta: rope scaling     = linear
0.00.050.049 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.049 I llm_load_print_meta: freq_scale_train = 1
0.00.050.049 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.049 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.049 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.049 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.050 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.050 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.050 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.062 I llm_load_print_meta: model type       = 1.4B
0.00.050.062 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.062 I llm_load_print_meta: model params     = 1.41 B
0.00.050.063 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.063 I llm_load_print_meta: general.name     = 1.4B
0.00.050.063 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.063 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.064 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.064 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.064 I llm_load_print_meta: LF token         = 128 ''
0.00.050.064 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.064 I llm_load_print_meta: max token length = 1024
0.00.052.014 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.014 I llm_load_tensors: offloading output layer to GPU
0.00.052.015 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.025 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.026 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.911 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.912 I llama_new_context_with_model: n_ctx         = 128
0.00.052.912 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.913 I llama_new_context_with_model: n_batch       = 128
0.00.052.913 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.913 I llama_new_context_with_model: flash_attn    = 0
0.00.052.913 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.914 I llama_new_context_with_model: freq_scale    = 1
0.00.052.914 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.914 I ggml_metal_init: allocating
0.00.052.920 I ggml_metal_init: found device: Apple M4
0.00.052.924 I ggml_metal_init: picking default device: Apple M4
0.00.053.456 I ggml_metal_init: using embedded metal library
0.00.055.762 I ggml_metal_init: GPU name:   Apple M4
0.00.055.763 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.764 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.764 I ggml_metal_init: simdgroup reduction   = true
0.00.055.764 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.765 I ggml_metal_init: has bfloat            = true
0.00.055.765 I ggml_metal_init: use bfloat            = true
0.00.055.765 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.766 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.540 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.543 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.557 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.414 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.415 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.416 I llama_new_context_with_model: graph nodes  = 967
0.00.067.416 I llama_new_context_with_model: graph splits = 2
0.00.067.428 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.120 I 
0.00.689.158 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.689.175 I perplexity: tokenizing the input ..
0.00.696.655 I perplexity: tokenization took 7.476 ms
0.00.696.665 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.423 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.820.793 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.820.819 I llama_perf_context_print:        load time =     680.37 ms
0.00.820.820 I llama_perf_context_print: prompt eval time =     122.53 ms /   128 tokens (    0.96 ms per token,  1044.67 tokens per second)
0.00.820.821 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.821 I llama_perf_context_print:       total time =     131.70 ms /   129 tokens
0.00.821.384 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.080s
sys	0m0.123s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.842 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.761 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.766 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.772 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.773 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.773 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.774 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.781 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.781 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.782 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.782 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.782 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.783 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.785 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.785 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.785 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.566 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.599 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.381 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.382 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.383 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.383 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.383 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.383 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.384 I llama_model_loader: - type  f32:  194 tensors
0.00.025.384 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.385 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.492 I llm_load_vocab: special tokens cache size = 25
0.00.051.546 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.548 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.549 I llm_load_print_meta: arch             = gptneox
0.00.051.549 I llm_load_print_meta: vocab type       = BPE
0.00.051.549 I llm_load_print_meta: n_vocab          = 50304
0.00.051.550 I llm_load_print_meta: n_merges         = 50009
0.00.051.550 I llm_load_print_meta: vocab_only       = 0
0.00.051.550 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.550 I llm_load_print_meta: n_embd           = 2048
0.00.051.550 I llm_load_print_meta: n_layer          = 24
0.00.051.553 I llm_load_print_meta: n_head           = 16
0.00.051.554 I llm_load_print_meta: n_head_kv        = 16
0.00.051.554 I llm_load_print_meta: n_rot            = 32
0.00.051.554 I llm_load_print_meta: n_swa            = 0
0.00.051.554 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.557 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.557 I llm_load_print_meta: n_gqa            = 1
0.00.051.558 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.559 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.559 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.561 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.561 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.561 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.562 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.562 I llm_load_print_meta: n_ff             = 8192
0.00.051.562 I llm_load_print_meta: n_expert         = 0
0.00.051.563 I llm_load_print_meta: n_expert_used    = 0
0.00.051.563 I llm_load_print_meta: causal attn      = 1
0.00.051.563 I llm_load_print_meta: pooling type     = 0
0.00.051.563 I llm_load_print_meta: rope type        = 2
0.00.051.563 I llm_load_print_meta: rope scaling     = linear
0.00.051.564 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.564 I llm_load_print_meta: freq_scale_train = 1
0.00.051.564 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.564 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.564 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.565 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.565 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.565 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.565 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.577 I llm_load_print_meta: model type       = 1.4B
0.00.051.577 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.577 I llm_load_print_meta: model params     = 1.41 B
0.00.051.579 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.579 I llm_load_print_meta: general.name     = 1.4B
0.00.051.579 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.579 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.580 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.580 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.580 I llm_load_print_meta: LF token         = 128 ''
0.00.051.580 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.580 I llm_load_print_meta: max token length = 1024
0.00.053.588 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.588 I llm_load_tensors: offloading output layer to GPU
0.00.053.588 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.598 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.600 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.479 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.480 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.480 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.480 I llama_new_context_with_model: n_batch       = 2048
0.00.054.481 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.481 I llama_new_context_with_model: flash_attn    = 0
0.00.054.481 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.482 I llama_new_context_with_model: freq_scale    = 1
0.00.054.482 I ggml_metal_init: allocating
0.00.054.488 I ggml_metal_init: found device: Apple M4
0.00.054.490 I ggml_metal_init: picking default device: Apple M4
0.00.055.040 I ggml_metal_init: using embedded metal library
0.00.057.377 I ggml_metal_init: GPU name:   Apple M4
0.00.057.379 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.379 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.380 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.380 I ggml_metal_init: simdgroup reduction   = true
0.00.057.380 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.380 I ggml_metal_init: has bfloat            = true
0.00.057.380 I ggml_metal_init: use bfloat            = true
0.00.057.381 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.609 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.614 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.632 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.717 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.718 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.718 I llama_new_context_with_model: graph nodes  = 967
0.00.086.719 I llama_new_context_with_model: graph splits = 2
0.00.086.732 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.206 I main: llama threadpool init, n_threads = 4
0.00.787.243 I 
0.00.787.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.787.273 I 
0.00.787.500 I sampler seed: 1234
0.00.787.507 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.787.518 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.787.520 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.787.520 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.575.373 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.575.373 I llama_perf_context_print:        load time =     777.36 ms
0.01.575.374 I llama_perf_context_print: prompt eval time =      43.23 ms /     7 tokens (    6.18 ms per token,   161.93 tokens per second)
0.01.575.375 I llama_perf_context_print:        eval time =     741.61 ms /    63 runs   (   11.77 ms per token,    84.95 tokens per second)
0.01.575.375 I llama_perf_context_print:       total time =     788.17 ms /    70 tokens
0.01.575.554 I ggml_metal_free: deallocating

real	0m1.591s
user	0m0.109s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.492 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.080 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.084 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.087 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.087 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.088 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.088 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.088 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.095 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.095 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.096 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.096 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.096 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.097 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.098 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.099 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.099 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.999 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.082 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.849 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.850 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.850 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.851 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.851 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.851 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.852 I llama_model_loader: - type  f32:  194 tensors
0.00.023.852 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.852 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.921 I llm_load_vocab: special tokens cache size = 25
0.00.049.854 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.857 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.857 I llm_load_print_meta: arch             = gptneox
0.00.049.858 I llm_load_print_meta: vocab type       = BPE
0.00.049.858 I llm_load_print_meta: n_vocab          = 50304
0.00.049.858 I llm_load_print_meta: n_merges         = 50009
0.00.049.858 I llm_load_print_meta: vocab_only       = 0
0.00.049.858 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.858 I llm_load_print_meta: n_embd           = 2048
0.00.049.859 I llm_load_print_meta: n_layer          = 24
0.00.049.861 I llm_load_print_meta: n_head           = 16
0.00.049.862 I llm_load_print_meta: n_head_kv        = 16
0.00.049.862 I llm_load_print_meta: n_rot            = 32
0.00.049.862 I llm_load_print_meta: n_swa            = 0
0.00.049.862 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.862 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.863 I llm_load_print_meta: n_gqa            = 1
0.00.049.864 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.865 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.865 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.866 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.866 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.866 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.866 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.867 I llm_load_print_meta: n_ff             = 8192
0.00.049.867 I llm_load_print_meta: n_expert         = 0
0.00.049.867 I llm_load_print_meta: n_expert_used    = 0
0.00.049.867 I llm_load_print_meta: causal attn      = 1
0.00.049.867 I llm_load_print_meta: pooling type     = 0
0.00.049.868 I llm_load_print_meta: rope type        = 2
0.00.049.868 I llm_load_print_meta: rope scaling     = linear
0.00.049.868 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.869 I llm_load_print_meta: freq_scale_train = 1
0.00.049.869 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.869 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.869 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.869 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.869 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.870 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.870 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.881 I llm_load_print_meta: model type       = 1.4B
0.00.049.881 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.881 I llm_load_print_meta: model params     = 1.41 B
0.00.049.882 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.882 I llm_load_print_meta: general.name     = 1.4B
0.00.049.882 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.883 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.883 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.883 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.883 I llm_load_print_meta: LF token         = 128 ''
0.00.049.883 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.883 I llm_load_print_meta: max token length = 1024
0.00.051.852 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.852 I llm_load_tensors: offloading output layer to GPU
0.00.051.852 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.863 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.864 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.753 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.754 I llama_new_context_with_model: n_ctx         = 128
0.00.052.754 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.754 I llama_new_context_with_model: n_batch       = 128
0.00.052.754 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.754 I llama_new_context_with_model: flash_attn    = 0
0.00.052.755 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.755 I llama_new_context_with_model: freq_scale    = 1
0.00.052.755 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.756 I ggml_metal_init: allocating
0.00.052.759 I ggml_metal_init: found device: Apple M4
0.00.052.761 I ggml_metal_init: picking default device: Apple M4
0.00.053.302 I ggml_metal_init: using embedded metal library
0.00.055.573 I ggml_metal_init: GPU name:   Apple M4
0.00.055.575 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.575 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.575 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.576 I ggml_metal_init: simdgroup reduction   = true
0.00.055.576 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.576 I ggml_metal_init: has bfloat            = true
0.00.055.576 I ggml_metal_init: use bfloat            = true
0.00.055.577 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.337 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.339 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.353 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.247 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.248 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.248 I llama_new_context_with_model: graph nodes  = 967
0.00.067.248 I llama_new_context_with_model: graph splits = 2
0.00.067.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.986 I 
0.00.733.019 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.733.035 I perplexity: tokenizing the input ..
0.00.740.294 I perplexity: tokenization took 7.258 ms
0.00.740.304 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.875.323 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.876.668 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.876.684 I llama_perf_context_print:        load time =     723.49 ms
0.00.876.687 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.80 tokens per second)
0.00.876.688 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.876.689 I llama_perf_context_print:       total time =     143.70 ms /   129 tokens
0.00.877.263 I ggml_metal_free: deallocating

real	0m0.893s
user	0m0.079s
sys	0m0.135s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.244 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.167 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.171 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.173 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.177 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.178 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.178 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.178 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.186 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.186 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.187 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.189 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.189 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.189 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.108 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.961 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.963 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.963 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.964 I llama_model_loader: - type  f32:  194 tensors
0.00.023.964 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.965 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.063 I llm_load_vocab: special tokens cache size = 25
0.00.050.056 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.058 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.058 I llm_load_print_meta: arch             = gptneox
0.00.050.059 I llm_load_print_meta: vocab type       = BPE
0.00.050.059 I llm_load_print_meta: n_vocab          = 50304
0.00.050.059 I llm_load_print_meta: n_merges         = 50009
0.00.050.059 I llm_load_print_meta: vocab_only       = 0
0.00.050.060 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.060 I llm_load_print_meta: n_embd           = 2048
0.00.050.060 I llm_load_print_meta: n_layer          = 24
0.00.050.062 I llm_load_print_meta: n_head           = 16
0.00.050.063 I llm_load_print_meta: n_head_kv        = 16
0.00.050.063 I llm_load_print_meta: n_rot            = 32
0.00.050.064 I llm_load_print_meta: n_swa            = 0
0.00.050.064 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.064 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.065 I llm_load_print_meta: n_gqa            = 1
0.00.050.065 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.066 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.067 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.067 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.067 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.067 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.068 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.068 I llm_load_print_meta: n_ff             = 8192
0.00.050.068 I llm_load_print_meta: n_expert         = 0
0.00.050.068 I llm_load_print_meta: n_expert_used    = 0
0.00.050.071 I llm_load_print_meta: causal attn      = 1
0.00.050.073 I llm_load_print_meta: pooling type     = 0
0.00.050.073 I llm_load_print_meta: rope type        = 2
0.00.050.073 I llm_load_print_meta: rope scaling     = linear
0.00.050.074 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.074 I llm_load_print_meta: freq_scale_train = 1
0.00.050.074 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.074 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.075 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.075 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.075 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.075 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.075 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.086 I llm_load_print_meta: model type       = 1.4B
0.00.050.087 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.087 I llm_load_print_meta: model params     = 1.41 B
0.00.050.088 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.089 I llm_load_print_meta: general.name     = 1.4B
0.00.050.089 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.089 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.089 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.090 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.090 I llm_load_print_meta: LF token         = 128 ''
0.00.050.091 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.091 I llm_load_print_meta: max token length = 1024
0.00.052.113 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.113 I llm_load_tensors: offloading output layer to GPU
0.00.052.113 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.124 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.125 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.055 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.056 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.057 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.057 I llama_new_context_with_model: n_batch       = 2048
0.00.053.057 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.057 I llama_new_context_with_model: flash_attn    = 0
0.00.053.058 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.058 I llama_new_context_with_model: freq_scale    = 1
0.00.053.059 I ggml_metal_init: allocating
0.00.053.065 I ggml_metal_init: found device: Apple M4
0.00.053.067 I ggml_metal_init: picking default device: Apple M4
0.00.053.646 I ggml_metal_init: using embedded metal library
0.00.056.002 I ggml_metal_init: GPU name:   Apple M4
0.00.056.003 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.004 I ggml_metal_init: simdgroup reduction   = true
0.00.056.006 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.006 I ggml_metal_init: has bfloat            = true
0.00.056.006 I ggml_metal_init: use bfloat            = true
0.00.056.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.719 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.726 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.744 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.897 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.898 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.899 I llama_new_context_with_model: graph nodes  = 967
0.00.085.899 I llama_new_context_with_model: graph splits = 2
0.00.085.905 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.085 I main: llama threadpool init, n_threads = 4
0.00.719.122 I 
0.00.719.162 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.719.164 I 
0.00.719.408 I sampler seed: 1234
0.00.719.412 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.719.422 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.719.423 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.719.425 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.558.184 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.558.185 I llama_perf_context_print:        load time =     710.84 ms
0.01.558.185 I llama_perf_context_print: prompt eval time =      42.29 ms /     7 tokens (    6.04 ms per token,   165.54 tokens per second)
0.01.558.186 I llama_perf_context_print:        eval time =     793.45 ms /    63 runs   (   12.59 ms per token,    79.40 tokens per second)
0.01.558.187 I llama_perf_context_print:       total time =     839.10 ms /    70 tokens
0.01.558.385 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.108s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.116 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.998 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.822 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.827 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.828 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.829 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.837 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.837 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.837 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.838 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.839 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.839 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.840 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.842 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.843 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.843 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.652 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.797 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.641 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.642 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.643 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.643 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.643 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.644 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.644 I llama_model_loader: - type  f32:  194 tensors
0.00.023.645 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.645 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.499 I llm_load_vocab: special tokens cache size = 25
0.00.049.501 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.503 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.504 I llm_load_print_meta: arch             = gptneox
0.00.049.504 I llm_load_print_meta: vocab type       = BPE
0.00.049.504 I llm_load_print_meta: n_vocab          = 50304
0.00.049.504 I llm_load_print_meta: n_merges         = 50009
0.00.049.504 I llm_load_print_meta: vocab_only       = 0
0.00.049.505 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.505 I llm_load_print_meta: n_embd           = 2048
0.00.049.505 I llm_load_print_meta: n_layer          = 24
0.00.049.508 I llm_load_print_meta: n_head           = 16
0.00.049.509 I llm_load_print_meta: n_head_kv        = 16
0.00.049.509 I llm_load_print_meta: n_rot            = 32
0.00.049.509 I llm_load_print_meta: n_swa            = 0
0.00.049.509 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.509 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.510 I llm_load_print_meta: n_gqa            = 1
0.00.049.511 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.511 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.512 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.512 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.513 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.513 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.513 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.514 I llm_load_print_meta: n_ff             = 8192
0.00.049.514 I llm_load_print_meta: n_expert         = 0
0.00.049.514 I llm_load_print_meta: n_expert_used    = 0
0.00.049.517 I llm_load_print_meta: causal attn      = 1
0.00.049.517 I llm_load_print_meta: pooling type     = 0
0.00.049.517 I llm_load_print_meta: rope type        = 2
0.00.049.517 I llm_load_print_meta: rope scaling     = linear
0.00.049.518 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.519 I llm_load_print_meta: freq_scale_train = 1
0.00.049.519 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.519 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.519 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.519 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.519 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.520 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.520 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.531 I llm_load_print_meta: model type       = 1.4B
0.00.049.531 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.532 I llm_load_print_meta: model params     = 1.41 B
0.00.049.532 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.532 I llm_load_print_meta: general.name     = 1.4B
0.00.049.533 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.537 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.537 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.537 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.537 I llm_load_print_meta: LF token         = 128 ''
0.00.049.538 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.538 I llm_load_print_meta: max token length = 1024
0.00.051.149 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.149 I llm_load_tensors: offloading output layer to GPU
0.00.051.149 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.159 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.160 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.031 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.032 I llama_new_context_with_model: n_ctx         = 128
0.00.052.032 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.032 I llama_new_context_with_model: n_batch       = 128
0.00.052.032 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.033 I llama_new_context_with_model: flash_attn    = 0
0.00.052.033 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.033 I llama_new_context_with_model: freq_scale    = 1
0.00.052.033 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.034 I ggml_metal_init: allocating
0.00.052.037 I ggml_metal_init: found device: Apple M4
0.00.052.039 I ggml_metal_init: picking default device: Apple M4
0.00.052.584 I ggml_metal_init: using embedded metal library
0.00.054.885 I ggml_metal_init: GPU name:   Apple M4
0.00.054.886 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.887 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.887 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.887 I ggml_metal_init: simdgroup reduction   = true
0.00.054.887 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.888 I ggml_metal_init: has bfloat            = true
0.00.054.888 I ggml_metal_init: use bfloat            = true
0.00.054.889 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.889 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.531 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.533 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.545 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.399 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.400 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.401 I llama_new_context_with_model: graph nodes  = 967
0.00.066.401 I llama_new_context_with_model: graph splits = 2
0.00.066.413 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.169 I 
0.00.671.198 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.671.216 I perplexity: tokenizing the input ..
0.00.678.292 I perplexity: tokenization took 7.074 ms
0.00.678.302 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.813.167 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.814.518 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.814.532 I llama_perf_context_print:        load time =     662.17 ms
0.00.814.533 I llama_perf_context_print: prompt eval time =     134.61 ms /   128 tokens (    1.05 ms per token,   950.90 tokens per second)
0.00.814.534 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.534 I llama_perf_context_print:       total time =     143.36 ms /   129 tokens
0.00.815.089 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.079s
sys	0m0.131s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.010.291 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.116 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.120 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.127 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.128 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.128 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.137 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.138 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.141 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.053 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.128 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.997 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.998 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.998 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.999 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.999 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.999 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.000 I llama_model_loader: - type  f32:  194 tensors
0.00.025.000 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.000 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.001 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.887 I llm_load_vocab: special tokens cache size = 25
0.00.051.801 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.804 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.804 I llm_load_print_meta: arch             = gptneox
0.00.051.805 I llm_load_print_meta: vocab type       = BPE
0.00.051.805 I llm_load_print_meta: n_vocab          = 50304
0.00.051.805 I llm_load_print_meta: n_merges         = 50009
0.00.051.805 I llm_load_print_meta: vocab_only       = 0
0.00.051.805 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.805 I llm_load_print_meta: n_embd           = 2048
0.00.051.806 I llm_load_print_meta: n_layer          = 24
0.00.051.809 I llm_load_print_meta: n_head           = 16
0.00.051.809 I llm_load_print_meta: n_head_kv        = 16
0.00.051.810 I llm_load_print_meta: n_rot            = 32
0.00.051.810 I llm_load_print_meta: n_swa            = 0
0.00.051.810 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.810 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.811 I llm_load_print_meta: n_gqa            = 1
0.00.051.812 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.812 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.813 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.813 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.813 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.813 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.814 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.814 I llm_load_print_meta: n_ff             = 8192
0.00.051.814 I llm_load_print_meta: n_expert         = 0
0.00.051.815 I llm_load_print_meta: n_expert_used    = 0
0.00.051.815 I llm_load_print_meta: causal attn      = 1
0.00.051.815 I llm_load_print_meta: pooling type     = 0
0.00.051.815 I llm_load_print_meta: rope type        = 2
0.00.051.815 I llm_load_print_meta: rope scaling     = linear
0.00.051.816 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.816 I llm_load_print_meta: freq_scale_train = 1
0.00.051.816 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.816 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.817 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.817 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.817 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.817 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.817 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.829 I llm_load_print_meta: model type       = 1.4B
0.00.051.829 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.830 I llm_load_print_meta: model params     = 1.41 B
0.00.051.830 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.830 I llm_load_print_meta: general.name     = 1.4B
0.00.051.830 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.831 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.832 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.832 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.833 I llm_load_print_meta: LF token         = 128 ''
0.00.051.833 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.833 I llm_load_print_meta: max token length = 1024
0.00.053.749 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.750 I llm_load_tensors: offloading output layer to GPU
0.00.053.750 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.760 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.761 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.700 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.701 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.701 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.702 I llama_new_context_with_model: n_batch       = 2048
0.00.054.702 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.702 I llama_new_context_with_model: flash_attn    = 0
0.00.054.702 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.703 I llama_new_context_with_model: freq_scale    = 1
0.00.054.703 I ggml_metal_init: allocating
0.00.054.706 I ggml_metal_init: found device: Apple M4
0.00.054.708 I ggml_metal_init: picking default device: Apple M4
0.00.055.283 I ggml_metal_init: using embedded metal library
0.00.057.622 I ggml_metal_init: GPU name:   Apple M4
0.00.057.624 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.624 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.625 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.625 I ggml_metal_init: simdgroup reduction   = true
0.00.057.625 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.625 I ggml_metal_init: has bfloat            = true
0.00.057.625 I ggml_metal_init: use bfloat            = true
0.00.057.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.626 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.020 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.029 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.047 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.154 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.155 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.156 I llama_new_context_with_model: graph nodes  = 967
0.00.088.156 I llama_new_context_with_model: graph splits = 2
0.00.088.170 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.455.805 I main: llama threadpool init, n_threads = 4
0.00.455.843 I 
0.00.455.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.455.874 I 
0.00.456.109 I sampler seed: 1234
0.00.456.114 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.456.144 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.456.146 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.456.146 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.135.375 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60169.49 tokens per second)
0.01.135.376 I llama_perf_context_print:        load time =     445.51 ms
0.01.135.376 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.30 tokens per second)
0.01.135.378 I llama_perf_context_print:        eval time =     640.40 ms /    63 runs   (   10.17 ms per token,    98.38 tokens per second)
0.01.135.380 I llama_perf_context_print:       total time =     679.57 ms /    70 tokens
0.01.135.555 I ggml_metal_free: deallocating

real	0m1.153s
user	0m0.110s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.098 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.610 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.614 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.616 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.616 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.617 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.617 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.624 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.624 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.625 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.625 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.627 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.629 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.629 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.629 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.397 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.406 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.183 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.184 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.184 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.185 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.185 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.185 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.186 I llama_model_loader: - type  f32:  194 tensors
0.00.024.186 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.186 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.186 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.212 I llm_load_vocab: special tokens cache size = 25
0.00.050.083 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.086 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.087 I llm_load_print_meta: arch             = gptneox
0.00.050.087 I llm_load_print_meta: vocab type       = BPE
0.00.050.087 I llm_load_print_meta: n_vocab          = 50304
0.00.050.088 I llm_load_print_meta: n_merges         = 50009
0.00.050.088 I llm_load_print_meta: vocab_only       = 0
0.00.050.088 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.088 I llm_load_print_meta: n_embd           = 2048
0.00.050.088 I llm_load_print_meta: n_layer          = 24
0.00.050.091 I llm_load_print_meta: n_head           = 16
0.00.050.092 I llm_load_print_meta: n_head_kv        = 16
0.00.050.092 I llm_load_print_meta: n_rot            = 32
0.00.050.092 I llm_load_print_meta: n_swa            = 0
0.00.050.093 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.093 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.093 I llm_load_print_meta: n_gqa            = 1
0.00.050.096 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.096 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.097 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.097 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.098 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.098 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.100 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.101 I llm_load_print_meta: n_ff             = 8192
0.00.050.101 I llm_load_print_meta: n_expert         = 0
0.00.050.101 I llm_load_print_meta: n_expert_used    = 0
0.00.050.101 I llm_load_print_meta: causal attn      = 1
0.00.050.101 I llm_load_print_meta: pooling type     = 0
0.00.050.101 I llm_load_print_meta: rope type        = 2
0.00.050.102 I llm_load_print_meta: rope scaling     = linear
0.00.050.102 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.102 I llm_load_print_meta: freq_scale_train = 1
0.00.050.103 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.103 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.103 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.103 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.103 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.103 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.107 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.119 I llm_load_print_meta: model type       = 1.4B
0.00.050.119 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.119 I llm_load_print_meta: model params     = 1.41 B
0.00.050.121 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.121 I llm_load_print_meta: general.name     = 1.4B
0.00.050.121 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.121 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.121 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.123 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.123 I llm_load_print_meta: LF token         = 128 ''
0.00.050.123 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.123 I llm_load_print_meta: max token length = 1024
0.00.051.680 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.681 I llm_load_tensors: offloading output layer to GPU
0.00.051.681 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.691 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.692 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.540 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.541 I llama_new_context_with_model: n_ctx         = 128
0.00.052.541 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.541 I llama_new_context_with_model: n_batch       = 128
0.00.052.541 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.542 I llama_new_context_with_model: flash_attn    = 0
0.00.052.542 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.542 I llama_new_context_with_model: freq_scale    = 1
0.00.052.543 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.543 I ggml_metal_init: allocating
0.00.052.549 I ggml_metal_init: found device: Apple M4
0.00.052.551 I ggml_metal_init: picking default device: Apple M4
0.00.053.096 I ggml_metal_init: using embedded metal library
0.00.055.439 I ggml_metal_init: GPU name:   Apple M4
0.00.055.440 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.440 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.441 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.441 I ggml_metal_init: simdgroup reduction   = true
0.00.055.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.441 I ggml_metal_init: has bfloat            = true
0.00.055.442 I ggml_metal_init: use bfloat            = true
0.00.055.442 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.953 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.956 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.970 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.821 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.822 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.822 I llama_new_context_with_model: graph nodes  = 967
0.00.066.822 I llama_new_context_with_model: graph splits = 2
0.00.066.834 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.397.376 I 
0.00.397.422 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.397.443 I perplexity: tokenizing the input ..
0.00.405.042 I perplexity: tokenization took 7.597 ms
0.00.405.052 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.536.604 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.537.977 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.537.991 I llama_perf_context_print:        load time =     387.27 ms
0.00.537.992 I llama_perf_context_print: prompt eval time =     131.32 ms /   128 tokens (    1.03 ms per token,   974.71 tokens per second)
0.00.537.993 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.537.993 I llama_perf_context_print:       total time =     140.62 ms /   129 tokens
0.00.538.454 I ggml_metal_free: deallocating

real	0m0.555s
user	0m0.078s
sys	0m0.073s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.438 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.013.937 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.013.942 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.944 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.013.945 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.945 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.013.946 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.013.946 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.013.952 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.013.953 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.013.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.013.954 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.013.954 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.013.954 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.013.957 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.013.958 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.013.959 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.013.959 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.728 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.764 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.523 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.524 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.525 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.525 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.525 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.526 I llama_model_loader: - type  f32:  194 tensors
0.00.022.526 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.527 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.527 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.527 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.745 I llm_load_vocab: special tokens cache size = 25
0.00.048.545 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.548 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.548 I llm_load_print_meta: arch             = gptneox
0.00.048.549 I llm_load_print_meta: vocab type       = BPE
0.00.048.549 I llm_load_print_meta: n_vocab          = 50304
0.00.048.549 I llm_load_print_meta: n_merges         = 50009
0.00.048.549 I llm_load_print_meta: vocab_only       = 0
0.00.048.549 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.549 I llm_load_print_meta: n_embd           = 2048
0.00.048.550 I llm_load_print_meta: n_layer          = 24
0.00.048.552 I llm_load_print_meta: n_head           = 16
0.00.048.553 I llm_load_print_meta: n_head_kv        = 16
0.00.048.553 I llm_load_print_meta: n_rot            = 32
0.00.048.553 I llm_load_print_meta: n_swa            = 0
0.00.048.553 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.553 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.554 I llm_load_print_meta: n_gqa            = 1
0.00.048.555 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.556 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.556 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.557 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.557 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.557 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.557 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.558 I llm_load_print_meta: n_ff             = 8192
0.00.048.558 I llm_load_print_meta: n_expert         = 0
0.00.048.558 I llm_load_print_meta: n_expert_used    = 0
0.00.048.558 I llm_load_print_meta: causal attn      = 1
0.00.048.558 I llm_load_print_meta: pooling type     = 0
0.00.048.558 I llm_load_print_meta: rope type        = 2
0.00.048.559 I llm_load_print_meta: rope scaling     = linear
0.00.048.559 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.559 I llm_load_print_meta: freq_scale_train = 1
0.00.048.560 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.560 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.562 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.562 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.562 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.562 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.563 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.575 I llm_load_print_meta: model type       = 1.4B
0.00.048.575 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.575 I llm_load_print_meta: model params     = 1.41 B
0.00.048.576 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.576 I llm_load_print_meta: general.name     = 1.4B
0.00.048.576 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.576 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.576 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.576 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.577 I llm_load_print_meta: LF token         = 128 ''
0.00.048.577 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.578 I llm_load_print_meta: max token length = 1024
0.00.050.501 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.501 I llm_load_tensors: offloading output layer to GPU
0.00.050.501 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.511 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.512 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.410 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.411 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.411 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.411 I llama_new_context_with_model: n_batch       = 2048
0.00.051.411 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.411 I llama_new_context_with_model: flash_attn    = 0
0.00.051.412 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.412 I llama_new_context_with_model: freq_scale    = 1
0.00.051.412 I ggml_metal_init: allocating
0.00.051.416 I ggml_metal_init: found device: Apple M4
0.00.051.418 I ggml_metal_init: picking default device: Apple M4
0.00.051.967 I ggml_metal_init: using embedded metal library
0.00.054.254 I ggml_metal_init: GPU name:   Apple M4
0.00.054.256 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.256 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.257 I ggml_metal_init: simdgroup reduction   = true
0.00.054.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.257 I ggml_metal_init: has bfloat            = true
0.00.054.257 I ggml_metal_init: use bfloat            = true
0.00.054.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.726 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.734 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.753 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.734 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.735 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.735 I llama_new_context_with_model: graph nodes  = 967
0.00.083.736 I llama_new_context_with_model: graph splits = 2
0.00.083.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.538.706 I main: llama threadpool init, n_threads = 4
0.00.538.748 I 
0.00.538.797 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.538.798 I 
0.00.539.036 I sampler seed: 1234
0.00.539.042 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.539.078 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.539.082 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.539.082 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.289.694 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56170.89 tokens per second)
0.01.289.695 I llama_perf_context_print:        load time =     530.26 ms
0.01.289.696 I llama_perf_context_print: prompt eval time =      44.31 ms /     7 tokens (    6.33 ms per token,   157.98 tokens per second)
0.01.289.696 I llama_perf_context_print:        eval time =     703.21 ms /    63 runs   (   11.16 ms per token,    89.59 tokens per second)
0.01.289.696 I llama_perf_context_print:       total time =     750.99 ms /    70 tokens
0.01.289.904 I ggml_metal_free: deallocating

real	0m1.304s
user	0m0.109s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.749 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.240 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.245 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.247 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.248 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.248 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.249 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.256 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.257 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.258 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.258 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.258 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.259 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.259 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.263 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.264 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.264 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.052 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.133 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.022 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.022 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.023 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.023 I llama_model_loader: - type  f32:  194 tensors
0.00.023.024 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.024 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.024 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.025 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.859 I llm_load_vocab: special tokens cache size = 25
0.00.049.837 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.840 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.840 I llm_load_print_meta: arch             = gptneox
0.00.049.840 I llm_load_print_meta: vocab type       = BPE
0.00.049.841 I llm_load_print_meta: n_vocab          = 50304
0.00.049.841 I llm_load_print_meta: n_merges         = 50009
0.00.049.841 I llm_load_print_meta: vocab_only       = 0
0.00.049.841 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.841 I llm_load_print_meta: n_embd           = 2048
0.00.049.841 I llm_load_print_meta: n_layer          = 24
0.00.049.844 I llm_load_print_meta: n_head           = 16
0.00.049.845 I llm_load_print_meta: n_head_kv        = 16
0.00.049.848 I llm_load_print_meta: n_rot            = 32
0.00.049.848 I llm_load_print_meta: n_swa            = 0
0.00.049.848 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.848 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.849 I llm_load_print_meta: n_gqa            = 1
0.00.049.849 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.850 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.851 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.851 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.851 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.851 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.852 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.852 I llm_load_print_meta: n_ff             = 8192
0.00.049.852 I llm_load_print_meta: n_expert         = 0
0.00.049.852 I llm_load_print_meta: n_expert_used    = 0
0.00.049.853 I llm_load_print_meta: causal attn      = 1
0.00.049.853 I llm_load_print_meta: pooling type     = 0
0.00.049.855 I llm_load_print_meta: rope type        = 2
0.00.049.856 I llm_load_print_meta: rope scaling     = linear
0.00.049.857 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.857 I llm_load_print_meta: freq_scale_train = 1
0.00.049.857 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.857 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.857 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.858 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.858 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.858 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.858 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.869 I llm_load_print_meta: model type       = 1.4B
0.00.049.870 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.870 I llm_load_print_meta: model params     = 1.41 B
0.00.049.871 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.871 I llm_load_print_meta: general.name     = 1.4B
0.00.049.871 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.872 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.872 I llm_load_print_meta: LF token         = 128 ''
0.00.049.872 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.872 I llm_load_print_meta: max token length = 1024
0.00.051.840 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.840 I llm_load_tensors: offloading output layer to GPU
0.00.051.840 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.850 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.851 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.810 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.811 I llama_new_context_with_model: n_ctx         = 128
0.00.052.811 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.811 I llama_new_context_with_model: n_batch       = 128
0.00.052.811 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.812 I llama_new_context_with_model: flash_attn    = 0
0.00.052.812 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.812 I llama_new_context_with_model: freq_scale    = 1
0.00.052.813 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.813 I ggml_metal_init: allocating
0.00.052.816 I ggml_metal_init: found device: Apple M4
0.00.052.818 I ggml_metal_init: picking default device: Apple M4
0.00.053.362 I ggml_metal_init: using embedded metal library
0.00.055.695 I ggml_metal_init: GPU name:   Apple M4
0.00.055.697 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.697 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.698 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.699 I ggml_metal_init: simdgroup reduction   = true
0.00.055.699 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.700 I ggml_metal_init: has bfloat            = true
0.00.055.700 I ggml_metal_init: use bfloat            = true
0.00.055.700 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.521 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.523 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.544 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.464 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.465 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.465 I llama_new_context_with_model: graph nodes  = 967
0.00.067.465 I llama_new_context_with_model: graph splits = 2
0.00.067.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.488.601 I 
0.00.488.629 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.488.641 I perplexity: tokenizing the input ..
0.00.496.196 I perplexity: tokenization took 7.555 ms
0.00.496.207 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.628.241 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.629.568 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.629.584 I llama_perf_context_print:        load time =     479.85 ms
0.00.629.585 I llama_perf_context_print: prompt eval time =     131.81 ms /   128 tokens (    1.03 ms per token,   971.11 tokens per second)
0.00.629.586 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.629.586 I llama_perf_context_print:       total time =     140.98 ms /   129 tokens
0.00.629.981 I ggml_metal_free: deallocating

real	0m0.643s
user	0m0.080s
sys	0m0.085s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.363 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.538 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.542 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.548 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.556 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.557 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.557 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.558 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.558 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.558 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.559 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.562 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.562 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.562 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.416 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.445 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.239 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.241 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.241 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.241 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.241 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.242 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.242 I llama_model_loader: - type  f32:  194 tensors
0.00.025.243 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.243 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.243 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.489 I llm_load_vocab: special tokens cache size = 25
0.00.051.409 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.412 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.413 I llm_load_print_meta: arch             = gptneox
0.00.051.413 I llm_load_print_meta: vocab type       = BPE
0.00.051.413 I llm_load_print_meta: n_vocab          = 50304
0.00.051.413 I llm_load_print_meta: n_merges         = 50009
0.00.051.413 I llm_load_print_meta: vocab_only       = 0
0.00.051.414 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.414 I llm_load_print_meta: n_embd           = 2048
0.00.051.414 I llm_load_print_meta: n_layer          = 24
0.00.051.417 I llm_load_print_meta: n_head           = 16
0.00.051.418 I llm_load_print_meta: n_head_kv        = 16
0.00.051.418 I llm_load_print_meta: n_rot            = 32
0.00.051.418 I llm_load_print_meta: n_swa            = 0
0.00.051.418 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.418 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.419 I llm_load_print_meta: n_gqa            = 1
0.00.051.420 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.420 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.421 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.421 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.426 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.426 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.426 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.427 I llm_load_print_meta: n_ff             = 8192
0.00.051.427 I llm_load_print_meta: n_expert         = 0
0.00.051.427 I llm_load_print_meta: n_expert_used    = 0
0.00.051.428 I llm_load_print_meta: causal attn      = 1
0.00.051.428 I llm_load_print_meta: pooling type     = 0
0.00.051.428 I llm_load_print_meta: rope type        = 2
0.00.051.428 I llm_load_print_meta: rope scaling     = linear
0.00.051.428 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.429 I llm_load_print_meta: freq_scale_train = 1
0.00.051.429 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.429 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.429 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.430 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.430 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.430 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.430 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.441 I llm_load_print_meta: model type       = 1.4B
0.00.051.442 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.442 I llm_load_print_meta: model params     = 1.41 B
0.00.051.442 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.443 I llm_load_print_meta: general.name     = 1.4B
0.00.051.443 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.443 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.443 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.443 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.444 I llm_load_print_meta: LF token         = 128 ''
0.00.051.444 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.444 I llm_load_print_meta: max token length = 1024
0.00.052.990 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.990 I llm_load_tensors: offloading output layer to GPU
0.00.052.991 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.000 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.002 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.892 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.893 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.893 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.894 I llama_new_context_with_model: n_batch       = 2048
0.00.053.894 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.894 I llama_new_context_with_model: flash_attn    = 0
0.00.053.894 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.895 I llama_new_context_with_model: freq_scale    = 1
0.00.053.895 I ggml_metal_init: allocating
0.00.053.898 I ggml_metal_init: found device: Apple M4
0.00.053.900 I ggml_metal_init: picking default device: Apple M4
0.00.054.441 I ggml_metal_init: using embedded metal library
0.00.056.701 I ggml_metal_init: GPU name:   Apple M4
0.00.056.703 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.703 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.704 I ggml_metal_init: simdgroup reduction   = true
0.00.056.704 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.704 I ggml_metal_init: has bfloat            = true
0.00.056.704 I ggml_metal_init: use bfloat            = true
0.00.056.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.705 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.545 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.550 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.567 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.567 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.569 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.569 I llama_new_context_with_model: graph nodes  = 967
0.00.086.569 I llama_new_context_with_model: graph splits = 2
0.00.086.582 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.624.872 I main: llama threadpool init, n_threads = 4
0.00.624.915 I 
0.00.624.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.624.941 I 
0.00.625.167 I sampler seed: 1234
0.00.625.171 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.625.214 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.625.215 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.625.215 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.388.196 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.388.196 I llama_perf_context_print:        load time =     614.50 ms
0.01.388.197 I llama_perf_context_print: prompt eval time =      50.50 ms /     7 tokens (    7.21 ms per token,   138.61 tokens per second)
0.01.388.198 I llama_perf_context_print:        eval time =     709.51 ms /    63 runs   (   11.26 ms per token,    88.79 tokens per second)
0.01.388.198 I llama_perf_context_print:       total time =     763.33 ms /    70 tokens
0.01.388.389 I ggml_metal_free: deallocating

real	0m1.405s
user	0m0.108s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.415 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.112 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.117 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.123 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.124 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.124 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.124 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.125 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.132 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.132 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.133 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.133 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.133 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.140 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.140 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.999 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.753 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.754 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.754 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.755 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.755 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.755 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.756 I llama_model_loader: - type  f32:  194 tensors
0.00.023.756 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.757 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.757 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.797 I llm_load_vocab: special tokens cache size = 25
0.00.049.602 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.605 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.605 I llm_load_print_meta: arch             = gptneox
0.00.049.606 I llm_load_print_meta: vocab type       = BPE
0.00.049.606 I llm_load_print_meta: n_vocab          = 50304
0.00.049.606 I llm_load_print_meta: n_merges         = 50009
0.00.049.606 I llm_load_print_meta: vocab_only       = 0
0.00.049.607 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.607 I llm_load_print_meta: n_embd           = 2048
0.00.049.607 I llm_load_print_meta: n_layer          = 24
0.00.049.610 I llm_load_print_meta: n_head           = 16
0.00.049.611 I llm_load_print_meta: n_head_kv        = 16
0.00.049.611 I llm_load_print_meta: n_rot            = 32
0.00.049.611 I llm_load_print_meta: n_swa            = 0
0.00.049.611 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.611 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.612 I llm_load_print_meta: n_gqa            = 1
0.00.049.613 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.613 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.614 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.614 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.617 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.617 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.617 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.618 I llm_load_print_meta: n_ff             = 8192
0.00.049.618 I llm_load_print_meta: n_expert         = 0
0.00.049.618 I llm_load_print_meta: n_expert_used    = 0
0.00.049.618 I llm_load_print_meta: causal attn      = 1
0.00.049.618 I llm_load_print_meta: pooling type     = 0
0.00.049.618 I llm_load_print_meta: rope type        = 2
0.00.049.620 I llm_load_print_meta: rope scaling     = linear
0.00.049.622 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.622 I llm_load_print_meta: freq_scale_train = 1
0.00.049.622 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.622 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.622 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.623 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.623 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.623 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.623 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.635 I llm_load_print_meta: model type       = 1.4B
0.00.049.635 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.636 I llm_load_print_meta: model params     = 1.41 B
0.00.049.636 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.636 I llm_load_print_meta: general.name     = 1.4B
0.00.049.636 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.637 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.637 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.637 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.637 I llm_load_print_meta: LF token         = 128 ''
0.00.049.637 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.637 I llm_load_print_meta: max token length = 1024
0.00.051.623 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.623 I llm_load_tensors: offloading output layer to GPU
0.00.051.623 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.634 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.635 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.653 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.654 I llama_new_context_with_model: n_ctx         = 128
0.00.052.655 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.655 I llama_new_context_with_model: n_batch       = 128
0.00.052.655 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.655 I llama_new_context_with_model: flash_attn    = 0
0.00.052.656 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.656 I llama_new_context_with_model: freq_scale    = 1
0.00.052.656 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.657 I ggml_metal_init: allocating
0.00.052.663 I ggml_metal_init: found device: Apple M4
0.00.052.665 I ggml_metal_init: picking default device: Apple M4
0.00.053.206 I ggml_metal_init: using embedded metal library
0.00.055.545 I ggml_metal_init: GPU name:   Apple M4
0.00.055.547 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.547 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.547 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.548 I ggml_metal_init: simdgroup reduction   = true
0.00.055.548 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.548 I ggml_metal_init: has bfloat            = true
0.00.055.549 I ggml_metal_init: use bfloat            = true
0.00.055.550 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.550 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.321 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.323 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.336 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.209 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.210 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.210 I llama_new_context_with_model: graph nodes  = 967
0.00.067.210 I llama_new_context_with_model: graph splits = 2
0.00.067.223 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.574.815 I 
0.00.574.845 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.574.864 I perplexity: tokenizing the input ..
0.00.582.494 I perplexity: tokenization took 7.628 ms
0.00.582.505 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.716.269 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.717.670 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.717.686 I llama_perf_context_print:        load time =     565.40 ms
0.00.717.687 I llama_perf_context_print: prompt eval time =     133.54 ms /   128 tokens (    1.04 ms per token,   958.53 tokens per second)
0.00.717.688 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.717.688 I llama_perf_context_print:       total time =     142.87 ms /   129 tokens
0.00.718.150 I ggml_metal_free: deallocating

real	0m0.734s
user	0m0.078s
sys	0m0.106s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.640 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.713 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.718 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.719 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.720 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.720 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.721 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.721 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.729 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.730 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.731 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.731 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.732 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.732 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.734 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.734 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.734 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.670 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.626 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.627 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.628 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.628 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.628 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.628 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.629 I llama_model_loader: - type  f32:  194 tensors
0.00.024.629 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.629 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.604 I llm_load_vocab: special tokens cache size = 25
0.00.051.626 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.629 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.629 I llm_load_print_meta: arch             = gptneox
0.00.051.629 I llm_load_print_meta: vocab type       = BPE
0.00.051.629 I llm_load_print_meta: n_vocab          = 50304
0.00.051.630 I llm_load_print_meta: n_merges         = 50009
0.00.051.630 I llm_load_print_meta: vocab_only       = 0
0.00.051.630 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.630 I llm_load_print_meta: n_embd           = 2048
0.00.051.630 I llm_load_print_meta: n_layer          = 24
0.00.051.633 I llm_load_print_meta: n_head           = 16
0.00.051.634 I llm_load_print_meta: n_head_kv        = 16
0.00.051.634 I llm_load_print_meta: n_rot            = 32
0.00.051.634 I llm_load_print_meta: n_swa            = 0
0.00.051.635 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.637 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.638 I llm_load_print_meta: n_gqa            = 1
0.00.051.639 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.639 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.640 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.640 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.641 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.641 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.641 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.642 I llm_load_print_meta: n_ff             = 8192
0.00.051.642 I llm_load_print_meta: n_expert         = 0
0.00.051.642 I llm_load_print_meta: n_expert_used    = 0
0.00.051.642 I llm_load_print_meta: causal attn      = 1
0.00.051.642 I llm_load_print_meta: pooling type     = 0
0.00.051.642 I llm_load_print_meta: rope type        = 2
0.00.051.643 I llm_load_print_meta: rope scaling     = linear
0.00.051.643 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.643 I llm_load_print_meta: freq_scale_train = 1
0.00.051.643 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.643 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.644 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.644 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.644 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.644 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.644 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.655 I llm_load_print_meta: model type       = 1.4B
0.00.051.656 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.656 I llm_load_print_meta: model params     = 1.41 B
0.00.051.658 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.658 I llm_load_print_meta: general.name     = 1.4B
0.00.051.658 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.658 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.659 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.659 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.660 I llm_load_print_meta: LF token         = 128 ''
0.00.051.660 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.660 I llm_load_print_meta: max token length = 1024
0.00.053.185 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.185 I llm_load_tensors: offloading output layer to GPU
0.00.053.185 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.195 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.196 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.020 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.021 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.021 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.022 I llama_new_context_with_model: n_batch       = 2048
0.00.054.022 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.022 I llama_new_context_with_model: flash_attn    = 0
0.00.054.023 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.023 I llama_new_context_with_model: freq_scale    = 1
0.00.054.023 I ggml_metal_init: allocating
0.00.054.030 I ggml_metal_init: found device: Apple M4
0.00.054.032 I ggml_metal_init: picking default device: Apple M4
0.00.054.609 I ggml_metal_init: using embedded metal library
0.00.056.901 I ggml_metal_init: GPU name:   Apple M4
0.00.056.902 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.903 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.903 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.903 I ggml_metal_init: simdgroup reduction   = true
0.00.056.903 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.904 I ggml_metal_init: has bfloat            = true
0.00.056.904 I ggml_metal_init: use bfloat            = true
0.00.056.904 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.905 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.660 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.664 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.682 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.603 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.604 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.604 I llama_new_context_with_model: graph nodes  = 967
0.00.086.605 I llama_new_context_with_model: graph splits = 2
0.00.086.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.087 I main: llama threadpool init, n_threads = 4
0.00.702.131 I 
0.00.702.186 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.702.187 I 
0.00.702.436 I sampler seed: 1234
0.00.702.441 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.702.483 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.702.487 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.702.487 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.551.180 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60118.54 tokens per second)
0.01.551.181 I llama_perf_context_print:        load time =     693.44 ms
0.01.551.181 I llama_perf_context_print: prompt eval time =      51.61 ms /     7 tokens (    7.37 ms per token,   135.64 tokens per second)
0.01.551.183 I llama_perf_context_print:        eval time =     794.12 ms /    63 runs   (   12.61 ms per token,    79.33 tokens per second)
0.01.551.184 I llama_perf_context_print:       total time =     849.10 ms /    70 tokens
0.01.551.360 I ggml_metal_free: deallocating

real	0m1.566s
user	0m0.110s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.468 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.350 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.353 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.354 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.355 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.362 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.362 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.362 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.363 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.363 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.364 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.364 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.366 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.366 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.366 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.174 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.201 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.028 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.030 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.030 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.030 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.030 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.031 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.031 I llama_model_loader: - type  f32:  194 tensors
0.00.023.032 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.032 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.805 I llm_load_vocab: special tokens cache size = 25
0.00.049.823 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.825 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.826 I llm_load_print_meta: arch             = gptneox
0.00.049.826 I llm_load_print_meta: vocab type       = BPE
0.00.049.826 I llm_load_print_meta: n_vocab          = 50304
0.00.049.826 I llm_load_print_meta: n_merges         = 50009
0.00.049.827 I llm_load_print_meta: vocab_only       = 0
0.00.049.827 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.827 I llm_load_print_meta: n_embd           = 2048
0.00.049.827 I llm_load_print_meta: n_layer          = 24
0.00.049.829 I llm_load_print_meta: n_head           = 16
0.00.049.830 I llm_load_print_meta: n_head_kv        = 16
0.00.049.830 I llm_load_print_meta: n_rot            = 32
0.00.049.831 I llm_load_print_meta: n_swa            = 0
0.00.049.831 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.831 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.832 I llm_load_print_meta: n_gqa            = 1
0.00.049.832 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.833 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.834 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.834 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.834 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.834 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.835 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.835 I llm_load_print_meta: n_ff             = 8192
0.00.049.835 I llm_load_print_meta: n_expert         = 0
0.00.049.836 I llm_load_print_meta: n_expert_used    = 0
0.00.049.836 I llm_load_print_meta: causal attn      = 1
0.00.049.836 I llm_load_print_meta: pooling type     = 0
0.00.049.836 I llm_load_print_meta: rope type        = 2
0.00.049.836 I llm_load_print_meta: rope scaling     = linear
0.00.049.837 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.837 I llm_load_print_meta: freq_scale_train = 1
0.00.049.837 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.837 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.837 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.838 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.838 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.838 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.838 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.850 I llm_load_print_meta: model type       = 1.4B
0.00.049.850 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.850 I llm_load_print_meta: model params     = 1.41 B
0.00.049.851 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.851 I llm_load_print_meta: general.name     = 1.4B
0.00.049.851 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.853 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.853 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.854 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.854 I llm_load_print_meta: LF token         = 128 ''
0.00.049.854 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.855 I llm_load_print_meta: max token length = 1024
0.00.051.891 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.892 I llm_load_tensors: offloading output layer to GPU
0.00.051.892 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.902 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.904 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.802 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.803 I llama_new_context_with_model: n_ctx         = 128
0.00.052.803 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.804 I llama_new_context_with_model: n_batch       = 128
0.00.052.804 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.804 I llama_new_context_with_model: flash_attn    = 0
0.00.052.804 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.805 I llama_new_context_with_model: freq_scale    = 1
0.00.052.805 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.805 I ggml_metal_init: allocating
0.00.052.811 I ggml_metal_init: found device: Apple M4
0.00.052.813 I ggml_metal_init: picking default device: Apple M4
0.00.053.370 I ggml_metal_init: using embedded metal library
0.00.055.681 I ggml_metal_init: GPU name:   Apple M4
0.00.055.683 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.683 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.683 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.684 I ggml_metal_init: simdgroup reduction   = true
0.00.055.684 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.684 I ggml_metal_init: has bfloat            = true
0.00.055.684 I ggml_metal_init: use bfloat            = true
0.00.055.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.192 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.194 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.207 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.078 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.079 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.079 I llama_new_context_with_model: graph nodes  = 967
0.00.067.079 I llama_new_context_with_model: graph splits = 2
0.00.067.092 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.910 I 
0.00.655.940 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.655.953 I perplexity: tokenizing the input ..
0.00.663.438 I perplexity: tokenization took 7.483 ms
0.00.663.451 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.814 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.805.250 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.805.260 I llama_perf_context_print:        load time =     647.44 ms
0.00.805.261 I llama_perf_context_print: prompt eval time =     140.14 ms /   128 tokens (    1.09 ms per token,   913.39 tokens per second)
0.00.805.262 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.262 I llama_perf_context_print:       total time =     149.35 ms /   129 tokens
0.00.805.566 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.079s
sys	0m0.123s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.777 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.887 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.891 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.893 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.893 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.894 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.894 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.894 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.901 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.902 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.902 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.902 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.903 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.903 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.903 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.905 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.905 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.906 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.713 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.746 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.578 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.579 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.580 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.580 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.580 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.580 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.581 I llama_model_loader: - type  f32:  194 tensors
0.00.025.581 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.750 I llm_load_vocab: special tokens cache size = 25
0.00.051.645 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.647 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.648 I llm_load_print_meta: arch             = gptneox
0.00.051.648 I llm_load_print_meta: vocab type       = BPE
0.00.051.648 I llm_load_print_meta: n_vocab          = 50304
0.00.051.648 I llm_load_print_meta: n_merges         = 50009
0.00.051.649 I llm_load_print_meta: vocab_only       = 0
0.00.051.649 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.649 I llm_load_print_meta: n_embd           = 2048
0.00.051.649 I llm_load_print_meta: n_layer          = 24
0.00.051.652 I llm_load_print_meta: n_head           = 16
0.00.051.652 I llm_load_print_meta: n_head_kv        = 16
0.00.051.653 I llm_load_print_meta: n_rot            = 32
0.00.051.653 I llm_load_print_meta: n_swa            = 0
0.00.051.653 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.653 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.655 I llm_load_print_meta: n_gqa            = 1
0.00.051.656 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.657 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.657 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.657 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.658 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.658 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.658 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.659 I llm_load_print_meta: n_ff             = 8192
0.00.051.660 I llm_load_print_meta: n_expert         = 0
0.00.051.660 I llm_load_print_meta: n_expert_used    = 0
0.00.051.660 I llm_load_print_meta: causal attn      = 1
0.00.051.661 I llm_load_print_meta: pooling type     = 0
0.00.051.661 I llm_load_print_meta: rope type        = 2
0.00.051.661 I llm_load_print_meta: rope scaling     = linear
0.00.051.661 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.662 I llm_load_print_meta: freq_scale_train = 1
0.00.051.662 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.662 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.662 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.662 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.664 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.664 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.664 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.671 I llm_load_print_meta: model type       = 1.4B
0.00.051.671 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.671 I llm_load_print_meta: model params     = 1.41 B
0.00.051.672 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.672 I llm_load_print_meta: general.name     = 1.4B
0.00.051.672 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.672 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.672 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: LF token         = 128 ''
0.00.051.673 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: max token length = 1024
0.00.053.459 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.460 I llm_load_tensors: offloading output layer to GPU
0.00.053.460 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.465 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.466 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.342 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.343 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.343 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.343 I llama_new_context_with_model: n_batch       = 2048
0.00.054.344 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.344 I llama_new_context_with_model: flash_attn    = 0
0.00.054.344 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.344 I llama_new_context_with_model: freq_scale    = 1
0.00.054.345 I ggml_metal_init: allocating
0.00.054.351 I ggml_metal_init: found device: Apple M4
0.00.054.354 I ggml_metal_init: picking default device: Apple M4
0.00.054.904 I ggml_metal_init: using embedded metal library
0.00.057.226 I ggml_metal_init: GPU name:   Apple M4
0.00.057.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.228 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.228 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.228 I ggml_metal_init: simdgroup reduction   = true
0.00.057.229 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.229 I ggml_metal_init: has bfloat            = true
0.00.057.229 I ggml_metal_init: use bfloat            = true
0.00.057.229 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.230 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.676 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.682 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.701 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.671 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.672 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.673 I llama_new_context_with_model: graph nodes  = 967
0.00.087.673 I llama_new_context_with_model: graph splits = 2
0.00.087.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.848 I main: llama threadpool init, n_threads = 4
0.00.772.891 I 
0.00.772.931 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.772.932 I 
0.00.773.180 I sampler seed: 1234
0.00.773.187 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.236 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.236 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.236 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.657.886 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.657.886 I llama_perf_context_print:        load time =     763.06 ms
0.01.657.887 I llama_perf_context_print: prompt eval time =      54.43 ms /     7 tokens (    7.78 ms per token,   128.60 tokens per second)
0.01.657.888 I llama_perf_context_print:        eval time =     827.23 ms /    63 runs   (   13.13 ms per token,    76.16 tokens per second)
0.01.657.888 I llama_perf_context_print:       total time =     885.04 ms /    70 tokens
0.01.658.072 I ggml_metal_free: deallocating

real	0m1.674s
user	0m0.109s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4257 (05837cfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.727 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.310 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.314 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.317 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.317 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.318 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.318 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.318 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.325 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.325 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.326 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.326 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.327 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.055 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.035 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.789 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.790 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.790 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.791 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.791 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.791 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.792 I llama_model_loader: - type  f32:  194 tensors
0.00.023.792 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.718 I llm_load_vocab: special tokens cache size = 25
0.00.049.566 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.569 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.569 I llm_load_print_meta: arch             = gptneox
0.00.049.569 I llm_load_print_meta: vocab type       = BPE
0.00.049.569 I llm_load_print_meta: n_vocab          = 50304
0.00.049.570 I llm_load_print_meta: n_merges         = 50009
0.00.049.570 I llm_load_print_meta: vocab_only       = 0
0.00.049.570 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.570 I llm_load_print_meta: n_embd           = 2048
0.00.049.570 I llm_load_print_meta: n_layer          = 24
0.00.049.573 I llm_load_print_meta: n_head           = 16
0.00.049.574 I llm_load_print_meta: n_head_kv        = 16
0.00.049.574 I llm_load_print_meta: n_rot            = 32
0.00.049.574 I llm_load_print_meta: n_swa            = 0
0.00.049.574 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.574 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.575 I llm_load_print_meta: n_gqa            = 1
0.00.049.576 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.576 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.577 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.577 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.577 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.578 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.578 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.578 I llm_load_print_meta: n_ff             = 8192
0.00.049.579 I llm_load_print_meta: n_expert         = 0
0.00.049.579 I llm_load_print_meta: n_expert_used    = 0
0.00.049.579 I llm_load_print_meta: causal attn      = 1
0.00.049.579 I llm_load_print_meta: pooling type     = 0
0.00.049.579 I llm_load_print_meta: rope type        = 2
0.00.049.580 I llm_load_print_meta: rope scaling     = linear
0.00.049.580 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.580 I llm_load_print_meta: freq_scale_train = 1
0.00.049.580 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.581 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.581 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.581 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.581 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.581 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.581 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.595 I llm_load_print_meta: model type       = 1.4B
0.00.049.595 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.595 I llm_load_print_meta: model params     = 1.41 B
0.00.049.596 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.596 I llm_load_print_meta: general.name     = 1.4B
0.00.049.596 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.596 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.596 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.597 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.597 I llm_load_print_meta: LF token         = 128 ''
0.00.049.597 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.597 I llm_load_print_meta: max token length = 1024
0.00.051.207 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.207 I llm_load_tensors: offloading output layer to GPU
0.00.051.208 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.218 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.219 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.069 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.070 I llama_new_context_with_model: n_ctx         = 128
0.00.052.070 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.071 I llama_new_context_with_model: n_batch       = 128
0.00.052.071 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.071 I llama_new_context_with_model: flash_attn    = 0
0.00.052.071 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.072 I llama_new_context_with_model: freq_scale    = 1
0.00.052.072 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.072 I ggml_metal_init: allocating
0.00.052.075 I ggml_metal_init: found device: Apple M4
0.00.052.077 I ggml_metal_init: picking default device: Apple M4
0.00.052.595 I ggml_metal_init: using embedded metal library
0.00.054.884 I ggml_metal_init: GPU name:   Apple M4
0.00.054.886 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.886 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.886 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.887 I ggml_metal_init: simdgroup reduction   = true
0.00.054.887 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.887 I ggml_metal_init: has bfloat            = true
0.00.054.887 I ggml_metal_init: use bfloat            = true
0.00.054.887 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.888 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.542 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.547 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.567 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.427 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.428 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.428 I llama_new_context_with_model: graph nodes  = 967
0.00.066.428 I llama_new_context_with_model: graph splits = 2
0.00.066.440 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.376.376 I 
0.00.376.437 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.376.462 I perplexity: tokenizing the input ..
0.00.384.175 I perplexity: tokenization took 7.711 ms
0.00.384.187 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.523.821 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.525.164 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.525.184 I llama_perf_context_print:        load time =     366.64 ms
0.00.525.185 I llama_perf_context_print: prompt eval time =     139.39 ms /   128 tokens (    1.09 ms per token,   918.27 tokens per second)
0.00.525.186 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.525.187 I llama_perf_context_print:       total time =     148.82 ms /   129 tokens
0.00.525.672 I ggml_metal_free: deallocating

real	0m0.542s
user	0m0.078s
sys	0m0.086s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4257 (05837cfa)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145b0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145b0a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145b0af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145b0b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145b0baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145b0c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145b0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145b0cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145b0d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145b0d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145b0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145b0e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145b0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145b0f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145b0fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145b102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145b109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145b110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145b11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145b11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145b12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145b12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145b13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145b13de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145b14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145b147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145b14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145b15a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145b15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145b16240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145b166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145b169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145b17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145b17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145b17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145b17ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145b18370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145b18810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145b18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145b19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145b195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145b19a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145b19f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145b1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145b1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145b1aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145b1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145b1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145b1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145b1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145b1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145b1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145b1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145b1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145b1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145b1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145b1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145b1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145b1fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145b20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145b204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145b20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145b20e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145b212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145b21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145b21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145b220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145b22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145b229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145b22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145b23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145b237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145b23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145b241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145b24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145b24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145b251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145b256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145b25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145b26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145b266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145b26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145b27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145b276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145b27c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145b28170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145b286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145b28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145b29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145b296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145b29c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145b2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145b2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145b2abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145b2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145b2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145b2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145b1b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145b2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145b2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145b2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145b2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145b2d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145b2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145b2e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145b2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145b2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145b2f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145b2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145b2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145b30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145b307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145b30d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145b311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145b31650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145b31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145b31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145b32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145b328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145b32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145b33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145b336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145b33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145b33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145b34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145b34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145b34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145b35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145b35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145b35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145b36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145b364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145b36990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145b36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145b372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145b37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145b37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145b380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145b38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145b389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145b38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145b39330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145b397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145b39c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145b3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145b3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145b3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145b3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145b3b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145b3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145b3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145b3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145b3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145b3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145b3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145b3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145b3d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145b3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145b3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145b3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145b3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145b3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145b3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145b3f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145b3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145b40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145b406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145b40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145b41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145b414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145b41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145b41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145b42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145b42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145b42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145b43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145b43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145b439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145b43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145b442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145b44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145b44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145b450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145b45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145b45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145b45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145b46350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145b467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145b46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145b47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145b475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145b47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145b47f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145b48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145b489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145b48f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145b49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145b49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145b49d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145b4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145b4a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145b4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145b4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145b4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145b4bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145b4c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145b4cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145b4cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145b4d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145b4dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145b4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145b4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145b4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145b4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145b4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145b4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145b50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145b506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145b50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145b51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145b51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145b51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145b52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145b52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145b52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145b53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145b53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145b53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145b54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145b54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145b54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145b55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145b55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145b55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145b560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145b56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145b56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145b570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145b57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145b57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145b580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145b58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145b58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145b590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145b59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145b59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145b5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145b5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145b5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145b5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145b5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145b5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145b5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145b5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145b5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145b5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145b5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145b5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145b5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145b5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145b5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145b5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145b5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145b5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145b60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145b605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145b60a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145b60ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145b61380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145b61820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145b61cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145b62160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145b62600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145b62aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145b62f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145b633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145b63880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145b63d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145b641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145b64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145b64e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145b65550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145b65c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145b66390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145b66650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145b66e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145b67100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145b67710 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.139.933 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145b0db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145b0df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145b0e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145b0e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145b0ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145b0f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145b0f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145b0fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145b0fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145b10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145b10780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145b10bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145b114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145b11c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145b12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145b12b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145b13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145b13910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145b14000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145b14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145b15070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145b15760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145b15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145b16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145b16c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145b170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145b17510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145b17980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145b17df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145b18260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145b186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145b18b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145b18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145b19270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145b196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145b19b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145b19fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145b1a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145b1a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145b1ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145b1b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145b1b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145b1ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145b1bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145b1c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145b1c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145b1cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145b1d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145b1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145b1d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145b1dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145b1e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145b1e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145b1eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145b1efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145b1f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145b1f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145b1fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145b20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145b205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145b20a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145b20eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145b21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145b21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145b21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145b22070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145b224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145b22950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145b22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145b23230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145b236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145b23b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145b23f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145b243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145b24860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145b24cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145b25140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145b255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145b25a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145b25e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145b26300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145b26770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145b26be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145b27050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145b274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145b27930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145b27da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145b28210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145b28680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145b28af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145b28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145b293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145b29840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145b29cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145b2a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145b2a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145b2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145b2ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145b2b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145b2b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145b2bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145b2c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145b2c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145b2c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145b2cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145b2d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145b2d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145b2dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145b2df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145b2e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145b2e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145b2ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145b2f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145b2f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145b2f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145b2fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145b302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145b30730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145b30ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145b31010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145b31480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145b318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145b31d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145b321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145b32640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145b32ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145b32f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145b33390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145b33800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145b33c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145b340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145b34550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145b349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145b34e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145b352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145b35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145b35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145b35ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145b36460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145b368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145b36d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145b371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145b37620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145b37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145b37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145b38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145b387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145b38c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145b390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145b39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145b399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145b39e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145b3a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145b3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145b3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145b3afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145b3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145b3b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145b3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145b3c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145b3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145b3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145b3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145b3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145b3d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145b3dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145b3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145b3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145b3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145b3edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145b3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145b3f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145b3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145b3ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145b40420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145b40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145b40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145b41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145b415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145b41a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145b41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145b42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145b427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145b42c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145b43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145b434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145b43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145b43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145b44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145b446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145b44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145b44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145b45400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145b45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145b45ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145b46150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145b465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145b46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145b46ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145b47310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145b47780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145b47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145b48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145b484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145b48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145b48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145b49220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145b49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145b49b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145b4a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145b4a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145b4ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145b4afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145b4b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145b4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145b4bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145b4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145b4c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145b4ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145b4cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145b4d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145b4d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145b4dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145b4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145b4e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145b4e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145b4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145b4f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145b4f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145b4fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145b4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145b50420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145b50890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145b50d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145b51170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145b515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145b51a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145b51ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145b52330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145b527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145b52c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145b53080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145b534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145b53960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145b53dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145b54240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145b546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145b54b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145b54f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145b55400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145b55870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145b55ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145b56150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145b565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145b56a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145b56ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145b57310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145b57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145b57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145b58060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145b584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145b58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145b58db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145b59220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145b59690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145b59b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145b59f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145b5a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145b5a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145b5acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145b5b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145b5b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145b5ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145b5be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145b5c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145b5c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145b5cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145b5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145b5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145b5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145b5e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145b5e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145b5edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145b5f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145b5f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145b5fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145b60230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145b606a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145b0db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145b0df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145b0e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145b0e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145b0ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145b0f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145b0f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145b0fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145b0fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145b10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145b10780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145b10bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145b114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145b11c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145b12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145b12b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145b13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145b13910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145b14000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145b14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145b15070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145b15760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145b15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145b16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145b16c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145b170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145b17510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145b17980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145b17df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145b18260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145b186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145b18b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145b18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145b19270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145b196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145b19b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145b19fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145b1a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145b1a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145b1ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145b1b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145b1b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145b1ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145b1bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145b1c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145b1c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145b1cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145b1d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145b1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145b1d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145b1dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145b1e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145b1e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145b1eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145b1efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145b1f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145b1f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145b1fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145b20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145b205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145b20a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145b20eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145b21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145b21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145b21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145b22070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145b224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145b22950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145b22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145b23230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145b236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145b23b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145b23f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145b243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145b24860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145b24cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145b25140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145b255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145b25a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145b25e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145b26300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145b26770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145b26be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145b27050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145b274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145b27930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145b27da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145b28210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145b28680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145b28af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145b28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145b293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145b29840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145b29cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145b2a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145b2a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145b2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145b2ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145b2b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145b2b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145b2bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145b2c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145b2c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145b2c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145b2cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145b2d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145b2d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145b2dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145b2df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145b2e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145b2e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145b2ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145b2f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145b2f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145b2f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145b2fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145b302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145b30730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145b30ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145b31010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145b31480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145b318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145b31d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145b321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145b32640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145b32ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145b32f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145b33390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145b33800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145b33c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145b340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145b34550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145b349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145b34e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145b352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145b35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145b35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145b35ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145b36460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145b368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145b36d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145b371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145b37620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145b37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145b37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145b38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145b387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145b38c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145b390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145b39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145b399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145b39e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145b3a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145b3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145b3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145b3afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145b3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145b3b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145b3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145b3c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145b3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145b3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145b3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145b3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145b3d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145b3dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145b3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145b3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145b3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145b3edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145b3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145b3f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145b3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145b3ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145b40420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145b40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145b40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145b41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145b415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145b41a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145b41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145b42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145b427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145b42c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145b43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145b434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145b43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145b43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145b44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145b446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145a05590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145a05850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145a06130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145a063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145a06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145a06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145a07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145a075b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145a07a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145a07e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145a04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145a046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145a04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145a08460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145a088d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145a08d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145a091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145a096e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145a09bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145a0a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145a0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145a0afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145a0b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145a0bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145a0c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145a0c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145a0cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145a0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145a0d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145a0dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145a0e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145a0e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145a0ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145a0f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145a0faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145a10060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145a10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145a10be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145a111a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145a11760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145a11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145a122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145a128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145a12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145a13420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145a139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145a13fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145a14560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145a14b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145a150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145a156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145a15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145a16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145a167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145a16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145a17360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145a17920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145a17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145a184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145a18a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145a19020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145a195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145a19ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145a1a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145a1a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145a1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145a1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145a1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145a1be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145a1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145a1c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145a1cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145a1d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145a1dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145a1e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145a1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145a1ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145a1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145a1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145a1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145a20020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145a20520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145a20a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145a20f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145a21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145a21920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145a21e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145a22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145a22820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145a22d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145a23730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145a23e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145a24570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145a24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145a24f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145a25740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145a25a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145a26010 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.823s
user	0m0.292s
sys	0m0.300s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4257 (05837cfa)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15660c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15660c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15660cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15660d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15660d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15660de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15660e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15660e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15660ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15660f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15660f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15660fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156610940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1566110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156611900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156612020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156612740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156612e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156613580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156613d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156614470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156614b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1566152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156615b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156616270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156616530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156616b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1566177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156617cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156617fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156618450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156618710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156618fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1566194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1566197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156619c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15661a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15661a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15661aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15661aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15661b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15661b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15661bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15661c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15661c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15661ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15661d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15661d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15661df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15661e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15661eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15661f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15661f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15661fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156620590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156620a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156620ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156621190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1566217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156621f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156622250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1566226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156622b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x156623030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1566234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156623970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156623e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1566242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156624750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156624bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156625090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156625530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1566259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156625f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156626470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1566269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156626f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156627460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1566279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156627f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156628450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1566289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156628ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156629440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156629990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156629ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15662a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15662a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15662aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15662b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15662b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15662bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15662c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15662c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15662ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15662d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15662d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15661d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15662ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15662e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15662eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15662f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15662f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15662fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156630000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156630550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156630aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156630ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156631540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156631a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156631fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156632530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156632a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156632f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1566333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156633860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156633d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1566341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156634640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156634ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156634f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156635420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1566358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156635d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156636200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1566366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156636b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156636fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156637480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156637920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156637dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156638260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156638700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156638ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156639040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1566394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156639980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156639e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15663a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15663a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15663ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15663b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15663b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15663b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15663be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15663c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15663c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15663cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15663d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15663d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15663da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15663dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15663e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15663e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15663ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15663f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15663f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15663faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15663ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1566403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156640880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156640d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1566411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156641660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156641b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156641fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156642440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1566428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156642d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156643220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1566436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156643b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156644000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1566444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156644940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156644de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156645280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156645720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156645bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156646060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156646500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1566469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156646e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1566472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156647780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156647c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1566480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156648560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156648a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156648ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156649340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1566497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156649c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15664a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15664a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15664ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15664b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15664b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15664ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15664c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15664c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15664cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15664d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15664d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15664dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15664e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15664e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15664ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15664f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15664f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15664fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156650430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156650980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156650ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156651420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156651970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156651ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156652410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156652960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156652eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156653400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156653950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156653ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1566543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156654940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156654e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1566553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156655930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156655e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1566563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156656920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156656e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1566573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156657910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156657e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1566583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156658900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156658e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1566593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1566598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156659e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15665a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15665a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15665ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15665b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15665b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15665be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15665c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15665c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15665ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15665d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15665d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15665de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15665e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15665e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15665edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15665f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15665f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15665fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156660330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156660880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156660dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156661320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156661870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156661dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156662310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1566627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156662c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1566630f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156663590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156663a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156663ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156664370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156664810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156664cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156665150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1566655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156665a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156665f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156666480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156666ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1566672c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1566679e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156668100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1566683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156668bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156668e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156669480 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.646 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x163f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x163f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x163f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x163f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x163f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x163f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x163f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x163f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x163f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x163f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x163f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x163f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x163f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x163f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x163f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x163f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x163f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x163f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x163f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x163f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x163f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x163f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x163f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x163f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x163f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x163f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x163f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x163f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x163f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x163f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x163f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x163f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x163f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x163f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x163f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x163f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x163f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x163f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x163f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x163f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x163f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x163f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x163f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x163f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x163f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x163f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x163f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x163f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x163f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x163f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x163f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x163f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x163f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x163f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x163f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x163f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x163f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x163f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x163f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x163f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x163f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x163f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x163f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x163f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x163f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x163f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x163f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x163f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x163f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x163f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x163f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x163f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x163f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x163f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x163f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x163f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x163f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x163f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x163f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x163f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x163f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x163f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x163f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x163f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x163f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x163f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x163f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x163f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x163f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x163f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x163f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x163f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x163f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x163f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x163f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x163f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x163f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x163f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x163f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x163f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x163f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x163f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x163f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x163f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x163f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x163f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x163f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x163f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x163f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x163f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x163f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x163f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x163f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x163f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x163f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x163f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x163f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x163f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x163f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x163f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x163f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x163f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x163f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x163f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x163f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x163f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x163f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x163f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x163f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x163f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x163f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x163f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x163f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x163f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x163f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x163f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x163f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x163f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x163f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x163f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x163f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x163f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x163f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x163f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x163f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x163f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x163f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x163f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x163f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x163f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x163f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x163f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x163f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x163f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x163f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x163f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x163f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x163f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x163f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x163f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x163f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x163f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x163f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x163f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x163f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x163f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x163f35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x163f35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x163f35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x163f36150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x163f365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x163f36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x163f36ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x163f37310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x163f37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x163f37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x163f38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x163f384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x163f38940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x163f38db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x163f39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x163f39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x163f39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x163f39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x163f3a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x163f3a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x163f3acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x163f3b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x163f3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x163f3ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x163f3be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x163f3c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x163f3c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x163f3cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x163f3d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x163f3d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x163f3d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x163f3dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x163f3e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x163f3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x163f3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x163f3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x163f3f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x163f3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x163f3fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x163f40110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x163f40580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x163f409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x163f40e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x163f419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x163f41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x163f41f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x163f423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x163f42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x163f42ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x163f43110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x163f43580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x163f439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x163f43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x163f442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x163f44740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x163f44bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x163f45020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x163f45490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x163f45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x163f45d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x163f461e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x163f46650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x163f46ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x163f46f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x163f473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x163f47810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x163f47c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x163f480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x163f48560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x163f489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x163f48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x163f492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x163f49720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x163f49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x163f4a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x163f4a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x163f4a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x163f4ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x163f4b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x163f4b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x163f4baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x163f4bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x163f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x163f4c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x163f4cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x163f4d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x163f4d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x163f4d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x163f4de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x163f4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x163f4e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x163f4eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x163f4efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x163f4f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x163f4f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x163f4fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x163f501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x163f50610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x163f50a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x163f50ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x163f51360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x163f517d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x163f51c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x163f520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x163f52520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x163f52990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x163f52e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x163f53270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x163f536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x163f53b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x163f53fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x163f54430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x163f548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x163f54d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x163f55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x163f55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x163f56690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x163f56db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x163f57070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x163f57330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x163f577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x163f57c10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x163f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x163f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x163f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x163f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x163f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x163f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x163f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x163f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x163f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x163f072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x163f07740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x163f07d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x163f08610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x163f08d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x163f09570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x163f09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x163f0a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x163f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x163f0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x163f0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x163f0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x163f0c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x163f0cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x163f0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x163f0dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x163f0e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x163f0e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x163f0eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x163f0ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x163f0f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x163f0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x163f0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x163f100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x163f103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x163f10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x163f10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x163f110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x163f11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x163f119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x163f11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x163f122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x163f12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x163f12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x163f13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x163f13470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x163f138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x163f13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x163f141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x163f14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x163f14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x163f14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x163f15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x163f157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x163f15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x163f160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x163f16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x163f169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x163f16e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x163f17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x163f17700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x163f17b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x163f17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x163f18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x163f188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x163f18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x163f191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x163f19610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x163f19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x163f19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x163f1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x163f1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x163f1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x163f1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x163f1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x163f1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x163f1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x163f1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x163f1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x163f1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x163f1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x163f1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x163f1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x163f1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x163f1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x163f1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x163f1ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x163f1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x163f1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x163f1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x163f1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x163f20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x163f20500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x163f20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x163f20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x163f21250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x163f216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x163f21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x163f21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x163f22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x163f22880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x163f22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x163f23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x163f235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x163f23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x163f23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x163f24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x163f24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x163f24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x163f25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x163f254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x163f25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x163f25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x163f26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x163f266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x163f26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x163f26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x163f273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x163f27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x163f27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x163f28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x163f285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x163f28a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x163f28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x163f29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x163f29770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x163f29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x163f2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x163f2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x163f2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x163f2ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x163f2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x163f2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x163f2baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x163f2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x163f2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x163f2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x163f2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x163f2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x163f2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x163f2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x163f2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x163f2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x163f2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x163f2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x163f2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x163f2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x163f2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x163f2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x163f301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x163f30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x163f30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x163f30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x163f313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x163f31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x163f31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x163f32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x163f32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x163f329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x163f32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x163f332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x163f33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x163f33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x163f34010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x163f34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x163f348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x163f34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x163f351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x163f35640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x163f35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x163f35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x163f36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x163f36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x163f36c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x163f370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x163f37550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x163f379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x163f37e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x163f382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x163f38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x163f38b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x163f38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x163f39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x163f398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x163f39d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x163f3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x163f3a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x163f3aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x163f3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x163f3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x163f3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x163f3bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x163f3c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x163f3c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x163f3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x163f3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x163f3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x163f3d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x163f3db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x163f3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x163f3e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x163f3e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x163f3ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x163f3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x163f3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x163f3fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x163f3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x163f40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x163f407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x163f40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x163f413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x163f41820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x163f41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x163f42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x163f42570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x163f429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x163f42e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x163f432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x163f43730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x163f43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x163f44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x163f44480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x163f448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x163f44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x163f451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x163f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x163f45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x163f45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x163f46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x163f46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x163f46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x163f470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x163f47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x163f479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x163f47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x163f482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x163f48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x163f48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x163f48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x163f49460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x163f498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x163f49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x163f4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x163f4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x163f4aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x163f4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x163f4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x163f4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x163f4bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x163f4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x163f4c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x163f4c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x163f4ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x163f4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x163f4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x163f4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x163f4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x163f4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x163f4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x163f4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x163f4f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x163f4f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x163f4fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x163f4fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x163f50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x163f507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x163f50c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x163f510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x163f51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x163f51980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x163f51df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x163f52260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x163f526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x163f52b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x163f52fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x163f53420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x163f53890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x163f53d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x163f54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x163f545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x163f54a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x163f552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x163f559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x163f56090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x163f56780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x163f56bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x163f57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x163f574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x163f57940 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.923s
user	0m0.245s
sys	0m0.131s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.55 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.17 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.29 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.56 sec*proc (2 tests)

Total Test time (real) =   0.57 sec
        0.58 real         0.16 user         0.05 sys
```
