Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.080s
user	0m1.018s
sys	0m1.475s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Built target build_info
[  4%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 29%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 29%] Built target llava
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 35%] Built target llama-quantize-stats
[ 35%] Built target test-c
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-sampling
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 56%] Linking CXX executable ../bin/test-autorelease
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Built target test-arg-parser
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-autorelease
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-barrier
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Built target test-quantize-perf
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Built target test-quantize-fns
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target test-rope
[ 69%] Built target llama-batched-bench
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-batched
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Built target llama-imatrix
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-bench
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Built target llama-gritlm
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Built target llama-lookahead
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup-stats
[ 80%] Generating loading.html.hpp
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-cli
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Built target llama-parallel
[ 84%] Built target llama-passkey
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Built target llama-perplexity
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Built target llama-quantize
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-gen-docs
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-save-load-state
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-run
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Built target llama-cvector-generator
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.200s
user	0m6.544s
sys	0m9.903s

main: quantize time =  7607.76 ms
main:    total time =  7607.76 ms

main: quantize time =  3368.38 ms
main:    total time =  3368.38 ms

main: quantize time =  4558.10 ms
main:    total time =  4558.10 ms

main: quantize time =  3361.88 ms
main:    total time =  3361.88 ms

main: quantize time =  2953.49 ms
main:    total time =  2953.49 ms

main: quantize time =  6247.33 ms
main:    total time =  6247.33 ms

main: quantize time =  5495.52 ms
main:    total time =  5495.52 ms

main: quantize time =  6603.90 ms
main:    total time =  6603.90 ms

main: quantize time =  5663.05 ms
main:    total time =  5663.05 ms

main: quantize time =  4367.26 ms
main:    total time =  4367.26 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.291 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.544 I main: llama backend init
0.00.000.558 I main: load the model and apply lora adapter, if any
0.00.087.707 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.100.688 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.100.718 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.100.723 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.100.724 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.100.725 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.100.742 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.100.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.100.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.100.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.100.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.100.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.100.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.100.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.100.757 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.100.764 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.100.768 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.100.768 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.108.097 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.110.574 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.119.727 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.119.732 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.119.732 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.119.733 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.119.733 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.119.735 I llama_model_loader: - type  f32:  194 tensors
0.00.119.735 I llama_model_loader: - type  f16:   98 tensors
0.00.119.737 I print_info: file format = GGUF V3 (latest)
0.00.119.738 I print_info: file type   = all F32 (guessed)
0.00.119.740 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.135.312 I load: special tokens cache size = 25
0.00.144.637 I load: token to piece cache size = 0.2984 MB
0.00.144.642 I print_info: arch             = gptneox
0.00.144.642 I print_info: vocab_only       = 0
0.00.144.643 I print_info: n_ctx_train      = 2048
0.00.144.643 I print_info: n_embd           = 2048
0.00.144.643 I print_info: n_layer          = 24
0.00.144.649 I print_info: n_head           = 16
0.00.144.649 I print_info: n_head_kv        = 16
0.00.144.650 I print_info: n_rot            = 32
0.00.144.650 I print_info: n_swa            = 0
0.00.144.650 I print_info: n_embd_head_k    = 128
0.00.144.650 I print_info: n_embd_head_v    = 128
0.00.144.653 I print_info: n_gqa            = 1
0.00.144.654 I print_info: n_embd_k_gqa     = 2048
0.00.144.655 I print_info: n_embd_v_gqa     = 2048
0.00.144.656 I print_info: f_norm_eps       = 1.0e-05
0.00.144.656 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.144.656 I print_info: f_clamp_kqv      = 0.0e+00
0.00.144.656 I print_info: f_max_alibi_bias = 0.0e+00
0.00.144.657 I print_info: f_logit_scale    = 0.0e+00
0.00.144.658 I print_info: n_ff             = 8192
0.00.144.658 I print_info: n_expert         = 0
0.00.144.658 I print_info: n_expert_used    = 0
0.00.144.658 I print_info: causal attn      = 1
0.00.144.658 I print_info: pooling type     = 0
0.00.144.658 I print_info: rope type        = 2
0.00.144.659 I print_info: rope scaling     = linear
0.00.144.659 I print_info: freq_base_train  = 10000.0
0.00.144.660 I print_info: freq_scale_train = 1
0.00.144.660 I print_info: n_ctx_orig_yarn  = 2048
0.00.144.660 I print_info: rope_finetuned   = unknown
0.00.144.660 I print_info: ssm_d_conv       = 0
0.00.144.661 I print_info: ssm_d_inner      = 0
0.00.144.661 I print_info: ssm_d_state      = 0
0.00.144.661 I print_info: ssm_dt_rank      = 0
0.00.144.662 I print_info: ssm_dt_b_c_rms   = 0
0.00.144.662 I print_info: model type       = 1.4B
0.00.144.663 I print_info: model params     = 1.41 B
0.00.144.665 I print_info: general.name     = 1.4B
0.00.144.666 I print_info: vocab type       = BPE
0.00.144.667 I print_info: n_vocab          = 50304
0.00.144.667 I print_info: n_merges         = 50009
0.00.144.669 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.144.669 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.144.669 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.144.669 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.144.670 I print_info: LF token         = 187 'Ċ'
0.00.144.670 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.144.671 I print_info: max token length = 1024
0.00.144.671 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.186.086 I load_tensors: offloading 24 repeating layers to GPU
0.00.186.089 I load_tensors: offloading output layer to GPU
0.00.186.089 I load_tensors: offloaded 25/25 layers to GPU
0.00.186.111 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.186.113 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.186.756 I llama_init_from_model: n_seq_max     = 1
0.00.186.757 I llama_init_from_model: n_ctx         = 2048
0.00.186.757 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.186.758 I llama_init_from_model: n_batch       = 2048
0.00.186.758 I llama_init_from_model: n_ubatch      = 512
0.00.186.758 I llama_init_from_model: flash_attn    = 0
0.00.186.759 I llama_init_from_model: freq_base     = 10000.0
0.00.186.759 I llama_init_from_model: freq_scale    = 1
0.00.186.759 I ggml_metal_init: allocating
0.00.186.801 I ggml_metal_init: found device: Apple M4
0.00.186.807 I ggml_metal_init: picking default device: Apple M4
0.00.187.455 I ggml_metal_init: using embedded metal library
0.00.464.031 I ggml_metal_init: GPU name:   Apple M4
0.00.464.049 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.464.050 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.464.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.464.051 I ggml_metal_init: simdgroup reduction   = true
0.00.464.052 I ggml_metal_init: simdgroup matrix mul. = true
0.00.464.052 I ggml_metal_init: has residency sets    = true
0.00.464.052 I ggml_metal_init: has bfloat            = true
0.00.464.053 I ggml_metal_init: use bfloat            = true
0.00.464.055 I ggml_metal_init: hasUnifiedMemory      = true
0.00.464.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.501.347 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.538.575 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.538.583 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.538.630 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.542.869 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.542.872 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.542.872 I llama_init_from_model: graph nodes  = 967
0.00.542.872 I llama_init_from_model: graph splits = 2
0.00.542.878 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.542.993 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.542.994 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.211 I main: llama threadpool init, n_threads = 4
0.00.609.257 I 
0.00.609.288 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.290 I 
0.00.609.467 I sampler seed: 1234
0.00.609.472 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.609.496 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.609.498 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.609.498 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.444.309 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.02.444.310 I llama_perf_context_print:        load time =     520.55 ms
0.02.444.311 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.18 tokens per second)
0.02.444.312 I llama_perf_context_print:        eval time =    1788.32 ms /    63 runs   (   28.39 ms per token,    35.23 tokens per second)
0.02.444.312 I llama_perf_context_print:       total time =    1836.04 ms /    70 tokens
0.02.444.516 I ggml_metal_free: deallocating

real	0m2.744s
user	0m0.147s
sys	0m0.159s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.009.935 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.049 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.054 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.056 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.057 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.057 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.057 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.058 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.058 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.059 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.059 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.059 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.060 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.060 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.062 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.062 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.062 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.940 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.111 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.932 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.933 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.934 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.934 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.935 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.935 I llama_model_loader: - type  f32:  194 tensors
0.00.028.935 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.936 I print_info: file format = GGUF V3 (latest)
0.00.028.937 I print_info: file type   = Q8_0
0.00.028.938 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.037.167 I load: special tokens cache size = 25
0.00.043.083 I load: token to piece cache size = 0.2984 MB
0.00.043.088 I print_info: arch             = gptneox
0.00.043.089 I print_info: vocab_only       = 0
0.00.043.089 I print_info: n_ctx_train      = 2048
0.00.043.089 I print_info: n_embd           = 2048
0.00.043.089 I print_info: n_layer          = 24
0.00.043.097 I print_info: n_head           = 16
0.00.043.098 I print_info: n_head_kv        = 16
0.00.043.098 I print_info: n_rot            = 32
0.00.043.098 I print_info: n_swa            = 0
0.00.043.099 I print_info: n_embd_head_k    = 128
0.00.043.099 I print_info: n_embd_head_v    = 128
0.00.043.099 I print_info: n_gqa            = 1
0.00.043.100 I print_info: n_embd_k_gqa     = 2048
0.00.043.100 I print_info: n_embd_v_gqa     = 2048
0.00.043.101 I print_info: f_norm_eps       = 1.0e-05
0.00.043.102 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.102 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.102 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.102 I print_info: f_logit_scale    = 0.0e+00
0.00.043.103 I print_info: n_ff             = 8192
0.00.043.103 I print_info: n_expert         = 0
0.00.043.103 I print_info: n_expert_used    = 0
0.00.043.103 I print_info: causal attn      = 1
0.00.043.103 I print_info: pooling type     = 0
0.00.043.106 I print_info: rope type        = 2
0.00.043.106 I print_info: rope scaling     = linear
0.00.043.107 I print_info: freq_base_train  = 10000.0
0.00.043.107 I print_info: freq_scale_train = 1
0.00.043.107 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.107 I print_info: rope_finetuned   = unknown
0.00.043.107 I print_info: ssm_d_conv       = 0
0.00.043.107 I print_info: ssm_d_inner      = 0
0.00.043.108 I print_info: ssm_d_state      = 0
0.00.043.108 I print_info: ssm_dt_rank      = 0
0.00.043.108 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.108 I print_info: model type       = 1.4B
0.00.043.109 I print_info: model params     = 1.41 B
0.00.043.109 I print_info: general.name     = 1.4B
0.00.043.110 I print_info: vocab type       = BPE
0.00.043.110 I print_info: n_vocab          = 50304
0.00.043.110 I print_info: n_merges         = 50009
0.00.043.110 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.110 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.110 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.111 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.111 I print_info: LF token         = 187 'Ċ'
0.00.043.111 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.111 I print_info: max token length = 1024
0.00.043.112 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.016.467 I load_tensors: offloading 24 repeating layers to GPU
0.01.016.471 I load_tensors: offloading output layer to GPU
0.01.016.472 I load_tensors: offloaded 25/25 layers to GPU
0.01.016.492 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.016.495 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.017.402 I llama_init_from_model: n_seq_max     = 1
0.01.017.404 I llama_init_from_model: n_ctx         = 2048
0.01.017.404 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.017.404 I llama_init_from_model: n_batch       = 2048
0.01.017.405 I llama_init_from_model: n_ubatch      = 512
0.01.017.405 I llama_init_from_model: flash_attn    = 0
0.01.017.406 I llama_init_from_model: freq_base     = 10000.0
0.01.017.407 I llama_init_from_model: freq_scale    = 1
0.01.017.408 I ggml_metal_init: allocating
0.01.017.425 I ggml_metal_init: found device: Apple M4
0.01.017.433 I ggml_metal_init: picking default device: Apple M4
0.01.018.728 I ggml_metal_init: using embedded metal library
0.01.024.435 I ggml_metal_init: GPU name:   Apple M4
0.01.024.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.024.439 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.024.439 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.024.439 I ggml_metal_init: simdgroup reduction   = true
0.01.024.440 I ggml_metal_init: simdgroup matrix mul. = true
0.01.024.440 I ggml_metal_init: has residency sets    = true
0.01.024.440 I ggml_metal_init: has bfloat            = true
0.01.024.440 I ggml_metal_init: use bfloat            = true
0.01.024.443 I ggml_metal_init: hasUnifiedMemory      = true
0.01.024.445 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.040.348 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.094.679 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.094.687 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.094.718 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.100.111 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.100.113 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.100.114 I llama_init_from_model: graph nodes  = 967
0.01.100.114 I llama_init_from_model: graph splits = 2
0.01.100.118 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.100.260 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.100.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.148.999 I main: llama threadpool init, n_threads = 4
0.01.149.044 I 
0.01.149.069 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.149.071 I 
0.01.149.189 I sampler seed: 1234
0.01.149.193 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.149.210 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.149.210 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.149.210 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.241.137 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53910.40 tokens per second)
0.02.241.138 I llama_perf_context_print:        load time =    1138.33 ms
0.02.241.139 I llama_perf_context_print: prompt eval time =      48.56 ms /     7 tokens (    6.94 ms per token,   144.14 tokens per second)
0.02.241.139 I llama_perf_context_print:        eval time =    1040.46 ms /    63 runs   (   16.52 ms per token,    60.55 tokens per second)
0.02.241.140 I llama_perf_context_print:       total time =    1092.87 ms /    70 tokens
0.02.241.418 I ggml_metal_free: deallocating

real	0m2.261s
user	0m0.111s
sys	0m0.258s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.011.061 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.176 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.180 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.189 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.191 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.191 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.192 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.192 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.192 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.193 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.193 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.195 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.195 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.195 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.110 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.193 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.954 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.956 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.956 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.956 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.957 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.957 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.958 I llama_model_loader: - type  f32:  194 tensors
0.00.027.958 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.958 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.959 I print_info: file format = GGUF V3 (latest)
0.00.027.960 I print_info: file type   = Q4_0
0.00.027.961 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.429 I load: special tokens cache size = 25
0.00.042.395 I load: token to piece cache size = 0.2984 MB
0.00.042.398 I print_info: arch             = gptneox
0.00.042.398 I print_info: vocab_only       = 0
0.00.042.399 I print_info: n_ctx_train      = 2048
0.00.042.399 I print_info: n_embd           = 2048
0.00.042.399 I print_info: n_layer          = 24
0.00.042.404 I print_info: n_head           = 16
0.00.042.405 I print_info: n_head_kv        = 16
0.00.042.405 I print_info: n_rot            = 32
0.00.042.405 I print_info: n_swa            = 0
0.00.042.405 I print_info: n_embd_head_k    = 128
0.00.042.406 I print_info: n_embd_head_v    = 128
0.00.042.406 I print_info: n_gqa            = 1
0.00.042.410 I print_info: n_embd_k_gqa     = 2048
0.00.042.410 I print_info: n_embd_v_gqa     = 2048
0.00.042.412 I print_info: f_norm_eps       = 1.0e-05
0.00.042.413 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.413 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.413 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.413 I print_info: f_logit_scale    = 0.0e+00
0.00.042.414 I print_info: n_ff             = 8192
0.00.042.415 I print_info: n_expert         = 0
0.00.042.415 I print_info: n_expert_used    = 0
0.00.042.415 I print_info: causal attn      = 1
0.00.042.415 I print_info: pooling type     = 0
0.00.042.415 I print_info: rope type        = 2
0.00.042.415 I print_info: rope scaling     = linear
0.00.042.416 I print_info: freq_base_train  = 10000.0
0.00.042.416 I print_info: freq_scale_train = 1
0.00.042.416 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.417 I print_info: rope_finetuned   = unknown
0.00.042.417 I print_info: ssm_d_conv       = 0
0.00.042.417 I print_info: ssm_d_inner      = 0
0.00.042.417 I print_info: ssm_d_state      = 0
0.00.042.417 I print_info: ssm_dt_rank      = 0
0.00.042.418 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.418 I print_info: model type       = 1.4B
0.00.042.418 I print_info: model params     = 1.41 B
0.00.042.418 I print_info: general.name     = 1.4B
0.00.042.419 I print_info: vocab type       = BPE
0.00.042.419 I print_info: n_vocab          = 50304
0.00.042.420 I print_info: n_merges         = 50009
0.00.042.420 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.421 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.421 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.422 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.422 I print_info: LF token         = 187 'Ċ'
0.00.042.422 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.422 I print_info: max token length = 1024
0.00.042.422 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.584.791 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.805 I load_tensors: offloading output layer to GPU
0.00.584.806 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.839 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.584.845 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.586.491 I llama_init_from_model: n_seq_max     = 1
0.00.586.495 I llama_init_from_model: n_ctx         = 2048
0.00.586.495 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.586.496 I llama_init_from_model: n_batch       = 2048
0.00.586.496 I llama_init_from_model: n_ubatch      = 512
0.00.586.496 I llama_init_from_model: flash_attn    = 0
0.00.586.498 I llama_init_from_model: freq_base     = 10000.0
0.00.586.499 I llama_init_from_model: freq_scale    = 1
0.00.586.501 I ggml_metal_init: allocating
0.00.586.573 I ggml_metal_init: found device: Apple M4
0.00.586.587 I ggml_metal_init: picking default device: Apple M4
0.00.588.408 I ggml_metal_init: using embedded metal library
0.00.595.110 I ggml_metal_init: GPU name:   Apple M4
0.00.595.115 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.116 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.117 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.118 I ggml_metal_init: simdgroup reduction   = true
0.00.595.118 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.118 I ggml_metal_init: has residency sets    = true
0.00.595.119 I ggml_metal_init: has bfloat            = true
0.00.595.119 I ggml_metal_init: use bfloat            = true
0.00.595.120 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.122 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.042 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.670.947 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.670.954 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.670.989 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.764 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.676.766 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.676.766 I llama_init_from_model: graph nodes  = 967
0.00.676.766 I llama_init_from_model: graph splits = 2
0.00.676.770 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.676.899 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.676.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.076 I main: llama threadpool init, n_threads = 4
0.00.732.120 I 
0.00.732.145 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.146 I 
0.00.732.293 I sampler seed: 1234
0.00.732.297 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.732.309 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.732.309 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.732.309 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.423.448 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49754.73 tokens per second)
0.01.423.449 I llama_perf_context_print:        load time =     720.28 ms
0.01.423.450 I llama_perf_context_print: prompt eval time =      48.96 ms /     7 tokens (    6.99 ms per token,   142.97 tokens per second)
0.01.423.451 I llama_perf_context_print:        eval time =     639.26 ms /    63 runs   (   10.15 ms per token,    98.55 tokens per second)
0.01.423.451 I llama_perf_context_print:       total time =     692.10 ms /    70 tokens
0.01.423.709 I ggml_metal_free: deallocating

real	0m1.442s
user	0m0.112s
sys	0m0.205s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.009.232 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.232 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.237 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.239 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.239 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.240 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.240 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.240 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.241 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.241 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.242 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.243 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.243 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.244 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.244 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.246 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.246 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.246 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.006 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.825 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.826 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.827 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.827 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.827 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.827 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.828 I llama_model_loader: - type  f32:  194 tensors
0.00.024.828 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.829 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.829 I print_info: file format = GGUF V3 (latest)
0.00.024.830 I print_info: file type   = Q4_1
0.00.024.831 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.079 I load: special tokens cache size = 25
0.00.038.808 I load: token to piece cache size = 0.2984 MB
0.00.038.811 I print_info: arch             = gptneox
0.00.038.811 I print_info: vocab_only       = 0
0.00.038.811 I print_info: n_ctx_train      = 2048
0.00.038.812 I print_info: n_embd           = 2048
0.00.038.812 I print_info: n_layer          = 24
0.00.038.815 I print_info: n_head           = 16
0.00.038.815 I print_info: n_head_kv        = 16
0.00.038.815 I print_info: n_rot            = 32
0.00.038.816 I print_info: n_swa            = 0
0.00.038.816 I print_info: n_embd_head_k    = 128
0.00.038.816 I print_info: n_embd_head_v    = 128
0.00.038.817 I print_info: n_gqa            = 1
0.00.038.818 I print_info: n_embd_k_gqa     = 2048
0.00.038.818 I print_info: n_embd_v_gqa     = 2048
0.00.038.819 I print_info: f_norm_eps       = 1.0e-05
0.00.038.819 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.820 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.820 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.820 I print_info: f_logit_scale    = 0.0e+00
0.00.038.821 I print_info: n_ff             = 8192
0.00.038.821 I print_info: n_expert         = 0
0.00.038.821 I print_info: n_expert_used    = 0
0.00.038.821 I print_info: causal attn      = 1
0.00.038.821 I print_info: pooling type     = 0
0.00.038.821 I print_info: rope type        = 2
0.00.038.822 I print_info: rope scaling     = linear
0.00.038.822 I print_info: freq_base_train  = 10000.0
0.00.038.823 I print_info: freq_scale_train = 1
0.00.038.823 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.823 I print_info: rope_finetuned   = unknown
0.00.038.823 I print_info: ssm_d_conv       = 0
0.00.038.824 I print_info: ssm_d_inner      = 0
0.00.038.824 I print_info: ssm_d_state      = 0
0.00.038.826 I print_info: ssm_dt_rank      = 0
0.00.038.826 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.826 I print_info: model type       = 1.4B
0.00.038.827 I print_info: model params     = 1.41 B
0.00.038.827 I print_info: general.name     = 1.4B
0.00.038.827 I print_info: vocab type       = BPE
0.00.038.827 I print_info: n_vocab          = 50304
0.00.038.828 I print_info: n_merges         = 50009
0.00.038.828 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.828 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.828 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.828 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.833 I print_info: LF token         = 187 'Ċ'
0.00.038.833 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.833 I print_info: max token length = 1024
0.00.038.834 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.609.961 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.978 I load_tensors: offloading output layer to GPU
0.00.609.978 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.016 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.610.017 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.611.671 I llama_init_from_model: n_seq_max     = 1
0.00.611.676 I llama_init_from_model: n_ctx         = 2048
0.00.611.676 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.611.677 I llama_init_from_model: n_batch       = 2048
0.00.611.677 I llama_init_from_model: n_ubatch      = 512
0.00.611.678 I llama_init_from_model: flash_attn    = 0
0.00.611.680 I llama_init_from_model: freq_base     = 10000.0
0.00.611.680 I llama_init_from_model: freq_scale    = 1
0.00.611.684 I ggml_metal_init: allocating
0.00.611.802 I ggml_metal_init: found device: Apple M4
0.00.611.816 I ggml_metal_init: picking default device: Apple M4
0.00.613.763 I ggml_metal_init: using embedded metal library
0.00.619.784 I ggml_metal_init: GPU name:   Apple M4
0.00.619.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.791 I ggml_metal_init: simdgroup reduction   = true
0.00.619.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.792 I ggml_metal_init: has residency sets    = true
0.00.619.792 I ggml_metal_init: has bfloat            = true
0.00.619.792 I ggml_metal_init: use bfloat            = true
0.00.619.793 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.795 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.092 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.698.536 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.698.546 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.698.578 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.988 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.990 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.990 I llama_init_from_model: graph nodes  = 967
0.00.703.990 I llama_init_from_model: graph splits = 2
0.00.703.996 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.704.126 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.127 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.537 I main: llama threadpool init, n_threads = 4
0.00.758.578 I 
0.00.758.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.601 I 
0.00.758.781 I sampler seed: 1234
0.00.758.786 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.797 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.798 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.798 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.492.532 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.492.533 I llama_perf_context_print:        load time =     748.52 ms
0.01.492.534 I llama_perf_context_print: prompt eval time =      48.87 ms /     7 tokens (    6.98 ms per token,   143.25 tokens per second)
0.01.492.534 I llama_perf_context_print:        eval time =     682.25 ms /    63 runs   (   10.83 ms per token,    92.34 tokens per second)
0.01.492.535 I llama_perf_context_print:       total time =     734.77 ms /    70 tokens
0.01.492.765 I ggml_metal_free: deallocating

real	0m1.508s
user	0m0.110s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.881 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.906 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.918 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.921 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.922 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.922 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.922 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.923 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.924 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.924 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.924 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.925 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.925 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.925 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.926 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.927 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.927 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.928 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.718 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.762 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.483 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.484 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.484 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.485 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.485 I llama_model_loader: - type  f32:  194 tensors
0.00.024.485 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.486 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.486 I print_info: file format = GGUF V3 (latest)
0.00.024.487 I print_info: file type   = Q5_0
0.00.024.488 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.692 I load: special tokens cache size = 25
0.00.038.742 I load: token to piece cache size = 0.2984 MB
0.00.038.745 I print_info: arch             = gptneox
0.00.038.745 I print_info: vocab_only       = 0
0.00.038.746 I print_info: n_ctx_train      = 2048
0.00.038.746 I print_info: n_embd           = 2048
0.00.038.746 I print_info: n_layer          = 24
0.00.038.748 I print_info: n_head           = 16
0.00.038.749 I print_info: n_head_kv        = 16
0.00.038.749 I print_info: n_rot            = 32
0.00.038.750 I print_info: n_swa            = 0
0.00.038.750 I print_info: n_embd_head_k    = 128
0.00.038.750 I print_info: n_embd_head_v    = 128
0.00.038.751 I print_info: n_gqa            = 1
0.00.038.751 I print_info: n_embd_k_gqa     = 2048
0.00.038.752 I print_info: n_embd_v_gqa     = 2048
0.00.038.753 I print_info: f_norm_eps       = 1.0e-05
0.00.038.753 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.755 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.755 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.755 I print_info: f_logit_scale    = 0.0e+00
0.00.038.756 I print_info: n_ff             = 8192
0.00.038.756 I print_info: n_expert         = 0
0.00.038.756 I print_info: n_expert_used    = 0
0.00.038.758 I print_info: causal attn      = 1
0.00.038.758 I print_info: pooling type     = 0
0.00.038.758 I print_info: rope type        = 2
0.00.038.758 I print_info: rope scaling     = linear
0.00.038.759 I print_info: freq_base_train  = 10000.0
0.00.038.759 I print_info: freq_scale_train = 1
0.00.038.759 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.760 I print_info: rope_finetuned   = unknown
0.00.038.760 I print_info: ssm_d_conv       = 0
0.00.038.760 I print_info: ssm_d_inner      = 0
0.00.038.760 I print_info: ssm_d_state      = 0
0.00.038.760 I print_info: ssm_dt_rank      = 0
0.00.038.762 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.762 I print_info: model type       = 1.4B
0.00.038.762 I print_info: model params     = 1.41 B
0.00.038.763 I print_info: general.name     = 1.4B
0.00.038.763 I print_info: vocab type       = BPE
0.00.038.763 I print_info: n_vocab          = 50304
0.00.038.770 I print_info: n_merges         = 50009
0.00.038.771 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.771 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.771 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.771 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.772 I print_info: LF token         = 187 'Ċ'
0.00.038.772 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.772 I print_info: max token length = 1024
0.00.038.773 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.624.442 I load_tensors: offloading 24 repeating layers to GPU
0.00.624.457 I load_tensors: offloading output layer to GPU
0.00.624.458 I load_tensors: offloaded 25/25 layers to GPU
0.00.624.492 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.624.493 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.626.138 I llama_init_from_model: n_seq_max     = 1
0.00.626.141 I llama_init_from_model: n_ctx         = 2048
0.00.626.142 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.626.142 I llama_init_from_model: n_batch       = 2048
0.00.626.143 I llama_init_from_model: n_ubatch      = 512
0.00.626.143 I llama_init_from_model: flash_attn    = 0
0.00.626.144 I llama_init_from_model: freq_base     = 10000.0
0.00.626.145 I llama_init_from_model: freq_scale    = 1
0.00.626.146 I ggml_metal_init: allocating
0.00.626.183 I ggml_metal_init: found device: Apple M4
0.00.626.199 I ggml_metal_init: picking default device: Apple M4
0.00.627.788 I ggml_metal_init: using embedded metal library
0.00.634.338 I ggml_metal_init: GPU name:   Apple M4
0.00.634.342 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.634.343 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.634.344 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.634.344 I ggml_metal_init: simdgroup reduction   = true
0.00.634.344 I ggml_metal_init: simdgroup matrix mul. = true
0.00.634.345 I ggml_metal_init: has residency sets    = true
0.00.634.345 I ggml_metal_init: has bfloat            = true
0.00.634.345 I ggml_metal_init: use bfloat            = true
0.00.634.346 I ggml_metal_init: hasUnifiedMemory      = true
0.00.634.347 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.496 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.848 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.705.854 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.705.887 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.710.620 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.710.622 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.710.623 I llama_init_from_model: graph nodes  = 967
0.00.710.623 I llama_init_from_model: graph splits = 2
0.00.710.629 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.710.757 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.710.758 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.372 I main: llama threadpool init, n_threads = 4
0.00.768.416 I 
0.00.768.437 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.437 I 
0.00.768.612 I sampler seed: 1234
0.00.768.618 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.628 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.629 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.629 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.559.684 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51976.57 tokens per second)
0.01.559.685 I llama_perf_context_print:        load time =     758.71 ms
0.01.559.686 I llama_perf_context_print: prompt eval time =      53.68 ms /     7 tokens (    7.67 ms per token,   130.40 tokens per second)
0.01.559.687 I llama_perf_context_print:        eval time =     734.52 ms /    63 runs   (   11.66 ms per token,    85.77 tokens per second)
0.01.559.687 I llama_perf_context_print:       total time =     792.09 ms /    70 tokens
0.01.559.942 I ggml_metal_free: deallocating

real	0m1.575s
user	0m0.109s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.442 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.777 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.782 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.784 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.784 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.785 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.785 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.785 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.786 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.787 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.787 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.788 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.788 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.788 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.789 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.791 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.792 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.792 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.660 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.355 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.356 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.356 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.357 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.357 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.357 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.358 I llama_model_loader: - type  f32:  194 tensors
0.00.027.358 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.358 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.359 I print_info: file format = GGUF V3 (latest)
0.00.027.359 I print_info: file type   = Q5_1
0.00.027.365 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.648 I load: special tokens cache size = 25
0.00.041.525 I load: token to piece cache size = 0.2984 MB
0.00.041.527 I print_info: arch             = gptneox
0.00.041.528 I print_info: vocab_only       = 0
0.00.041.528 I print_info: n_ctx_train      = 2048
0.00.041.528 I print_info: n_embd           = 2048
0.00.041.528 I print_info: n_layer          = 24
0.00.041.531 I print_info: n_head           = 16
0.00.041.531 I print_info: n_head_kv        = 16
0.00.041.532 I print_info: n_rot            = 32
0.00.041.534 I print_info: n_swa            = 0
0.00.041.534 I print_info: n_embd_head_k    = 128
0.00.041.534 I print_info: n_embd_head_v    = 128
0.00.041.535 I print_info: n_gqa            = 1
0.00.041.536 I print_info: n_embd_k_gqa     = 2048
0.00.041.536 I print_info: n_embd_v_gqa     = 2048
0.00.041.537 I print_info: f_norm_eps       = 1.0e-05
0.00.041.537 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.537 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.538 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.538 I print_info: f_logit_scale    = 0.0e+00
0.00.041.538 I print_info: n_ff             = 8192
0.00.041.539 I print_info: n_expert         = 0
0.00.041.539 I print_info: n_expert_used    = 0
0.00.041.539 I print_info: causal attn      = 1
0.00.041.539 I print_info: pooling type     = 0
0.00.041.539 I print_info: rope type        = 2
0.00.041.540 I print_info: rope scaling     = linear
0.00.041.540 I print_info: freq_base_train  = 10000.0
0.00.041.540 I print_info: freq_scale_train = 1
0.00.041.540 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.541 I print_info: rope_finetuned   = unknown
0.00.041.541 I print_info: ssm_d_conv       = 0
0.00.041.541 I print_info: ssm_d_inner      = 0
0.00.041.541 I print_info: ssm_d_state      = 0
0.00.041.541 I print_info: ssm_dt_rank      = 0
0.00.041.541 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.542 I print_info: model type       = 1.4B
0.00.041.542 I print_info: model params     = 1.41 B
0.00.041.542 I print_info: general.name     = 1.4B
0.00.041.546 I print_info: vocab type       = BPE
0.00.041.547 I print_info: n_vocab          = 50304
0.00.041.547 I print_info: n_merges         = 50009
0.00.041.547 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.549 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.549 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.549 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.549 I print_info: LF token         = 187 'Ċ'
0.00.041.549 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.550 I print_info: max token length = 1024
0.00.041.550 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.626.897 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.912 I load_tensors: offloading output layer to GPU
0.00.626.913 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.949 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.626.950 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.628.404 I llama_init_from_model: n_seq_max     = 1
0.00.628.406 I llama_init_from_model: n_ctx         = 2048
0.00.628.407 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.628.407 I llama_init_from_model: n_batch       = 2048
0.00.628.408 I llama_init_from_model: n_ubatch      = 512
0.00.628.408 I llama_init_from_model: flash_attn    = 0
0.00.628.409 I llama_init_from_model: freq_base     = 10000.0
0.00.628.410 I llama_init_from_model: freq_scale    = 1
0.00.628.411 I ggml_metal_init: allocating
0.00.628.438 I ggml_metal_init: found device: Apple M4
0.00.628.451 I ggml_metal_init: picking default device: Apple M4
0.00.630.015 I ggml_metal_init: using embedded metal library
0.00.636.563 I ggml_metal_init: GPU name:   Apple M4
0.00.636.566 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.567 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.568 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.568 I ggml_metal_init: simdgroup reduction   = true
0.00.636.569 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.569 I ggml_metal_init: has residency sets    = true
0.00.636.569 I ggml_metal_init: has bfloat            = true
0.00.636.569 I ggml_metal_init: use bfloat            = true
0.00.636.570 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.572 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.654.788 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.711.038 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.711.046 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.711.091 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.715.504 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.715.507 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.715.507 I llama_init_from_model: graph nodes  = 967
0.00.715.507 I llama_init_from_model: graph splits = 2
0.00.715.514 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.715.626 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.715.627 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.501 I main: llama threadpool init, n_threads = 4
0.00.773.540 I 
0.00.773.562 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.565 I 
0.00.773.740 I sampler seed: 1234
0.00.773.745 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.764 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.764 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.765 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.614.053 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.614.053 I llama_perf_context_print:        load time =     762.27 ms
0.01.614.054 I llama_perf_context_print: prompt eval time =      52.43 ms /     7 tokens (    7.49 ms per token,   133.52 tokens per second)
0.01.614.055 I llama_perf_context_print:        eval time =     785.02 ms /    63 runs   (   12.46 ms per token,    80.25 tokens per second)
0.01.614.055 I llama_perf_context_print:       total time =     841.34 ms /    70 tokens
0.01.614.288 I ggml_metal_free: deallocating

real	0m1.633s
user	0m0.109s
sys	0m0.233s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.870 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.875 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.877 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.878 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.878 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.878 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.881 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.882 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.882 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.883 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.883 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.887 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.888 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.889 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.889 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.698 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.799 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.589 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.591 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.591 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.592 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.592 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.593 I llama_model_loader: - type  f32:  194 tensors
0.00.024.593 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.593 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.593 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.594 I print_info: file format = GGUF V3 (latest)
0.00.024.594 I print_info: file type   = Q2_K - Medium
0.00.024.599 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.804 I load: special tokens cache size = 25
0.00.038.802 I load: token to piece cache size = 0.2984 MB
0.00.038.805 I print_info: arch             = gptneox
0.00.038.805 I print_info: vocab_only       = 0
0.00.038.806 I print_info: n_ctx_train      = 2048
0.00.038.806 I print_info: n_embd           = 2048
0.00.038.806 I print_info: n_layer          = 24
0.00.038.809 I print_info: n_head           = 16
0.00.038.809 I print_info: n_head_kv        = 16
0.00.038.811 I print_info: n_rot            = 32
0.00.038.812 I print_info: n_swa            = 0
0.00.038.812 I print_info: n_embd_head_k    = 128
0.00.038.812 I print_info: n_embd_head_v    = 128
0.00.038.813 I print_info: n_gqa            = 1
0.00.038.813 I print_info: n_embd_k_gqa     = 2048
0.00.038.814 I print_info: n_embd_v_gqa     = 2048
0.00.038.815 I print_info: f_norm_eps       = 1.0e-05
0.00.038.815 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.815 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.816 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.816 I print_info: f_logit_scale    = 0.0e+00
0.00.038.816 I print_info: n_ff             = 8192
0.00.038.817 I print_info: n_expert         = 0
0.00.038.817 I print_info: n_expert_used    = 0
0.00.038.817 I print_info: causal attn      = 1
0.00.038.817 I print_info: pooling type     = 0
0.00.038.817 I print_info: rope type        = 2
0.00.038.818 I print_info: rope scaling     = linear
0.00.038.818 I print_info: freq_base_train  = 10000.0
0.00.038.818 I print_info: freq_scale_train = 1
0.00.038.818 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.822 I print_info: rope_finetuned   = unknown
0.00.038.822 I print_info: ssm_d_conv       = 0
0.00.038.823 I print_info: ssm_d_inner      = 0
0.00.038.824 I print_info: ssm_d_state      = 0
0.00.038.824 I print_info: ssm_dt_rank      = 0
0.00.038.824 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.825 I print_info: model type       = 1.4B
0.00.038.825 I print_info: model params     = 1.41 B
0.00.038.825 I print_info: general.name     = 1.4B
0.00.038.826 I print_info: vocab type       = BPE
0.00.038.826 I print_info: n_vocab          = 50304
0.00.038.826 I print_info: n_merges         = 50009
0.00.038.826 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.826 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.827 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.827 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.827 I print_info: LF token         = 187 'Ċ'
0.00.038.827 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.827 I print_info: max token length = 1024
0.00.038.828 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.338.433 I load_tensors: offloading 24 repeating layers to GPU
0.00.338.448 I load_tensors: offloading output layer to GPU
0.00.338.449 I load_tensors: offloaded 25/25 layers to GPU
0.00.338.487 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.338.488 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.340.123 I llama_init_from_model: n_seq_max     = 1
0.00.340.126 I llama_init_from_model: n_ctx         = 2048
0.00.340.127 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.340.127 I llama_init_from_model: n_batch       = 2048
0.00.340.127 I llama_init_from_model: n_ubatch      = 512
0.00.340.128 I llama_init_from_model: flash_attn    = 0
0.00.340.129 I llama_init_from_model: freq_base     = 10000.0
0.00.340.130 I llama_init_from_model: freq_scale    = 1
0.00.340.132 I ggml_metal_init: allocating
0.00.340.198 I ggml_metal_init: found device: Apple M4
0.00.340.211 I ggml_metal_init: picking default device: Apple M4
0.00.342.092 I ggml_metal_init: using embedded metal library
0.00.347.844 I ggml_metal_init: GPU name:   Apple M4
0.00.347.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.347.854 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.347.855 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.347.855 I ggml_metal_init: simdgroup reduction   = true
0.00.347.855 I ggml_metal_init: simdgroup matrix mul. = true
0.00.347.856 I ggml_metal_init: has residency sets    = true
0.00.347.856 I ggml_metal_init: has bfloat            = true
0.00.347.856 I ggml_metal_init: use bfloat            = true
0.00.347.863 I ggml_metal_init: hasUnifiedMemory      = true
0.00.347.866 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.369.388 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.430.197 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.430.205 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.430.239 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.434.329 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.434.331 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.434.331 I llama_init_from_model: graph nodes  = 967
0.00.434.331 I llama_init_from_model: graph splits = 2
0.00.434.337 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.434.467 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.434.467 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.493.381 I main: llama threadpool init, n_threads = 4
0.00.493.429 I 
0.00.493.449 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.493.451 I 
0.00.493.617 I sampler seed: 1234
0.00.493.622 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.493.633 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.493.633 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.493.633 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.166.977 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52014.65 tokens per second)
0.01.166.978 I llama_perf_context_print:        load time =     483.70 ms
0.01.166.978 I llama_perf_context_print: prompt eval time =      35.75 ms /     7 tokens (    5.11 ms per token,   195.82 tokens per second)
0.01.166.980 I llama_perf_context_print:        eval time =     634.79 ms /    63 runs   (   10.08 ms per token,    99.25 tokens per second)
0.01.166.980 I llama_perf_context_print:       total time =     674.36 ms /    70 tokens
0.01.167.213 I ggml_metal_free: deallocating

real	0m1.185s
user	0m0.113s
sys	0m0.168s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.811 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.569 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.576 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.576 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.577 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.577 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.578 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.578 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.579 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.579 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.579 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.580 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.581 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.582 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.583 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.584 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.584 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.378 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.447 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.221 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.222 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.222 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.222 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.223 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.223 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.223 I llama_model_loader: - type  f32:  194 tensors
0.00.024.224 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.224 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.224 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.224 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.225 I print_info: file format = GGUF V3 (latest)
0.00.024.225 I print_info: file type   = Q3_K - Medium
0.00.024.226 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.138 I load: special tokens cache size = 25
0.00.038.212 I load: token to piece cache size = 0.2984 MB
0.00.038.215 I print_info: arch             = gptneox
0.00.038.216 I print_info: vocab_only       = 0
0.00.038.216 I print_info: n_ctx_train      = 2048
0.00.038.216 I print_info: n_embd           = 2048
0.00.038.216 I print_info: n_layer          = 24
0.00.038.219 I print_info: n_head           = 16
0.00.038.220 I print_info: n_head_kv        = 16
0.00.038.220 I print_info: n_rot            = 32
0.00.038.220 I print_info: n_swa            = 0
0.00.038.221 I print_info: n_embd_head_k    = 128
0.00.038.221 I print_info: n_embd_head_v    = 128
0.00.038.221 I print_info: n_gqa            = 1
0.00.038.222 I print_info: n_embd_k_gqa     = 2048
0.00.038.223 I print_info: n_embd_v_gqa     = 2048
0.00.038.223 I print_info: f_norm_eps       = 1.0e-05
0.00.038.224 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.224 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.226 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.226 I print_info: f_logit_scale    = 0.0e+00
0.00.038.227 I print_info: n_ff             = 8192
0.00.038.227 I print_info: n_expert         = 0
0.00.038.228 I print_info: n_expert_used    = 0
0.00.038.229 I print_info: causal attn      = 1
0.00.038.229 I print_info: pooling type     = 0
0.00.038.229 I print_info: rope type        = 2
0.00.038.230 I print_info: rope scaling     = linear
0.00.038.234 I print_info: freq_base_train  = 10000.0
0.00.038.234 I print_info: freq_scale_train = 1
0.00.038.234 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.234 I print_info: rope_finetuned   = unknown
0.00.038.235 I print_info: ssm_d_conv       = 0
0.00.038.235 I print_info: ssm_d_inner      = 0
0.00.038.235 I print_info: ssm_d_state      = 0
0.00.038.235 I print_info: ssm_dt_rank      = 0
0.00.038.235 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.235 I print_info: model type       = 1.4B
0.00.038.236 I print_info: model params     = 1.41 B
0.00.038.236 I print_info: general.name     = 1.4B
0.00.038.236 I print_info: vocab type       = BPE
0.00.038.237 I print_info: n_vocab          = 50304
0.00.038.237 I print_info: n_merges         = 50009
0.00.038.240 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.240 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.240 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.240 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.240 I print_info: LF token         = 187 'Ċ'
0.00.038.240 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.241 I print_info: max token length = 1024
0.00.038.241 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.438.332 I load_tensors: offloading 24 repeating layers to GPU
0.00.438.348 I load_tensors: offloading output layer to GPU
0.00.438.348 I load_tensors: offloaded 25/25 layers to GPU
0.00.438.384 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.438.386 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.440.060 I llama_init_from_model: n_seq_max     = 1
0.00.440.062 I llama_init_from_model: n_ctx         = 2048
0.00.440.063 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.440.064 I llama_init_from_model: n_batch       = 2048
0.00.440.064 I llama_init_from_model: n_ubatch      = 512
0.00.440.064 I llama_init_from_model: flash_attn    = 0
0.00.440.067 I llama_init_from_model: freq_base     = 10000.0
0.00.440.067 I llama_init_from_model: freq_scale    = 1
0.00.440.069 I ggml_metal_init: allocating
0.00.440.180 I ggml_metal_init: found device: Apple M4
0.00.440.194 I ggml_metal_init: picking default device: Apple M4
0.00.442.232 I ggml_metal_init: using embedded metal library
0.00.448.792 I ggml_metal_init: GPU name:   Apple M4
0.00.448.796 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.448.797 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.448.798 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.448.799 I ggml_metal_init: simdgroup reduction   = true
0.00.448.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.448.799 I ggml_metal_init: has residency sets    = true
0.00.448.800 I ggml_metal_init: has bfloat            = true
0.00.448.800 I ggml_metal_init: use bfloat            = true
0.00.448.801 I ggml_metal_init: hasUnifiedMemory      = true
0.00.448.811 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.467.746 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.523.146 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.523.154 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.523.192 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.528.279 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.528.281 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.528.282 I llama_init_from_model: graph nodes  = 967
0.00.528.282 I llama_init_from_model: graph splits = 2
0.00.528.290 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.528.407 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.528.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.585.613 I main: llama threadpool init, n_threads = 4
0.00.585.658 I 
0.00.585.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.585.683 I 
0.00.585.864 I sampler seed: 1234
0.00.585.869 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.585.890 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.585.890 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.585.890 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.337.870 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.337.871 I llama_perf_context_print:        load time =     576.04 ms
0.01.337.871 I llama_perf_context_print: prompt eval time =      49.22 ms /     7 tokens (    7.03 ms per token,   142.23 tokens per second)
0.01.337.872 I llama_perf_context_print:        eval time =     699.97 ms /    63 runs   (   11.11 ms per token,    90.00 tokens per second)
0.01.337.872 I llama_perf_context_print:       total time =     753.01 ms /    70 tokens
0.01.338.129 I ggml_metal_free: deallocating

real	0m1.354s
user	0m0.111s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.450 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.244 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.251 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.252 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.254 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.254 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.255 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.255 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.255 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.256 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.256 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.257 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.258 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.258 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.033 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.097 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.866 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.868 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.868 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.869 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.869 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.869 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.870 I llama_model_loader: - type  f32:  194 tensors
0.00.024.870 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.870 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.871 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.871 I print_info: file format = GGUF V3 (latest)
0.00.024.872 I print_info: file type   = Q4_K - Medium
0.00.024.872 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.097 I load: special tokens cache size = 25
0.00.039.068 I load: token to piece cache size = 0.2984 MB
0.00.039.071 I print_info: arch             = gptneox
0.00.039.071 I print_info: vocab_only       = 0
0.00.039.072 I print_info: n_ctx_train      = 2048
0.00.039.072 I print_info: n_embd           = 2048
0.00.039.072 I print_info: n_layer          = 24
0.00.039.075 I print_info: n_head           = 16
0.00.039.075 I print_info: n_head_kv        = 16
0.00.039.076 I print_info: n_rot            = 32
0.00.039.076 I print_info: n_swa            = 0
0.00.039.076 I print_info: n_embd_head_k    = 128
0.00.039.077 I print_info: n_embd_head_v    = 128
0.00.039.078 I print_info: n_gqa            = 1
0.00.039.079 I print_info: n_embd_k_gqa     = 2048
0.00.039.079 I print_info: n_embd_v_gqa     = 2048
0.00.039.080 I print_info: f_norm_eps       = 1.0e-05
0.00.039.080 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.080 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.081 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.081 I print_info: f_logit_scale    = 0.0e+00
0.00.039.081 I print_info: n_ff             = 8192
0.00.039.082 I print_info: n_expert         = 0
0.00.039.082 I print_info: n_expert_used    = 0
0.00.039.082 I print_info: causal attn      = 1
0.00.039.082 I print_info: pooling type     = 0
0.00.039.084 I print_info: rope type        = 2
0.00.039.084 I print_info: rope scaling     = linear
0.00.039.085 I print_info: freq_base_train  = 10000.0
0.00.039.085 I print_info: freq_scale_train = 1
0.00.039.086 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.087 I print_info: rope_finetuned   = unknown
0.00.039.087 I print_info: ssm_d_conv       = 0
0.00.039.087 I print_info: ssm_d_inner      = 0
0.00.039.087 I print_info: ssm_d_state      = 0
0.00.039.087 I print_info: ssm_dt_rank      = 0
0.00.039.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.088 I print_info: model type       = 1.4B
0.00.039.088 I print_info: model params     = 1.41 B
0.00.039.088 I print_info: general.name     = 1.4B
0.00.039.089 I print_info: vocab type       = BPE
0.00.039.089 I print_info: n_vocab          = 50304
0.00.039.091 I print_info: n_merges         = 50009
0.00.039.091 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.092 I print_info: LF token         = 187 'Ċ'
0.00.039.092 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.092 I print_info: max token length = 1024
0.00.039.093 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.512.499 I load_tensors: offloading 24 repeating layers to GPU
0.00.512.512 I load_tensors: offloading output layer to GPU
0.00.512.512 I load_tensors: offloaded 25/25 layers to GPU
0.00.512.551 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.512.552 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.514.235 I llama_init_from_model: n_seq_max     = 1
0.00.514.238 I llama_init_from_model: n_ctx         = 2048
0.00.514.239 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.514.239 I llama_init_from_model: n_batch       = 2048
0.00.514.239 I llama_init_from_model: n_ubatch      = 512
0.00.514.240 I llama_init_from_model: flash_attn    = 0
0.00.514.242 I llama_init_from_model: freq_base     = 10000.0
0.00.514.242 I llama_init_from_model: freq_scale    = 1
0.00.514.244 I ggml_metal_init: allocating
0.00.514.346 I ggml_metal_init: found device: Apple M4
0.00.514.360 I ggml_metal_init: picking default device: Apple M4
0.00.516.322 I ggml_metal_init: using embedded metal library
0.00.523.534 I ggml_metal_init: GPU name:   Apple M4
0.00.523.539 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.523.540 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.523.540 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.523.541 I ggml_metal_init: simdgroup reduction   = true
0.00.523.541 I ggml_metal_init: simdgroup matrix mul. = true
0.00.523.542 I ggml_metal_init: has residency sets    = true
0.00.523.542 I ggml_metal_init: has bfloat            = true
0.00.523.542 I ggml_metal_init: use bfloat            = true
0.00.523.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.523.545 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.630 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.599.878 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.599.886 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.599.920 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.604.096 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.604.098 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.604.098 I llama_init_from_model: graph nodes  = 967
0.00.604.099 I llama_init_from_model: graph splits = 2
0.00.604.104 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.604.233 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.604.233 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.194 I main: llama threadpool init, n_threads = 4
0.00.663.239 I 
0.00.663.263 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.264 I 
0.00.663.444 I sampler seed: 1234
0.00.663.448 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.663.473 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.663.474 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.663.474 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.426.774 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.01.426.775 I llama_perf_context_print:        load time =     652.99 ms
0.01.426.776 I llama_perf_context_print: prompt eval time =      58.32 ms /     7 tokens (    8.33 ms per token,   120.03 tokens per second)
0.01.426.776 I llama_perf_context_print:        eval time =     702.07 ms /    63 runs   (   11.14 ms per token,    89.74 tokens per second)
0.01.426.777 I llama_perf_context_print:       total time =     764.34 ms /    70 tokens
0.01.427.001 I ggml_metal_free: deallocating

real	0m1.445s
user	0m0.112s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.221 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.245 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.252 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.252 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.253 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.254 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.255 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.255 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.255 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.256 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.256 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.257 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.261 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.261 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.035 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.106 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.868 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.869 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.870 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.870 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.870 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.871 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.871 I llama_model_loader: - type  f32:  194 tensors
0.00.024.872 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.872 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.872 I print_info: file format = GGUF V3 (latest)
0.00.024.873 I print_info: file type   = Q5_K - Medium
0.00.024.874 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.897 I load: special tokens cache size = 25
0.00.038.806 I load: token to piece cache size = 0.2984 MB
0.00.038.809 I print_info: arch             = gptneox
0.00.038.809 I print_info: vocab_only       = 0
0.00.038.809 I print_info: n_ctx_train      = 2048
0.00.038.809 I print_info: n_embd           = 2048
0.00.038.809 I print_info: n_layer          = 24
0.00.038.812 I print_info: n_head           = 16
0.00.038.813 I print_info: n_head_kv        = 16
0.00.038.813 I print_info: n_rot            = 32
0.00.038.813 I print_info: n_swa            = 0
0.00.038.815 I print_info: n_embd_head_k    = 128
0.00.038.816 I print_info: n_embd_head_v    = 128
0.00.038.816 I print_info: n_gqa            = 1
0.00.038.817 I print_info: n_embd_k_gqa     = 2048
0.00.038.822 I print_info: n_embd_v_gqa     = 2048
0.00.038.823 I print_info: f_norm_eps       = 1.0e-05
0.00.038.823 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.823 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.823 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.823 I print_info: f_logit_scale    = 0.0e+00
0.00.038.824 I print_info: n_ff             = 8192
0.00.038.824 I print_info: n_expert         = 0
0.00.038.825 I print_info: n_expert_used    = 0
0.00.038.825 I print_info: causal attn      = 1
0.00.038.825 I print_info: pooling type     = 0
0.00.038.827 I print_info: rope type        = 2
0.00.038.828 I print_info: rope scaling     = linear
0.00.038.829 I print_info: freq_base_train  = 10000.0
0.00.038.829 I print_info: freq_scale_train = 1
0.00.038.829 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.829 I print_info: rope_finetuned   = unknown
0.00.038.830 I print_info: ssm_d_conv       = 0
0.00.038.831 I print_info: ssm_d_inner      = 0
0.00.038.831 I print_info: ssm_d_state      = 0
0.00.038.831 I print_info: ssm_dt_rank      = 0
0.00.038.831 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.831 I print_info: model type       = 1.4B
0.00.038.832 I print_info: model params     = 1.41 B
0.00.038.832 I print_info: general.name     = 1.4B
0.00.038.832 I print_info: vocab type       = BPE
0.00.038.835 I print_info: n_vocab          = 50304
0.00.038.835 I print_info: n_merges         = 50009
0.00.038.835 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.836 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.836 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.836 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.837 I print_info: LF token         = 187 'Ċ'
0.00.038.837 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.837 I print_info: max token length = 1024
0.00.038.837 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.598.204 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.220 I load_tensors: offloading output layer to GPU
0.00.598.221 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.254 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.598.262 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.599.789 I llama_init_from_model: n_seq_max     = 1
0.00.599.792 I llama_init_from_model: n_ctx         = 2048
0.00.599.792 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.599.793 I llama_init_from_model: n_batch       = 2048
0.00.599.794 I llama_init_from_model: n_ubatch      = 512
0.00.599.794 I llama_init_from_model: flash_attn    = 0
0.00.599.796 I llama_init_from_model: freq_base     = 10000.0
0.00.599.797 I llama_init_from_model: freq_scale    = 1
0.00.599.799 I ggml_metal_init: allocating
0.00.599.876 I ggml_metal_init: found device: Apple M4
0.00.599.890 I ggml_metal_init: picking default device: Apple M4
0.00.601.881 I ggml_metal_init: using embedded metal library
0.00.608.426 I ggml_metal_init: GPU name:   Apple M4
0.00.608.431 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.432 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.432 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.433 I ggml_metal_init: simdgroup reduction   = true
0.00.608.433 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.433 I ggml_metal_init: has residency sets    = true
0.00.608.434 I ggml_metal_init: has bfloat            = true
0.00.608.434 I ggml_metal_init: use bfloat            = true
0.00.608.435 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.806 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.610 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.680.617 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.680.656 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.684.718 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.684.720 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.684.720 I llama_init_from_model: graph nodes  = 967
0.00.684.720 I llama_init_from_model: graph splits = 2
0.00.684.727 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.684.850 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.799 I main: llama threadpool init, n_threads = 4
0.00.745.855 I 
0.00.745.878 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.880 I 
0.00.746.043 I sampler seed: 1234
0.00.746.048 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.067 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.068 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.068 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.588.482 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.588.483 I llama_perf_context_print:        load time =     735.83 ms
0.01.588.484 I llama_perf_context_print: prompt eval time =      52.58 ms /     7 tokens (    7.51 ms per token,   133.12 tokens per second)
0.01.588.485 I llama_perf_context_print:        eval time =     787.10 ms /    63 runs   (   12.49 ms per token,    80.04 tokens per second)
0.01.588.485 I llama_perf_context_print:       total time =     843.43 ms /    70 tokens
0.01.588.688 I ggml_metal_free: deallocating

real	0m1.606s
user	0m0.110s
sys	0m0.216s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.959 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.794 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.798 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.800 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.802 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.803 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.803 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.803 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.809 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.809 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.809 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.810 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.810 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.811 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.812 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.813 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.813 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.551 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.632 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.368 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.369 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.370 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.370 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.370 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.370 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.371 I llama_model_loader: - type  f32:  194 tensors
0.00.024.371 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.372 I print_info: file format = GGUF V3 (latest)
0.00.024.372 I print_info: file type   = Q6_K
0.00.024.373 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.571 I load: special tokens cache size = 25
0.00.038.558 I load: token to piece cache size = 0.2984 MB
0.00.038.561 I print_info: arch             = gptneox
0.00.038.561 I print_info: vocab_only       = 0
0.00.038.561 I print_info: n_ctx_train      = 2048
0.00.038.562 I print_info: n_embd           = 2048
0.00.038.562 I print_info: n_layer          = 24
0.00.038.564 I print_info: n_head           = 16
0.00.038.565 I print_info: n_head_kv        = 16
0.00.038.565 I print_info: n_rot            = 32
0.00.038.566 I print_info: n_swa            = 0
0.00.038.566 I print_info: n_embd_head_k    = 128
0.00.038.566 I print_info: n_embd_head_v    = 128
0.00.038.567 I print_info: n_gqa            = 1
0.00.038.567 I print_info: n_embd_k_gqa     = 2048
0.00.038.568 I print_info: n_embd_v_gqa     = 2048
0.00.038.568 I print_info: f_norm_eps       = 1.0e-05
0.00.038.569 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.569 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.569 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.569 I print_info: f_logit_scale    = 0.0e+00
0.00.038.570 I print_info: n_ff             = 8192
0.00.038.570 I print_info: n_expert         = 0
0.00.038.570 I print_info: n_expert_used    = 0
0.00.038.570 I print_info: causal attn      = 1
0.00.038.571 I print_info: pooling type     = 0
0.00.038.571 I print_info: rope type        = 2
0.00.038.573 I print_info: rope scaling     = linear
0.00.038.573 I print_info: freq_base_train  = 10000.0
0.00.038.574 I print_info: freq_scale_train = 1
0.00.038.574 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.574 I print_info: rope_finetuned   = unknown
0.00.038.574 I print_info: ssm_d_conv       = 0
0.00.038.574 I print_info: ssm_d_inner      = 0
0.00.038.574 I print_info: ssm_d_state      = 0
0.00.038.575 I print_info: ssm_dt_rank      = 0
0.00.038.575 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.575 I print_info: model type       = 1.4B
0.00.038.576 I print_info: model params     = 1.41 B
0.00.038.576 I print_info: general.name     = 1.4B
0.00.038.576 I print_info: vocab type       = BPE
0.00.038.576 I print_info: n_vocab          = 50304
0.00.038.576 I print_info: n_merges         = 50009
0.00.038.577 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.577 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.577 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.577 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.578 I print_info: LF token         = 187 'Ċ'
0.00.038.578 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.578 I print_info: max token length = 1024
0.00.038.579 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.667.154 I load_tensors: offloading 24 repeating layers to GPU
0.00.667.157 I load_tensors: offloading output layer to GPU
0.00.667.157 I load_tensors: offloaded 25/25 layers to GPU
0.00.667.182 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.667.185 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.668.631 I llama_init_from_model: n_seq_max     = 1
0.00.668.634 I llama_init_from_model: n_ctx         = 2048
0.00.668.634 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.668.634 I llama_init_from_model: n_batch       = 2048
0.00.668.635 I llama_init_from_model: n_ubatch      = 512
0.00.668.636 I llama_init_from_model: flash_attn    = 0
0.00.668.637 I llama_init_from_model: freq_base     = 10000.0
0.00.668.637 I llama_init_from_model: freq_scale    = 1
0.00.668.639 I ggml_metal_init: allocating
0.00.668.682 I ggml_metal_init: found device: Apple M4
0.00.668.691 I ggml_metal_init: picking default device: Apple M4
0.00.670.274 I ggml_metal_init: using embedded metal library
0.00.676.297 I ggml_metal_init: GPU name:   Apple M4
0.00.676.300 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.676.301 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.676.302 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.676.303 I ggml_metal_init: simdgroup reduction   = true
0.00.676.303 I ggml_metal_init: simdgroup matrix mul. = true
0.00.676.303 I ggml_metal_init: has residency sets    = true
0.00.676.303 I ggml_metal_init: has bfloat            = true
0.00.676.304 I ggml_metal_init: use bfloat            = true
0.00.676.304 I ggml_metal_init: hasUnifiedMemory      = true
0.00.676.308 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.693.538 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.103 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.739.108 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.739.143 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.743.273 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.743.275 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.743.275 I llama_init_from_model: graph nodes  = 967
0.00.743.275 I llama_init_from_model: graph splits = 2
0.00.743.282 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.743.411 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.412 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.537 I main: llama threadpool init, n_threads = 4
0.00.807.583 I 
0.00.807.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.607 I 
0.00.807.783 I sampler seed: 1234
0.00.807.787 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.799 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.799 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.799 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.681.865 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51900.58 tokens per second)
0.01.681.866 I llama_perf_context_print:        load time =     797.83 ms
0.01.681.869 I llama_perf_context_print: prompt eval time =      57.79 ms /     7 tokens (    8.26 ms per token,   121.13 tokens per second)
0.01.681.872 I llama_perf_context_print:        eval time =     813.33 ms /    63 runs   (   12.91 ms per token,    77.46 tokens per second)
0.01.681.872 I llama_perf_context_print:       total time =     875.07 ms /    70 tokens
0.01.682.152 I ggml_metal_free: deallocating

real	0m1.698s
user	0m0.110s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.579 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.592 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.901 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.906 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.908 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.909 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.909 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.909 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.911 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.913 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.913 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.913 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.914 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.914 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.915 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.917 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.917 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.918 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.975 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.530 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.850 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.853 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.853 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.854 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.854 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.855 I llama_model_loader: - type  f32:  194 tensors
0.00.055.855 I llama_model_loader: - type  f16:   98 tensors
0.00.055.856 I print_info: file format = GGUF V3 (latest)
0.00.055.857 I print_info: file type   = all F32 (guessed)
0.00.055.858 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.174 I load: special tokens cache size = 25
0.00.077.369 I load: token to piece cache size = 0.2984 MB
0.00.077.372 I print_info: arch             = gptneox
0.00.077.372 I print_info: vocab_only       = 0
0.00.077.373 I print_info: n_ctx_train      = 2048
0.00.077.373 I print_info: n_embd           = 2048
0.00.077.373 I print_info: n_layer          = 24
0.00.077.376 I print_info: n_head           = 16
0.00.077.377 I print_info: n_head_kv        = 16
0.00.077.377 I print_info: n_rot            = 32
0.00.077.378 I print_info: n_swa            = 0
0.00.077.378 I print_info: n_embd_head_k    = 128
0.00.077.378 I print_info: n_embd_head_v    = 128
0.00.077.379 I print_info: n_gqa            = 1
0.00.077.380 I print_info: n_embd_k_gqa     = 2048
0.00.077.380 I print_info: n_embd_v_gqa     = 2048
0.00.077.381 I print_info: f_norm_eps       = 1.0e-05
0.00.077.381 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.381 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.382 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.382 I print_info: f_logit_scale    = 0.0e+00
0.00.077.383 I print_info: n_ff             = 8192
0.00.077.383 I print_info: n_expert         = 0
0.00.077.383 I print_info: n_expert_used    = 0
0.00.077.383 I print_info: causal attn      = 1
0.00.077.383 I print_info: pooling type     = 0
0.00.077.383 I print_info: rope type        = 2
0.00.077.384 I print_info: rope scaling     = linear
0.00.077.384 I print_info: freq_base_train  = 10000.0
0.00.077.384 I print_info: freq_scale_train = 1
0.00.077.385 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.385 I print_info: rope_finetuned   = unknown
0.00.077.385 I print_info: ssm_d_conv       = 0
0.00.077.385 I print_info: ssm_d_inner      = 0
0.00.077.385 I print_info: ssm_d_state      = 0
0.00.077.385 I print_info: ssm_dt_rank      = 0
0.00.077.386 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.386 I print_info: model type       = 1.4B
0.00.077.386 I print_info: model params     = 1.41 B
0.00.077.386 I print_info: general.name     = 1.4B
0.00.077.387 I print_info: vocab type       = BPE
0.00.077.387 I print_info: n_vocab          = 50304
0.00.077.387 I print_info: n_merges         = 50009
0.00.077.388 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.388 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.388 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.390 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.390 I print_info: LF token         = 187 'Ċ'
0.00.077.391 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.391 I print_info: max token length = 1024
0.00.077.391 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.430.792 I load_tensors: offloading 24 repeating layers to GPU
0.01.430.795 I load_tensors: offloading output layer to GPU
0.01.430.796 I load_tensors: offloaded 25/25 layers to GPU
0.01.430.821 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.430.822 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.432.112 I llama_init_from_model: n_seq_max     = 1
0.01.432.113 I llama_init_from_model: n_ctx         = 128
0.01.432.113 I llama_init_from_model: n_ctx_per_seq = 128
0.01.432.113 I llama_init_from_model: n_batch       = 128
0.01.432.114 I llama_init_from_model: n_ubatch      = 128
0.01.432.114 I llama_init_from_model: flash_attn    = 0
0.01.432.114 I llama_init_from_model: freq_base     = 10000.0
0.01.432.115 I llama_init_from_model: freq_scale    = 1
0.01.432.115 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.432.116 I ggml_metal_init: allocating
0.01.432.215 I ggml_metal_init: found device: Apple M4
0.01.432.228 I ggml_metal_init: picking default device: Apple M4
0.01.434.420 I ggml_metal_init: using embedded metal library
0.01.438.322 I ggml_metal_init: GPU name:   Apple M4
0.01.438.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.438.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.438.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.438.326 I ggml_metal_init: simdgroup reduction   = true
0.01.438.326 I ggml_metal_init: simdgroup matrix mul. = true
0.01.438.326 I ggml_metal_init: has residency sets    = true
0.01.438.327 I ggml_metal_init: has bfloat            = true
0.01.438.327 I ggml_metal_init: use bfloat            = true
0.01.438.327 I ggml_metal_init: hasUnifiedMemory      = true
0.01.438.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.449.645 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.451.401 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.451.403 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.451.429 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.453.225 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.453.227 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.453.227 I llama_init_from_model: graph nodes  = 967
0.01.453.227 I llama_init_from_model: graph splits = 2
0.01.453.229 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.453.229 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.488.514 I 
0.01.488.557 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.488.561 I perplexity: tokenizing the input ..
0.01.493.907 I perplexity: tokenization took 5.344 ms
0.01.493.912 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.613.372 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.616.118 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.616.175 I llama_perf_context_print:        load time =    1463.91 ms
0.01.616.176 I llama_perf_context_print: prompt eval time =     119.15 ms /   128 tokens (    0.93 ms per token,  1074.30 tokens per second)
0.01.616.177 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.616.180 I llama_perf_context_print:       total time =     127.66 ms /   129 tokens
0.01.616.860 I ggml_metal_free: deallocating

real	0m1.806s
user	0m0.107s
sys	0m0.271s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.169 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.493 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.507 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.508 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.508 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.509 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.510 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.510 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.511 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.511 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.513 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.514 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.514 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.376 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.461 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.253 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.255 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.255 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.256 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.257 I llama_model_loader: - type  f32:  194 tensors
0.00.025.257 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.258 I print_info: file format = GGUF V3 (latest)
0.00.025.258 I print_info: file type   = Q8_0
0.00.025.259 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.576 I load: special tokens cache size = 25
0.00.039.647 I load: token to piece cache size = 0.2984 MB
0.00.039.651 I print_info: arch             = gptneox
0.00.039.651 I print_info: vocab_only       = 0
0.00.039.652 I print_info: n_ctx_train      = 2048
0.00.039.652 I print_info: n_embd           = 2048
0.00.039.652 I print_info: n_layer          = 24
0.00.039.656 I print_info: n_head           = 16
0.00.039.657 I print_info: n_head_kv        = 16
0.00.039.657 I print_info: n_rot            = 32
0.00.039.658 I print_info: n_swa            = 0
0.00.039.658 I print_info: n_embd_head_k    = 128
0.00.039.658 I print_info: n_embd_head_v    = 128
0.00.039.659 I print_info: n_gqa            = 1
0.00.039.659 I print_info: n_embd_k_gqa     = 2048
0.00.039.660 I print_info: n_embd_v_gqa     = 2048
0.00.039.661 I print_info: f_norm_eps       = 1.0e-05
0.00.039.661 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.661 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.661 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.661 I print_info: f_logit_scale    = 0.0e+00
0.00.039.662 I print_info: n_ff             = 8192
0.00.039.662 I print_info: n_expert         = 0
0.00.039.665 I print_info: n_expert_used    = 0
0.00.039.665 I print_info: causal attn      = 1
0.00.039.666 I print_info: pooling type     = 0
0.00.039.666 I print_info: rope type        = 2
0.00.039.666 I print_info: rope scaling     = linear
0.00.039.666 I print_info: freq_base_train  = 10000.0
0.00.039.667 I print_info: freq_scale_train = 1
0.00.039.667 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.668 I print_info: rope_finetuned   = unknown
0.00.039.668 I print_info: ssm_d_conv       = 0
0.00.039.668 I print_info: ssm_d_inner      = 0
0.00.039.668 I print_info: ssm_d_state      = 0
0.00.039.668 I print_info: ssm_dt_rank      = 0
0.00.039.668 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.668 I print_info: model type       = 1.4B
0.00.039.669 I print_info: model params     = 1.41 B
0.00.039.669 I print_info: general.name     = 1.4B
0.00.039.669 I print_info: vocab type       = BPE
0.00.039.670 I print_info: n_vocab          = 50304
0.00.039.671 I print_info: n_merges         = 50009
0.00.039.671 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.671 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.671 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.672 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.672 I print_info: LF token         = 187 'Ċ'
0.00.039.672 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.672 I print_info: max token length = 1024
0.00.039.673 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.866.498 I load_tensors: offloading 24 repeating layers to GPU
0.00.866.506 I load_tensors: offloading output layer to GPU
0.00.866.506 I load_tensors: offloaded 25/25 layers to GPU
0.00.866.537 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.866.539 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.867.960 I llama_init_from_model: n_seq_max     = 1
0.00.867.962 I llama_init_from_model: n_ctx         = 128
0.00.867.962 I llama_init_from_model: n_ctx_per_seq = 128
0.00.867.963 I llama_init_from_model: n_batch       = 128
0.00.867.963 I llama_init_from_model: n_ubatch      = 128
0.00.867.963 I llama_init_from_model: flash_attn    = 0
0.00.867.964 I llama_init_from_model: freq_base     = 10000.0
0.00.867.964 I llama_init_from_model: freq_scale    = 1
0.00.867.965 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.867.966 I ggml_metal_init: allocating
0.00.868.023 I ggml_metal_init: found device: Apple M4
0.00.868.034 I ggml_metal_init: picking default device: Apple M4
0.00.869.481 I ggml_metal_init: using embedded metal library
0.00.874.901 I ggml_metal_init: GPU name:   Apple M4
0.00.874.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.874.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.874.906 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.874.906 I ggml_metal_init: simdgroup reduction   = true
0.00.874.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.874.907 I ggml_metal_init: has residency sets    = true
0.00.874.907 I ggml_metal_init: has bfloat            = true
0.00.874.907 I ggml_metal_init: use bfloat            = true
0.00.874.908 I ggml_metal_init: hasUnifiedMemory      = true
0.00.874.917 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.890.349 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.893.740 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.893.746 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.893.819 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.896.928 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.896.929 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.896.930 I llama_init_from_model: graph nodes  = 967
0.00.896.930 I llama_init_from_model: graph splits = 2
0.00.896.932 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.896.933 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.924.246 I 
0.00.924.337 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.924.353 I perplexity: tokenizing the input ..
0.00.931.377 I perplexity: tokenization took 7.023 ms
0.00.931.382 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.068.403 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.069.735 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.069.761 I llama_perf_context_print:        load time =     915.07 ms
0.01.069.764 I llama_perf_context_print: prompt eval time =     136.79 ms /   128 tokens (    1.07 ms per token,   935.73 tokens per second)
0.01.069.765 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.069.766 I llama_perf_context_print:       total time =     145.52 ms /   129 tokens
0.01.070.160 I ggml_metal_free: deallocating

real	0m1.084s
user	0m0.076s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.748 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.163 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.169 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.176 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.177 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.179 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.180 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.180 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.180 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.180 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.181 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.181 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.183 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.183 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.949 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.865 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.867 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.867 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.868 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.868 I llama_model_loader: - type  f32:  194 tensors
0.00.025.868 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.869 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.869 I print_info: file format = GGUF V3 (latest)
0.00.025.870 I print_info: file type   = Q4_0
0.00.025.871 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.476 I load: special tokens cache size = 25
0.00.040.380 I load: token to piece cache size = 0.2984 MB
0.00.040.384 I print_info: arch             = gptneox
0.00.040.384 I print_info: vocab_only       = 0
0.00.040.385 I print_info: n_ctx_train      = 2048
0.00.040.385 I print_info: n_embd           = 2048
0.00.040.385 I print_info: n_layer          = 24
0.00.040.389 I print_info: n_head           = 16
0.00.040.390 I print_info: n_head_kv        = 16
0.00.040.390 I print_info: n_rot            = 32
0.00.040.390 I print_info: n_swa            = 0
0.00.040.391 I print_info: n_embd_head_k    = 128
0.00.040.391 I print_info: n_embd_head_v    = 128
0.00.040.392 I print_info: n_gqa            = 1
0.00.040.392 I print_info: n_embd_k_gqa     = 2048
0.00.040.393 I print_info: n_embd_v_gqa     = 2048
0.00.040.394 I print_info: f_norm_eps       = 1.0e-05
0.00.040.394 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.394 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.394 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.395 I print_info: f_logit_scale    = 0.0e+00
0.00.040.395 I print_info: n_ff             = 8192
0.00.040.395 I print_info: n_expert         = 0
0.00.040.396 I print_info: n_expert_used    = 0
0.00.040.396 I print_info: causal attn      = 1
0.00.040.396 I print_info: pooling type     = 0
0.00.040.396 I print_info: rope type        = 2
0.00.040.397 I print_info: rope scaling     = linear
0.00.040.398 I print_info: freq_base_train  = 10000.0
0.00.040.398 I print_info: freq_scale_train = 1
0.00.040.398 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.398 I print_info: rope_finetuned   = unknown
0.00.040.398 I print_info: ssm_d_conv       = 0
0.00.040.398 I print_info: ssm_d_inner      = 0
0.00.040.399 I print_info: ssm_d_state      = 0
0.00.040.399 I print_info: ssm_dt_rank      = 0
0.00.040.400 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.401 I print_info: model type       = 1.4B
0.00.040.401 I print_info: model params     = 1.41 B
0.00.040.403 I print_info: general.name     = 1.4B
0.00.040.404 I print_info: vocab type       = BPE
0.00.040.404 I print_info: n_vocab          = 50304
0.00.040.404 I print_info: n_merges         = 50009
0.00.040.404 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.404 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.404 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.404 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.405 I print_info: LF token         = 187 'Ċ'
0.00.040.405 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.405 I print_info: max token length = 1024
0.00.040.406 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.603.178 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.192 I load_tensors: offloading output layer to GPU
0.00.603.193 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.225 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.603.226 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.604.826 I llama_init_from_model: n_seq_max     = 1
0.00.604.830 I llama_init_from_model: n_ctx         = 128
0.00.604.831 I llama_init_from_model: n_ctx_per_seq = 128
0.00.604.831 I llama_init_from_model: n_batch       = 128
0.00.604.831 I llama_init_from_model: n_ubatch      = 128
0.00.604.832 I llama_init_from_model: flash_attn    = 0
0.00.604.833 I llama_init_from_model: freq_base     = 10000.0
0.00.604.833 I llama_init_from_model: freq_scale    = 1
0.00.604.834 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.604.836 I ggml_metal_init: allocating
0.00.604.901 I ggml_metal_init: found device: Apple M4
0.00.604.916 I ggml_metal_init: picking default device: Apple M4
0.00.606.558 I ggml_metal_init: using embedded metal library
0.00.612.149 I ggml_metal_init: GPU name:   Apple M4
0.00.612.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.165 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.166 I ggml_metal_init: simdgroup reduction   = true
0.00.612.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.166 I ggml_metal_init: has residency sets    = true
0.00.612.167 I ggml_metal_init: has bfloat            = true
0.00.612.167 I ggml_metal_init: use bfloat            = true
0.00.612.170 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.633 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.637.370 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.637.382 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.637.448 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.641.140 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.641.142 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.641.142 I llama_init_from_model: graph nodes  = 967
0.00.641.143 I llama_init_from_model: graph splits = 2
0.00.641.146 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.641.146 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.441 I 
0.00.670.516 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.523 I perplexity: tokenizing the input ..
0.00.677.271 I perplexity: tokenization took 6.745 ms
0.00.677.277 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.813.260 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.814.603 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.814.627 I llama_perf_context_print:        load time =     660.69 ms
0.00.814.630 I llama_perf_context_print: prompt eval time =     135.06 ms /   128 tokens (    1.06 ms per token,   947.73 tokens per second)
0.00.814.632 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.633 I llama_perf_context_print:       total time =     144.19 ms /   129 tokens
0.00.815.041 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.082s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.117 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.023 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.413 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.419 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.420 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.426 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.427 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.428 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.430 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.430 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.431 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.431 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.433 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.434 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.191 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.249 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.977 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.978 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.978 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.979 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.979 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.979 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.980 I llama_model_loader: - type  f32:  194 tensors
0.00.024.980 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.981 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.981 I print_info: file format = GGUF V3 (latest)
0.00.024.982 I print_info: file type   = Q4_1
0.00.024.984 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.515 I load: special tokens cache size = 25
0.00.039.601 I load: token to piece cache size = 0.2984 MB
0.00.039.605 I print_info: arch             = gptneox
0.00.039.606 I print_info: vocab_only       = 0
0.00.039.606 I print_info: n_ctx_train      = 2048
0.00.039.606 I print_info: n_embd           = 2048
0.00.039.606 I print_info: n_layer          = 24
0.00.039.611 I print_info: n_head           = 16
0.00.039.611 I print_info: n_head_kv        = 16
0.00.039.611 I print_info: n_rot            = 32
0.00.039.615 I print_info: n_swa            = 0
0.00.039.615 I print_info: n_embd_head_k    = 128
0.00.039.615 I print_info: n_embd_head_v    = 128
0.00.039.615 I print_info: n_gqa            = 1
0.00.039.616 I print_info: n_embd_k_gqa     = 2048
0.00.039.617 I print_info: n_embd_v_gqa     = 2048
0.00.039.617 I print_info: f_norm_eps       = 1.0e-05
0.00.039.617 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.618 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.618 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.618 I print_info: f_logit_scale    = 0.0e+00
0.00.039.618 I print_info: n_ff             = 8192
0.00.039.622 I print_info: n_expert         = 0
0.00.039.622 I print_info: n_expert_used    = 0
0.00.039.624 I print_info: causal attn      = 1
0.00.039.624 I print_info: pooling type     = 0
0.00.039.624 I print_info: rope type        = 2
0.00.039.624 I print_info: rope scaling     = linear
0.00.039.624 I print_info: freq_base_train  = 10000.0
0.00.039.625 I print_info: freq_scale_train = 1
0.00.039.625 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.625 I print_info: rope_finetuned   = unknown
0.00.039.625 I print_info: ssm_d_conv       = 0
0.00.039.625 I print_info: ssm_d_inner      = 0
0.00.039.625 I print_info: ssm_d_state      = 0
0.00.039.625 I print_info: ssm_dt_rank      = 0
0.00.039.626 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.626 I print_info: model type       = 1.4B
0.00.039.626 I print_info: model params     = 1.41 B
0.00.039.626 I print_info: general.name     = 1.4B
0.00.039.629 I print_info: vocab type       = BPE
0.00.039.629 I print_info: n_vocab          = 50304
0.00.039.630 I print_info: n_merges         = 50009
0.00.039.630 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.630 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.630 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.630 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.631 I print_info: LF token         = 187 'Ċ'
0.00.039.631 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.631 I print_info: max token length = 1024
0.00.039.631 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.606.289 I load_tensors: offloading 24 repeating layers to GPU
0.00.606.301 I load_tensors: offloading output layer to GPU
0.00.606.301 I load_tensors: offloaded 25/25 layers to GPU
0.00.606.335 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.606.337 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.608.020 I llama_init_from_model: n_seq_max     = 1
0.00.608.023 I llama_init_from_model: n_ctx         = 128
0.00.608.023 I llama_init_from_model: n_ctx_per_seq = 128
0.00.608.024 I llama_init_from_model: n_batch       = 128
0.00.608.025 I llama_init_from_model: n_ubatch      = 128
0.00.608.025 I llama_init_from_model: flash_attn    = 0
0.00.608.027 I llama_init_from_model: freq_base     = 10000.0
0.00.608.028 I llama_init_from_model: freq_scale    = 1
0.00.608.028 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.608.030 I ggml_metal_init: allocating
0.00.608.115 I ggml_metal_init: found device: Apple M4
0.00.608.130 I ggml_metal_init: picking default device: Apple M4
0.00.609.917 I ggml_metal_init: using embedded metal library
0.00.616.841 I ggml_metal_init: GPU name:   Apple M4
0.00.616.850 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.850 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.851 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.852 I ggml_metal_init: simdgroup reduction   = true
0.00.616.852 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.852 I ggml_metal_init: has residency sets    = true
0.00.616.852 I ggml_metal_init: has bfloat            = true
0.00.616.853 I ggml_metal_init: use bfloat            = true
0.00.616.854 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.858 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.084 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.638.585 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.638.594 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.638.639 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.641.851 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.641.853 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.641.854 I llama_init_from_model: graph nodes  = 967
0.00.641.854 I llama_init_from_model: graph splits = 2
0.00.641.857 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.641.857 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.454 I 
0.00.670.537 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.544 I perplexity: tokenizing the input ..
0.00.677.511 I perplexity: tokenization took 6.964 ms
0.00.677.515 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.234 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.813.571 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.813.596 I llama_perf_context_print:        load time =     661.42 ms
0.00.813.597 I llama_perf_context_print: prompt eval time =     134.43 ms /   128 tokens (    1.05 ms per token,   952.17 tokens per second)
0.00.813.598 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.598 I llama_perf_context_print:       total time =     143.15 ms /   129 tokens
0.00.814.037 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.080s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.959 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.014 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.020 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.021 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.022 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.022 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.029 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.030 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.030 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.031 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.031 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.031 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.031 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.032 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.034 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.034 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.034 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.880 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.963 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.813 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.815 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.815 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.816 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.816 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.816 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.817 I llama_model_loader: - type  f32:  194 tensors
0.00.024.817 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.817 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.818 I print_info: file format = GGUF V3 (latest)
0.00.024.818 I print_info: file type   = Q5_0
0.00.024.820 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.141 I load: special tokens cache size = 25
0.00.039.231 I load: token to piece cache size = 0.2984 MB
0.00.039.235 I print_info: arch             = gptneox
0.00.039.236 I print_info: vocab_only       = 0
0.00.039.236 I print_info: n_ctx_train      = 2048
0.00.039.236 I print_info: n_embd           = 2048
0.00.039.236 I print_info: n_layer          = 24
0.00.039.241 I print_info: n_head           = 16
0.00.039.242 I print_info: n_head_kv        = 16
0.00.039.242 I print_info: n_rot            = 32
0.00.039.242 I print_info: n_swa            = 0
0.00.039.242 I print_info: n_embd_head_k    = 128
0.00.039.242 I print_info: n_embd_head_v    = 128
0.00.039.243 I print_info: n_gqa            = 1
0.00.039.244 I print_info: n_embd_k_gqa     = 2048
0.00.039.245 I print_info: n_embd_v_gqa     = 2048
0.00.039.245 I print_info: f_norm_eps       = 1.0e-05
0.00.039.247 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.247 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.247 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.247 I print_info: f_logit_scale    = 0.0e+00
0.00.039.248 I print_info: n_ff             = 8192
0.00.039.248 I print_info: n_expert         = 0
0.00.039.248 I print_info: n_expert_used    = 0
0.00.039.248 I print_info: causal attn      = 1
0.00.039.249 I print_info: pooling type     = 0
0.00.039.249 I print_info: rope type        = 2
0.00.039.249 I print_info: rope scaling     = linear
0.00.039.250 I print_info: freq_base_train  = 10000.0
0.00.039.250 I print_info: freq_scale_train = 1
0.00.039.250 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.251 I print_info: rope_finetuned   = unknown
0.00.039.251 I print_info: ssm_d_conv       = 0
0.00.039.252 I print_info: ssm_d_inner      = 0
0.00.039.252 I print_info: ssm_d_state      = 0
0.00.039.252 I print_info: ssm_dt_rank      = 0
0.00.039.253 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.253 I print_info: model type       = 1.4B
0.00.039.253 I print_info: model params     = 1.41 B
0.00.039.253 I print_info: general.name     = 1.4B
0.00.039.254 I print_info: vocab type       = BPE
0.00.039.254 I print_info: n_vocab          = 50304
0.00.039.254 I print_info: n_merges         = 50009
0.00.039.254 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.254 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.255 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.255 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.255 I print_info: LF token         = 187 'Ċ'
0.00.039.255 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.255 I print_info: max token length = 1024
0.00.039.256 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.457 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.470 I load_tensors: offloading output layer to GPU
0.00.619.471 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.505 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.619.507 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.621.176 I llama_init_from_model: n_seq_max     = 1
0.00.621.179 I llama_init_from_model: n_ctx         = 128
0.00.621.180 I llama_init_from_model: n_ctx_per_seq = 128
0.00.621.180 I llama_init_from_model: n_batch       = 128
0.00.621.181 I llama_init_from_model: n_ubatch      = 128
0.00.621.181 I llama_init_from_model: flash_attn    = 0
0.00.621.183 I llama_init_from_model: freq_base     = 10000.0
0.00.621.184 I llama_init_from_model: freq_scale    = 1
0.00.621.184 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.621.188 I ggml_metal_init: allocating
0.00.621.277 I ggml_metal_init: found device: Apple M4
0.00.621.290 I ggml_metal_init: picking default device: Apple M4
0.00.623.046 I ggml_metal_init: using embedded metal library
0.00.629.840 I ggml_metal_init: GPU name:   Apple M4
0.00.629.847 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.848 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.849 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.850 I ggml_metal_init: simdgroup reduction   = true
0.00.629.850 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.850 I ggml_metal_init: has residency sets    = true
0.00.629.850 I ggml_metal_init: has bfloat            = true
0.00.629.851 I ggml_metal_init: use bfloat            = true
0.00.629.852 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.855 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.648.362 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.652.034 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.652.037 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.652.089 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.655.363 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.655.365 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.655.366 I llama_init_from_model: graph nodes  = 967
0.00.655.366 I llama_init_from_model: graph splits = 2
0.00.655.369 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.655.369 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.297 I 
0.00.684.377 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.385 I perplexity: tokenizing the input ..
0.00.692.122 I perplexity: tokenization took 7.732 ms
0.00.692.136 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.917 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.829.247 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.829.269 I llama_perf_context_print:        load time =     675.33 ms
0.00.829.272 I llama_perf_context_print: prompt eval time =     134.88 ms /   128 tokens (    1.05 ms per token,   949.01 tokens per second)
0.00.829.273 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.273 I llama_perf_context_print:       total time =     144.97 ms /   129 tokens
0.00.829.647 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.082s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.035 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.122 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.140 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.142 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.142 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.143 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.143 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.144 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.144 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.144 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.145 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.148 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.149 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.149 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.911 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.010 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.895 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.897 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.898 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.898 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.899 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.899 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.900 I llama_model_loader: - type  f32:  194 tensors
0.00.027.900 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.900 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.901 I print_info: file format = GGUF V3 (latest)
0.00.027.901 I print_info: file type   = Q5_1
0.00.027.903 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.036.219 I load: special tokens cache size = 25
0.00.042.398 I load: token to piece cache size = 0.2984 MB
0.00.042.402 I print_info: arch             = gptneox
0.00.042.402 I print_info: vocab_only       = 0
0.00.042.403 I print_info: n_ctx_train      = 2048
0.00.042.403 I print_info: n_embd           = 2048
0.00.042.403 I print_info: n_layer          = 24
0.00.042.407 I print_info: n_head           = 16
0.00.042.408 I print_info: n_head_kv        = 16
0.00.042.408 I print_info: n_rot            = 32
0.00.042.409 I print_info: n_swa            = 0
0.00.042.409 I print_info: n_embd_head_k    = 128
0.00.042.409 I print_info: n_embd_head_v    = 128
0.00.042.410 I print_info: n_gqa            = 1
0.00.042.410 I print_info: n_embd_k_gqa     = 2048
0.00.042.411 I print_info: n_embd_v_gqa     = 2048
0.00.042.412 I print_info: f_norm_eps       = 1.0e-05
0.00.042.412 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.412 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.412 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.413 I print_info: f_logit_scale    = 0.0e+00
0.00.042.413 I print_info: n_ff             = 8192
0.00.042.413 I print_info: n_expert         = 0
0.00.042.414 I print_info: n_expert_used    = 0
0.00.042.414 I print_info: causal attn      = 1
0.00.042.414 I print_info: pooling type     = 0
0.00.042.414 I print_info: rope type        = 2
0.00.042.414 I print_info: rope scaling     = linear
0.00.042.414 I print_info: freq_base_train  = 10000.0
0.00.042.415 I print_info: freq_scale_train = 1
0.00.042.415 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.415 I print_info: rope_finetuned   = unknown
0.00.042.415 I print_info: ssm_d_conv       = 0
0.00.042.415 I print_info: ssm_d_inner      = 0
0.00.042.415 I print_info: ssm_d_state      = 0
0.00.042.415 I print_info: ssm_dt_rank      = 0
0.00.042.415 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.416 I print_info: model type       = 1.4B
0.00.042.416 I print_info: model params     = 1.41 B
0.00.042.416 I print_info: general.name     = 1.4B
0.00.042.417 I print_info: vocab type       = BPE
0.00.042.417 I print_info: n_vocab          = 50304
0.00.042.417 I print_info: n_merges         = 50009
0.00.042.420 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.420 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.420 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.420 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.421 I print_info: LF token         = 187 'Ċ'
0.00.042.421 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.421 I print_info: max token length = 1024
0.00.042.421 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.625.327 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.343 I load_tensors: offloading output layer to GPU
0.00.625.343 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.381 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.625.383 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.626.918 I llama_init_from_model: n_seq_max     = 1
0.00.626.922 I llama_init_from_model: n_ctx         = 128
0.00.626.923 I llama_init_from_model: n_ctx_per_seq = 128
0.00.626.924 I llama_init_from_model: n_batch       = 128
0.00.626.925 I llama_init_from_model: n_ubatch      = 128
0.00.626.925 I llama_init_from_model: flash_attn    = 0
0.00.626.928 I llama_init_from_model: freq_base     = 10000.0
0.00.626.928 I llama_init_from_model: freq_scale    = 1
0.00.626.929 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.626.934 I ggml_metal_init: allocating
0.00.627.087 I ggml_metal_init: found device: Apple M4
0.00.627.101 I ggml_metal_init: picking default device: Apple M4
0.00.629.093 I ggml_metal_init: using embedded metal library
0.00.635.919 I ggml_metal_init: GPU name:   Apple M4
0.00.635.925 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.926 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.927 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.927 I ggml_metal_init: simdgroup reduction   = true
0.00.635.928 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.928 I ggml_metal_init: has residency sets    = true
0.00.635.928 I ggml_metal_init: has bfloat            = true
0.00.635.928 I ggml_metal_init: use bfloat            = true
0.00.635.930 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.933 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.653.548 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.656.943 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.656.947 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.656.991 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.660.418 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.660.420 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.660.420 I llama_init_from_model: graph nodes  = 967
0.00.660.421 I llama_init_from_model: graph splits = 2
0.00.660.424 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.660.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.232 I 
0.00.689.306 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.315 I perplexity: tokenizing the input ..
0.00.696.165 I perplexity: tokenization took 6.847 ms
0.00.696.178 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.831.102 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.832.417 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.832.441 I llama_perf_context_print:        load time =     679.19 ms
0.00.832.442 I llama_perf_context_print: prompt eval time =     134.37 ms /   128 tokens (    1.05 ms per token,   952.57 tokens per second)
0.00.832.443 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.832.443 I llama_perf_context_print:       total time =     143.21 ms /   129 tokens
0.00.832.783 I ggml_metal_free: deallocating

real	0m0.849s
user	0m0.082s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.068 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.218 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.225 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.226 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.227 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.230 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.231 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.231 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.092 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.264 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.093 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.095 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.095 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.096 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.096 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.096 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.097 I llama_model_loader: - type  f32:  194 tensors
0.00.025.097 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.098 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.098 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.098 I print_info: file format = GGUF V3 (latest)
0.00.025.099 I print_info: file type   = Q2_K - Medium
0.00.025.101 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.703 I load: special tokens cache size = 25
0.00.039.562 I load: token to piece cache size = 0.2984 MB
0.00.039.566 I print_info: arch             = gptneox
0.00.039.566 I print_info: vocab_only       = 0
0.00.039.567 I print_info: n_ctx_train      = 2048
0.00.039.567 I print_info: n_embd           = 2048
0.00.039.567 I print_info: n_layer          = 24
0.00.039.571 I print_info: n_head           = 16
0.00.039.571 I print_info: n_head_kv        = 16
0.00.039.572 I print_info: n_rot            = 32
0.00.039.572 I print_info: n_swa            = 0
0.00.039.572 I print_info: n_embd_head_k    = 128
0.00.039.572 I print_info: n_embd_head_v    = 128
0.00.039.573 I print_info: n_gqa            = 1
0.00.039.577 I print_info: n_embd_k_gqa     = 2048
0.00.039.578 I print_info: n_embd_v_gqa     = 2048
0.00.039.578 I print_info: f_norm_eps       = 1.0e-05
0.00.039.579 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.579 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.579 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.579 I print_info: f_logit_scale    = 0.0e+00
0.00.039.580 I print_info: n_ff             = 8192
0.00.039.580 I print_info: n_expert         = 0
0.00.039.580 I print_info: n_expert_used    = 0
0.00.039.580 I print_info: causal attn      = 1
0.00.039.580 I print_info: pooling type     = 0
0.00.039.581 I print_info: rope type        = 2
0.00.039.581 I print_info: rope scaling     = linear
0.00.039.581 I print_info: freq_base_train  = 10000.0
0.00.039.581 I print_info: freq_scale_train = 1
0.00.039.581 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.582 I print_info: rope_finetuned   = unknown
0.00.039.582 I print_info: ssm_d_conv       = 0
0.00.039.582 I print_info: ssm_d_inner      = 0
0.00.039.582 I print_info: ssm_d_state      = 0
0.00.039.582 I print_info: ssm_dt_rank      = 0
0.00.039.582 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.583 I print_info: model type       = 1.4B
0.00.039.583 I print_info: model params     = 1.41 B
0.00.039.583 I print_info: general.name     = 1.4B
0.00.039.584 I print_info: vocab type       = BPE
0.00.039.584 I print_info: n_vocab          = 50304
0.00.039.584 I print_info: n_merges         = 50009
0.00.039.584 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.584 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.584 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.585 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.585 I print_info: LF token         = 187 'Ċ'
0.00.039.585 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.585 I print_info: max token length = 1024
0.00.039.586 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.332.869 I load_tensors: offloading 24 repeating layers to GPU
0.00.332.884 I load_tensors: offloading output layer to GPU
0.00.332.884 I load_tensors: offloaded 25/25 layers to GPU
0.00.332.918 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.332.920 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.334.662 I llama_init_from_model: n_seq_max     = 1
0.00.334.665 I llama_init_from_model: n_ctx         = 128
0.00.334.666 I llama_init_from_model: n_ctx_per_seq = 128
0.00.334.666 I llama_init_from_model: n_batch       = 128
0.00.334.667 I llama_init_from_model: n_ubatch      = 128
0.00.334.667 I llama_init_from_model: flash_attn    = 0
0.00.334.669 I llama_init_from_model: freq_base     = 10000.0
0.00.334.670 I llama_init_from_model: freq_scale    = 1
0.00.334.670 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.334.673 I ggml_metal_init: allocating
0.00.334.753 I ggml_metal_init: found device: Apple M4
0.00.334.767 I ggml_metal_init: picking default device: Apple M4
0.00.336.529 I ggml_metal_init: using embedded metal library
0.00.342.119 I ggml_metal_init: GPU name:   Apple M4
0.00.342.131 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.342.132 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.342.132 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.342.133 I ggml_metal_init: simdgroup reduction   = true
0.00.342.133 I ggml_metal_init: simdgroup matrix mul. = true
0.00.342.134 I ggml_metal_init: has residency sets    = true
0.00.342.134 I ggml_metal_init: has bfloat            = true
0.00.342.134 I ggml_metal_init: use bfloat            = true
0.00.342.136 I ggml_metal_init: hasUnifiedMemory      = true
0.00.342.140 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.363.555 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.367.210 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.367.218 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.367.279 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.370.637 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.370.639 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.370.640 I llama_init_from_model: graph nodes  = 967
0.00.370.640 I llama_init_from_model: graph splits = 2
0.00.370.643 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.370.643 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.402.806 I 
0.00.402.895 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.402.902 I perplexity: tokenizing the input ..
0.00.409.315 I perplexity: tokenization took 6.411 ms
0.00.409.320 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.553.693 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.555.034 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.555.056 I llama_perf_context_print:        load time =     393.73 ms
0.00.555.057 I llama_perf_context_print: prompt eval time =     143.97 ms /   128 tokens (    1.12 ms per token,   889.05 tokens per second)
0.00.555.059 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.555.061 I llama_perf_context_print:       total time =     152.25 ms /   129 tokens
0.00.555.441 I ggml_metal_free: deallocating

real	0m0.569s
user	0m0.081s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.823 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.977 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.984 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.986 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.986 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.987 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.987 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.987 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.988 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.988 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.989 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.990 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.990 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.992 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.993 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.993 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.767 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.878 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.622 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.623 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.624 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.624 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.624 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.625 I llama_model_loader: - type  f32:  194 tensors
0.00.024.625 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.625 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.626 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.626 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.629 I print_info: file format = GGUF V3 (latest)
0.00.024.629 I print_info: file type   = Q3_K - Medium
0.00.024.631 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.258 I load: special tokens cache size = 25
0.00.039.521 I load: token to piece cache size = 0.2984 MB
0.00.039.526 I print_info: arch             = gptneox
0.00.039.526 I print_info: vocab_only       = 0
0.00.039.526 I print_info: n_ctx_train      = 2048
0.00.039.526 I print_info: n_embd           = 2048
0.00.039.526 I print_info: n_layer          = 24
0.00.039.531 I print_info: n_head           = 16
0.00.039.532 I print_info: n_head_kv        = 16
0.00.039.532 I print_info: n_rot            = 32
0.00.039.532 I print_info: n_swa            = 0
0.00.039.532 I print_info: n_embd_head_k    = 128
0.00.039.533 I print_info: n_embd_head_v    = 128
0.00.039.533 I print_info: n_gqa            = 1
0.00.039.534 I print_info: n_embd_k_gqa     = 2048
0.00.039.535 I print_info: n_embd_v_gqa     = 2048
0.00.039.535 I print_info: f_norm_eps       = 1.0e-05
0.00.039.536 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.536 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.536 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.536 I print_info: f_logit_scale    = 0.0e+00
0.00.039.537 I print_info: n_ff             = 8192
0.00.039.537 I print_info: n_expert         = 0
0.00.039.537 I print_info: n_expert_used    = 0
0.00.039.537 I print_info: causal attn      = 1
0.00.039.537 I print_info: pooling type     = 0
0.00.039.537 I print_info: rope type        = 2
0.00.039.541 I print_info: rope scaling     = linear
0.00.039.541 I print_info: freq_base_train  = 10000.0
0.00.039.541 I print_info: freq_scale_train = 1
0.00.039.541 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.542 I print_info: rope_finetuned   = unknown
0.00.039.542 I print_info: ssm_d_conv       = 0
0.00.039.542 I print_info: ssm_d_inner      = 0
0.00.039.542 I print_info: ssm_d_state      = 0
0.00.039.542 I print_info: ssm_dt_rank      = 0
0.00.039.542 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.542 I print_info: model type       = 1.4B
0.00.039.543 I print_info: model params     = 1.41 B
0.00.039.543 I print_info: general.name     = 1.4B
0.00.039.543 I print_info: vocab type       = BPE
0.00.039.543 I print_info: n_vocab          = 50304
0.00.039.544 I print_info: n_merges         = 50009
0.00.039.544 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.544 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.544 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.544 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: LF token         = 187 'Ċ'
0.00.039.546 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: max token length = 1024
0.00.039.547 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.437.669 I load_tensors: offloading 24 repeating layers to GPU
0.00.437.675 I load_tensors: offloading output layer to GPU
0.00.437.676 I load_tensors: offloaded 25/25 layers to GPU
0.00.437.708 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.437.712 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.439.240 I llama_init_from_model: n_seq_max     = 1
0.00.439.243 I llama_init_from_model: n_ctx         = 128
0.00.439.243 I llama_init_from_model: n_ctx_per_seq = 128
0.00.439.244 I llama_init_from_model: n_batch       = 128
0.00.439.244 I llama_init_from_model: n_ubatch      = 128
0.00.439.245 I llama_init_from_model: flash_attn    = 0
0.00.439.247 I llama_init_from_model: freq_base     = 10000.0
0.00.439.247 I llama_init_from_model: freq_scale    = 1
0.00.439.248 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.439.250 I ggml_metal_init: allocating
0.00.439.335 I ggml_metal_init: found device: Apple M4
0.00.439.347 I ggml_metal_init: picking default device: Apple M4
0.00.441.175 I ggml_metal_init: using embedded metal library
0.00.446.836 I ggml_metal_init: GPU name:   Apple M4
0.00.446.842 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.446.843 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.446.843 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.446.844 I ggml_metal_init: simdgroup reduction   = true
0.00.446.845 I ggml_metal_init: simdgroup matrix mul. = true
0.00.446.845 I ggml_metal_init: has residency sets    = true
0.00.446.845 I ggml_metal_init: has bfloat            = true
0.00.446.845 I ggml_metal_init: use bfloat            = true
0.00.446.847 I ggml_metal_init: hasUnifiedMemory      = true
0.00.446.852 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.466.160 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.469.759 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.469.763 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.469.807 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.472.930 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.472.932 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.472.933 I llama_init_from_model: graph nodes  = 967
0.00.472.933 I llama_init_from_model: graph splits = 2
0.00.472.936 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.472.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.961 I 
0.00.501.033 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.501.042 I perplexity: tokenizing the input ..
0.00.508.632 I perplexity: tokenization took 7.588 ms
0.00.508.641 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.653.368 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.654.728 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.654.755 I llama_perf_context_print:        load time =     492.13 ms
0.00.654.755 I llama_perf_context_print: prompt eval time =     143.87 ms /   128 tokens (    1.12 ms per token,   889.72 tokens per second)
0.00.654.756 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.654.756 I llama_perf_context_print:       total time =     153.80 ms /   129 tokens
0.00.655.139 I ggml_metal_free: deallocating

real	0m0.669s
user	0m0.081s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.884 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.907 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.913 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.914 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.915 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.915 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.915 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.916 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.917 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.917 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.917 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.918 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.918 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.919 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.919 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.921 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.921 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.922 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.720 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.919 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.726 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.728 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.728 I llama_model_loader: - type  f32:  194 tensors
0.00.024.729 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.729 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.729 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.732 I print_info: file format = GGUF V3 (latest)
0.00.024.732 I print_info: file type   = Q4_K - Medium
0.00.024.733 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.295 I load: special tokens cache size = 25
0.00.039.342 I load: token to piece cache size = 0.2984 MB
0.00.039.347 I print_info: arch             = gptneox
0.00.039.348 I print_info: vocab_only       = 0
0.00.039.348 I print_info: n_ctx_train      = 2048
0.00.039.348 I print_info: n_embd           = 2048
0.00.039.348 I print_info: n_layer          = 24
0.00.039.353 I print_info: n_head           = 16
0.00.039.353 I print_info: n_head_kv        = 16
0.00.039.353 I print_info: n_rot            = 32
0.00.039.354 I print_info: n_swa            = 0
0.00.039.354 I print_info: n_embd_head_k    = 128
0.00.039.354 I print_info: n_embd_head_v    = 128
0.00.039.355 I print_info: n_gqa            = 1
0.00.039.355 I print_info: n_embd_k_gqa     = 2048
0.00.039.356 I print_info: n_embd_v_gqa     = 2048
0.00.039.357 I print_info: f_norm_eps       = 1.0e-05
0.00.039.357 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.357 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.360 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.360 I print_info: f_logit_scale    = 0.0e+00
0.00.039.360 I print_info: n_ff             = 8192
0.00.039.361 I print_info: n_expert         = 0
0.00.039.361 I print_info: n_expert_used    = 0
0.00.039.361 I print_info: causal attn      = 1
0.00.039.361 I print_info: pooling type     = 0
0.00.039.361 I print_info: rope type        = 2
0.00.039.361 I print_info: rope scaling     = linear
0.00.039.362 I print_info: freq_base_train  = 10000.0
0.00.039.362 I print_info: freq_scale_train = 1
0.00.039.362 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.362 I print_info: rope_finetuned   = unknown
0.00.039.366 I print_info: ssm_d_conv       = 0
0.00.039.367 I print_info: ssm_d_inner      = 0
0.00.039.368 I print_info: ssm_d_state      = 0
0.00.039.368 I print_info: ssm_dt_rank      = 0
0.00.039.368 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.368 I print_info: model type       = 1.4B
0.00.039.368 I print_info: model params     = 1.41 B
0.00.039.369 I print_info: general.name     = 1.4B
0.00.039.369 I print_info: vocab type       = BPE
0.00.039.369 I print_info: n_vocab          = 50304
0.00.039.369 I print_info: n_merges         = 50009
0.00.039.370 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.370 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.370 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.370 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.370 I print_info: LF token         = 187 'Ċ'
0.00.039.371 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.371 I print_info: max token length = 1024
0.00.039.371 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.530.361 I load_tensors: offloading 24 repeating layers to GPU
0.00.530.377 I load_tensors: offloading output layer to GPU
0.00.530.377 I load_tensors: offloaded 25/25 layers to GPU
0.00.530.408 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.530.410 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.531.986 I llama_init_from_model: n_seq_max     = 1
0.00.531.991 I llama_init_from_model: n_ctx         = 128
0.00.531.992 I llama_init_from_model: n_ctx_per_seq = 128
0.00.531.992 I llama_init_from_model: n_batch       = 128
0.00.531.992 I llama_init_from_model: n_ubatch      = 128
0.00.531.993 I llama_init_from_model: flash_attn    = 0
0.00.531.994 I llama_init_from_model: freq_base     = 10000.0
0.00.531.994 I llama_init_from_model: freq_scale    = 1
0.00.531.995 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.531.997 I ggml_metal_init: allocating
0.00.532.052 I ggml_metal_init: found device: Apple M4
0.00.532.065 I ggml_metal_init: picking default device: Apple M4
0.00.533.793 I ggml_metal_init: using embedded metal library
0.00.540.718 I ggml_metal_init: GPU name:   Apple M4
0.00.540.724 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.540.725 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.540.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.540.730 I ggml_metal_init: simdgroup reduction   = true
0.00.540.730 I ggml_metal_init: simdgroup matrix mul. = true
0.00.540.730 I ggml_metal_init: has residency sets    = true
0.00.540.730 I ggml_metal_init: has bfloat            = true
0.00.540.731 I ggml_metal_init: use bfloat            = true
0.00.540.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.540.738 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.559.289 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.562.914 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.562.919 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.562.965 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.566.214 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.566.216 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.566.216 I llama_init_from_model: graph nodes  = 967
0.00.566.217 I llama_init_from_model: graph splits = 2
0.00.566.220 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.566.220 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.376 I 
0.00.596.467 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.476 I perplexity: tokenizing the input ..
0.00.603.868 I perplexity: tokenization took 7.388 ms
0.00.603.874 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.570 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.748.929 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.748.959 I llama_perf_context_print:        load time =     587.48 ms
0.00.748.960 I llama_perf_context_print: prompt eval time =     142.72 ms /   128 tokens (    1.11 ms per token,   896.87 tokens per second)
0.00.748.960 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.960 I llama_perf_context_print:       total time =     152.59 ms /   129 tokens
0.00.749.422 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.081s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.947 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.908 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.915 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.921 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.922 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.922 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.922 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.923 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.923 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.924 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.924 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.924 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.925 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.925 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.927 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.927 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.928 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.736 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.827 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.628 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.630 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.630 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.631 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.631 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.631 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.632 I llama_model_loader: - type  f32:  194 tensors
0.00.025.632 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.632 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.633 I print_info: file format = GGUF V3 (latest)
0.00.025.634 I print_info: file type   = Q5_K - Medium
0.00.025.635 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.466 I load: special tokens cache size = 25
0.00.040.323 I load: token to piece cache size = 0.2984 MB
0.00.040.327 I print_info: arch             = gptneox
0.00.040.327 I print_info: vocab_only       = 0
0.00.040.328 I print_info: n_ctx_train      = 2048
0.00.040.328 I print_info: n_embd           = 2048
0.00.040.328 I print_info: n_layer          = 24
0.00.040.333 I print_info: n_head           = 16
0.00.040.333 I print_info: n_head_kv        = 16
0.00.040.334 I print_info: n_rot            = 32
0.00.040.334 I print_info: n_swa            = 0
0.00.040.334 I print_info: n_embd_head_k    = 128
0.00.040.334 I print_info: n_embd_head_v    = 128
0.00.040.335 I print_info: n_gqa            = 1
0.00.040.336 I print_info: n_embd_k_gqa     = 2048
0.00.040.336 I print_info: n_embd_v_gqa     = 2048
0.00.040.337 I print_info: f_norm_eps       = 1.0e-05
0.00.040.340 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.341 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.341 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.341 I print_info: f_logit_scale    = 0.0e+00
0.00.040.342 I print_info: n_ff             = 8192
0.00.040.342 I print_info: n_expert         = 0
0.00.040.342 I print_info: n_expert_used    = 0
0.00.040.342 I print_info: causal attn      = 1
0.00.040.342 I print_info: pooling type     = 0
0.00.040.342 I print_info: rope type        = 2
0.00.040.343 I print_info: rope scaling     = linear
0.00.040.343 I print_info: freq_base_train  = 10000.0
0.00.040.343 I print_info: freq_scale_train = 1
0.00.040.343 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.345 I print_info: rope_finetuned   = unknown
0.00.040.345 I print_info: ssm_d_conv       = 0
0.00.040.345 I print_info: ssm_d_inner      = 0
0.00.040.345 I print_info: ssm_d_state      = 0
0.00.040.345 I print_info: ssm_dt_rank      = 0
0.00.040.345 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.346 I print_info: model type       = 1.4B
0.00.040.346 I print_info: model params     = 1.41 B
0.00.040.346 I print_info: general.name     = 1.4B
0.00.040.347 I print_info: vocab type       = BPE
0.00.040.347 I print_info: n_vocab          = 50304
0.00.040.347 I print_info: n_merges         = 50009
0.00.040.347 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.347 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.347 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: LF token         = 187 'Ċ'
0.00.040.348 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: max token length = 1024
0.00.040.349 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.246 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.260 I load_tensors: offloading output layer to GPU
0.00.587.261 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.301 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.587.303 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.589.100 I llama_init_from_model: n_seq_max     = 1
0.00.589.103 I llama_init_from_model: n_ctx         = 128
0.00.589.103 I llama_init_from_model: n_ctx_per_seq = 128
0.00.589.104 I llama_init_from_model: n_batch       = 128
0.00.589.104 I llama_init_from_model: n_ubatch      = 128
0.00.589.105 I llama_init_from_model: flash_attn    = 0
0.00.589.108 I llama_init_from_model: freq_base     = 10000.0
0.00.589.109 I llama_init_from_model: freq_scale    = 1
0.00.589.109 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.112 I ggml_metal_init: allocating
0.00.589.193 I ggml_metal_init: found device: Apple M4
0.00.589.206 I ggml_metal_init: picking default device: Apple M4
0.00.590.931 I ggml_metal_init: using embedded metal library
0.00.597.519 I ggml_metal_init: GPU name:   Apple M4
0.00.597.524 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.525 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.525 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.526 I ggml_metal_init: simdgroup reduction   = true
0.00.597.526 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.527 I ggml_metal_init: has residency sets    = true
0.00.597.527 I ggml_metal_init: has bfloat            = true
0.00.597.527 I ggml_metal_init: use bfloat            = true
0.00.597.528 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.541 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.455 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.972 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.979 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.030 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.173 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.622.174 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.622.175 I llama_init_from_model: graph nodes  = 967
0.00.622.175 I llama_init_from_model: graph splits = 2
0.00.622.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.622.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.610 I 
0.00.653.697 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.705 I perplexity: tokenizing the input ..
0.00.660.948 I perplexity: tokenization took 7.239 ms
0.00.660.963 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.064 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.800.401 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.800.422 I llama_perf_context_print:        load time =     643.65 ms
0.00.800.422 I llama_perf_context_print: prompt eval time =     137.15 ms /   128 tokens (    1.07 ms per token,   933.28 tokens per second)
0.00.800.423 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.423 I llama_perf_context_print:       total time =     146.82 ms /   129 tokens
0.00.800.799 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.081s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.179 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.039 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.049 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.049 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.049 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.050 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.051 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.051 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.051 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.052 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.055 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.055 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.058 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.058 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.058 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.872 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.957 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.718 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.719 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.720 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.720 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.720 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.721 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.721 I llama_model_loader: - type  f32:  194 tensors
0.00.024.721 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.722 I print_info: file format = GGUF V3 (latest)
0.00.024.723 I print_info: file type   = Q6_K
0.00.024.724 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.880 I load: special tokens cache size = 25
0.00.038.832 I load: token to piece cache size = 0.2984 MB
0.00.038.836 I print_info: arch             = gptneox
0.00.038.836 I print_info: vocab_only       = 0
0.00.038.836 I print_info: n_ctx_train      = 2048
0.00.038.837 I print_info: n_embd           = 2048
0.00.038.837 I print_info: n_layer          = 24
0.00.038.841 I print_info: n_head           = 16
0.00.038.842 I print_info: n_head_kv        = 16
0.00.038.842 I print_info: n_rot            = 32
0.00.038.843 I print_info: n_swa            = 0
0.00.038.843 I print_info: n_embd_head_k    = 128
0.00.038.843 I print_info: n_embd_head_v    = 128
0.00.038.844 I print_info: n_gqa            = 1
0.00.038.844 I print_info: n_embd_k_gqa     = 2048
0.00.038.846 I print_info: n_embd_v_gqa     = 2048
0.00.038.848 I print_info: f_norm_eps       = 1.0e-05
0.00.038.849 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.849 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.849 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.850 I print_info: f_logit_scale    = 0.0e+00
0.00.038.851 I print_info: n_ff             = 8192
0.00.038.852 I print_info: n_expert         = 0
0.00.038.852 I print_info: n_expert_used    = 0
0.00.038.852 I print_info: causal attn      = 1
0.00.038.852 I print_info: pooling type     = 0
0.00.038.852 I print_info: rope type        = 2
0.00.038.853 I print_info: rope scaling     = linear
0.00.038.853 I print_info: freq_base_train  = 10000.0
0.00.038.853 I print_info: freq_scale_train = 1
0.00.038.853 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.854 I print_info: rope_finetuned   = unknown
0.00.038.854 I print_info: ssm_d_conv       = 0
0.00.038.854 I print_info: ssm_d_inner      = 0
0.00.038.854 I print_info: ssm_d_state      = 0
0.00.038.854 I print_info: ssm_dt_rank      = 0
0.00.038.854 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.854 I print_info: model type       = 1.4B
0.00.038.855 I print_info: model params     = 1.41 B
0.00.038.855 I print_info: general.name     = 1.4B
0.00.038.857 I print_info: vocab type       = BPE
0.00.038.857 I print_info: n_vocab          = 50304
0.00.038.857 I print_info: n_merges         = 50009
0.00.038.857 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.857 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.857 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.857 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.858 I print_info: LF token         = 187 'Ċ'
0.00.038.858 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.858 I print_info: max token length = 1024
0.00.038.859 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.593.935 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.944 I load_tensors: offloading output layer to GPU
0.00.593.945 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.975 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.593.978 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.595.548 I llama_init_from_model: n_seq_max     = 1
0.00.595.550 I llama_init_from_model: n_ctx         = 128
0.00.595.550 I llama_init_from_model: n_ctx_per_seq = 128
0.00.595.551 I llama_init_from_model: n_batch       = 128
0.00.595.551 I llama_init_from_model: n_ubatch      = 128
0.00.595.552 I llama_init_from_model: flash_attn    = 0
0.00.595.553 I llama_init_from_model: freq_base     = 10000.0
0.00.595.553 I llama_init_from_model: freq_scale    = 1
0.00.595.554 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.556 I ggml_metal_init: allocating
0.00.595.612 I ggml_metal_init: found device: Apple M4
0.00.595.625 I ggml_metal_init: picking default device: Apple M4
0.00.597.077 I ggml_metal_init: using embedded metal library
0.00.603.141 I ggml_metal_init: GPU name:   Apple M4
0.00.603.145 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.603.146 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.603.147 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.147 I ggml_metal_init: simdgroup reduction   = true
0.00.603.147 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.148 I ggml_metal_init: has residency sets    = true
0.00.603.148 I ggml_metal_init: has bfloat            = true
0.00.603.148 I ggml_metal_init: use bfloat            = true
0.00.603.149 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.154 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.459 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.957 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.623.963 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.624.016 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.627.086 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.627.087 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.627.088 I llama_init_from_model: graph nodes  = 967
0.00.627.088 I llama_init_from_model: graph splits = 2
0.00.627.091 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.627.091 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.134 I 
0.00.658.222 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.230 I perplexity: tokenizing the input ..
0.00.665.462 I perplexity: tokenization took 7.229 ms
0.00.665.468 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.264 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.798.597 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.798.623 I llama_perf_context_print:        load time =     648.95 ms
0.00.798.624 I llama_perf_context_print: prompt eval time =     131.39 ms /   128 tokens (    1.03 ms per token,   974.20 tokens per second)
0.00.798.624 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.625 I llama_perf_context_print:       total time =     140.49 ms /   129 tokens
0.00.799.026 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.079s
sys	0m0.131s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.289 I build: 4788 (05e6f5aa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.272 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.853 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.860 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.862 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.863 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.869 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.870 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.872 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.873 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.873 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.874 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.876 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.877 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.881 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.882 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.882 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.601 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.098 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.100 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.100 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.101 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.101 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.102 I llama_model_loader: - type  f32:  194 tensors
0.00.055.103 I llama_model_loader: - type  f16:   98 tensors
0.00.055.103 I print_info: file format = GGUF V3 (latest)
0.00.055.104 I print_info: file type   = all F32 (guessed)
0.00.055.106 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.190 I load: special tokens cache size = 25
0.00.076.610 I load: token to piece cache size = 0.2984 MB
0.00.076.614 I print_info: arch             = gptneox
0.00.076.614 I print_info: vocab_only       = 0
0.00.076.614 I print_info: n_ctx_train      = 2048
0.00.076.614 I print_info: n_embd           = 2048
0.00.076.615 I print_info: n_layer          = 24
0.00.076.618 I print_info: n_head           = 16
0.00.076.619 I print_info: n_head_kv        = 16
0.00.076.619 I print_info: n_rot            = 32
0.00.076.619 I print_info: n_swa            = 0
0.00.076.619 I print_info: n_embd_head_k    = 128
0.00.076.619 I print_info: n_embd_head_v    = 128
0.00.076.620 I print_info: n_gqa            = 1
0.00.076.621 I print_info: n_embd_k_gqa     = 2048
0.00.076.622 I print_info: n_embd_v_gqa     = 2048
0.00.076.622 I print_info: f_norm_eps       = 1.0e-05
0.00.076.623 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.623 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.623 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.623 I print_info: f_logit_scale    = 0.0e+00
0.00.076.624 I print_info: n_ff             = 8192
0.00.076.624 I print_info: n_expert         = 0
0.00.076.624 I print_info: n_expert_used    = 0
0.00.076.625 I print_info: causal attn      = 1
0.00.076.625 I print_info: pooling type     = 0
0.00.076.625 I print_info: rope type        = 2
0.00.076.625 I print_info: rope scaling     = linear
0.00.076.627 I print_info: freq_base_train  = 10000.0
0.00.076.628 I print_info: freq_scale_train = 1
0.00.076.628 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.628 I print_info: rope_finetuned   = unknown
0.00.076.629 I print_info: ssm_d_conv       = 0
0.00.076.629 I print_info: ssm_d_inner      = 0
0.00.076.629 I print_info: ssm_d_state      = 0
0.00.076.629 I print_info: ssm_dt_rank      = 0
0.00.076.629 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.630 I print_info: model type       = 1.4B
0.00.076.630 I print_info: model params     = 1.41 B
0.00.076.630 I print_info: general.name     = 1.4B
0.00.076.631 I print_info: vocab type       = BPE
0.00.076.631 I print_info: n_vocab          = 50304
0.00.076.631 I print_info: n_merges         = 50009
0.00.076.632 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.632 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.632 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.632 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.632 I print_info: LF token         = 187 'Ċ'
0.00.076.633 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.634 I print_info: max token length = 1024
0.00.076.635 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.329.101 I load_tensors: offloading 24 repeating layers to GPU
0.01.329.105 I load_tensors: offloading output layer to GPU
0.01.329.105 I load_tensors: offloaded 25/25 layers to GPU
0.01.329.130 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.329.131 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.330.288 I llama_init_from_model: n_seq_max     = 1
0.01.330.289 I llama_init_from_model: n_ctx         = 128
0.01.330.289 I llama_init_from_model: n_ctx_per_seq = 128
0.01.330.290 I llama_init_from_model: n_batch       = 128
0.01.330.290 I llama_init_from_model: n_ubatch      = 128
0.01.330.290 I llama_init_from_model: flash_attn    = 0
0.01.330.291 I llama_init_from_model: freq_base     = 10000.0
0.01.330.291 I llama_init_from_model: freq_scale    = 1
0.01.330.292 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.330.292 I ggml_metal_init: allocating
0.01.330.368 I ggml_metal_init: found device: Apple M4
0.01.330.374 I ggml_metal_init: picking default device: Apple M4
0.01.331.598 I ggml_metal_init: using embedded metal library
0.01.335.516 I ggml_metal_init: GPU name:   Apple M4
0.01.335.519 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.335.519 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.335.520 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.335.520 I ggml_metal_init: simdgroup reduction   = true
0.01.335.520 I ggml_metal_init: simdgroup matrix mul. = true
0.01.335.520 I ggml_metal_init: has residency sets    = true
0.01.335.520 I ggml_metal_init: has bfloat            = true
0.01.335.520 I ggml_metal_init: use bfloat            = true
0.01.335.521 I ggml_metal_init: hasUnifiedMemory      = true
0.01.335.522 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.346.359 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.348.096 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.348.099 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.348.126 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.349.764 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.349.766 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.349.766 I llama_init_from_model: graph nodes  = 967
0.01.349.766 I llama_init_from_model: graph splits = 2
0.01.349.767 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.349.767 I 
0.01.349.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.349.805 I compute_imatrix: tokenizing the input ..
0.01.353.905 I compute_imatrix: tokenization took 4.099 ms
0.01.353.907 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.619.476 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.621.820 I llama_perf_context_print:        load time =    1596.20 ms
0.01.621.820 I llama_perf_context_print: prompt eval time =     263.83 ms /   128 tokens (    2.06 ms per token,   485.16 tokens per second)
0.01.621.821 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.621.821 I llama_perf_context_print:       total time =    1598.54 ms /   129 tokens
0.01.622.310 I ggml_metal_free: deallocating

real	0m1.814s
user	0m0.127s
sys	0m0.254s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4788 (05e6f5aa)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139306940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139306fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139309ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13930a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13930aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13930afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13930b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13930ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13930bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13930c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13930c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13930cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13930d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13930dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13930e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13930ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13930f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13930fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139310150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139310b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139311220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139311940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139312060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139312780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139312ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139313160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139313770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1393143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139314920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139314be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139315080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139315340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139315bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139316110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1393163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139316870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139316d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1393171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139317650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139317af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139317f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139318430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1393188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139318d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139319030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139319640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139319c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13931a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13931ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13931b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13931b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13931bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13931c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13931c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13931d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13931d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13931db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13931ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13931e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13931ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13931ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13931f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13931f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13931fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139320100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1393205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139320a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139320ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139321380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139321820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139321cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139322160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139322600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139322b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1393230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1393235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139323b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139324090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1393245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139324b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139325080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1393255d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139325b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139326070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1393265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139326b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139327060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1393275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139327b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139328050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1393285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139328af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139329040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139329590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139329ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13932a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13932a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13931a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13932a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13932b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13932b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13932bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13932c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13932c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13932cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13932d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13932d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13932dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13932e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13932e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13932ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13932f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13932f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13932fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13932fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139330490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139330930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139330dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139331270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139331710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139331bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139332050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1393324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139332990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139332e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1393332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139333770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139333c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1393340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139334550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1393349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139334e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139335330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1393357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139335c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139336110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1393365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139336a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139336ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139337390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139337830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139337cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139338170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139338610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139338ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139338f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1393393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139339890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139339d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13933a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13933a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13933ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13933afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13933b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13933b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13933bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13933c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13933c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13933cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13933d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13933d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13933d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13933ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13933e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13933e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13933ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13933f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13933f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13933f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13933fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1393402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139340790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139340c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1393410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139341570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139341a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139341eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139342350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1393427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139342c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139343130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1393435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139343a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139343f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1393443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139344850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139344cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139345190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139345630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139345ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139345f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139346410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1393468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139346e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139347350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1393478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139347df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1393480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1393486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139348cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1393492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139349ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139349f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13934a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13934a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13934ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13934b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13934bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13934bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13934c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13934cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13934d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13934d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13934dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13934e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13934e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13934ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13934f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13934f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13934fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1393500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139350640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139350b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1393510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139351630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139351b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1393520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139352620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139352b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1393530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139353610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139353b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1393540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139354600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139354b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1393550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1393555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139355b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139356090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1393565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139356b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139357080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1393575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139357b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139358070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1393585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139358b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139359060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1393595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139359b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13935a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13935a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13935aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13935b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13935b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13935bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13935c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13935c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13935cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13935d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13935d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13935dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13935e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13935e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13935eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13935f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13935f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13935f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13935fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139360330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1393607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139360c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139361110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1393615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139361a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139361ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139362390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139362830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139362cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139363170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139363610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139363ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x139363f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1393643f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x139364890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x139364d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1393651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x139365670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x139365b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x139365fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x139366450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1393668f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139366e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139367560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139367c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1393683a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139368ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139368d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139369570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139369830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139369e40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.688.049 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.053 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x104604dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x104605240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1046056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x104605b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x104605f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x104606400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104606870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x104606ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x104607150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1046075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x104607a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x104608120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x104608c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1046093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x104609c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10460a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10460aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10460b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10460b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10460bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10460c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10460cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10460d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10460dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10460e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10460e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10460e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10460ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10460f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10460f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10460fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10460ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x104610430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1046106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x104610b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x104610fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104611440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1046118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104611d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104612190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x104612600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x104612a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x104612ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x104613350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1046137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x104613c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1046140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x104614510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x104614980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104614df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104615260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1046156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x104615b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x104615fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x104616420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x104616890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x104616e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x104617300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x104617770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x104617be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x104618050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1046184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x104618930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x104618da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x104619210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x104619680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x104619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x104619f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10461a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10461a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10461acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10461b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10461b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10461ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10461be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10461c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10461c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10461cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10461d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10461d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10461d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10461dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10461e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10461e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10461ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10461ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10461f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10461f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10461fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x104620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104620570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1046209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x104620e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1046212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x104621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x104621ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x104622010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x104622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1046228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x104622d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1046231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x104623640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x104623ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x104623f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x104624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x104624800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x104624c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1046250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x104625550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1046259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x104625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1046262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x104626710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x104626b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x104626ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x104627460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1046278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x104627d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1046281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x104628620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x104628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x104628f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x104629370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1046297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x104629c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10462a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10462a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10462a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10462ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10462b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10462b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10462bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10462bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10462c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10462c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10462cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10462d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10462d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10462da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10462dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10462e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10462e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10462ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10462f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10462f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10462f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10462fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x104630260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1046306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x104630b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104630fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x104631420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104631890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x104631d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104632170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1046325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104632a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104632ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x104633330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1046337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x104633c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x104634080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1046344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x104634960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x104634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104635e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104636130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1046363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104636860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104636cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x104637140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1046375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x104637a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x104637e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x104638300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x104638770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x104638be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x104639050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1046394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x104639930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104639da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10463a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10463a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10463aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10463af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10463b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10463b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10463bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10463c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10463c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10463ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10463ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10463d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10463d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10463dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10463e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10463e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10463e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10463ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10463f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10463f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10463fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1046400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x104640540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1046409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x104640e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x104641290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1046417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x104641cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x104642830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104642af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1046430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x104643670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104643c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1046441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1046447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104644d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x104645330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1046458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x104645eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x104646470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x104646a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x104646ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1046475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x104647b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x104648130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1046486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x104648cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x104649270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104649830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104649df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10464a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10464a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10464af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10464b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10464bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10464c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10464c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10464cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10464d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10464d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10464dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10464e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10464e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10464ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10464f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10464f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10464ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x104650570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x104650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1046510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1046516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x104651c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x104652230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1046527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x104652db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x104653370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x104653930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x104653ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1046544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x104654a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x104655030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1046555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x104655bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x104656170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x104656730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x104656cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1046571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1046576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x104657bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1046580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1046585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104658af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x104658ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1046594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1046599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104659ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10465a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10465a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10465adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10465b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10465b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10465bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10465c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10465c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10465cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10465d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10465d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10465daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10465dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10465e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10465e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10465f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10465fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104660240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x104660960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104660c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x104661410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1046616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104661ce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10465ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10464c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10464b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1046483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x104645bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1046552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104652ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x104650830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10464e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x104646730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x104643ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x104648f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10464a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10464f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10464c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1046541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x104647e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1046513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10464ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10464ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x104647870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1046558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x104644a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x104643370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1046455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x104655e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10464b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x104653630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x104649530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10464bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10464fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1046472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x104650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x104651970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x104646170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x104654770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104651f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10464da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1046569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104645030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x104656430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1046444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x104654d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10464eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x104650df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x104653bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1046524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10464a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x104641f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104604880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104660ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10460bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x104662140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x104662400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1046626c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x104662980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x104662c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x104662f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1046631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x104663480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x104663740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x104663a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x104663cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x104663f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x104664240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x104664500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1046647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x104664a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136604230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1366046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136604b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136604f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1366053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136605860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136605cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136606140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1366065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136606a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136606e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136607300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136607770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136607be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136608050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1366084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136608930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136608da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136609210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136609680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136609af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136609f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13660a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13660a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13660acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13660b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13660b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13660ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13660be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13660c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13660cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13660d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13660d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13660dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13660e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13660e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13660ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13660f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13660f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13660fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1366103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136610970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136610f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1366114d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136611a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136612030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136612530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136612a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136612f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136613430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136613930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136613e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136614330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136614830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136614d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136615230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136615730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136615c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136616130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136616630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136616b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136617030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136617530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136617a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136618430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136618930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136618e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136619330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136619830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136619d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13661a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13661a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13661ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13661b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13661b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13661bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13661c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13661c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13661ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13661cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13661d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13661d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13661de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13661e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13661e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13661ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13661f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13661f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13661fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136620130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136620630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136620b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136621030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136621530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136621a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136621f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136622430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136622930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136622e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136623330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136623830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136623d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136624230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136624730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136624c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136625130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136625630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136625b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136626030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136626530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136626a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136626f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136627430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136627930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136627e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136628330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136628830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136628d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136629230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136629730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136629c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13662a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13662a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13662ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13662b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13662b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13662bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13662c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13662c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13662cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13662d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13662d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13662e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13662e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13662e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13662ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13662f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13662fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136630120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1366305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136630a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136631210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136631760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136631cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136632200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136632750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136632ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1366331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136633740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136633c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1366341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136634730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136634c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1366351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136635720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136635c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1366361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136636710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136636c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1366371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136637700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136637c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1366381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1366386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136638c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136639190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1366396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136639c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13663a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13663a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13663ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13663b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13663b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13663bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13663c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13663c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13663cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13663d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13663d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13663dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13663e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13663e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13663ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13663f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13663f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13663fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136640120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136640670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136640bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136641110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136641660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136641bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136642100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136642650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136642ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1366430f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136643640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136643b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136644030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1366444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136644970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136644e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1366452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136645750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136645bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136646090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136646530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1366469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136646e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136647310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1366477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136647c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1366480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x136648590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x136648a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x136648ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x136649370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x136649810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x136649cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13664a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13664a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13664aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13664af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13664b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13664bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13664c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13664c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13664d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13664d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13664dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13664de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13664e480 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.753s
user	0m0.279s
sys	0m0.304s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4788 (05e6f5aa)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1428079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142808120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1428086d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142808c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142809230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1428097e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142809d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14280a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14280a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14280adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14280b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14280b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14280c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14280cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14280d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14280d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14280e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14280e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14280ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14280f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14280fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142810560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142810c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142811520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142811c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142811f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142812510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142813180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1428136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142813980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142813e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1428140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142814970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142814eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142815170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142815610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142815ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142815f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1428163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142816890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142816d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1428171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142817670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142817b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142817dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1428183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1428189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142819310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142819920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142819f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14281a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14281ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14281b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14281b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14281bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14281c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14281c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14281cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14281d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14281d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14281dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14281e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14281e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14281ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14281eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14281f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14281f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14281fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142820120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1428205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142820a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142820f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1428213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1428218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142821e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142822390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1428228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142822e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142823380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1428238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142823e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142824370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1428248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142824e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142825360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1428258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142825e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142826350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1428268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142826df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142827340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142827890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142827de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142828330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142828880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142828dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142829320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142819000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142829790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142829f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14282a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14282a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14282af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14282b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14282b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14282bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14282c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14282c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14282cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14282d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14282d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14282df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14282e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14282e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14282ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14282f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14282f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14282fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142830010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1428304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142830950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142830df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142831730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142831bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142832070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142832510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1428329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142832e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1428332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142833790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142833c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1428340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142834570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142834a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142834eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x142835350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1428357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142835c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142836130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1428365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142836f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1428373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142837850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142837cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142838190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142838630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142838ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142838f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142839410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1428398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142839d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14283a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14283a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14283ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14283afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14283b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14283b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14283bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14283c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14283c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14283cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14283d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14283d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14283d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14283de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14283e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14283e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14283ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14283f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14283f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14283f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14283fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142840310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1428407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142840c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1428410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142841590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142841a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142841ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142842370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142842810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142842cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142843150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1428435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142843a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142843f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1428443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142844d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1428451b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142845650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142845ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1428460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142846640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142846b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142846e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x142847460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142847a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142848080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x142848870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x142848d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142848fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1428495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142849bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14284a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14284a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14284ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14284b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14284b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14284bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14284c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14284c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14284ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14284d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14284d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14284dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14284e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14284e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14284ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14284f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14284f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14284fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1428503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142850920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142850e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1428513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142851910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142851e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1428523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142852900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142852e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1428533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1428538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142853e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142854390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1428548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142854e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142855380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1428558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x142855e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142856370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1428568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142856e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142857360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1428578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142857e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142858350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1428588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142858df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142859340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142859890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142859de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14285a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14285a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14285add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14285b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14285b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14285bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14285c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14285c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14285cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14285d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14285d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14285dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14285e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14285e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14285ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14285f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14285f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14285fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14285feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142860350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1428607f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142860c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142861130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1428615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142861a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142861f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1428623b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142862850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x142862cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x142863190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x142863630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x142863ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x142863f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x142864410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1428648b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x142864d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1428651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x142865690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142865be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142866300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142866a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142867140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142867860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142867b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x142868310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1428685d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142868be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.106.135 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142847720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142849290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142868890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142847110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142847d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14281ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14281a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14281ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1428498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1428121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142818cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1428195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1428186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14281b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14281a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1428111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142829a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142867de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1428143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142814660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x142849eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142848340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1428127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142812a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142812d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142869040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142869300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1428695c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142869880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142869b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142869e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14286a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14286a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14286a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14286a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14286abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14286ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14286b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14286b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14286b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14286b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14286bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14286bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14286c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14286c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14286c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14286ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14286ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14286cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14286d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14286d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14286d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14286da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14286dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14286e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14286e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14286e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14286e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14286eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14286edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14286f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14286f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14286f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14286f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14286fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14286fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142870100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1428703c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142870680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142870940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142870c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142870ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142871180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142871440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142871700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1428719c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142871c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142871f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142872200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1428724c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142872780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142872a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x142872d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142872fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142873280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142873540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142873800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142873ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142873d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142874040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142874300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1428745c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142874880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142874b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142874e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1428750c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142875380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142875640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142875900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142875bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142875e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142876140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142876400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1428766c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142876980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142876c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142876f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1428771c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142877480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142877740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142877a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142877cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142877f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142878240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142878500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1428787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142878a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142878d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142879000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1428792c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142879580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142879840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142879b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142879dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14287a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14287a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14287a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14287a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14287ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14287ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14287b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14287b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14287b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14287b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14287bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14287bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14287c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14287c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14287c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14287c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14287cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14287cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14287d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14287d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14287d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14287da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14287dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14287dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14287e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14287e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14287e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14287eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14287ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14287f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14287f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14287f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14287f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14287fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14287fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1428800c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142880380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142880640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142880900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142880bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142880e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142881140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142881400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1428816c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142881980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142881c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142881f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1428821c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142882480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142882740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142882a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142882cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142882f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142883240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142883500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1428837c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142883a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142883d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142884000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1428842c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142884580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142884840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142884b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142884dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142885080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142885340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142885600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1428858c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142885b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142885e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x142886100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1428863c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142886680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142886940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x142886c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142886ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142887180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x142887440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x142887700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1428879c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142887c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142887f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142888200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1428884c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142888780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142888a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142889010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1428892d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142889590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142889850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142889b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142889dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14288a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14288a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14288a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14288a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14288ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14288ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14288b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14288b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14288b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14288b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14288bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14288bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14288c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14288c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14288cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14288d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14288d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14288dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14288e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14288e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14288ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14288f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14288f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14288fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142890150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1428906a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142890bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142891140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142891690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142891be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142892130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142892680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142892bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142893120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142893670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142893bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142894110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142894660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142894bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x142895100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142895650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142895ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1428960f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142896640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142896b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1428970e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142897630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142897b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1428980d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142898620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142898b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142898e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1428990f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1428993b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142899820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142899c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14289a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14289a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14289a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14289ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14289b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14289b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14289bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14289c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14289c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14289c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14289cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14289d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14289d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14289dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14289df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14289e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14289e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14289ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14289f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14289f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14289f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1428a0420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1428a0b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1428a1260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1428a1980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1428a1c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1428a2430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1428a26f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1428a2d00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141607720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141607b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141608000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141608470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1416088e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141608d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1416091c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141609630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141609aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141609f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14160a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14160aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14160b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14160bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14160c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14160cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14160d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14160dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14160e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14160e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14160f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14160f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14160fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1416105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141610cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141610f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141611250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1416116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141611b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141611fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141612410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141612940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141612db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141613070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1416134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141613950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141613dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141614230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1416146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141614b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141614f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1416153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141615860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141615cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141616140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1416165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141616a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141616e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141617300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141617770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141617be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141618050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1416184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141618930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141618da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141619210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141619780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141619c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14161a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14161a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14161a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14161ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14161b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14161b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14161bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14161c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14161c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14161c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14161cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14161d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14161d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14161daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14161df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14161e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14161e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14161ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14161f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14161f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14161f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14161fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141620290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141620700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141620b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141620fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141621450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1416218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141621d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1416221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141622610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141622a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141622ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141623360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1416237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141623c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1416240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141624520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141624990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141624e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141625660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141625b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141626130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1416266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141626c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141627240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1416277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141627da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141628350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141628900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141628eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141629460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141629a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141629fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14162a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14162ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14162b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14162b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14162ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14162bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14162c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14162c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14162ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14162d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14162d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14162dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14162e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14162e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14162ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14162f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14162f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14162fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141630020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141630520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141630f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141631420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141631920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141631e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141632320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141632820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141632d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141633220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141633720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141633c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141634120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141634620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141634b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141635020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141635520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141635a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141635f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141636420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141636920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141636e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141637320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141637820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141637d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141638220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141638720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141638c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141639120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141639620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141639b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14163a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14163a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14163aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14163af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14163b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14163b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14163be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14163c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14163c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14163cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14163d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14163d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14163dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14163e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14163e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14163eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14163f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14163f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14163fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14163ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141640420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141640920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141640e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141641320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141641820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141641d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141642220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141642720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141642c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141643120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141643620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141643b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1416440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141644680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141644c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1416451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1416457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141645e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141646410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141646c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1416470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141647360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141647970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141647f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141648770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141648c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1416490b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141649550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141649d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14164a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14164a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14164acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14164b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14164b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14164bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14164c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14164c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14164ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14164d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14164d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14164dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14164e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14164e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14164ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14164f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14164f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14164fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1416501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141650740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141650c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1416511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141651730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141651c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1416521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141652720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141652c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1416531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141653710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141653c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1416541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141654700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141654c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1416551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1416556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141655c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141656190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1416566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141656c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141657180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1416576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141657c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141658170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1416586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141658c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141659160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1416596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141659c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14165a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14165a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14165abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14165b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14165b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14165bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14165c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14165c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14165cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14165cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14165d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14165d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14165dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14165e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14165e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14165eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14165f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14165f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14165f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14165fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1416602a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141660740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141660be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x141661080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x141661520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1416619c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x141661e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x141662300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1416627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x141662c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1416630e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x141663580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x141663a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141663f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141664690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141664db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1416654d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141665bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141665eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1416666a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141666960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141666f70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.973s
user	0m0.234s
sys	0m0.193s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
