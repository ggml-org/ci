### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.41 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.45 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.14 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  180.40 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.90 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.96 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.24 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 222.87 sec*proc (27 tests)

Total Test time (real) = 222.89 sec

real	3m42.915s
user	7m40.282s
sys	0m6.242s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.13 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.19 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.20 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.95 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.19 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.41 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.36 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.01 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.23 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.13 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.01 sec*proc (27 tests)

Total Test time (real) =  51.02 sec

real	0m51.032s
user	1m11.282s
sys	0m5.669s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.146 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.695 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.731 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.739 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.743 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.030.744 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.745 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.030.746 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.030.746 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.030.748 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.030.749 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.030.750 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.030.751 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.030.752 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.030.756 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.756 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.757 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.030.758 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.030.758 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.030.759 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.030.760 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.036.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.037.645 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.647 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.037.647 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.037.648 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.037.649 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.037.649 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.037.650 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.037.650 I llama_model_loader: - type  f32:  124 tensors
0.00.037.651 I llama_model_loader: - type  f16:   73 tensors
0.00.042.844 I llm_load_vocab: special tokens cache size = 5
0.00.045.293 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.045.297 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.045.297 I llm_load_print_meta: arch             = bert
0.00.045.298 I llm_load_print_meta: vocab type       = WPM
0.00.045.299 I llm_load_print_meta: n_vocab          = 30522
0.00.045.299 I llm_load_print_meta: n_merges         = 0
0.00.045.299 I llm_load_print_meta: vocab_only       = 0
0.00.045.299 I llm_load_print_meta: n_ctx_train      = 512
0.00.045.300 I llm_load_print_meta: n_embd           = 384
0.00.045.302 I llm_load_print_meta: n_layer          = 12
0.00.045.308 I llm_load_print_meta: n_head           = 12
0.00.045.309 I llm_load_print_meta: n_head_kv        = 12
0.00.045.337 I llm_load_print_meta: n_rot            = 32
0.00.045.338 I llm_load_print_meta: n_swa            = 0
0.00.045.339 I llm_load_print_meta: n_embd_head_k    = 32
0.00.045.339 I llm_load_print_meta: n_embd_head_v    = 32
0.00.045.340 I llm_load_print_meta: n_gqa            = 1
0.00.045.341 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.045.342 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.045.343 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.045.344 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.045.347 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.045.347 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.045.347 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.045.348 I llm_load_print_meta: n_ff             = 1536
0.00.045.350 I llm_load_print_meta: n_expert         = 0
0.00.045.350 I llm_load_print_meta: n_expert_used    = 0
0.00.045.351 I llm_load_print_meta: causal attn      = 0
0.00.045.351 I llm_load_print_meta: pooling type     = 2
0.00.045.351 I llm_load_print_meta: rope type        = 2
0.00.045.351 I llm_load_print_meta: rope scaling     = linear
0.00.045.352 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.045.353 I llm_load_print_meta: freq_scale_train = 1
0.00.045.353 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.045.353 I llm_load_print_meta: rope_finetuned   = unknown
0.00.045.354 I llm_load_print_meta: ssm_d_conv       = 0
0.00.045.354 I llm_load_print_meta: ssm_d_inner      = 0
0.00.045.354 I llm_load_print_meta: ssm_d_state      = 0
0.00.045.354 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.045.355 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.045.368 I llm_load_print_meta: model type       = 33M
0.00.045.369 I llm_load_print_meta: model ftype      = F16
0.00.045.369 I llm_load_print_meta: model params     = 33.21 M
0.00.045.370 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.045.371 I llm_load_print_meta: general.name     = Bge Small
0.00.045.371 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.045.371 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.045.372 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.045.374 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.045.374 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.045.374 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.045.375 I llm_load_print_meta: max token length = 21
0.00.047.737 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.047.738 I llm_load_tensors: offloading output layer to GPU
0.00.047.739 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.047.770 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.047.772 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.048.441 I llama_new_context_with_model: n_seq_max     = 1
0.00.048.443 I llama_new_context_with_model: n_ctx         = 512
0.00.048.443 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.048.444 I llama_new_context_with_model: n_batch       = 2048
0.00.048.444 I llama_new_context_with_model: n_ubatch      = 2048
0.00.048.444 I llama_new_context_with_model: flash_attn    = 0
0.00.048.445 I llama_new_context_with_model: freq_base     = 10000.0
0.00.048.445 I llama_new_context_with_model: freq_scale    = 1
0.00.048.446 I ggml_metal_init: allocating
0.00.048.459 I ggml_metal_init: found device: Apple M4
0.00.048.466 I ggml_metal_init: picking default device: Apple M4
0.00.049.677 I ggml_metal_init: using embedded metal library
0.00.054.529 I ggml_metal_init: GPU name:   Apple M4
0.00.054.532 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.533 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.534 I ggml_metal_init: simdgroup reduction   = true
0.00.054.535 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.535 I ggml_metal_init: has bfloat            = true
0.00.054.535 I ggml_metal_init: use bfloat            = true
0.00.054.536 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.540 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.269 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.068.272 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.068.273 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.069.106 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.069.108 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.069.108 I llama_new_context_with_model: graph nodes  = 429
0.00.069.108 I llama_new_context_with_model: graph splits = 2
0.00.069.130 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.075.704 I 
0.00.075.732 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.076.427 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.081.254 I llama_perf_context_print:        load time =      50.00 ms
0.00.081.255 I llama_perf_context_print: prompt eval time =       4.67 ms /     9 tokens (    0.52 ms per token,  1926.37 tokens per second)
0.00.081.256 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.081.256 I llama_perf_context_print:       total time =       5.55 ms /    10 tokens
0.00.081.401 I ggml_metal_free: deallocating

real	0m0.331s
user	0m0.054s
sys	0m0.035s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.034 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.252 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.415 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.418 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.420 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.421 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.422 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.423 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.423 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.424 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.424 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.425 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.425 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.425 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.429 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.429 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.429 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.430 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.433 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.434 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.434 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.997 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.673 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.674 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.674 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.674 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.675 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.675 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.675 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.676 I llama_model_loader: - type  f32:  124 tensors
0.00.014.676 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.362 I llm_load_vocab: special tokens cache size = 5
0.00.018.712 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.717 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.717 I llm_load_print_meta: arch             = bert
0.00.018.717 I llm_load_print_meta: vocab type       = WPM
0.00.018.719 I llm_load_print_meta: n_vocab          = 30522
0.00.018.719 I llm_load_print_meta: n_merges         = 0
0.00.018.719 I llm_load_print_meta: vocab_only       = 0
0.00.018.720 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.720 I llm_load_print_meta: n_embd           = 384
0.00.018.720 I llm_load_print_meta: n_layer          = 12
0.00.018.722 I llm_load_print_meta: n_head           = 12
0.00.018.723 I llm_load_print_meta: n_head_kv        = 12
0.00.018.730 I llm_load_print_meta: n_rot            = 32
0.00.018.730 I llm_load_print_meta: n_swa            = 0
0.00.018.730 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.730 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.731 I llm_load_print_meta: n_gqa            = 1
0.00.018.731 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.732 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.732 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.732 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.733 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.733 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.733 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.735 I llm_load_print_meta: n_ff             = 1536
0.00.018.735 I llm_load_print_meta: n_expert         = 0
0.00.018.735 I llm_load_print_meta: n_expert_used    = 0
0.00.018.736 I llm_load_print_meta: causal attn      = 0
0.00.018.736 I llm_load_print_meta: pooling type     = 2
0.00.018.736 I llm_load_print_meta: rope type        = 2
0.00.018.736 I llm_load_print_meta: rope scaling     = linear
0.00.018.736 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.737 I llm_load_print_meta: freq_scale_train = 1
0.00.018.737 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.737 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.737 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.737 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.737 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.737 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.738 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.742 I llm_load_print_meta: model type       = 33M
0.00.018.742 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.743 I llm_load_print_meta: model params     = 33.21 M
0.00.018.743 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.743 I llm_load_print_meta: general.name     = Bge Small
0.00.018.744 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.744 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.744 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.744 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.744 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.745 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.745 I llm_load_print_meta: max token length = 21
0.00.020.071 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.071 I llm_load_tensors: offloading output layer to GPU
0.00.020.074 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.082 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.083 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.452 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.452 I llama_new_context_with_model: n_ctx         = 512
0.00.020.453 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.453 I llama_new_context_with_model: n_batch       = 2048
0.00.020.453 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.453 I llama_new_context_with_model: flash_attn    = 0
0.00.020.454 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.454 I llama_new_context_with_model: freq_scale    = 1
0.00.020.454 I ggml_metal_init: allocating
0.00.020.457 I ggml_metal_init: found device: Apple M4
0.00.020.459 I ggml_metal_init: picking default device: Apple M4
0.00.021.093 I ggml_metal_init: using embedded metal library
0.00.023.678 I ggml_metal_init: GPU name:   Apple M4
0.00.023.679 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.680 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.681 I ggml_metal_init: simdgroup reduction   = true
0.00.023.681 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.681 I ggml_metal_init: has bfloat            = true
0.00.023.681 I ggml_metal_init: use bfloat            = true
0.00.023.682 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.682 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.428 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.430 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.431 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.019 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.020 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.020 I llama_new_context_with_model: graph nodes  = 429
0.00.035.020 I llama_new_context_with_model: graph splits = 2
0.00.035.033 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.306 I 
0.00.040.336 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.857 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.400 I llama_perf_context_print:        load time =      31.05 ms
0.00.045.401 I llama_perf_context_print: prompt eval time =       4.42 ms /     9 tokens (    0.49 ms per token,  2037.12 tokens per second)
0.00.045.402 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.403 I llama_perf_context_print:       total time =       5.09 ms /    10 tokens
0.00.045.572 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.132 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.226 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.355 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.360 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.362 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.363 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.364 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.365 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.366 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.367 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.368 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.369 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.369 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.370 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.373 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.378 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.379 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.379 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.380 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.605 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.818 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.820 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.822 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.822 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.822 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.049.823 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.823 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.823 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.824 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.824 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.049.830 I llama_model_loader: - type  f32:   41 tensors
0.00.049.831 I llama_model_loader: - type  f16:   29 tensors
0.00.068.018 W llm_load_vocab: empty token at index 5
0.00.072.526 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.073.798 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.073.824 I llm_load_vocab: special tokens cache size = 5
0.00.331.528 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.331.538 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.331.538 I llm_load_print_meta: arch             = jina-bert-v2
0.00.331.539 I llm_load_print_meta: vocab type       = BPE
0.00.331.539 I llm_load_print_meta: n_vocab          = 61056
0.00.331.539 I llm_load_print_meta: n_merges         = 39382
0.00.331.540 I llm_load_print_meta: vocab_only       = 0
0.00.331.540 I llm_load_print_meta: n_ctx_train      = 8192
0.00.331.540 I llm_load_print_meta: n_embd           = 384
0.00.331.540 I llm_load_print_meta: n_layer          = 4
0.00.331.544 I llm_load_print_meta: n_head           = 12
0.00.331.545 I llm_load_print_meta: n_head_kv        = 12
0.00.331.568 I llm_load_print_meta: n_rot            = 32
0.00.331.569 I llm_load_print_meta: n_swa            = 0
0.00.331.569 I llm_load_print_meta: n_embd_head_k    = 32
0.00.331.569 I llm_load_print_meta: n_embd_head_v    = 32
0.00.331.570 I llm_load_print_meta: n_gqa            = 1
0.00.331.570 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.331.571 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.331.571 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.331.572 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.331.572 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.331.572 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.331.572 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.331.573 I llm_load_print_meta: n_ff             = 1536
0.00.331.573 I llm_load_print_meta: n_expert         = 0
0.00.331.573 I llm_load_print_meta: n_expert_used    = 0
0.00.331.573 I llm_load_print_meta: causal attn      = 0
0.00.331.573 I llm_load_print_meta: pooling type     = -1
0.00.331.573 I llm_load_print_meta: rope type        = -1
0.00.331.574 I llm_load_print_meta: rope scaling     = linear
0.00.331.574 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.331.574 I llm_load_print_meta: freq_scale_train = 1
0.00.331.574 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.331.574 I llm_load_print_meta: rope_finetuned   = unknown
0.00.331.575 I llm_load_print_meta: ssm_d_conv       = 0
0.00.331.575 I llm_load_print_meta: ssm_d_inner      = 0
0.00.331.575 I llm_load_print_meta: ssm_d_state      = 0
0.00.331.575 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.331.578 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.331.594 I llm_load_print_meta: model type       = 33M
0.00.331.595 I llm_load_print_meta: model ftype      = F16
0.00.331.596 I llm_load_print_meta: model params     = 32.90 M
0.00.331.596 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.331.596 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.331.597 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.331.597 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.331.597 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.331.597 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.331.598 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.331.598 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.331.598 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.331.598 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.331.598 I llm_load_print_meta: max token length = 45
0.00.332.443 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.332.443 I llm_load_tensors: offloading output layer to GPU
0.00.332.444 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.332.464 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.332.465 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.333.227 I llama_new_context_with_model: n_seq_max     = 1
0.00.333.228 I llama_new_context_with_model: n_ctx         = 8192
0.00.333.228 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.333.228 I llama_new_context_with_model: n_batch       = 2048
0.00.333.228 I llama_new_context_with_model: n_ubatch      = 2048
0.00.333.228 I llama_new_context_with_model: flash_attn    = 0
0.00.333.229 I llama_new_context_with_model: freq_base     = 10000.0
0.00.333.229 I llama_new_context_with_model: freq_scale    = 1
0.00.333.229 I ggml_metal_init: allocating
0.00.333.233 I ggml_metal_init: found device: Apple M4
0.00.333.235 I ggml_metal_init: picking default device: Apple M4
0.00.333.885 I ggml_metal_init: using embedded metal library
0.00.336.471 I ggml_metal_init: GPU name:   Apple M4
0.00.336.473 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.336.473 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.336.474 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.336.474 I ggml_metal_init: simdgroup reduction   = true
0.00.336.474 I ggml_metal_init: simdgroup matrix mul. = true
0.00.336.474 I ggml_metal_init: has bfloat            = true
0.00.336.474 I ggml_metal_init: use bfloat            = true
0.00.336.475 I ggml_metal_init: hasUnifiedMemory      = true
0.00.336.476 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.387 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.349.389 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.349.393 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.350.063 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.064 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.065 I llama_new_context_with_model: graph nodes  = 154
0.00.350.065 I llama_new_context_with_model: graph splits = 2
0.00.350.085 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.364.835 I 
0.00.364.880 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.219 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.365.220 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.365.224 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.365.226 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.365.230 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.365.230 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.365.794 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.369.357 I llama_perf_context_print:        load time =     341.60 ms
0.00.369.358 I llama_perf_context_print: prompt eval time =       3.55 ms /    62 tokens (    0.06 ms per token, 17445.13 tokens per second)
0.00.369.359 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.369.359 I llama_perf_context_print:       total time =       4.52 ms /    63 tokens
0.00.369.588 I ggml_metal_free: deallocating

real	0m1.064s
user	0m0.339s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.102 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.230 I main: llama backend init
0.00.000.236 I main: load the model and apply lora adapter, if any
0.00.030.513 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.711 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.739 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.741 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.744 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.745 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.746 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.746 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.747 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.748 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.752 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.752 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.753 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.744 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.425 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.064.559 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.064.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.064.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.064.563 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.064.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.064.564 I llama_model_loader: - type  f32:  194 tensors
0.00.064.565 I llama_model_loader: - type  f16:   98 tensors
0.00.095.370 I llm_load_vocab: special tokens cache size = 25
0.00.102.429 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.102.432 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.102.433 I llm_load_print_meta: arch             = gptneox
0.00.102.433 I llm_load_print_meta: vocab type       = BPE
0.00.102.433 I llm_load_print_meta: n_vocab          = 50304
0.00.102.433 I llm_load_print_meta: n_merges         = 50009
0.00.102.433 I llm_load_print_meta: vocab_only       = 0
0.00.102.434 I llm_load_print_meta: n_ctx_train      = 2048
0.00.102.434 I llm_load_print_meta: n_embd           = 2048
0.00.102.434 I llm_load_print_meta: n_layer          = 24
0.00.102.437 I llm_load_print_meta: n_head           = 16
0.00.102.438 I llm_load_print_meta: n_head_kv        = 16
0.00.102.456 I llm_load_print_meta: n_rot            = 32
0.00.102.457 I llm_load_print_meta: n_swa            = 0
0.00.102.457 I llm_load_print_meta: n_embd_head_k    = 128
0.00.102.457 I llm_load_print_meta: n_embd_head_v    = 128
0.00.102.458 I llm_load_print_meta: n_gqa            = 1
0.00.102.459 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.102.459 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.102.460 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.102.460 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.102.460 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.102.460 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.102.460 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.102.463 I llm_load_print_meta: n_ff             = 8192
0.00.102.464 I llm_load_print_meta: n_expert         = 0
0.00.102.464 I llm_load_print_meta: n_expert_used    = 0
0.00.102.464 I llm_load_print_meta: causal attn      = 1
0.00.102.464 I llm_load_print_meta: pooling type     = 0
0.00.102.464 I llm_load_print_meta: rope type        = 2
0.00.102.468 I llm_load_print_meta: rope scaling     = linear
0.00.102.468 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.102.469 I llm_load_print_meta: freq_scale_train = 1
0.00.102.469 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.102.469 I llm_load_print_meta: rope_finetuned   = unknown
0.00.102.469 I llm_load_print_meta: ssm_d_conv       = 0
0.00.102.469 I llm_load_print_meta: ssm_d_inner      = 0
0.00.102.469 I llm_load_print_meta: ssm_d_state      = 0
0.00.102.470 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.102.470 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.102.480 I llm_load_print_meta: model type       = 1.4B
0.00.102.480 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.102.482 I llm_load_print_meta: model params     = 1.41 B
0.00.102.482 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.102.482 I llm_load_print_meta: general.name     = 1.4B
0.00.102.482 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.102.483 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.102.483 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.102.483 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.102.483 I llm_load_print_meta: LF token         = 128 ''
0.00.102.483 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.102.484 I llm_load_print_meta: max token length = 1024
0.00.104.965 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.104.965 I llm_load_tensors: offloading output layer to GPU
0.00.104.965 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.104.983 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.104.984 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.105.937 I llama_new_context_with_model: n_seq_max     = 1
0.00.105.939 I llama_new_context_with_model: n_ctx         = 2048
0.00.105.939 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.105.939 I llama_new_context_with_model: n_batch       = 2048
0.00.105.939 I llama_new_context_with_model: n_ubatch      = 512
0.00.105.939 I llama_new_context_with_model: flash_attn    = 0
0.00.105.940 I llama_new_context_with_model: freq_base     = 10000.0
0.00.105.940 I llama_new_context_with_model: freq_scale    = 1
0.00.105.941 I ggml_metal_init: allocating
0.00.105.950 I ggml_metal_init: found device: Apple M4
0.00.105.953 I ggml_metal_init: picking default device: Apple M4
0.00.106.634 I ggml_metal_init: using embedded metal library
0.00.116.220 I ggml_metal_init: GPU name:   Apple M4
0.00.116.222 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.116.223 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.116.223 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.116.223 I ggml_metal_init: simdgroup reduction   = true
0.00.116.223 I ggml_metal_init: simdgroup matrix mul. = true
0.00.116.224 I ggml_metal_init: has bfloat            = true
0.00.116.224 I ggml_metal_init: use bfloat            = true
0.00.116.224 I ggml_metal_init: hasUnifiedMemory      = true
0.00.116.225 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.160.745 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.160.750 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.160.774 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.161.723 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.161.725 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.161.726 I llama_new_context_with_model: graph nodes  = 967
0.00.161.726 I llama_new_context_with_model: graph splits = 2
0.00.161.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.240.347 I main: llama threadpool init, n_threads = 4
0.00.240.377 I 
0.00.240.411 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.240.412 I 
0.00.240.493 I sampler seed: 1234
0.00.240.498 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.240.545 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.240.549 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.240.549 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.090.402 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.02.090.403 I llama_perf_context_print:        load time =     209.82 ms
0.02.090.403 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.06 tokens per second)
0.02.090.404 I llama_perf_context_print:        eval time =    1803.12 ms /    63 runs   (   28.62 ms per token,    34.94 tokens per second)
0.02.090.404 I llama_perf_context_print:       total time =    1850.06 ms /    70 tokens
0.02.090.593 I ggml_metal_free: deallocating

real	0m2.373s
user	0m0.145s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.579 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.148 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.912 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.918 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.920 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.922 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.923 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.923 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.923 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.926 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.926 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.927 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.928 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.928 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.932 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.933 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.935 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.936 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.936 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.330 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.053 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.053 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.054 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.054 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.055 I llama_model_loader: - type  f32:  194 tensors
0.00.054.055 I llama_model_loader: - type  f16:   98 tensors
0.00.083.852 I llm_load_vocab: special tokens cache size = 25
0.00.090.318 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.321 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.321 I llm_load_print_meta: arch             = gptneox
0.00.090.321 I llm_load_print_meta: vocab type       = BPE
0.00.090.322 I llm_load_print_meta: n_vocab          = 50304
0.00.090.322 I llm_load_print_meta: n_merges         = 50009
0.00.090.322 I llm_load_print_meta: vocab_only       = 0
0.00.090.322 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.322 I llm_load_print_meta: n_embd           = 2048
0.00.090.322 I llm_load_print_meta: n_layer          = 24
0.00.090.325 I llm_load_print_meta: n_head           = 16
0.00.090.326 I llm_load_print_meta: n_head_kv        = 16
0.00.090.338 I llm_load_print_meta: n_rot            = 32
0.00.090.338 I llm_load_print_meta: n_swa            = 0
0.00.090.338 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.338 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.339 I llm_load_print_meta: n_gqa            = 1
0.00.090.340 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.340 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.341 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.341 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.341 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.341 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.341 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.342 I llm_load_print_meta: n_ff             = 8192
0.00.090.342 I llm_load_print_meta: n_expert         = 0
0.00.090.342 I llm_load_print_meta: n_expert_used    = 0
0.00.090.343 I llm_load_print_meta: causal attn      = 1
0.00.090.343 I llm_load_print_meta: pooling type     = 0
0.00.090.343 I llm_load_print_meta: rope type        = 2
0.00.090.343 I llm_load_print_meta: rope scaling     = linear
0.00.090.343 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.344 I llm_load_print_meta: freq_scale_train = 1
0.00.090.344 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.344 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.344 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.344 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.344 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.344 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.345 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.354 I llm_load_print_meta: model type       = 1.4B
0.00.090.355 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.355 I llm_load_print_meta: model params     = 1.41 B
0.00.090.356 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.356 I llm_load_print_meta: general.name     = 1.4B
0.00.090.356 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.356 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.356 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.357 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.357 I llm_load_print_meta: LF token         = 128 ''
0.00.090.358 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.358 I llm_load_print_meta: max token length = 1024
0.00.092.843 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.843 I llm_load_tensors: offloading output layer to GPU
0.00.092.843 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.854 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.855 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.776 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.777 I llama_new_context_with_model: n_ctx         = 128
0.00.093.777 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.777 I llama_new_context_with_model: n_batch       = 128
0.00.093.777 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.777 I llama_new_context_with_model: flash_attn    = 0
0.00.093.778 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.778 I llama_new_context_with_model: freq_scale    = 1
0.00.093.778 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.779 I ggml_metal_init: allocating
0.00.093.782 I ggml_metal_init: found device: Apple M4
0.00.093.784 I ggml_metal_init: picking default device: Apple M4
0.00.094.400 I ggml_metal_init: using embedded metal library
0.00.096.928 I ggml_metal_init: GPU name:   Apple M4
0.00.096.929 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.930 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.930 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.930 I ggml_metal_init: simdgroup reduction   = true
0.00.096.930 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.931 I ggml_metal_init: has bfloat            = true
0.00.096.931 I ggml_metal_init: use bfloat            = true
0.00.096.931 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.932 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.244 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.247 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.261 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.131 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.132 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.132 I llama_new_context_with_model: graph nodes  = 967
0.00.109.133 I llama_new_context_with_model: graph splits = 2
0.00.109.145 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.851.650 I 
0.00.851.716 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.851.757 I perplexity: tokenizing the input ..
0.00.865.139 I perplexity: tokenization took 13.377 ms
0.00.865.170 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.001.623 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.003.112 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.003.140 I llama_perf_context_print:        load time =     828.49 ms
0.01.003.141 I llama_perf_context_print: prompt eval time =     135.55 ms /   128 tokens (    1.06 ms per token,   944.27 tokens per second)
0.01.003.146 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.003.149 I llama_perf_context_print:       total time =     151.49 ms /   129 tokens
0.01.003.725 I ggml_metal_free: deallocating

real	0m1.191s
user	0m0.123s
sys	0m0.202s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.756 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.812 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.817 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.819 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.824 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.824 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.825 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.825 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.826 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.826 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.829 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.831 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.754 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.819 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.744 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.746 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.746 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.746 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.747 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.747 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.748 I llama_model_loader: - type  f32:  194 tensors
0.00.035.748 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.579 I llm_load_vocab: special tokens cache size = 25
0.00.064.480 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.484 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.485 I llm_load_print_meta: arch             = gptneox
0.00.064.485 I llm_load_print_meta: vocab type       = BPE
0.00.064.485 I llm_load_print_meta: n_vocab          = 50304
0.00.064.485 I llm_load_print_meta: n_merges         = 50009
0.00.064.488 I llm_load_print_meta: vocab_only       = 0
0.00.064.488 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.488 I llm_load_print_meta: n_embd           = 2048
0.00.064.488 I llm_load_print_meta: n_layer          = 24
0.00.064.494 I llm_load_print_meta: n_head           = 16
0.00.064.495 I llm_load_print_meta: n_head_kv        = 16
0.00.064.507 I llm_load_print_meta: n_rot            = 32
0.00.064.508 I llm_load_print_meta: n_swa            = 0
0.00.064.508 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.508 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.509 I llm_load_print_meta: n_gqa            = 1
0.00.064.510 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.510 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.511 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.511 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.511 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.511 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.512 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.512 I llm_load_print_meta: n_ff             = 8192
0.00.064.513 I llm_load_print_meta: n_expert         = 0
0.00.064.513 I llm_load_print_meta: n_expert_used    = 0
0.00.064.513 I llm_load_print_meta: causal attn      = 1
0.00.064.513 I llm_load_print_meta: pooling type     = 0
0.00.064.513 I llm_load_print_meta: rope type        = 2
0.00.064.513 I llm_load_print_meta: rope scaling     = linear
0.00.064.514 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.514 I llm_load_print_meta: freq_scale_train = 1
0.00.064.514 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.514 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.514 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.514 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.514 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.515 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.515 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.525 I llm_load_print_meta: model type       = 1.4B
0.00.064.525 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.526 I llm_load_print_meta: model params     = 1.41 B
0.00.064.526 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.526 I llm_load_print_meta: general.name     = 1.4B
0.00.064.526 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.526 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.527 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.527 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.527 I llm_load_print_meta: LF token         = 128 ''
0.00.064.527 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.528 I llm_load_print_meta: max token length = 1024
0.00.066.323 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.323 I llm_load_tensors: offloading output layer to GPU
0.00.066.324 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.334 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.335 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.207 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.208 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.208 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.208 I llama_new_context_with_model: n_batch       = 2048
0.00.067.208 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.208 I llama_new_context_with_model: flash_attn    = 0
0.00.067.209 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.209 I llama_new_context_with_model: freq_scale    = 1
0.00.067.209 I ggml_metal_init: allocating
0.00.067.213 I ggml_metal_init: found device: Apple M4
0.00.067.215 I ggml_metal_init: picking default device: Apple M4
0.00.067.964 I ggml_metal_init: using embedded metal library
0.00.070.622 I ggml_metal_init: GPU name:   Apple M4
0.00.070.623 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.624 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.624 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.625 I ggml_metal_init: simdgroup reduction   = true
0.00.070.625 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.625 I ggml_metal_init: has bfloat            = true
0.00.070.625 I ggml_metal_init: use bfloat            = true
0.00.070.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.626 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.895 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.902 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.927 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.011 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.013 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.013 I llama_new_context_with_model: graph nodes  = 967
0.00.107.013 I llama_new_context_with_model: graph splits = 2
0.00.107.031 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.221.748 I main: llama threadpool init, n_threads = 4
0.01.221.782 I 
0.01.221.811 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.221.811 I 
0.01.222.030 I sampler seed: 1234
0.01.222.035 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.222.077 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.222.080 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.222.081 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.321.032 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60322.85 tokens per second)
0.02.321.033 I llama_perf_context_print:        load time =    1211.99 ms
0.02.321.034 I llama_perf_context_print: prompt eval time =      43.76 ms /     7 tokens (    6.25 ms per token,   159.96 tokens per second)
0.02.321.035 I llama_perf_context_print:        eval time =    1052.25 ms /    63 runs   (   16.70 ms per token,    59.87 tokens per second)
0.02.321.035 I llama_perf_context_print:       total time =    1099.29 ms /    70 tokens
0.02.321.232 I ggml_metal_free: deallocating

real	0m2.340s
user	0m0.114s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.116 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.094 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.358 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.363 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.365 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.365 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.366 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.366 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.367 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.367 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.368 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.368 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.369 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.369 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.371 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.372 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.372 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.234 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.581 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.397 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.399 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.400 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.400 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.400 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.401 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.401 I llama_model_loader: - type  f32:  194 tensors
0.00.029.401 I llama_model_loader: - type q8_0:   98 tensors
0.00.052.767 I llm_load_vocab: special tokens cache size = 25
0.00.058.842 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.845 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.846 I llm_load_print_meta: arch             = gptneox
0.00.058.846 I llm_load_print_meta: vocab type       = BPE
0.00.058.846 I llm_load_print_meta: n_vocab          = 50304
0.00.058.847 I llm_load_print_meta: n_merges         = 50009
0.00.058.847 I llm_load_print_meta: vocab_only       = 0
0.00.058.847 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.847 I llm_load_print_meta: n_embd           = 2048
0.00.058.847 I llm_load_print_meta: n_layer          = 24
0.00.058.855 I llm_load_print_meta: n_head           = 16
0.00.058.856 I llm_load_print_meta: n_head_kv        = 16
0.00.058.869 I llm_load_print_meta: n_rot            = 32
0.00.058.869 I llm_load_print_meta: n_swa            = 0
0.00.058.869 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.870 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.870 I llm_load_print_meta: n_gqa            = 1
0.00.058.871 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.871 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.872 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.876 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.876 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.876 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.876 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.877 I llm_load_print_meta: n_ff             = 8192
0.00.058.877 I llm_load_print_meta: n_expert         = 0
0.00.058.877 I llm_load_print_meta: n_expert_used    = 0
0.00.058.877 I llm_load_print_meta: causal attn      = 1
0.00.058.877 I llm_load_print_meta: pooling type     = 0
0.00.058.878 I llm_load_print_meta: rope type        = 2
0.00.058.878 I llm_load_print_meta: rope scaling     = linear
0.00.058.878 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.878 I llm_load_print_meta: freq_scale_train = 1
0.00.058.879 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.879 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.879 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.879 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.879 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.879 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.880 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.890 I llm_load_print_meta: model type       = 1.4B
0.00.058.890 I llm_load_print_meta: model ftype      = Q8_0
0.00.058.891 I llm_load_print_meta: model params     = 1.41 B
0.00.058.891 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.058.892 I llm_load_print_meta: general.name     = 1.4B
0.00.058.892 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.892 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.892 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.892 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.893 I llm_load_print_meta: LF token         = 128 ''
0.00.058.893 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.893 I llm_load_print_meta: max token length = 1024
0.00.061.179 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.179 I llm_load_tensors: offloading output layer to GPU
0.00.061.180 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.191 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.192 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.154 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.155 I llama_new_context_with_model: n_ctx         = 128
0.00.062.155 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.155 I llama_new_context_with_model: n_batch       = 128
0.00.062.156 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.156 I llama_new_context_with_model: flash_attn    = 0
0.00.062.156 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.156 I llama_new_context_with_model: freq_scale    = 1
0.00.062.157 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.157 I ggml_metal_init: allocating
0.00.062.164 I ggml_metal_init: found device: Apple M4
0.00.062.170 I ggml_metal_init: picking default device: Apple M4
0.00.062.814 I ggml_metal_init: using embedded metal library
0.00.065.387 I ggml_metal_init: GPU name:   Apple M4
0.00.065.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.389 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.390 I ggml_metal_init: simdgroup reduction   = true
0.00.065.390 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.390 I ggml_metal_init: has bfloat            = true
0.00.065.391 I ggml_metal_init: use bfloat            = true
0.00.065.391 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.392 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.112 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.077.115 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.077.134 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.078.048 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.078.049 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.078.050 I llama_new_context_with_model: graph nodes  = 967
0.00.078.050 I llama_new_context_with_model: graph splits = 2
0.00.078.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.027 I 
0.00.879.057 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.879.064 I perplexity: tokenizing the input ..
0.00.886.854 I perplexity: tokenization took 7.788 ms
0.00.886.864 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.010.796 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.011.971 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.011.988 I llama_perf_context_print:        load time =     867.93 ms
0.01.011.989 I llama_perf_context_print: prompt eval time =     123.71 ms /   128 tokens (    0.97 ms per token,  1034.71 tokens per second)
0.01.011.991 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.011.992 I llama_perf_context_print:       total time =     132.96 ms /   129 tokens
0.01.012.480 I ggml_metal_free: deallocating

real	0m1.028s
user	0m0.087s
sys	0m0.164s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.648 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.424 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.026.436 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.439 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.440 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.441 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.442 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.442 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.447 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.448 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.448 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.449 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.450 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.450 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.451 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.453 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.454 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.454 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.313 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.397 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.428 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.429 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.430 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.430 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.430 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.431 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.431 I llama_model_loader: - type  f32:  194 tensors
0.00.035.432 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.432 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.085 I llm_load_vocab: special tokens cache size = 25
0.00.065.043 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.046 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.046 I llm_load_print_meta: arch             = gptneox
0.00.065.046 I llm_load_print_meta: vocab type       = BPE
0.00.065.047 I llm_load_print_meta: n_vocab          = 50304
0.00.065.047 I llm_load_print_meta: n_merges         = 50009
0.00.065.047 I llm_load_print_meta: vocab_only       = 0
0.00.065.047 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.047 I llm_load_print_meta: n_embd           = 2048
0.00.065.047 I llm_load_print_meta: n_layer          = 24
0.00.065.052 I llm_load_print_meta: n_head           = 16
0.00.065.052 I llm_load_print_meta: n_head_kv        = 16
0.00.065.065 I llm_load_print_meta: n_rot            = 32
0.00.065.066 I llm_load_print_meta: n_swa            = 0
0.00.065.066 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.066 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.067 I llm_load_print_meta: n_gqa            = 1
0.00.065.067 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.068 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.069 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.069 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.071 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.071 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.072 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.072 I llm_load_print_meta: n_ff             = 8192
0.00.065.073 I llm_load_print_meta: n_expert         = 0
0.00.065.073 I llm_load_print_meta: n_expert_used    = 0
0.00.065.073 I llm_load_print_meta: causal attn      = 1
0.00.065.073 I llm_load_print_meta: pooling type     = 0
0.00.065.073 I llm_load_print_meta: rope type        = 2
0.00.065.073 I llm_load_print_meta: rope scaling     = linear
0.00.065.073 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.074 I llm_load_print_meta: freq_scale_train = 1
0.00.065.074 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.074 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.074 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.074 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.074 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.075 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.075 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.085 I llm_load_print_meta: model type       = 1.4B
0.00.065.085 I llm_load_print_meta: model ftype      = Q4_0
0.00.065.086 I llm_load_print_meta: model params     = 1.41 B
0.00.065.086 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.065.086 I llm_load_print_meta: general.name     = 1.4B
0.00.065.087 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.087 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.087 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.087 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.088 I llm_load_print_meta: LF token         = 128 ''
0.00.065.088 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.088 I llm_load_print_meta: max token length = 1024
0.00.067.312 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.313 I llm_load_tensors: offloading output layer to GPU
0.00.067.313 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.324 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.067.325 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.068.291 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.292 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.292 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.293 I llama_new_context_with_model: n_batch       = 2048
0.00.068.293 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.293 I llama_new_context_with_model: flash_attn    = 0
0.00.068.293 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.294 I llama_new_context_with_model: freq_scale    = 1
0.00.068.294 I ggml_metal_init: allocating
0.00.068.297 I ggml_metal_init: found device: Apple M4
0.00.068.299 I ggml_metal_init: picking default device: Apple M4
0.00.069.045 I ggml_metal_init: using embedded metal library
0.00.071.783 I ggml_metal_init: GPU name:   Apple M4
0.00.071.785 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.785 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.785 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.786 I ggml_metal_init: simdgroup reduction   = true
0.00.071.786 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.786 I ggml_metal_init: has bfloat            = true
0.00.071.786 I ggml_metal_init: use bfloat            = true
0.00.071.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.787 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.467 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.476 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.501 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.741 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.743 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.743 I llama_new_context_with_model: graph nodes  = 967
0.00.107.743 I llama_new_context_with_model: graph splits = 2
0.00.107.760 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.358 I main: llama threadpool init, n_threads = 4
0.00.731.400 I 
0.00.731.428 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.429 I 
0.00.731.687 I sampler seed: 1234
0.00.731.691 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.703 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.703 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.704 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.415.830 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.01.415.831 I llama_perf_context_print:        load time =     720.70 ms
0.01.415.831 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.03 tokens per second)
0.01.415.832 I llama_perf_context_print:        eval time =     637.24 ms /    63 runs   (   10.11 ms per token,    98.86 tokens per second)
0.01.415.835 I llama_perf_context_print:       total time =     684.48 ms /    70 tokens
0.01.416.030 I ggml_metal_free: deallocating

real	0m1.434s
user	0m0.114s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.924 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.701 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.707 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.708 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.708 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.708 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.710 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.711 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.711 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.712 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.713 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.713 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.443 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.554 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.370 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.371 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.372 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.372 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.372 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.373 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.373 I llama_model_loader: - type  f32:  194 tensors
0.00.024.374 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.374 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.354 I llm_load_vocab: special tokens cache size = 25
0.00.050.259 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.262 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.263 I llm_load_print_meta: arch             = gptneox
0.00.050.263 I llm_load_print_meta: vocab type       = BPE
0.00.050.263 I llm_load_print_meta: n_vocab          = 50304
0.00.050.264 I llm_load_print_meta: n_merges         = 50009
0.00.050.264 I llm_load_print_meta: vocab_only       = 0
0.00.050.264 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.266 I llm_load_print_meta: n_embd           = 2048
0.00.050.266 I llm_load_print_meta: n_layer          = 24
0.00.050.269 I llm_load_print_meta: n_head           = 16
0.00.050.270 I llm_load_print_meta: n_head_kv        = 16
0.00.050.282 I llm_load_print_meta: n_rot            = 32
0.00.050.282 I llm_load_print_meta: n_swa            = 0
0.00.050.282 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.282 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.283 I llm_load_print_meta: n_gqa            = 1
0.00.050.284 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.284 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.285 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.285 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.285 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.285 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.285 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.286 I llm_load_print_meta: n_ff             = 8192
0.00.050.286 I llm_load_print_meta: n_expert         = 0
0.00.050.286 I llm_load_print_meta: n_expert_used    = 0
0.00.050.286 I llm_load_print_meta: causal attn      = 1
0.00.050.287 I llm_load_print_meta: pooling type     = 0
0.00.050.287 I llm_load_print_meta: rope type        = 2
0.00.050.287 I llm_load_print_meta: rope scaling     = linear
0.00.050.287 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.288 I llm_load_print_meta: freq_scale_train = 1
0.00.050.288 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.288 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.288 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.288 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.288 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.288 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.288 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.298 I llm_load_print_meta: model type       = 1.4B
0.00.050.298 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.299 I llm_load_print_meta: model params     = 1.41 B
0.00.050.299 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.299 I llm_load_print_meta: general.name     = 1.4B
0.00.050.299 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.299 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.300 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.300 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.300 I llm_load_print_meta: LF token         = 128 ''
0.00.050.300 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.301 I llm_load_print_meta: max token length = 1024
0.00.052.238 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.239 I llm_load_tensors: offloading output layer to GPU
0.00.052.239 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.249 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.250 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.175 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.176 I llama_new_context_with_model: n_ctx         = 128
0.00.053.176 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.176 I llama_new_context_with_model: n_batch       = 128
0.00.053.177 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.177 I llama_new_context_with_model: flash_attn    = 0
0.00.053.177 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.178 I llama_new_context_with_model: freq_scale    = 1
0.00.053.178 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.178 I ggml_metal_init: allocating
0.00.053.185 I ggml_metal_init: found device: Apple M4
0.00.053.188 I ggml_metal_init: picking default device: Apple M4
0.00.053.761 I ggml_metal_init: using embedded metal library
0.00.056.083 I ggml_metal_init: GPU name:   Apple M4
0.00.056.084 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.085 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.085 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.086 I ggml_metal_init: simdgroup reduction   = true
0.00.056.086 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.086 I ggml_metal_init: has bfloat            = true
0.00.056.086 I ggml_metal_init: use bfloat            = true
0.00.056.086 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.088 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.686 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.688 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.701 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.566 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.567 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.568 I llama_new_context_with_model: graph nodes  = 967
0.00.067.568 I llama_new_context_with_model: graph splits = 2
0.00.067.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.574 I 
0.00.588.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.620 I perplexity: tokenizing the input ..
0.00.596.822 I perplexity: tokenization took 8.2 ms
0.00.596.832 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.719.405 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.720.556 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.720.578 I llama_perf_context_print:        load time =     578.65 ms
0.00.720.580 I llama_perf_context_print: prompt eval time =     122.34 ms /   128 tokens (    0.96 ms per token,  1046.22 tokens per second)
0.00.720.581 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.720.581 I llama_perf_context_print:       total time =     132.00 ms /   129 tokens
0.00.721.182 I ggml_metal_free: deallocating

real	0m0.737s
user	0m0.077s
sys	0m0.105s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.012.004 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.174 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.184 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.184 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.185 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.186 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.186 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.187 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.187 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.188 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.188 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.189 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.192 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.193 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.193 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.194 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.387 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.670 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.234 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.235 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.235 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.236 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.236 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.236 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.237 I llama_model_loader: - type  f32:  194 tensors
0.00.029.237 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.237 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.653 I llm_load_vocab: special tokens cache size = 25
0.00.061.261 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.263 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.264 I llm_load_print_meta: arch             = gptneox
0.00.061.264 I llm_load_print_meta: vocab type       = BPE
0.00.061.264 I llm_load_print_meta: n_vocab          = 50304
0.00.061.264 I llm_load_print_meta: n_merges         = 50009
0.00.061.264 I llm_load_print_meta: vocab_only       = 0
0.00.061.265 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.265 I llm_load_print_meta: n_embd           = 2048
0.00.061.265 I llm_load_print_meta: n_layer          = 24
0.00.061.267 I llm_load_print_meta: n_head           = 16
0.00.061.268 I llm_load_print_meta: n_head_kv        = 16
0.00.061.279 I llm_load_print_meta: n_rot            = 32
0.00.061.281 I llm_load_print_meta: n_swa            = 0
0.00.061.282 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.282 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.282 I llm_load_print_meta: n_gqa            = 1
0.00.061.283 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.284 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.284 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.285 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.285 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.285 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.285 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.286 I llm_load_print_meta: n_ff             = 8192
0.00.061.286 I llm_load_print_meta: n_expert         = 0
0.00.061.286 I llm_load_print_meta: n_expert_used    = 0
0.00.061.286 I llm_load_print_meta: causal attn      = 1
0.00.061.286 I llm_load_print_meta: pooling type     = 0
0.00.061.286 I llm_load_print_meta: rope type        = 2
0.00.061.287 I llm_load_print_meta: rope scaling     = linear
0.00.061.287 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.287 I llm_load_print_meta: freq_scale_train = 1
0.00.061.287 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.288 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.288 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.288 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.288 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.289 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.289 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.299 I llm_load_print_meta: model type       = 1.4B
0.00.061.299 I llm_load_print_meta: model ftype      = Q4_1
0.00.061.300 I llm_load_print_meta: model params     = 1.41 B
0.00.061.300 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.061.300 I llm_load_print_meta: general.name     = 1.4B
0.00.061.300 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.301 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.301 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.301 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.301 I llm_load_print_meta: LF token         = 128 ''
0.00.061.302 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.302 I llm_load_print_meta: max token length = 1024
0.00.063.424 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.425 I llm_load_tensors: offloading output layer to GPU
0.00.063.425 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.435 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.063.437 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.064.392 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.393 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.393 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.393 I llama_new_context_with_model: n_batch       = 2048
0.00.064.394 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.394 I llama_new_context_with_model: flash_attn    = 0
0.00.064.394 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.395 I llama_new_context_with_model: freq_scale    = 1
0.00.064.395 I ggml_metal_init: allocating
0.00.064.401 I ggml_metal_init: found device: Apple M4
0.00.064.403 I ggml_metal_init: picking default device: Apple M4
0.00.064.998 I ggml_metal_init: using embedded metal library
0.00.067.503 I ggml_metal_init: GPU name:   Apple M4
0.00.067.504 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.505 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.505 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.505 I ggml_metal_init: simdgroup reduction   = true
0.00.067.505 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.506 I ggml_metal_init: has bfloat            = true
0.00.067.506 I ggml_metal_init: use bfloat            = true
0.00.067.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.530 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.535 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.553 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.503 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.101.504 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.101.504 I llama_new_context_with_model: graph nodes  = 967
0.00.101.505 I llama_new_context_with_model: graph splits = 2
0.00.101.514 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.359 I main: llama threadpool init, n_threads = 4
0.00.730.399 I 
0.00.730.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.434 I 
0.00.730.667 I sampler seed: 1234
0.00.730.671 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.693 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.693 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.693 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.463.824 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66108.01 tokens per second)
0.01.463.825 I llama_perf_context_print:        load time =     718.35 ms
0.01.463.826 I llama_perf_context_print: prompt eval time =      43.83 ms /     7 tokens (    6.26 ms per token,   159.70 tokens per second)
0.01.463.826 I llama_perf_context_print:        eval time =     686.52 ms /    63 runs   (   10.90 ms per token,    91.77 tokens per second)
0.01.463.827 I llama_perf_context_print:       total time =     733.47 ms /    70 tokens
0.01.464.021 I ggml_metal_free: deallocating

real	0m1.493s
user	0m0.117s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.919 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.756 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.760 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.761 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.762 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.762 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.762 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.763 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.764 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.764 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.764 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.765 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.765 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.765 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.766 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.767 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.767 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.768 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.511 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.321 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.322 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.323 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.323 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.323 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.323 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.324 I llama_model_loader: - type  f32:  194 tensors
0.00.023.324 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.324 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.392 I llm_load_vocab: special tokens cache size = 25
0.00.049.404 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.407 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.407 I llm_load_print_meta: arch             = gptneox
0.00.049.407 I llm_load_print_meta: vocab type       = BPE
0.00.049.408 I llm_load_print_meta: n_vocab          = 50304
0.00.049.408 I llm_load_print_meta: n_merges         = 50009
0.00.049.408 I llm_load_print_meta: vocab_only       = 0
0.00.049.408 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.408 I llm_load_print_meta: n_embd           = 2048
0.00.049.408 I llm_load_print_meta: n_layer          = 24
0.00.049.412 I llm_load_print_meta: n_head           = 16
0.00.049.412 I llm_load_print_meta: n_head_kv        = 16
0.00.049.424 I llm_load_print_meta: n_rot            = 32
0.00.049.425 I llm_load_print_meta: n_swa            = 0
0.00.049.425 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.425 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.426 I llm_load_print_meta: n_gqa            = 1
0.00.049.429 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.429 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.430 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.430 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.430 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.430 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.430 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.432 I llm_load_print_meta: n_ff             = 8192
0.00.049.432 I llm_load_print_meta: n_expert         = 0
0.00.049.432 I llm_load_print_meta: n_expert_used    = 0
0.00.049.433 I llm_load_print_meta: causal attn      = 1
0.00.049.433 I llm_load_print_meta: pooling type     = 0
0.00.049.434 I llm_load_print_meta: rope type        = 2
0.00.049.434 I llm_load_print_meta: rope scaling     = linear
0.00.049.434 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.434 I llm_load_print_meta: freq_scale_train = 1
0.00.049.435 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.435 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.435 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.435 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.435 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.435 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.435 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.445 I llm_load_print_meta: model type       = 1.4B
0.00.049.445 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.445 I llm_load_print_meta: model params     = 1.41 B
0.00.049.446 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.446 I llm_load_print_meta: general.name     = 1.4B
0.00.049.446 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.446 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.446 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.447 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.447 I llm_load_print_meta: LF token         = 128 ''
0.00.049.448 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.448 I llm_load_print_meta: max token length = 1024
0.00.051.385 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.385 I llm_load_tensors: offloading output layer to GPU
0.00.051.385 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.396 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.397 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.282 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.283 I llama_new_context_with_model: n_ctx         = 128
0.00.052.283 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.283 I llama_new_context_with_model: n_batch       = 128
0.00.052.284 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.284 I llama_new_context_with_model: flash_attn    = 0
0.00.052.284 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.284 I llama_new_context_with_model: freq_scale    = 1
0.00.052.285 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.285 I ggml_metal_init: allocating
0.00.052.292 I ggml_metal_init: found device: Apple M4
0.00.052.294 I ggml_metal_init: picking default device: Apple M4
0.00.052.861 I ggml_metal_init: using embedded metal library
0.00.055.232 I ggml_metal_init: GPU name:   Apple M4
0.00.055.234 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.234 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.235 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.235 I ggml_metal_init: simdgroup reduction   = true
0.00.055.235 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.235 I ggml_metal_init: has bfloat            = true
0.00.055.235 I ggml_metal_init: use bfloat            = true
0.00.055.236 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.236 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.928 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.934 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.948 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.827 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.828 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.829 I llama_new_context_with_model: graph nodes  = 967
0.00.066.829 I llama_new_context_with_model: graph splits = 2
0.00.066.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.812 I 
0.00.656.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.853 I perplexity: tokenizing the input ..
0.00.665.061 I perplexity: tokenization took 8.206 ms
0.00.665.077 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.240 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.789.454 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.789.471 I llama_perf_context_print:        load time =     647.89 ms
0.00.789.473 I llama_perf_context_print: prompt eval time =     122.94 ms /   128 tokens (    0.96 ms per token,  1041.20 tokens per second)
0.00.789.474 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.474 I llama_perf_context_print:       total time =     132.66 ms /   129 tokens
0.00.789.953 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.077s
sys	0m0.102s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.671 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.607 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.025.611 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.617 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.617 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.618 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.620 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.620 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.621 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.622 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.622 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.623 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.623 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.624 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.625 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.625 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.575 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.636 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.572 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.573 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.573 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.574 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.574 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.574 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.034.575 I llama_model_loader: - type  f32:  194 tensors
0.00.034.575 I llama_model_loader: - type q5_0:   97 tensors
0.00.034.575 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.458 I llm_load_vocab: special tokens cache size = 25
0.00.066.323 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.326 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.327 I llm_load_print_meta: arch             = gptneox
0.00.066.327 I llm_load_print_meta: vocab type       = BPE
0.00.066.327 I llm_load_print_meta: n_vocab          = 50304
0.00.066.328 I llm_load_print_meta: n_merges         = 50009
0.00.066.328 I llm_load_print_meta: vocab_only       = 0
0.00.066.328 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.328 I llm_load_print_meta: n_embd           = 2048
0.00.066.328 I llm_load_print_meta: n_layer          = 24
0.00.066.331 I llm_load_print_meta: n_head           = 16
0.00.066.332 I llm_load_print_meta: n_head_kv        = 16
0.00.066.343 I llm_load_print_meta: n_rot            = 32
0.00.066.344 I llm_load_print_meta: n_swa            = 0
0.00.066.344 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.344 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.345 I llm_load_print_meta: n_gqa            = 1
0.00.066.345 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.346 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.347 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.347 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.347 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.348 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.348 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.348 I llm_load_print_meta: n_ff             = 8192
0.00.066.349 I llm_load_print_meta: n_expert         = 0
0.00.066.349 I llm_load_print_meta: n_expert_used    = 0
0.00.066.349 I llm_load_print_meta: causal attn      = 1
0.00.066.349 I llm_load_print_meta: pooling type     = 0
0.00.066.349 I llm_load_print_meta: rope type        = 2
0.00.066.349 I llm_load_print_meta: rope scaling     = linear
0.00.066.350 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.350 I llm_load_print_meta: freq_scale_train = 1
0.00.066.350 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.351 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.351 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.351 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.353 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.353 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.353 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.363 I llm_load_print_meta: model type       = 1.4B
0.00.066.365 I llm_load_print_meta: model ftype      = Q5_0
0.00.066.365 I llm_load_print_meta: model params     = 1.41 B
0.00.066.366 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.066.366 I llm_load_print_meta: general.name     = 1.4B
0.00.066.368 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.368 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.368 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.368 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.368 I llm_load_print_meta: LF token         = 128 ''
0.00.066.369 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.369 I llm_load_print_meta: max token length = 1024
0.00.068.746 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.746 I llm_load_tensors: offloading output layer to GPU
0.00.068.747 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.758 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.068.759 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.069.933 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.934 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.934 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.934 I llama_new_context_with_model: n_batch       = 2048
0.00.069.934 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.935 I llama_new_context_with_model: flash_attn    = 0
0.00.069.935 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.935 I llama_new_context_with_model: freq_scale    = 1
0.00.069.936 I ggml_metal_init: allocating
0.00.069.939 I ggml_metal_init: found device: Apple M4
0.00.069.942 I ggml_metal_init: picking default device: Apple M4
0.00.070.632 I ggml_metal_init: using embedded metal library
0.00.073.332 I ggml_metal_init: GPU name:   Apple M4
0.00.073.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.334 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.334 I ggml_metal_init: simdgroup reduction   = true
0.00.073.335 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.335 I ggml_metal_init: has bfloat            = true
0.00.073.335 I ggml_metal_init: use bfloat            = true
0.00.073.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.566 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.571 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.587 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.586 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.587 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.588 I llama_new_context_with_model: graph nodes  = 967
0.00.105.588 I llama_new_context_with_model: graph splits = 2
0.00.105.601 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.907.765 I main: llama threadpool init, n_threads = 4
0.00.907.803 I 
0.00.907.835 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.907.836 I 
0.00.908.092 I sampler seed: 1234
0.00.908.098 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.908.134 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.908.135 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.908.135 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.708.938 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.708.938 I llama_perf_context_print:        load time =     899.09 ms
0.01.708.939 I llama_perf_context_print: prompt eval time =      47.07 ms /     7 tokens (    6.72 ms per token,   148.73 tokens per second)
0.01.708.943 I llama_perf_context_print:        eval time =     750.74 ms /    63 runs   (   11.92 ms per token,    83.92 tokens per second)
0.01.708.943 I llama_perf_context_print:       total time =     801.18 ms /    70 tokens
0.01.709.141 I ggml_metal_free: deallocating

real	0m1.727s
user	0m0.116s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.351 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.170 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.174 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.177 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.178 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.178 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.178 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.179 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.180 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.181 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.182 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.182 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.182 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.183 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.183 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.185 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.185 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.185 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.997 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.082 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.866 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.867 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.868 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.868 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.869 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.869 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.869 I llama_model_loader: - type  f32:  194 tensors
0.00.023.870 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.870 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.800 I llm_load_vocab: special tokens cache size = 25
0.00.049.821 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.823 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.824 I llm_load_print_meta: arch             = gptneox
0.00.049.824 I llm_load_print_meta: vocab type       = BPE
0.00.049.824 I llm_load_print_meta: n_vocab          = 50304
0.00.049.824 I llm_load_print_meta: n_merges         = 50009
0.00.049.825 I llm_load_print_meta: vocab_only       = 0
0.00.049.825 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.825 I llm_load_print_meta: n_embd           = 2048
0.00.049.825 I llm_load_print_meta: n_layer          = 24
0.00.049.828 I llm_load_print_meta: n_head           = 16
0.00.049.829 I llm_load_print_meta: n_head_kv        = 16
0.00.049.836 I llm_load_print_meta: n_rot            = 32
0.00.049.836 I llm_load_print_meta: n_swa            = 0
0.00.049.836 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.836 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.837 I llm_load_print_meta: n_gqa            = 1
0.00.049.838 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.838 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.839 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.839 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.840 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.840 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.840 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.841 I llm_load_print_meta: n_ff             = 8192
0.00.049.841 I llm_load_print_meta: n_expert         = 0
0.00.049.841 I llm_load_print_meta: n_expert_used    = 0
0.00.049.841 I llm_load_print_meta: causal attn      = 1
0.00.049.841 I llm_load_print_meta: pooling type     = 0
0.00.049.841 I llm_load_print_meta: rope type        = 2
0.00.049.842 I llm_load_print_meta: rope scaling     = linear
0.00.049.842 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.842 I llm_load_print_meta: freq_scale_train = 1
0.00.049.843 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.843 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.843 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.843 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.843 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.843 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.844 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.848 I llm_load_print_meta: model type       = 1.4B
0.00.049.848 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.849 I llm_load_print_meta: model params     = 1.41 B
0.00.049.849 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.849 I llm_load_print_meta: general.name     = 1.4B
0.00.049.851 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.851 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.851 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.851 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.852 I llm_load_print_meta: LF token         = 128 ''
0.00.049.852 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.852 I llm_load_print_meta: max token length = 1024
0.00.051.599 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.599 I llm_load_tensors: offloading output layer to GPU
0.00.051.599 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.605 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.605 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.499 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.499 I llama_new_context_with_model: n_ctx         = 128
0.00.052.500 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.500 I llama_new_context_with_model: n_batch       = 128
0.00.052.500 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.500 I llama_new_context_with_model: flash_attn    = 0
0.00.052.500 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.501 I llama_new_context_with_model: freq_scale    = 1
0.00.052.501 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.501 I ggml_metal_init: allocating
0.00.052.504 I ggml_metal_init: found device: Apple M4
0.00.052.506 I ggml_metal_init: picking default device: Apple M4
0.00.053.053 I ggml_metal_init: using embedded metal library
0.00.055.343 I ggml_metal_init: GPU name:   Apple M4
0.00.055.345 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.345 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.346 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.346 I ggml_metal_init: simdgroup reduction   = true
0.00.055.346 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.346 I ggml_metal_init: has bfloat            = true
0.00.055.346 I ggml_metal_init: use bfloat            = true
0.00.055.347 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.347 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.000 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.004 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.017 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.938 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.939 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.940 I llama_new_context_with_model: graph nodes  = 967
0.00.066.940 I llama_new_context_with_model: graph splits = 2
0.00.066.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.627 I 
0.00.690.669 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.679 I perplexity: tokenizing the input ..
0.00.698.992 I perplexity: tokenization took 8.311 ms
0.00.699.002 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.388 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.834.695 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.834.715 I llama_perf_context_print:        load time =     681.27 ms
0.00.834.716 I llama_perf_context_print: prompt eval time =     134.16 ms /   128 tokens (    1.05 ms per token,   954.11 tokens per second)
0.00.834.717 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.718 I llama_perf_context_print:       total time =     144.09 ms /   129 tokens
0.00.835.224 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.077s
sys	0m0.113s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.016.700 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.208 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.036.213 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.219 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.219 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.220 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.220 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.221 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.222 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.222 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.222 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.223 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.223 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.223 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.224 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.225 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.226 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.226 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.338 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.187 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.048.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.190 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.190 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.191 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.048.192 I llama_model_loader: - type  f32:  194 tensors
0.00.048.192 I llama_model_loader: - type q5_1:   97 tensors
0.00.048.192 I llama_model_loader: - type q6_K:    1 tensors
0.00.084.055 I llm_load_vocab: special tokens cache size = 25
0.00.093.672 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.093.675 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.093.676 I llm_load_print_meta: arch             = gptneox
0.00.093.676 I llm_load_print_meta: vocab type       = BPE
0.00.093.676 I llm_load_print_meta: n_vocab          = 50304
0.00.093.677 I llm_load_print_meta: n_merges         = 50009
0.00.093.677 I llm_load_print_meta: vocab_only       = 0
0.00.093.677 I llm_load_print_meta: n_ctx_train      = 2048
0.00.093.677 I llm_load_print_meta: n_embd           = 2048
0.00.093.677 I llm_load_print_meta: n_layer          = 24
0.00.093.681 I llm_load_print_meta: n_head           = 16
0.00.093.682 I llm_load_print_meta: n_head_kv        = 16
0.00.093.689 I llm_load_print_meta: n_rot            = 32
0.00.093.689 I llm_load_print_meta: n_swa            = 0
0.00.093.690 I llm_load_print_meta: n_embd_head_k    = 128
0.00.093.690 I llm_load_print_meta: n_embd_head_v    = 128
0.00.093.691 I llm_load_print_meta: n_gqa            = 1
0.00.093.692 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.093.693 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.093.693 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.093.693 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.093.694 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.093.694 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.093.694 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.093.695 I llm_load_print_meta: n_ff             = 8192
0.00.093.695 I llm_load_print_meta: n_expert         = 0
0.00.093.695 I llm_load_print_meta: n_expert_used    = 0
0.00.093.696 I llm_load_print_meta: causal attn      = 1
0.00.093.696 I llm_load_print_meta: pooling type     = 0
0.00.093.696 I llm_load_print_meta: rope type        = 2
0.00.093.699 I llm_load_print_meta: rope scaling     = linear
0.00.093.699 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.093.699 I llm_load_print_meta: freq_scale_train = 1
0.00.093.700 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.093.700 I llm_load_print_meta: rope_finetuned   = unknown
0.00.093.700 I llm_load_print_meta: ssm_d_conv       = 0
0.00.093.700 I llm_load_print_meta: ssm_d_inner      = 0
0.00.093.700 I llm_load_print_meta: ssm_d_state      = 0
0.00.093.701 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.093.701 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.093.706 I llm_load_print_meta: model type       = 1.4B
0.00.093.706 I llm_load_print_meta: model ftype      = Q5_1
0.00.093.707 I llm_load_print_meta: model params     = 1.41 B
0.00.093.708 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.093.708 I llm_load_print_meta: general.name     = 1.4B
0.00.093.708 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.093.709 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.093.709 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.093.709 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.093.710 I llm_load_print_meta: LF token         = 128 ''
0.00.093.710 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.093.710 I llm_load_print_meta: max token length = 1024
0.00.096.176 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.176 I llm_load_tensors: offloading output layer to GPU
0.00.096.176 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.183 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.096.183 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.097.519 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.521 I llama_new_context_with_model: n_ctx         = 2048
0.00.097.521 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.097.521 I llama_new_context_with_model: n_batch       = 2048
0.00.097.521 I llama_new_context_with_model: n_ubatch      = 512
0.00.097.522 I llama_new_context_with_model: flash_attn    = 0
0.00.097.523 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.523 I llama_new_context_with_model: freq_scale    = 1
0.00.097.524 I ggml_metal_init: allocating
0.00.097.533 I ggml_metal_init: found device: Apple M4
0.00.097.536 I ggml_metal_init: picking default device: Apple M4
0.00.098.343 I ggml_metal_init: using embedded metal library
0.00.101.785 I ggml_metal_init: GPU name:   Apple M4
0.00.101.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.788 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.788 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.789 I ggml_metal_init: simdgroup reduction   = true
0.00.101.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.789 I ggml_metal_init: has bfloat            = true
0.00.101.789 I ggml_metal_init: use bfloat            = true
0.00.101.790 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.791 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.135.122 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.135.129 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.135.158 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.136.063 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.136.064 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.136.065 I llama_new_context_with_model: graph nodes  = 967
0.00.136.065 I llama_new_context_with_model: graph splits = 2
0.00.136.079 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.995.558 I main: llama threadpool init, n_threads = 4
0.00.995.649 I 
0.00.995.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.995.723 I 
0.00.996.290 I sampler seed: 1234
0.00.996.296 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.996.328 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.996.330 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.996.331 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.852.989 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.852.990 I llama_perf_context_print:        load time =     978.85 ms
0.01.852.991 I llama_perf_context_print: prompt eval time =      53.61 ms /     7 tokens (    7.66 ms per token,   130.57 tokens per second)
0.01.852.991 I llama_perf_context_print:        eval time =     800.02 ms /    63 runs   (   12.70 ms per token,    78.75 tokens per second)
0.01.852.992 I llama_perf_context_print:       total time =     857.44 ms /    70 tokens
0.01.853.205 I ggml_metal_free: deallocating

real	0m1.884s
user	0m0.150s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.324 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.188 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.193 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.198 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.199 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.199 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.200 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.200 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.202 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.203 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.203 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.203 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.203 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.204 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.208 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.209 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.209 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.975 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.012 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.889 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.890 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.890 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.891 I llama_model_loader: - type  f32:  194 tensors
0.00.023.891 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.892 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.032 I llm_load_vocab: special tokens cache size = 25
0.00.049.928 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.931 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.931 I llm_load_print_meta: arch             = gptneox
0.00.049.931 I llm_load_print_meta: vocab type       = BPE
0.00.049.931 I llm_load_print_meta: n_vocab          = 50304
0.00.049.932 I llm_load_print_meta: n_merges         = 50009
0.00.049.932 I llm_load_print_meta: vocab_only       = 0
0.00.049.932 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.932 I llm_load_print_meta: n_embd           = 2048
0.00.049.932 I llm_load_print_meta: n_layer          = 24
0.00.049.936 I llm_load_print_meta: n_head           = 16
0.00.049.936 I llm_load_print_meta: n_head_kv        = 16
0.00.049.948 I llm_load_print_meta: n_rot            = 32
0.00.049.948 I llm_load_print_meta: n_swa            = 0
0.00.049.948 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.948 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.950 I llm_load_print_meta: n_gqa            = 1
0.00.049.951 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.951 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.952 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.952 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.952 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.953 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.954 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.954 I llm_load_print_meta: n_ff             = 8192
0.00.049.955 I llm_load_print_meta: n_expert         = 0
0.00.049.955 I llm_load_print_meta: n_expert_used    = 0
0.00.049.956 I llm_load_print_meta: causal attn      = 1
0.00.049.956 I llm_load_print_meta: pooling type     = 0
0.00.049.956 I llm_load_print_meta: rope type        = 2
0.00.049.956 I llm_load_print_meta: rope scaling     = linear
0.00.049.957 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.957 I llm_load_print_meta: freq_scale_train = 1
0.00.049.957 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.957 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.957 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.957 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.958 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.958 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.958 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.967 I llm_load_print_meta: model type       = 1.4B
0.00.049.968 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.968 I llm_load_print_meta: model params     = 1.41 B
0.00.049.969 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.969 I llm_load_print_meta: general.name     = 1.4B
0.00.049.969 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.969 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.969 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.969 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.971 I llm_load_print_meta: LF token         = 128 ''
0.00.049.971 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.971 I llm_load_print_meta: max token length = 1024
0.00.051.984 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.984 I llm_load_tensors: offloading output layer to GPU
0.00.051.984 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.995 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.996 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.921 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.921 I llama_new_context_with_model: n_ctx         = 128
0.00.052.922 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.922 I llama_new_context_with_model: n_batch       = 128
0.00.052.922 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.922 I llama_new_context_with_model: flash_attn    = 0
0.00.052.922 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.923 I llama_new_context_with_model: freq_scale    = 1
0.00.052.923 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.924 I ggml_metal_init: allocating
0.00.052.928 I ggml_metal_init: found device: Apple M4
0.00.052.931 I ggml_metal_init: picking default device: Apple M4
0.00.053.505 I ggml_metal_init: using embedded metal library
0.00.055.795 I ggml_metal_init: GPU name:   Apple M4
0.00.055.797 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.797 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.797 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.798 I ggml_metal_init: simdgroup reduction   = true
0.00.055.798 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.798 I ggml_metal_init: has bfloat            = true
0.00.055.798 I ggml_metal_init: use bfloat            = true
0.00.055.798 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.799 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.528 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.530 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.544 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.444 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.445 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.445 I llama_new_context_with_model: graph nodes  = 967
0.00.067.445 I llama_new_context_with_model: graph splits = 2
0.00.067.458 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.974 I 
0.00.745.017 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.027 I perplexity: tokenizing the input ..
0.00.752.530 I perplexity: tokenization took 7.501 ms
0.00.752.540 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.887.800 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.889.056 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.889.073 I llama_perf_context_print:        load time =     735.64 ms
0.00.889.074 I llama_perf_context_print: prompt eval time =     135.02 ms /   128 tokens (    1.05 ms per token,   947.99 tokens per second)
0.00.889.075 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.889.075 I llama_perf_context_print:       total time =     144.10 ms /   129 tokens
0.00.889.564 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.077s
sys	0m0.120s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.016.377 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.935 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.023.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.941 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.941 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.942 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.944 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.944 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.945 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.945 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.945 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.946 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.947 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.947 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.947 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.853 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.581 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.582 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.582 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.583 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.583 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.583 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.033.584 I llama_model_loader: - type  f32:  194 tensors
0.00.033.584 I llama_model_loader: - type q2_K:   49 tensors
0.00.033.584 I llama_model_loader: - type q3_K:   48 tensors
0.00.033.585 I llama_model_loader: - type q6_K:    1 tensors
0.00.060.624 I llm_load_vocab: special tokens cache size = 25
0.00.070.011 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.014 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.014 I llm_load_print_meta: arch             = gptneox
0.00.070.015 I llm_load_print_meta: vocab type       = BPE
0.00.070.015 I llm_load_print_meta: n_vocab          = 50304
0.00.070.015 I llm_load_print_meta: n_merges         = 50009
0.00.070.016 I llm_load_print_meta: vocab_only       = 0
0.00.070.016 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.016 I llm_load_print_meta: n_embd           = 2048
0.00.070.016 I llm_load_print_meta: n_layer          = 24
0.00.070.020 I llm_load_print_meta: n_head           = 16
0.00.070.021 I llm_load_print_meta: n_head_kv        = 16
0.00.070.033 I llm_load_print_meta: n_rot            = 32
0.00.070.033 I llm_load_print_meta: n_swa            = 0
0.00.070.033 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.034 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.035 I llm_load_print_meta: n_gqa            = 1
0.00.070.036 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.036 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.037 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.038 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.038 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.038 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.038 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.039 I llm_load_print_meta: n_ff             = 8192
0.00.070.040 I llm_load_print_meta: n_expert         = 0
0.00.070.040 I llm_load_print_meta: n_expert_used    = 0
0.00.070.040 I llm_load_print_meta: causal attn      = 1
0.00.070.040 I llm_load_print_meta: pooling type     = 0
0.00.070.040 I llm_load_print_meta: rope type        = 2
0.00.070.041 I llm_load_print_meta: rope scaling     = linear
0.00.070.041 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.042 I llm_load_print_meta: freq_scale_train = 1
0.00.070.042 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.042 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.042 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.042 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.044 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.044 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.044 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.054 I llm_load_print_meta: model type       = 1.4B
0.00.070.055 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.070.055 I llm_load_print_meta: model params     = 1.41 B
0.00.070.056 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.070.056 I llm_load_print_meta: general.name     = 1.4B
0.00.070.056 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.057 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.057 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.059 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.059 I llm_load_print_meta: LF token         = 128 ''
0.00.070.060 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.060 I llm_load_print_meta: max token length = 1024
0.00.072.437 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.438 I llm_load_tensors: offloading output layer to GPU
0.00.072.438 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.449 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.072.450 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.073.643 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.644 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.644 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.644 I llama_new_context_with_model: n_batch       = 2048
0.00.073.645 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.645 I llama_new_context_with_model: flash_attn    = 0
0.00.073.645 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.646 I llama_new_context_with_model: freq_scale    = 1
0.00.073.646 I ggml_metal_init: allocating
0.00.073.650 I ggml_metal_init: found device: Apple M4
0.00.073.653 I ggml_metal_init: picking default device: Apple M4
0.00.074.376 I ggml_metal_init: using embedded metal library
0.00.078.028 I ggml_metal_init: GPU name:   Apple M4
0.00.078.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.032 I ggml_metal_init: simdgroup reduction   = true
0.00.078.032 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.032 I ggml_metal_init: has bfloat            = true
0.00.078.033 I ggml_metal_init: use bfloat            = true
0.00.078.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.844 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.850 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.870 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.910 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.114.911 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.114.912 I llama_new_context_with_model: graph nodes  = 967
0.00.114.912 I llama_new_context_with_model: graph splits = 2
0.00.114.926 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.557.839 I main: llama threadpool init, n_threads = 4
0.00.557.882 I 
0.00.557.913 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.557.913 I 
0.00.558.146 I sampler seed: 1234
0.00.558.151 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.558.162 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.558.163 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.558.163 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.237.692 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.01.237.693 I llama_perf_context_print:        load time =     541.46 ms
0.01.237.694 I llama_perf_context_print: prompt eval time =      35.75 ms /     7 tokens (    5.11 ms per token,   195.78 tokens per second)
0.01.237.695 I llama_perf_context_print:        eval time =     640.76 ms /    63 runs   (   10.17 ms per token,    98.32 tokens per second)
0.01.237.695 I llama_perf_context_print:       total time =     679.86 ms /    70 tokens
0.01.237.889 I ggml_metal_free: deallocating

real	0m1.263s
user	0m0.126s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.941 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.288 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.295 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.295 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.295 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.296 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.296 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.297 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.297 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.297 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.298 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.298 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.299 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.300 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.039 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.096 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.989 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.990 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.990 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.990 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.990 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.991 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.991 I llama_model_loader: - type  f32:  194 tensors
0.00.023.991 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.992 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.992 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.965 I llm_load_vocab: special tokens cache size = 25
0.00.050.039 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.041 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.042 I llm_load_print_meta: arch             = gptneox
0.00.050.042 I llm_load_print_meta: vocab type       = BPE
0.00.050.042 I llm_load_print_meta: n_vocab          = 50304
0.00.050.043 I llm_load_print_meta: n_merges         = 50009
0.00.050.043 I llm_load_print_meta: vocab_only       = 0
0.00.050.043 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.043 I llm_load_print_meta: n_embd           = 2048
0.00.050.043 I llm_load_print_meta: n_layer          = 24
0.00.050.046 I llm_load_print_meta: n_head           = 16
0.00.050.047 I llm_load_print_meta: n_head_kv        = 16
0.00.050.059 I llm_load_print_meta: n_rot            = 32
0.00.050.060 I llm_load_print_meta: n_swa            = 0
0.00.050.060 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.060 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.061 I llm_load_print_meta: n_gqa            = 1
0.00.050.062 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.062 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.063 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.063 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.063 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.063 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.063 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.064 I llm_load_print_meta: n_ff             = 8192
0.00.050.064 I llm_load_print_meta: n_expert         = 0
0.00.050.064 I llm_load_print_meta: n_expert_used    = 0
0.00.050.066 I llm_load_print_meta: causal attn      = 1
0.00.050.066 I llm_load_print_meta: pooling type     = 0
0.00.050.066 I llm_load_print_meta: rope type        = 2
0.00.050.066 I llm_load_print_meta: rope scaling     = linear
0.00.050.067 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.067 I llm_load_print_meta: freq_scale_train = 1
0.00.050.067 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.067 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.069 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.069 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.069 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.069 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.070 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.079 I llm_load_print_meta: model type       = 1.4B
0.00.050.079 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.080 I llm_load_print_meta: model params     = 1.41 B
0.00.050.080 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.080 I llm_load_print_meta: general.name     = 1.4B
0.00.050.081 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.081 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.081 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.081 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.081 I llm_load_print_meta: LF token         = 128 ''
0.00.050.081 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.082 I llm_load_print_meta: max token length = 1024
0.00.051.994 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.994 I llm_load_tensors: offloading output layer to GPU
0.00.051.995 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.005 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.006 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.950 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.951 I llama_new_context_with_model: n_ctx         = 128
0.00.052.951 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.951 I llama_new_context_with_model: n_batch       = 128
0.00.052.951 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.951 I llama_new_context_with_model: flash_attn    = 0
0.00.052.952 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.952 I llama_new_context_with_model: freq_scale    = 1
0.00.052.952 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.953 I ggml_metal_init: allocating
0.00.052.956 I ggml_metal_init: found device: Apple M4
0.00.052.958 I ggml_metal_init: picking default device: Apple M4
0.00.053.547 I ggml_metal_init: using embedded metal library
0.00.055.859 I ggml_metal_init: GPU name:   Apple M4
0.00.055.860 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.860 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.861 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.861 I ggml_metal_init: simdgroup reduction   = true
0.00.055.861 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.861 I ggml_metal_init: has bfloat            = true
0.00.055.862 I ggml_metal_init: use bfloat            = true
0.00.055.862 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.863 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.649 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.651 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.664 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.597 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.599 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.599 I llama_new_context_with_model: graph nodes  = 967
0.00.067.599 I llama_new_context_with_model: graph splits = 2
0.00.067.612 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.462.116 I 
0.00.462.150 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.462.158 I perplexity: tokenizing the input ..
0.00.470.044 I perplexity: tokenization took 7.884 ms
0.00.470.057 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.601.962 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.603.289 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.603.304 I llama_perf_context_print:        load time =     452.17 ms
0.00.603.305 I llama_perf_context_print: prompt eval time =     131.68 ms /   128 tokens (    1.03 ms per token,   972.07 tokens per second)
0.00.603.306 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.603.308 I llama_perf_context_print:       total time =     141.19 ms /   129 tokens
0.00.603.704 I ggml_metal_free: deallocating

real	0m0.617s
user	0m0.076s
sys	0m0.074s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.011.972 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.636 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.030.641 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.646 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.646 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.647 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.647 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.647 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.648 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.649 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.649 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.649 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.651 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.652 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.652 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.735 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.825 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.933 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.935 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.935 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.935 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.936 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.936 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.039.937 I llama_model_loader: - type  f32:  194 tensors
0.00.039.937 I llama_model_loader: - type q3_K:   25 tensors
0.00.039.937 I llama_model_loader: - type q4_K:   71 tensors
0.00.039.937 I llama_model_loader: - type q5_K:    1 tensors
0.00.039.938 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.160 I llm_load_vocab: special tokens cache size = 25
0.00.073.825 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.829 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.829 I llm_load_print_meta: arch             = gptneox
0.00.073.830 I llm_load_print_meta: vocab type       = BPE
0.00.073.830 I llm_load_print_meta: n_vocab          = 50304
0.00.073.830 I llm_load_print_meta: n_merges         = 50009
0.00.073.831 I llm_load_print_meta: vocab_only       = 0
0.00.073.831 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.831 I llm_load_print_meta: n_embd           = 2048
0.00.073.831 I llm_load_print_meta: n_layer          = 24
0.00.073.834 I llm_load_print_meta: n_head           = 16
0.00.073.835 I llm_load_print_meta: n_head_kv        = 16
0.00.073.852 I llm_load_print_meta: n_rot            = 32
0.00.073.853 I llm_load_print_meta: n_swa            = 0
0.00.073.853 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.853 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.863 I llm_load_print_meta: n_gqa            = 1
0.00.073.866 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.867 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.868 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.868 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.869 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.869 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.871 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.871 I llm_load_print_meta: n_ff             = 8192
0.00.073.872 I llm_load_print_meta: n_expert         = 0
0.00.073.872 I llm_load_print_meta: n_expert_used    = 0
0.00.073.872 I llm_load_print_meta: causal attn      = 1
0.00.073.872 I llm_load_print_meta: pooling type     = 0
0.00.073.874 I llm_load_print_meta: rope type        = 2
0.00.073.874 I llm_load_print_meta: rope scaling     = linear
0.00.073.875 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.875 I llm_load_print_meta: freq_scale_train = 1
0.00.073.875 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.875 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.876 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.876 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.876 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.876 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.876 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.882 I llm_load_print_meta: model type       = 1.4B
0.00.073.882 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.073.883 I llm_load_print_meta: model params     = 1.41 B
0.00.073.883 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.073.884 I llm_load_print_meta: general.name     = 1.4B
0.00.073.884 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.884 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.884 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.885 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.885 I llm_load_print_meta: LF token         = 128 ''
0.00.073.885 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.886 I llm_load_print_meta: max token length = 1024
0.00.076.389 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.390 I llm_load_tensors: offloading output layer to GPU
0.00.076.390 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.401 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.076.402 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.077.748 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.749 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.749 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.750 I llama_new_context_with_model: n_batch       = 2048
0.00.077.750 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.750 I llama_new_context_with_model: flash_attn    = 0
0.00.077.751 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.751 I llama_new_context_with_model: freq_scale    = 1
0.00.077.752 I ggml_metal_init: allocating
0.00.077.755 I ggml_metal_init: found device: Apple M4
0.00.077.758 I ggml_metal_init: picking default device: Apple M4
0.00.078.546 I ggml_metal_init: using embedded metal library
0.00.082.014 I ggml_metal_init: GPU name:   Apple M4
0.00.082.016 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.016 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.016 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.017 I ggml_metal_init: simdgroup reduction   = true
0.00.082.017 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.017 I ggml_metal_init: has bfloat            = true
0.00.082.017 I ggml_metal_init: use bfloat            = true
0.00.082.018 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.018 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.683 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.690 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.713 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.659 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.660 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.661 I llama_new_context_with_model: graph nodes  = 967
0.00.115.661 I llama_new_context_with_model: graph splits = 2
0.00.115.676 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.574.163 I main: llama threadpool init, n_threads = 4
0.00.574.201 I 
0.00.574.233 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.574.233 I 
0.00.574.462 I sampler seed: 1234
0.00.574.467 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.574.477 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.574.481 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.574.481 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.320.874 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.320.875 I llama_perf_context_print:        load time =     562.19 ms
0.01.320.876 I llama_perf_context_print: prompt eval time =      40.63 ms /     7 tokens (    5.80 ms per token,   172.29 tokens per second)
0.01.320.877 I llama_perf_context_print:        eval time =     702.79 ms /    63 runs   (   11.16 ms per token,    89.64 tokens per second)
0.01.320.877 I llama_perf_context_print:       total time =     746.71 ms /    70 tokens
0.01.321.071 I ggml_metal_free: deallocating

real	0m1.339s
user	0m0.123s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.818 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.454 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.459 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.460 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.461 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.461 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.462 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.462 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.463 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.463 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.463 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.464 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.464 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.464 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.465 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.466 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.467 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.467 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.426 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.666 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.662 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.663 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.663 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.664 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.664 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.664 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.664 I llama_model_loader: - type  f32:  194 tensors
0.00.023.665 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.665 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.665 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.665 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.762 I llm_load_vocab: special tokens cache size = 25
0.00.049.664 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.668 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.668 I llm_load_print_meta: arch             = gptneox
0.00.049.668 I llm_load_print_meta: vocab type       = BPE
0.00.049.669 I llm_load_print_meta: n_vocab          = 50304
0.00.049.669 I llm_load_print_meta: n_merges         = 50009
0.00.049.670 I llm_load_print_meta: vocab_only       = 0
0.00.049.670 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.671 I llm_load_print_meta: n_embd           = 2048
0.00.049.675 I llm_load_print_meta: n_layer          = 24
0.00.049.678 I llm_load_print_meta: n_head           = 16
0.00.049.678 I llm_load_print_meta: n_head_kv        = 16
0.00.049.690 I llm_load_print_meta: n_rot            = 32
0.00.049.691 I llm_load_print_meta: n_swa            = 0
0.00.049.692 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.692 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.692 I llm_load_print_meta: n_gqa            = 1
0.00.049.693 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.693 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.694 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.694 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.694 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.694 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.694 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.695 I llm_load_print_meta: n_ff             = 8192
0.00.049.695 I llm_load_print_meta: n_expert         = 0
0.00.049.695 I llm_load_print_meta: n_expert_used    = 0
0.00.049.696 I llm_load_print_meta: causal attn      = 1
0.00.049.696 I llm_load_print_meta: pooling type     = 0
0.00.049.696 I llm_load_print_meta: rope type        = 2
0.00.049.696 I llm_load_print_meta: rope scaling     = linear
0.00.049.696 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.697 I llm_load_print_meta: freq_scale_train = 1
0.00.049.697 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.697 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.697 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.697 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.697 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.699 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.699 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.708 I llm_load_print_meta: model type       = 1.4B
0.00.049.708 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.709 I llm_load_print_meta: model params     = 1.41 B
0.00.049.709 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.709 I llm_load_print_meta: general.name     = 1.4B
0.00.049.710 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.710 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.710 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.710 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.710 I llm_load_print_meta: LF token         = 128 ''
0.00.049.711 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.711 I llm_load_print_meta: max token length = 1024
0.00.051.483 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.483 I llm_load_tensors: offloading output layer to GPU
0.00.051.483 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.488 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.488 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.387 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.388 I llama_new_context_with_model: n_ctx         = 128
0.00.052.388 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.388 I llama_new_context_with_model: n_batch       = 128
0.00.052.388 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.388 I llama_new_context_with_model: flash_attn    = 0
0.00.052.389 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.389 I llama_new_context_with_model: freq_scale    = 1
0.00.052.389 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.390 I ggml_metal_init: allocating
0.00.052.396 I ggml_metal_init: found device: Apple M4
0.00.052.398 I ggml_metal_init: picking default device: Apple M4
0.00.052.965 I ggml_metal_init: using embedded metal library
0.00.055.269 I ggml_metal_init: GPU name:   Apple M4
0.00.055.270 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.271 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.272 I ggml_metal_init: simdgroup reduction   = true
0.00.055.272 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.272 I ggml_metal_init: has bfloat            = true
0.00.055.272 I ggml_metal_init: use bfloat            = true
0.00.055.273 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.273 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.835 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.837 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.849 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.732 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.733 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.734 I llama_new_context_with_model: graph nodes  = 967
0.00.066.734 I llama_new_context_with_model: graph splits = 2
0.00.066.741 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.492.943 I 
0.00.492.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.492.997 I perplexity: tokenizing the input ..
0.00.501.103 I perplexity: tokenization took 8.104 ms
0.00.501.114 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.633.253 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.634.412 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.634.430 I llama_perf_context_print:        load time =     484.12 ms
0.00.634.431 I llama_perf_context_print: prompt eval time =     131.91 ms /   128 tokens (    1.03 ms per token,   970.36 tokens per second)
0.00.634.432 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.634.432 I llama_perf_context_print:       total time =     141.49 ms /   129 tokens
0.00.634.973 I ggml_metal_free: deallocating

real	0m0.651s
user	0m0.078s
sys	0m0.095s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.010.931 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.248 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.249 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.250 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.250 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.250 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.251 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.252 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.252 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.253 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.253 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.256 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.256 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.256 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.138 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.189 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.045 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.046 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.046 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.047 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.047 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.047 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.048 I llama_model_loader: - type  f32:  194 tensors
0.00.026.048 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.048 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.048 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.332 I llm_load_vocab: special tokens cache size = 25
0.00.052.160 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.163 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.163 I llm_load_print_meta: arch             = gptneox
0.00.052.163 I llm_load_print_meta: vocab type       = BPE
0.00.052.164 I llm_load_print_meta: n_vocab          = 50304
0.00.052.164 I llm_load_print_meta: n_merges         = 50009
0.00.052.164 I llm_load_print_meta: vocab_only       = 0
0.00.052.164 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.164 I llm_load_print_meta: n_embd           = 2048
0.00.052.164 I llm_load_print_meta: n_layer          = 24
0.00.052.168 I llm_load_print_meta: n_head           = 16
0.00.052.168 I llm_load_print_meta: n_head_kv        = 16
0.00.052.180 I llm_load_print_meta: n_rot            = 32
0.00.052.181 I llm_load_print_meta: n_swa            = 0
0.00.052.181 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.181 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.182 I llm_load_print_meta: n_gqa            = 1
0.00.052.183 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.183 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.184 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.184 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.184 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.184 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.185 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.185 I llm_load_print_meta: n_ff             = 8192
0.00.052.185 I llm_load_print_meta: n_expert         = 0
0.00.052.187 I llm_load_print_meta: n_expert_used    = 0
0.00.052.189 I llm_load_print_meta: causal attn      = 1
0.00.052.189 I llm_load_print_meta: pooling type     = 0
0.00.052.189 I llm_load_print_meta: rope type        = 2
0.00.052.189 I llm_load_print_meta: rope scaling     = linear
0.00.052.189 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.189 I llm_load_print_meta: freq_scale_train = 1
0.00.052.190 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.190 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.190 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.190 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.190 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.190 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.190 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.200 I llm_load_print_meta: model type       = 1.4B
0.00.052.201 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.201 I llm_load_print_meta: model params     = 1.41 B
0.00.052.201 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.202 I llm_load_print_meta: general.name     = 1.4B
0.00.052.202 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.202 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.203 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.203 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.204 I llm_load_print_meta: LF token         = 128 ''
0.00.052.204 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.204 I llm_load_print_meta: max token length = 1024
0.00.054.110 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.110 I llm_load_tensors: offloading output layer to GPU
0.00.054.110 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.121 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.122 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.071 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.072 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.072 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.072 I llama_new_context_with_model: n_batch       = 2048
0.00.055.072 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.072 I llama_new_context_with_model: flash_attn    = 0
0.00.055.073 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.073 I llama_new_context_with_model: freq_scale    = 1
0.00.055.074 I ggml_metal_init: allocating
0.00.055.080 I ggml_metal_init: found device: Apple M4
0.00.055.082 I ggml_metal_init: picking default device: Apple M4
0.00.055.689 I ggml_metal_init: using embedded metal library
0.00.058.031 I ggml_metal_init: GPU name:   Apple M4
0.00.058.032 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.033 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.033 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.035 I ggml_metal_init: simdgroup reduction   = true
0.00.058.035 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.035 I ggml_metal_init: has bfloat            = true
0.00.058.035 I ggml_metal_init: use bfloat            = true
0.00.058.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.382 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.386 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.406 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.363 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.364 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.365 I llama_new_context_with_model: graph nodes  = 967
0.00.087.365 I llama_new_context_with_model: graph splits = 2
0.00.087.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.085 I main: llama threadpool init, n_threads = 4
0.00.609.127 I 
0.00.609.162 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.163 I 
0.00.609.320 I sampler seed: 1234
0.00.609.325 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.609.360 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.609.362 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.609.362 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.372.845 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.372.845 I llama_perf_context_print:        load time =     598.15 ms
0.01.372.846 I llama_perf_context_print: prompt eval time =      47.04 ms /     7 tokens (    6.72 ms per token,   148.81 tokens per second)
0.01.372.847 I llama_perf_context_print:        eval time =     713.39 ms /    63 runs   (   11.32 ms per token,    88.31 tokens per second)
0.01.372.847 I llama_perf_context_print:       total time =     763.76 ms /    70 tokens
0.01.373.014 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.108s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.442 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.192 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.196 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.198 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.198 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.198 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.199 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.199 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.200 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.200 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.201 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.201 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.201 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.202 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.202 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.204 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.204 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.204 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.596 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.604 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.605 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.606 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.606 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.606 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.607 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.607 I llama_model_loader: - type  f32:  194 tensors
0.00.024.608 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.608 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.608 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.485 I llm_load_vocab: special tokens cache size = 25
0.00.051.513 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.515 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.516 I llm_load_print_meta: arch             = gptneox
0.00.051.516 I llm_load_print_meta: vocab type       = BPE
0.00.051.516 I llm_load_print_meta: n_vocab          = 50304
0.00.051.516 I llm_load_print_meta: n_merges         = 50009
0.00.051.517 I llm_load_print_meta: vocab_only       = 0
0.00.051.517 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.517 I llm_load_print_meta: n_embd           = 2048
0.00.051.517 I llm_load_print_meta: n_layer          = 24
0.00.051.520 I llm_load_print_meta: n_head           = 16
0.00.051.521 I llm_load_print_meta: n_head_kv        = 16
0.00.051.533 I llm_load_print_meta: n_rot            = 32
0.00.051.533 I llm_load_print_meta: n_swa            = 0
0.00.051.533 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.533 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.534 I llm_load_print_meta: n_gqa            = 1
0.00.051.535 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.536 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.536 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.536 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.537 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.537 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.537 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.538 I llm_load_print_meta: n_ff             = 8192
0.00.051.538 I llm_load_print_meta: n_expert         = 0
0.00.051.538 I llm_load_print_meta: n_expert_used    = 0
0.00.051.538 I llm_load_print_meta: causal attn      = 1
0.00.051.538 I llm_load_print_meta: pooling type     = 0
0.00.051.538 I llm_load_print_meta: rope type        = 2
0.00.051.538 I llm_load_print_meta: rope scaling     = linear
0.00.051.539 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.539 I llm_load_print_meta: freq_scale_train = 1
0.00.051.539 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.540 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.540 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.540 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.540 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.540 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.540 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.550 I llm_load_print_meta: model type       = 1.4B
0.00.051.550 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.551 I llm_load_print_meta: model params     = 1.41 B
0.00.051.551 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.551 I llm_load_print_meta: general.name     = 1.4B
0.00.051.551 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.552 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.552 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.552 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.552 I llm_load_print_meta: LF token         = 128 ''
0.00.051.552 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.553 I llm_load_print_meta: max token length = 1024
0.00.053.444 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.444 I llm_load_tensors: offloading output layer to GPU
0.00.053.444 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.455 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.456 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.370 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.371 I llama_new_context_with_model: n_ctx         = 128
0.00.054.371 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.371 I llama_new_context_with_model: n_batch       = 128
0.00.054.371 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.371 I llama_new_context_with_model: flash_attn    = 0
0.00.054.372 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.372 I llama_new_context_with_model: freq_scale    = 1
0.00.054.372 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.373 I ggml_metal_init: allocating
0.00.054.376 I ggml_metal_init: found device: Apple M4
0.00.054.378 I ggml_metal_init: picking default device: Apple M4
0.00.054.937 I ggml_metal_init: using embedded metal library
0.00.057.269 I ggml_metal_init: GPU name:   Apple M4
0.00.057.270 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.270 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.271 I ggml_metal_init: simdgroup reduction   = true
0.00.057.271 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.271 I ggml_metal_init: has bfloat            = true
0.00.057.272 I ggml_metal_init: use bfloat            = true
0.00.057.272 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.273 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.269 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.271 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.295 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.237 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.238 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.238 I llama_new_context_with_model: graph nodes  = 967
0.00.069.238 I llama_new_context_with_model: graph splits = 2
0.00.069.251 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.773 I 
0.00.587.853 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.867 I perplexity: tokenizing the input ..
0.00.595.807 I perplexity: tokenization took 7.938 ms
0.00.595.817 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.730.271 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.731.479 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.731.495 I llama_perf_context_print:        load time =     578.32 ms
0.00.731.496 I llama_perf_context_print: prompt eval time =     134.21 ms /   128 tokens (    1.05 ms per token,   953.70 tokens per second)
0.00.731.497 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.497 I llama_perf_context_print:       total time =     143.72 ms /   129 tokens
0.00.731.882 I ggml_metal_free: deallocating

real	0m0.746s
user	0m0.079s
sys	0m0.118s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.678 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.126 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.130 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.132 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.137 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.138 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.139 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.139 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.140 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.140 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.140 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.141 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.145 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.145 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.145 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.061 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.151 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.100 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.102 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.102 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.102 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.102 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.103 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.103 I llama_model_loader: - type  f32:  194 tensors
0.00.025.104 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.104 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.247 I llm_load_vocab: special tokens cache size = 25
0.00.052.229 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.232 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.232 I llm_load_print_meta: arch             = gptneox
0.00.052.233 I llm_load_print_meta: vocab type       = BPE
0.00.052.233 I llm_load_print_meta: n_vocab          = 50304
0.00.052.233 I llm_load_print_meta: n_merges         = 50009
0.00.052.233 I llm_load_print_meta: vocab_only       = 0
0.00.052.233 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.234 I llm_load_print_meta: n_embd           = 2048
0.00.052.234 I llm_load_print_meta: n_layer          = 24
0.00.052.237 I llm_load_print_meta: n_head           = 16
0.00.052.238 I llm_load_print_meta: n_head_kv        = 16
0.00.052.250 I llm_load_print_meta: n_rot            = 32
0.00.052.250 I llm_load_print_meta: n_swa            = 0
0.00.052.250 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.250 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.251 I llm_load_print_meta: n_gqa            = 1
0.00.052.252 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.253 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.255 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.255 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.256 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.256 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.256 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.257 I llm_load_print_meta: n_ff             = 8192
0.00.052.257 I llm_load_print_meta: n_expert         = 0
0.00.052.257 I llm_load_print_meta: n_expert_used    = 0
0.00.052.259 I llm_load_print_meta: causal attn      = 1
0.00.052.260 I llm_load_print_meta: pooling type     = 0
0.00.052.260 I llm_load_print_meta: rope type        = 2
0.00.052.260 I llm_load_print_meta: rope scaling     = linear
0.00.052.260 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.261 I llm_load_print_meta: freq_scale_train = 1
0.00.052.261 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.261 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.261 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.261 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.261 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.261 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.261 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.271 I llm_load_print_meta: model type       = 1.4B
0.00.052.271 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.271 I llm_load_print_meta: model params     = 1.41 B
0.00.052.272 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.273 I llm_load_print_meta: general.name     = 1.4B
0.00.052.274 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.274 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.274 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.275 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.275 I llm_load_print_meta: LF token         = 128 ''
0.00.052.275 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.275 I llm_load_print_meta: max token length = 1024
0.00.054.337 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.337 I llm_load_tensors: offloading output layer to GPU
0.00.054.337 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.348 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.349 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.309 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.310 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.310 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.310 I llama_new_context_with_model: n_batch       = 2048
0.00.055.310 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.310 I llama_new_context_with_model: flash_attn    = 0
0.00.055.311 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.311 I llama_new_context_with_model: freq_scale    = 1
0.00.055.312 I ggml_metal_init: allocating
0.00.055.317 I ggml_metal_init: found device: Apple M4
0.00.055.319 I ggml_metal_init: picking default device: Apple M4
0.00.055.887 I ggml_metal_init: using embedded metal library
0.00.058.228 I ggml_metal_init: GPU name:   Apple M4
0.00.058.229 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.231 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.232 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.232 I ggml_metal_init: simdgroup reduction   = true
0.00.058.232 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.232 I ggml_metal_init: has bfloat            = true
0.00.058.232 I ggml_metal_init: use bfloat            = true
0.00.058.233 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.233 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.580 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.595 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.615 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.577 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.578 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.579 I llama_new_context_with_model: graph nodes  = 967
0.00.087.579 I llama_new_context_with_model: graph splits = 2
0.00.087.592 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.227 I main: llama threadpool init, n_threads = 4
0.00.703.268 I 
0.00.703.299 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.299 I 
0.00.703.535 I sampler seed: 1234
0.00.703.540 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.568 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.569 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.569 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.556.149 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.556.150 I llama_perf_context_print:        load time =     694.54 ms
0.01.556.150 I llama_perf_context_print: prompt eval time =      51.63 ms /     7 tokens (    7.38 ms per token,   135.57 tokens per second)
0.01.556.151 I llama_perf_context_print:        eval time =     797.86 ms /    63 runs   (   12.66 ms per token,    78.96 tokens per second)
0.01.556.151 I llama_perf_context_print:       total time =     852.93 ms /    70 tokens
0.01.556.340 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.110s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.945 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.687 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.696 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.697 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.698 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.698 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.698 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.700 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.700 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.701 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.568 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.603 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.468 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.469 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.469 I llama_model_loader: - type  f32:  194 tensors
0.00.023.470 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.470 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.271 I llm_load_vocab: special tokens cache size = 25
0.00.050.374 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.377 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.377 I llm_load_print_meta: arch             = gptneox
0.00.050.378 I llm_load_print_meta: vocab type       = BPE
0.00.050.378 I llm_load_print_meta: n_vocab          = 50304
0.00.050.378 I llm_load_print_meta: n_merges         = 50009
0.00.050.378 I llm_load_print_meta: vocab_only       = 0
0.00.050.379 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.379 I llm_load_print_meta: n_embd           = 2048
0.00.050.379 I llm_load_print_meta: n_layer          = 24
0.00.050.381 I llm_load_print_meta: n_head           = 16
0.00.050.382 I llm_load_print_meta: n_head_kv        = 16
0.00.050.394 I llm_load_print_meta: n_rot            = 32
0.00.050.394 I llm_load_print_meta: n_swa            = 0
0.00.050.394 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.394 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.395 I llm_load_print_meta: n_gqa            = 1
0.00.050.396 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.396 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.397 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.397 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.398 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.398 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.398 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.399 I llm_load_print_meta: n_ff             = 8192
0.00.050.399 I llm_load_print_meta: n_expert         = 0
0.00.050.399 I llm_load_print_meta: n_expert_used    = 0
0.00.050.399 I llm_load_print_meta: causal attn      = 1
0.00.050.399 I llm_load_print_meta: pooling type     = 0
0.00.050.399 I llm_load_print_meta: rope type        = 2
0.00.050.400 I llm_load_print_meta: rope scaling     = linear
0.00.050.400 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.400 I llm_load_print_meta: freq_scale_train = 1
0.00.050.400 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.401 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.401 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.401 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.401 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.401 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.401 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.411 I llm_load_print_meta: model type       = 1.4B
0.00.050.411 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.411 I llm_load_print_meta: model params     = 1.41 B
0.00.050.412 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.412 I llm_load_print_meta: general.name     = 1.4B
0.00.050.412 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.415 I llm_load_print_meta: LF token         = 128 ''
0.00.050.415 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.415 I llm_load_print_meta: max token length = 1024
0.00.052.391 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.391 I llm_load_tensors: offloading output layer to GPU
0.00.052.391 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.402 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.403 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.312 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.313 I llama_new_context_with_model: n_ctx         = 128
0.00.053.313 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.313 I llama_new_context_with_model: n_batch       = 128
0.00.053.314 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.314 I llama_new_context_with_model: flash_attn    = 0
0.00.053.314 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.314 I llama_new_context_with_model: freq_scale    = 1
0.00.053.315 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.315 I ggml_metal_init: allocating
0.00.053.321 I ggml_metal_init: found device: Apple M4
0.00.053.323 I ggml_metal_init: picking default device: Apple M4
0.00.053.863 I ggml_metal_init: using embedded metal library
0.00.056.175 I ggml_metal_init: GPU name:   Apple M4
0.00.056.176 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.176 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.177 I ggml_metal_init: simdgroup reduction   = true
0.00.056.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.177 I ggml_metal_init: has bfloat            = true
0.00.056.177 I ggml_metal_init: use bfloat            = true
0.00.056.178 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.178 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.090 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.093 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.107 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.006 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.007 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.007 I llama_new_context_with_model: graph nodes  = 967
0.00.068.008 I llama_new_context_with_model: graph splits = 2
0.00.068.020 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.109 I 
0.00.662.140 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.152 I perplexity: tokenizing the input ..
0.00.670.063 I perplexity: tokenization took 7.912 ms
0.00.670.073 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.133 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.812.303 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.812.331 I llama_perf_context_print:        load time =     653.16 ms
0.00.812.332 I llama_perf_context_print: prompt eval time =     140.83 ms /   128 tokens (    1.10 ms per token,   908.90 tokens per second)
0.00.812.333 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.333 I llama_perf_context_print:       total time =     150.22 ms /   129 tokens
0.00.812.874 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.079s
sys	0m0.128s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.624 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.379 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.383 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.385 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.386 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.386 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.387 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.387 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.387 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.388 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.388 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.388 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.389 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.391 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.391 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.391 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.235 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.253 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.045 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.046 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.046 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.046 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.047 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.047 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.048 I llama_model_loader: - type  f32:  194 tensors
0.00.025.048 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.400 I llm_load_vocab: special tokens cache size = 25
0.00.051.433 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.435 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.435 I llm_load_print_meta: arch             = gptneox
0.00.051.436 I llm_load_print_meta: vocab type       = BPE
0.00.051.436 I llm_load_print_meta: n_vocab          = 50304
0.00.051.436 I llm_load_print_meta: n_merges         = 50009
0.00.051.436 I llm_load_print_meta: vocab_only       = 0
0.00.051.437 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.437 I llm_load_print_meta: n_embd           = 2048
0.00.051.437 I llm_load_print_meta: n_layer          = 24
0.00.051.439 I llm_load_print_meta: n_head           = 16
0.00.051.440 I llm_load_print_meta: n_head_kv        = 16
0.00.051.452 I llm_load_print_meta: n_rot            = 32
0.00.051.452 I llm_load_print_meta: n_swa            = 0
0.00.051.452 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.452 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.453 I llm_load_print_meta: n_gqa            = 1
0.00.051.454 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.455 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.455 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.455 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.456 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.456 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.456 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.457 I llm_load_print_meta: n_ff             = 8192
0.00.051.457 I llm_load_print_meta: n_expert         = 0
0.00.051.457 I llm_load_print_meta: n_expert_used    = 0
0.00.051.457 I llm_load_print_meta: causal attn      = 1
0.00.051.460 I llm_load_print_meta: pooling type     = 0
0.00.051.461 I llm_load_print_meta: rope type        = 2
0.00.051.461 I llm_load_print_meta: rope scaling     = linear
0.00.051.462 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.462 I llm_load_print_meta: freq_scale_train = 1
0.00.051.465 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.466 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.466 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.466 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.466 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.466 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.466 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.476 I llm_load_print_meta: model type       = 1.4B
0.00.051.476 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.477 I llm_load_print_meta: model params     = 1.41 B
0.00.051.477 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.477 I llm_load_print_meta: general.name     = 1.4B
0.00.051.477 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: LF token         = 128 ''
0.00.051.478 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: max token length = 1024
0.00.053.495 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.495 I llm_load_tensors: offloading output layer to GPU
0.00.053.496 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.506 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.507 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.451 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.452 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.452 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.452 I llama_new_context_with_model: n_batch       = 2048
0.00.054.452 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.453 I llama_new_context_with_model: flash_attn    = 0
0.00.054.453 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.453 I llama_new_context_with_model: freq_scale    = 1
0.00.054.454 I ggml_metal_init: allocating
0.00.054.456 I ggml_metal_init: found device: Apple M4
0.00.054.458 I ggml_metal_init: picking default device: Apple M4
0.00.055.059 I ggml_metal_init: using embedded metal library
0.00.057.337 I ggml_metal_init: GPU name:   Apple M4
0.00.057.338 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.339 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.339 I ggml_metal_init: simdgroup reduction   = true
0.00.057.341 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.341 I ggml_metal_init: has bfloat            = true
0.00.057.341 I ggml_metal_init: use bfloat            = true
0.00.057.342 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.342 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.060 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.065 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.083 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.136 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.138 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.138 I llama_new_context_with_model: graph nodes  = 967
0.00.088.138 I llama_new_context_with_model: graph splits = 2
0.00.088.152 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.043 I main: llama threadpool init, n_threads = 4
0.00.741.079 I 
0.00.741.105 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.107 I 
0.00.741.274 I sampler seed: 1234
0.00.741.279 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.289 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.290 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.290 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.628.346 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62117.24 tokens per second)
0.01.628.347 I llama_perf_context_print:        load time =     731.41 ms
0.01.628.348 I llama_perf_context_print: prompt eval time =      54.41 ms /     7 tokens (    7.77 ms per token,   128.64 tokens per second)
0.01.628.348 I llama_perf_context_print:        eval time =     829.69 ms /    63 runs   (   13.17 ms per token,    75.93 tokens per second)
0.01.628.349 I llama_perf_context_print:       total time =     887.31 ms /    70 tokens
0.01.628.533 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.110s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4281 (1881ffaf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.869 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.463 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.468 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.469 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.469 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.472 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.472 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.472 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.473 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.474 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.474 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.474 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.475 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.475 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.477 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.477 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.478 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.195 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.221 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.027 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.028 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.028 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.029 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.029 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.029 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.030 I llama_model_loader: - type  f32:  194 tensors
0.00.024.030 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.001 I llm_load_vocab: special tokens cache size = 25
0.00.049.917 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.919 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.920 I llm_load_print_meta: arch             = gptneox
0.00.049.920 I llm_load_print_meta: vocab type       = BPE
0.00.049.920 I llm_load_print_meta: n_vocab          = 50304
0.00.049.920 I llm_load_print_meta: n_merges         = 50009
0.00.049.921 I llm_load_print_meta: vocab_only       = 0
0.00.049.921 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.921 I llm_load_print_meta: n_embd           = 2048
0.00.049.921 I llm_load_print_meta: n_layer          = 24
0.00.049.924 I llm_load_print_meta: n_head           = 16
0.00.049.924 I llm_load_print_meta: n_head_kv        = 16
0.00.049.937 I llm_load_print_meta: n_rot            = 32
0.00.049.937 I llm_load_print_meta: n_swa            = 0
0.00.049.937 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.937 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.938 I llm_load_print_meta: n_gqa            = 1
0.00.049.939 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.939 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.940 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.942 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.942 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.942 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.943 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.943 I llm_load_print_meta: n_ff             = 8192
0.00.049.943 I llm_load_print_meta: n_expert         = 0
0.00.049.944 I llm_load_print_meta: n_expert_used    = 0
0.00.049.944 I llm_load_print_meta: causal attn      = 1
0.00.049.944 I llm_load_print_meta: pooling type     = 0
0.00.049.944 I llm_load_print_meta: rope type        = 2
0.00.049.944 I llm_load_print_meta: rope scaling     = linear
0.00.049.944 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.945 I llm_load_print_meta: freq_scale_train = 1
0.00.049.946 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.946 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.946 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.946 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.946 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.947 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.947 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.956 I llm_load_print_meta: model type       = 1.4B
0.00.049.956 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.957 I llm_load_print_meta: model params     = 1.41 B
0.00.049.957 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.957 I llm_load_print_meta: general.name     = 1.4B
0.00.049.957 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.958 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.958 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.958 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.958 I llm_load_print_meta: LF token         = 128 ''
0.00.049.958 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.959 I llm_load_print_meta: max token length = 1024
0.00.051.915 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.915 I llm_load_tensors: offloading output layer to GPU
0.00.051.915 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.926 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.927 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.805 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.806 I llama_new_context_with_model: n_ctx         = 128
0.00.052.806 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.806 I llama_new_context_with_model: n_batch       = 128
0.00.052.806 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.806 I llama_new_context_with_model: flash_attn    = 0
0.00.052.807 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.807 I llama_new_context_with_model: freq_scale    = 1
0.00.052.807 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.808 I ggml_metal_init: allocating
0.00.052.811 I ggml_metal_init: found device: Apple M4
0.00.052.813 I ggml_metal_init: picking default device: Apple M4
0.00.053.364 I ggml_metal_init: using embedded metal library
0.00.055.647 I ggml_metal_init: GPU name:   Apple M4
0.00.055.648 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.648 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.649 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.649 I ggml_metal_init: simdgroup reduction   = true
0.00.055.649 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.649 I ggml_metal_init: has bfloat            = true
0.00.055.649 I ggml_metal_init: use bfloat            = true
0.00.055.650 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.650 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.496 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.498 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.512 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.421 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.422 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.422 I llama_new_context_with_model: graph nodes  = 967
0.00.067.422 I llama_new_context_with_model: graph splits = 2
0.00.067.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.183.133 I 
0.00.183.178 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.183.190 I perplexity: tokenizing the input ..
0.00.190.550 I perplexity: tokenization took 7.359 ms
0.00.190.560 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.330.725 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.331.951 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.331.966 I llama_perf_context_print:        load time =     173.26 ms
0.00.331.967 I llama_perf_context_print: prompt eval time =     139.94 ms /   128 tokens (    1.09 ms per token,   914.68 tokens per second)
0.00.331.968 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.331.969 I llama_perf_context_print:       total time =     148.83 ms /   129 tokens
0.00.332.330 I ggml_metal_free: deallocating

real	0m0.346s
user	0m0.076s
sys	0m0.046s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4281 (1881ffaf)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10760a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10760a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10760af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10760b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10760ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10760c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10760c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10760cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10760d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10760d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10760db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10760e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10760eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10760f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10760fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107610250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107610970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107611090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1076117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107611f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1076126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107612dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1076134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107613d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1076144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107614760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107614d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1076159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107615f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1076161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107616680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107616940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1076171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107617710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1076179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107617e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107618310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1076187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107618c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1076190f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107619590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107619a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107619ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10761a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10761a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10761ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10761b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10761bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10761c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10761c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10761cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10761d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10761d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10761dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10761e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10761ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10761f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10761f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10761f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1076201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107620480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107620920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107620dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107621260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107621700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107621ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107622040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1076224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107622980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107622e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1076232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107623760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107623c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107624150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1076246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107624bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107625140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107625690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107625be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107626130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107626680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107626bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107627120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107627670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107627bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107628110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107628660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107628bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107629100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107629650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107629ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10762a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10762a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10762ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10762b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10762b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10762bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10761b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10762bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10762c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10762ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10762d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10762d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10762dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10762e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10762e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10762ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10762f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10762f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10762fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107630210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107630760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107630cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107631150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1076315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107631a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107631f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1076323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107632870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107632d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1076331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107633650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107633af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107633f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107634430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1076348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107634d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107635210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1076356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107635b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107635ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107636490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107636930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107636dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107637270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107637710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107637bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107638050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1076384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107638990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107638e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1076392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107639770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107639c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10763a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10763a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10763a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10763ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10763b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10763b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10763bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10763c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10763c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10763ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10763cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10763d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10763d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10763dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10763e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10763e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10763eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10763ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10763f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10763f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10763fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1076401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107640670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107640b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107640fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107641450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1076418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107641d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107642230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1076426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107642b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107643010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1076434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107643950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107643df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107644290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107644730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107644bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107645070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107645510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1076459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107645e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1076462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107646790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107646c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1076470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107647570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107647a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107647eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107648400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107648950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107648ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1076493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1076496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107649cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10764a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10764a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10764b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10764b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10764b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10764be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10764c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10764cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10764d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10764d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10764da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10764e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10764e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10764ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10764f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10764f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10764fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1076501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107650700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107650c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1076511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1076516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107651c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107652190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1076526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107652c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107653180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1076536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107653c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107654170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1076546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107654c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107655160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1076556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107655c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107656150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1076566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107656bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107657140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107657690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107657be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107658130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107658680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107658bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107659120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107659670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107659bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10765a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10765a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10765abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10765b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10765b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10765bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10765c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10765c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10765cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10765d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10765d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10765db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10765e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10765e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10765eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10765f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10765f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10765fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1076600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107660600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107660b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107660ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107661490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107661930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107661dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107662270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107662710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107662bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107663050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1076634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107663990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107663e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1076642d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107664770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107664c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1076650b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107665600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107665d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107666440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107666b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107667280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107667540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107667d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107667ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107668600 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.147.484 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1076251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107625630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107625aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107625f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107626380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1076267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107626c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1076270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107627540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1076279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107627e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107628400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107628cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107629470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107629c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10762a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10762aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10762b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10762b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10762c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10762c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10762cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10762d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10762dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10762e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10762e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10762ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10762f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10762f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10762fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10762fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107630350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1076307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107630a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107630ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107631360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1076317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107631c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1076320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107632520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107632990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107632e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1076336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107633b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107633fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107634430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1076348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107635180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1076355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107635a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107635ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107636340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1076367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107636c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107637090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107637500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107637970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107637de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107638250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1076386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107638b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107638fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107639410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107639880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107639cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10763a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10763a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10763aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10763aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10763b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10763b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10763bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10763c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10763c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10763c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10763cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10763d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10763d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10763db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10763df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10763e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10763e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10763ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10763f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10763f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10763fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10763fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107640300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107640770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107640be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107641050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1076414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107641930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107641da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107642210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107642680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107642af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107642f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1076433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107643840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107643cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107644120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107644590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107644a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107644e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1076452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107645750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107645bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107646030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1076464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107646910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107646d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1076471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107647660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107647f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1076483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107648820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107648c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107649100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107649570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1076499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107649e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10764a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10764a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10764aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10764b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10764b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10764b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10764bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10764c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10764c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10764cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10764cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10764d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10764d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10764dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10764e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10764e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10764e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10764ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10764f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10764f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10764fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10764fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107650460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1076508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107650d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1076511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107651620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107651a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107651f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107652370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1076527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107652c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1076530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107653530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1076539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107653e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107654280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1076546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107654b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107654fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107655440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1076558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107655d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107656190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107656600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107656a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107656ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107657350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1076577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107657c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1076580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107658510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107658980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107658df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107659260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1076596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107659b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107659fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10765a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10765a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10765ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10765b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10765b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10765ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10765bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10765c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10765c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10765cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10765d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10765d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10765d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10765ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10765e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10765e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10765eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10765ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10765f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10765f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10765fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107660150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1076605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107660a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107660ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107661310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107661780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107661f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107662370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1076627e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107662c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1076630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107663530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1076639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107663e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107664280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1076646f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107664b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107664fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107665440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1076658b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107665d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107666190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107666600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107666a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107666ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107667350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1076677c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107667c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1076680a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107668510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10760b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10760ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1076098e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1076178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107617d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107618190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107618600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107618a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107618ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107619350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1076197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107619c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10761a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10761a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10761a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10761adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10761b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10761b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10761bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10761bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10761c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10761c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10761cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10761d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10761d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10761da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10761dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10761e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10761e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10761ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10761f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10761f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10761f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10761fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107620240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1076206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107620b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107620f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107621400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107621870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107621ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107622150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1076225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107622a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107622ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107623310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107623780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107623bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107624060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107615f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107616450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107616b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10760b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10760bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10760c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10760c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10760c8f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10760ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107617700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107617b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107617fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107618450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1076188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107618d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1076191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107619610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107619a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107619ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10761a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10761adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10761b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10761bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10761c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10761cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10761d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10761d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10761e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10761e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10761f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10761f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10761fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107620510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107620980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107620df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107621260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1076216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107621b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107621fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107622420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107622890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107622b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107622fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107623430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1076238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107623d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107624180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1076245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1076160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107616530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1076169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107616e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107624d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107625020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107625490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107625900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107625d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1076261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107626650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107626ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107626f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1076273a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107627810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107627c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1076280f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107628560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1076289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107628e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1076292b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107629720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107629b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10762a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10762a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10762a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10762ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10762b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10762b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10762baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10762bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10762c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10762c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10762cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10762d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10762d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10762d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10762de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10762e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10762e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10762eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10762efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10762f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10762f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10762fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1076301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107630610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107630a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107630ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107631360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1076317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107631c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1076320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107632520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107632990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107632e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1076336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107633b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107633fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107634430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1076348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107635180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1076355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107635a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107635ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107636340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1076367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107636c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107637090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107637500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107637970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107637de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107638250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1076386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107638b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107638fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107639410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107639880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107639cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10763a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10763a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10763aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10763aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10763b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10763b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10763bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10763c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10763c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10763c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10763cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10763d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10763d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10763db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10763df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10763e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10763e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10763ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10763f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10763f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10763fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10763fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107640300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107640770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107640be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107641050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1076414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107641930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107641da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107642210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107642680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107642af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107642f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1076433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107643840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107643cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107644120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107644590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107644a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107644e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1076452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107645750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107645bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107646030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1076464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107646910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107646d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1076471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107647660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107647f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1076483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107648820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107648c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107649100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107649570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1076499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107649e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10764a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10764a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10764aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10764b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10764b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10764b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10764bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10764c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10764c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10764cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10764cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10764d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10764d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10764dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10764e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10764e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10764e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10764ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10764f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10764f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10764fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10764fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107650460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1076508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107650d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1076511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107651620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107651a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107651f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107652370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1076527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107652f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1076533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107653840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107653cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107654120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107654590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107654a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107654e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1076552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107655750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107655bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107656030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1076564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107656910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107656d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1076571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107657660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107657ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107657f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1076583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107658820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107658c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107659100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107659570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1076599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107659e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10765a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10765a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10765aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10765b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10765b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10765b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10765bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10765c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10765c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10765cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10765cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10765d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10765d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10765dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10765e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10765e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10765e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10765ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10765f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10765f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10765fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10765fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107660460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1076608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107660d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1076611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107661620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107661a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107661f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107662370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1076627e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107662c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1076630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107663530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1076639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107663e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107664280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1076646f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107664b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107664fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107665440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1076658b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107665d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107666190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107666600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107666a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107666ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107667740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107667e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107668520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10760b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10760bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10760c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10760c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10760c8f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.942s
user	0m0.311s
sys	0m0.270s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4281 (1881ffaf)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15a00a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15a00a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15a00ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15a00b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15a00b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15a00bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15a00c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15a00caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15a00d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15a00d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15a00da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15a00df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15a00ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15a00f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15a00fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15a010150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15a010870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15a010f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15a0116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15a011e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15a0125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15a012cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15a0133e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15a013c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15a0143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15a014660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15a014c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15a0158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15a015e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15a0160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15a016580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15a016840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15a0170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15a017610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15a0178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15a017d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15a018210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15a0186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15a018b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15a018ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15a019490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15a019930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15a019dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15a01a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15a01a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15a01ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15a01b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15a01ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15a01c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15a01c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15a01cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15a01d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15a01d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15a01ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15a01e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15a01eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15a01f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15a01f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15a01f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15a0200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15a020380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15a020820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15a020cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15a021160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15a021600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15a021aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15a021f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15a0223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15a022880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15a022d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15a0231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15a023660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15a023b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15a024050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15a0245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15a024af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15a025040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15a025590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15a025ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15a026030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15a026580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15a026ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15a027020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15a027570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15a027ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15a028010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15a028560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15a028ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15a029000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15a029550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15a029aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15a029ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15a02a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15a02aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15a02afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15a02b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15a02ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15a01b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15a02bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15a02c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15a02cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15a02d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15a02d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15a02dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15a02e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15a02e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15a02ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15a02f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15a02f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15a02fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15a030110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15a030660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15a030bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15a031050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15a0314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15a031990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15a031e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15a0322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15a032770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15a032c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15a0330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15a033550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15a0339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15a033e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15a034330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15a0347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15a034c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15a035110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15a0355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15a035a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15a035ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15a036390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15a036830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15a036cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15a037170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15a037610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15a037ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15a037f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15a0383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15a038890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15a038d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15a0391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15a039670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15a039b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15a039fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15a03a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15a03a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15a03ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15a03b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15a03b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15a03bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15a03c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15a03c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15a03c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15a03cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15a03d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15a03d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15a03dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15a03e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15a03e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15a03e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15a03ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15a03f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15a03f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15a03fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15a0400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15a040570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15a040a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15a040eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15a041350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15a0417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15a041c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15a042130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15a0425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15a042a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15a042f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15a0433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15a043850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15a043cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15a044190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15a044630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15a044ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15a044f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15a045410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15a0458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15a045d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15a0461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15a046690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15a046b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15a046fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15a047470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15a047910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15a047db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15a048300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15a048850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15a048da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15a0492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15a0495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15a049bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15a04a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15a04a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15a04afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15a04b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15a04b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15a04bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15a04c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15a04cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15a04cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15a04d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15a04d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15a04e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15a04e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15a04eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15a04f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15a04f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15a04fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15a0500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15a050600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15a050b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15a0510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15a0515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15a051b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15a052090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15a0525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15a052b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15a053080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15a0535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15a053b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15a054070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15a0545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15a054b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15a055060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15a0555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15a055b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15a056050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15a0565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15a056af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15a057040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15a057590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15a057ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15a058030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15a058580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15a058ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15a059020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15a059570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15a059ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15a05a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15a05a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15a05aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15a05b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15a05b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15a05baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15a05bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15a05c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15a05ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15a05cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15a05d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15a05da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15a05dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15a05e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15a05ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15a05efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15a05f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15a05fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15a05ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15a060500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15a060a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15a060ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15a061390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15a061830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15a061cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15a062170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15a062610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15a062ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15a062f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15a0633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15a063890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15a063d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15a0641d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15a064670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15a064b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15a064fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15a065500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15a065c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15a066340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15a066a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15a067180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15a067440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15a067c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15a067ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15a068500 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.867 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158707a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158707ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158708360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1587087d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158708c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1587090b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158709520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158709990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158709e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15870a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15870a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15870ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15870b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15870c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15870c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15870d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15870d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15870de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15870e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15870ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15870f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15870fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158710290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1587109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1587110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158711390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158711650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158711ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158711f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1587123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158712810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158712d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1587131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158713470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1587138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158713d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1587141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158714630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158714aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158714f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158715380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1587157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158715c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1587160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158716540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1587169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158716e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158717290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158717700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158717b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158717fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158718450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1587188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158718d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1587191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158719610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158719b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15871a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15871a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15871a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15871add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15871b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15871b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15871bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15871bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15871c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15871c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15871cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15871d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15871d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15871da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15871dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15871e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15871e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15871ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15871f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15871f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15871f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15871fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158720220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158720690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158720b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158720f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1587213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158721850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158721cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158722130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1587225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158722a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158722e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1587232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158723760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158723bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158724040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1587244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158724920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158724d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158725200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158725670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158725ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158725f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1587263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158726830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158726ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158727110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158727580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1587279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158727e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1587282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158728740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158728bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158729020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158729490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158729900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158729d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15872a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15872a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15872aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15872af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15872b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15872b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15872bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15872c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15872c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15872c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15872ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15872d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15872d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15872db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15872e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15872e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15872e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15872ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15872f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15872f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15872faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15872ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158730380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1587307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158730c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1587310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158731540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1587319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158731e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158732290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158732700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158732b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158732fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158733450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1587338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158733d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1587341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158734610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158734a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158734ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158735360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1587357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158735c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1587360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158736520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158736990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158736e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158737270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1587376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158737b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158737fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158738430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1587388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158738d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158739180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1587395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158739a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158739ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15873a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15873a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15873ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15873b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15873b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15873b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15873bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15873c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15873c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15873cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15873cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15873d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15873d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15873dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15873e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15873e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15873ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15873eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15873f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15873f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15873fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158740070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1587404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158740950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158740dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158741230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1587416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158741b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158741f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1587423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158742860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158742cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158743140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1587435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158743b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158743fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158744420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158744f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158745230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1587454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158745960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158745dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158746240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1587466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158746b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158746f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158747400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158747870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158747ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158748150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1587485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158748a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158748ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158749310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158749780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158749bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15874a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15874a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15874a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15874adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15874b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15874b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15874bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15874bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15874c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15874c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15874ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15874d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15874d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15874da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15874de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15874e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15874e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15874ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15874f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15874f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15874f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15874fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158750200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158750670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158750ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158750f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1587513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158751830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158751ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158752110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158752580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1587529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158752e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1587532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158753740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158753bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158754020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158754490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158754900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158754d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1587551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158755650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158755ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158755f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1587563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158756810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158756c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1587570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158757560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1587579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158757e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1587582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158758720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158758b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158759600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158759d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15875a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15875ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15875ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15875b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15875b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15875bea0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1586088d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158608d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1586091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158609620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158609a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158609f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15860a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15860a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15860ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15860b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15860b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15860bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15860c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15860cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15860d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15860de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15860e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15860eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15860f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15860fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1586102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1586109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1586110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158611810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158611f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1586121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1586124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158612920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158612d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158613200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158613700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158613c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158614080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158614340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1586147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158614c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158615180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158615680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158615b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158616080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158616580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158616a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158616f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158617480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158617980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158617df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158618260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1586186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158618b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158618fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158619420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158619890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158619d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15861a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15861a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15861adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15861b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15861b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15861bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15861c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15861c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15861cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15861d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15861d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15861da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15861ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15861e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15861e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15861ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15861f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15861f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15861fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15861ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158620480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1586209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158620f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158621470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1586219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158621f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158622460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1586229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158622f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158623450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1586239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158623ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158624440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158624990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158624ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158625430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158625980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158626420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158626970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158626ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158627410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158627960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158627eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158628400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158628950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158628ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1586293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158629940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158629e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15862a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15862a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15862ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15862b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15862b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15862be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15862c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15862c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15862ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15862d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15862d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15862dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15862e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15862e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15862ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15862ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15862f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15862f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15862fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1586301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158630690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158630b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158630fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158631470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158631910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158631db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158632250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1586326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158632b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158633030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1586334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158633970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158633e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1586342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158634750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158634bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158635090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158635530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1586359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158635e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158636310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1586367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158636c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1586370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158637590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158637a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158637ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158638370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158638810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158638cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158639150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1586395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158639a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158639f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15863a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15863a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15863ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15863b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15863b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15863baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15863bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15863c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15863c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15863cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15863d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15863d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15863db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15863dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15863e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15863e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15863edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15863f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15863f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15863fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158640050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1586404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158640990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158640e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1586412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158641770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158641c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1586420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158642550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1586429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158642e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158643330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1586437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158643c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158644110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1586445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158644b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158645050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1586455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158645af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158645db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1586463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1586469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158646fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1586477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158647c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158647f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158648540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158648b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158649340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1586497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158649c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15864a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15864a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15864ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15864b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15864b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15864be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15864c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15864c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15864ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15864d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15864d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15864ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15864e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15864e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15864ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15864f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15864f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15864fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158650320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158650870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158650dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158651310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158651860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158651db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158652300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158652850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158652da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1586532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158653840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158653d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1586542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158654830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158654d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1586552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158655820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158655d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1586562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158656810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158656d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1586572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158657800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158657d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1586582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1586587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158658d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158659290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1586597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158659d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15865a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15865a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15865ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15865b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15865b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15865bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15865c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15865c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15865cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15865d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15865d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15865db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15865e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15865e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15865e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15865ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15865f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15865f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15865fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158660090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158660530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1586609d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158660e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158661310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1586617b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158661d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158662420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158662b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158663260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158663980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158663c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158664430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1586646f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158664d00 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.942s
user	0m0.242s
sys	0m0.146s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.53 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.50 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.76 sec*proc (2 tests)

Total Test time (real) =   0.77 sec
        0.78 real         0.16 user         0.05 sys
```
