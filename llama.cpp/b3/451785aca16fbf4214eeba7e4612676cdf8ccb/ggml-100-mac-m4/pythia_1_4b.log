Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.612s
user	0m0.917s
sys	0m1.265s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Built target sha256
[  4%] Built target build_info
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-simple
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-sampling
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-chat
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Built target test-log
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 56%] Linking CXX executable ../bin/test-autorelease
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-autorelease
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-barrier
[ 62%] Built target test-arg-parser
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Built target test-quantize-perf
[ 69%] Built target test-quantize-fns
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target test-rope
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-batched-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 73%] Built target llama-bench
[ 74%] Linking CXX executable ../../bin/llama-lookup-merge
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Built target llama-imatrix
[ 78%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Built target llama-lookahead
[ 79%] Built target llama-lookup-stats
[ 79%] Built target llama-lookup-merge
[ 79%] Built target llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Generating loading.html.hpp
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-cli
[ 82%] Built target llama-lookup
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-parallel
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Built target llama-perplexity
[ 88%] Built target llama-passkey
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 91%] Built target llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-run
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Built target llama-tts
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Built target llama-gen-docs
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-cvector-generator
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.185s
user	0m6.451s
sys	0m9.862s

main: quantize time =  5295.79 ms
main:    total time =  5295.80 ms

main: quantize time =  2909.61 ms
main:    total time =  2909.61 ms

main: quantize time =  6050.09 ms
main:    total time =  6050.09 ms

main: quantize time =  1699.02 ms
main:    total time =  1699.02 ms

main: quantize time =  2515.75 ms
main:    total time =  2515.75 ms

main: quantize time =  5025.24 ms
main:    total time =  5025.24 ms

main: quantize time =  5739.33 ms
main:    total time =  5739.33 ms

main: quantize time =  6926.02 ms
main:    total time =  6926.02 ms

main: quantize time =  6097.77 ms
main:    total time =  6097.77 ms

main: quantize time =  5100.97 ms
main:    total time =  5100.97 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.145 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.302 I main: llama backend init
0.00.000.308 I main: load the model and apply lora adapter, if any
0.00.109.553 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.121.881 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.121.893 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.121.897 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.121.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.121.899 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.121.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.121.900 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.121.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.121.903 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.121.904 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.121.905 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.121.906 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.121.906 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.121.907 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.121.912 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.121.921 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.121.921 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.128.730 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.130.870 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.137.604 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.137.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.137.609 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.137.610 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.137.611 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.137.612 I llama_model_loader: - type  f32:  194 tensors
0.00.137.612 I llama_model_loader: - type  f16:   98 tensors
0.00.137.614 I print_info: file format = GGUF V3 (latest)
0.00.137.615 I print_info: file type   = all F32 (guessed)
0.00.137.618 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.155.969 I load: special tokens cache size = 25
0.00.165.521 I load: token to piece cache size = 0.2984 MB
0.00.165.527 I print_info: arch             = gptneox
0.00.165.527 I print_info: vocab_only       = 0
0.00.165.528 I print_info: n_ctx_train      = 2048
0.00.165.528 I print_info: n_embd           = 2048
0.00.165.528 I print_info: n_layer          = 24
0.00.165.534 I print_info: n_head           = 16
0.00.165.535 I print_info: n_head_kv        = 16
0.00.165.535 I print_info: n_rot            = 32
0.00.165.535 I print_info: n_swa            = 0
0.00.165.536 I print_info: n_embd_head_k    = 128
0.00.165.536 I print_info: n_embd_head_v    = 128
0.00.165.537 I print_info: n_gqa            = 1
0.00.165.538 I print_info: n_embd_k_gqa     = 2048
0.00.165.538 I print_info: n_embd_v_gqa     = 2048
0.00.165.539 I print_info: f_norm_eps       = 1.0e-05
0.00.165.540 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.165.544 I print_info: f_clamp_kqv      = 0.0e+00
0.00.165.544 I print_info: f_max_alibi_bias = 0.0e+00
0.00.165.544 I print_info: f_logit_scale    = 0.0e+00
0.00.165.545 I print_info: n_ff             = 8192
0.00.165.545 I print_info: n_expert         = 0
0.00.165.547 I print_info: n_expert_used    = 0
0.00.165.547 I print_info: causal attn      = 1
0.00.165.548 I print_info: pooling type     = 0
0.00.165.548 I print_info: rope type        = 2
0.00.165.548 I print_info: rope scaling     = linear
0.00.165.549 I print_info: freq_base_train  = 10000.0
0.00.165.549 I print_info: freq_scale_train = 1
0.00.165.549 I print_info: n_ctx_orig_yarn  = 2048
0.00.165.549 I print_info: rope_finetuned   = unknown
0.00.165.549 I print_info: ssm_d_conv       = 0
0.00.165.550 I print_info: ssm_d_inner      = 0
0.00.165.550 I print_info: ssm_d_state      = 0
0.00.165.550 I print_info: ssm_dt_rank      = 0
0.00.165.550 I print_info: ssm_dt_b_c_rms   = 0
0.00.165.550 I print_info: model type       = 1.4B
0.00.165.551 I print_info: model params     = 1.41 B
0.00.165.556 I print_info: general.name     = 1.4B
0.00.165.556 I print_info: vocab type       = BPE
0.00.165.557 I print_info: n_vocab          = 50304
0.00.165.557 I print_info: n_merges         = 50009
0.00.165.557 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.165.557 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.165.558 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.165.564 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.165.567 I print_info: LF token         = 187 'Ċ'
0.00.165.568 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.165.568 I print_info: max token length = 1024
0.00.216.899 I load_tensors: offloading 24 repeating layers to GPU
0.00.216.903 I load_tensors: offloading output layer to GPU
0.00.216.903 I load_tensors: offloaded 25/25 layers to GPU
0.00.216.931 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.216.932 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.217.519 I llama_init_from_model: n_seq_max     = 1
0.00.217.521 I llama_init_from_model: n_ctx         = 2048
0.00.217.521 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.217.522 I llama_init_from_model: n_batch       = 2048
0.00.217.523 I llama_init_from_model: n_ubatch      = 512
0.00.217.523 I llama_init_from_model: flash_attn    = 0
0.00.217.523 I llama_init_from_model: freq_base     = 10000.0
0.00.217.524 I llama_init_from_model: freq_scale    = 1
0.00.217.525 I ggml_metal_init: allocating
0.00.217.565 I ggml_metal_init: found device: Apple M4
0.00.217.570 I ggml_metal_init: picking default device: Apple M4
0.00.218.310 I ggml_metal_init: using embedded metal library
0.00.228.229 I ggml_metal_init: GPU name:   Apple M4
0.00.228.231 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.228.232 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.228.232 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.228.232 I ggml_metal_init: simdgroup reduction   = true
0.00.228.233 I ggml_metal_init: simdgroup matrix mul. = true
0.00.228.233 I ggml_metal_init: has residency sets    = true
0.00.228.233 I ggml_metal_init: has bfloat            = true
0.00.228.233 I ggml_metal_init: use bfloat            = true
0.00.228.233 I ggml_metal_init: hasUnifiedMemory      = true
0.00.228.234 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.259.337 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.289.266 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.289.272 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.289.316 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.293.176 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.293.178 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.293.178 I llama_init_from_model: graph nodes  = 967
0.00.293.179 I llama_init_from_model: graph splits = 2
0.00.293.187 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.293.317 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.293.318 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.193 I main: llama threadpool init, n_threads = 4
0.00.360.233 I 
0.00.360.265 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.266 I 
0.00.360.439 I sampler seed: 1234
0.00.360.444 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.360.489 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.360.492 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.360.492 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.188.959 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.02.188.960 I llama_perf_context_print:        load time =     249.60 ms
0.02.188.960 I llama_perf_context_print: prompt eval time =      43.97 ms /     7 tokens (    6.28 ms per token,   159.20 tokens per second)
0.02.188.962 I llama_perf_context_print:        eval time =    1781.59 ms /    63 runs   (   28.28 ms per token,    35.36 tokens per second)
0.02.188.962 I llama_perf_context_print:       total time =    1829.80 ms /    70 tokens
0.02.189.248 I ggml_metal_free: deallocating

real	0m2.541s
user	0m0.134s
sys	0m0.149s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.866 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.036 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.043 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.044 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.044 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.045 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.045 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.046 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.046 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.047 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.047 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.048 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.050 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.052 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.053 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.935 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.835 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.837 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.837 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.837 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.838 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.838 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.839 I llama_model_loader: - type  f32:  194 tensors
0.00.033.839 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.840 I print_info: file format = GGUF V3 (latest)
0.00.033.840 I print_info: file type   = Q8_0
0.00.033.842 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.122 I load: special tokens cache size = 25
0.00.048.358 I load: token to piece cache size = 0.2984 MB
0.00.048.363 I print_info: arch             = gptneox
0.00.048.363 I print_info: vocab_only       = 0
0.00.048.365 I print_info: n_ctx_train      = 2048
0.00.048.365 I print_info: n_embd           = 2048
0.00.048.366 I print_info: n_layer          = 24
0.00.048.372 I print_info: n_head           = 16
0.00.048.372 I print_info: n_head_kv        = 16
0.00.048.373 I print_info: n_rot            = 32
0.00.048.373 I print_info: n_swa            = 0
0.00.048.373 I print_info: n_embd_head_k    = 128
0.00.048.373 I print_info: n_embd_head_v    = 128
0.00.048.373 I print_info: n_gqa            = 1
0.00.048.374 I print_info: n_embd_k_gqa     = 2048
0.00.048.375 I print_info: n_embd_v_gqa     = 2048
0.00.048.375 I print_info: f_norm_eps       = 1.0e-05
0.00.048.376 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.376 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.376 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.377 I print_info: f_logit_scale    = 0.0e+00
0.00.048.377 I print_info: n_ff             = 8192
0.00.048.378 I print_info: n_expert         = 0
0.00.048.378 I print_info: n_expert_used    = 0
0.00.048.378 I print_info: causal attn      = 1
0.00.048.378 I print_info: pooling type     = 0
0.00.048.378 I print_info: rope type        = 2
0.00.048.379 I print_info: rope scaling     = linear
0.00.048.379 I print_info: freq_base_train  = 10000.0
0.00.048.379 I print_info: freq_scale_train = 1
0.00.048.380 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.380 I print_info: rope_finetuned   = unknown
0.00.048.380 I print_info: ssm_d_conv       = 0
0.00.048.380 I print_info: ssm_d_inner      = 0
0.00.048.380 I print_info: ssm_d_state      = 0
0.00.048.380 I print_info: ssm_dt_rank      = 0
0.00.048.381 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.381 I print_info: model type       = 1.4B
0.00.048.381 I print_info: model params     = 1.41 B
0.00.048.381 I print_info: general.name     = 1.4B
0.00.048.382 I print_info: vocab type       = BPE
0.00.048.382 I print_info: n_vocab          = 50304
0.00.048.383 I print_info: n_merges         = 50009
0.00.048.383 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.383 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.383 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.383 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.387 I print_info: LF token         = 187 'Ċ'
0.00.048.387 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.387 I print_info: max token length = 1024
0.01.198.890 I load_tensors: offloading 24 repeating layers to GPU
0.01.198.896 I load_tensors: offloading output layer to GPU
0.01.198.898 I load_tensors: offloaded 25/25 layers to GPU
0.01.198.921 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.198.922 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.199.674 I llama_init_from_model: n_seq_max     = 1
0.01.199.677 I llama_init_from_model: n_ctx         = 2048
0.01.199.677 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.199.678 I llama_init_from_model: n_batch       = 2048
0.01.199.678 I llama_init_from_model: n_ubatch      = 512
0.01.199.678 I llama_init_from_model: flash_attn    = 0
0.01.199.679 I llama_init_from_model: freq_base     = 10000.0
0.01.199.679 I llama_init_from_model: freq_scale    = 1
0.01.199.680 I ggml_metal_init: allocating
0.01.199.696 I ggml_metal_init: found device: Apple M4
0.01.199.705 I ggml_metal_init: picking default device: Apple M4
0.01.201.087 I ggml_metal_init: using embedded metal library
0.01.206.526 I ggml_metal_init: GPU name:   Apple M4
0.01.206.529 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.206.530 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.206.530 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.206.531 I ggml_metal_init: simdgroup reduction   = true
0.01.206.531 I ggml_metal_init: simdgroup matrix mul. = true
0.01.206.531 I ggml_metal_init: has residency sets    = true
0.01.206.531 I ggml_metal_init: has bfloat            = true
0.01.206.532 I ggml_metal_init: use bfloat            = true
0.01.206.532 I ggml_metal_init: hasUnifiedMemory      = true
0.01.206.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.223.521 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.276.210 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.276.218 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.276.253 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.280.544 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.280.547 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.280.547 I llama_init_from_model: graph nodes  = 967
0.01.280.547 I llama_init_from_model: graph splits = 2
0.01.280.554 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.280.678 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.280.678 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.335.633 I main: llama threadpool init, n_threads = 4
0.01.335.682 I 
0.01.335.710 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.335.710 I 
0.01.335.860 I sampler seed: 1234
0.01.335.864 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.335.886 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.335.887 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.335.887 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.429.049 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56126.48 tokens per second)
0.02.429.049 I llama_perf_context_print:        load time =    1324.84 ms
0.02.429.050 I llama_perf_context_print: prompt eval time =      48.91 ms /     7 tokens (    6.99 ms per token,   143.12 tokens per second)
0.02.429.051 I llama_perf_context_print:        eval time =    1041.43 ms /    63 runs   (   16.53 ms per token,    60.49 tokens per second)
0.02.429.051 I llama_perf_context_print:       total time =    1094.34 ms /    70 tokens
0.02.429.331 I ggml_metal_free: deallocating

real	0m2.450s
user	0m0.108s
sys	0m0.268s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.018.759 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.453 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.459 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.462 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.462 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.463 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.463 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.464 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.465 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.468 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.468 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.469 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.471 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.472 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.473 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.909 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.017 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.364 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.364 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.364 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.365 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.365 I llama_model_loader: - type  f32:  194 tensors
0.00.039.366 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.366 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.366 I print_info: file format = GGUF V3 (latest)
0.00.039.367 I print_info: file type   = Q4_0
0.00.039.368 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.049.787 I load: special tokens cache size = 25
0.00.057.957 I load: token to piece cache size = 0.2984 MB
0.00.057.961 I print_info: arch             = gptneox
0.00.057.961 I print_info: vocab_only       = 0
0.00.057.961 I print_info: n_ctx_train      = 2048
0.00.057.961 I print_info: n_embd           = 2048
0.00.057.961 I print_info: n_layer          = 24
0.00.057.966 I print_info: n_head           = 16
0.00.057.967 I print_info: n_head_kv        = 16
0.00.057.967 I print_info: n_rot            = 32
0.00.057.967 I print_info: n_swa            = 0
0.00.057.968 I print_info: n_embd_head_k    = 128
0.00.057.968 I print_info: n_embd_head_v    = 128
0.00.057.969 I print_info: n_gqa            = 1
0.00.057.970 I print_info: n_embd_k_gqa     = 2048
0.00.057.971 I print_info: n_embd_v_gqa     = 2048
0.00.057.971 I print_info: f_norm_eps       = 1.0e-05
0.00.057.972 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.972 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.972 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.972 I print_info: f_logit_scale    = 0.0e+00
0.00.057.973 I print_info: n_ff             = 8192
0.00.057.973 I print_info: n_expert         = 0
0.00.057.974 I print_info: n_expert_used    = 0
0.00.057.974 I print_info: causal attn      = 1
0.00.057.974 I print_info: pooling type     = 0
0.00.057.974 I print_info: rope type        = 2
0.00.057.975 I print_info: rope scaling     = linear
0.00.057.975 I print_info: freq_base_train  = 10000.0
0.00.057.975 I print_info: freq_scale_train = 1
0.00.057.976 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.976 I print_info: rope_finetuned   = unknown
0.00.057.976 I print_info: ssm_d_conv       = 0
0.00.057.976 I print_info: ssm_d_inner      = 0
0.00.057.976 I print_info: ssm_d_state      = 0
0.00.057.977 I print_info: ssm_dt_rank      = 0
0.00.057.977 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.977 I print_info: model type       = 1.4B
0.00.057.979 I print_info: model params     = 1.41 B
0.00.057.979 I print_info: general.name     = 1.4B
0.00.057.980 I print_info: vocab type       = BPE
0.00.057.980 I print_info: n_vocab          = 50304
0.00.057.980 I print_info: n_merges         = 50009
0.00.057.981 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.981 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.981 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.981 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.982 I print_info: LF token         = 187 'Ċ'
0.00.057.982 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.986 I print_info: max token length = 1024
0.00.652.264 I load_tensors: offloading 24 repeating layers to GPU
0.00.652.280 I load_tensors: offloading output layer to GPU
0.00.652.281 I load_tensors: offloaded 25/25 layers to GPU
0.00.652.315 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.652.316 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.653.589 I llama_init_from_model: n_seq_max     = 1
0.00.653.593 I llama_init_from_model: n_ctx         = 2048
0.00.653.594 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.653.594 I llama_init_from_model: n_batch       = 2048
0.00.653.595 I llama_init_from_model: n_ubatch      = 512
0.00.653.595 I llama_init_from_model: flash_attn    = 0
0.00.653.598 I llama_init_from_model: freq_base     = 10000.0
0.00.653.598 I llama_init_from_model: freq_scale    = 1
0.00.653.601 I ggml_metal_init: allocating
0.00.653.679 I ggml_metal_init: found device: Apple M4
0.00.653.694 I ggml_metal_init: picking default device: Apple M4
0.00.655.530 I ggml_metal_init: using embedded metal library
0.00.662.368 I ggml_metal_init: GPU name:   Apple M4
0.00.662.373 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.662.374 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.662.375 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.662.375 I ggml_metal_init: simdgroup reduction   = true
0.00.662.375 I ggml_metal_init: simdgroup matrix mul. = true
0.00.662.376 I ggml_metal_init: has residency sets    = true
0.00.662.376 I ggml_metal_init: has bfloat            = true
0.00.662.376 I ggml_metal_init: use bfloat            = true
0.00.662.377 I ggml_metal_init: hasUnifiedMemory      = true
0.00.662.379 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.971 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.984 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.736.991 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.028 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.146 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.741.148 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.741.148 I llama_init_from_model: graph nodes  = 967
0.00.741.148 I llama_init_from_model: graph splits = 2
0.00.741.153 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.741.288 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.289 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.132 I main: llama threadpool init, n_threads = 4
0.00.795.175 I 
0.00.795.199 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.201 I 
0.00.795.353 I sampler seed: 1234
0.00.795.357 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.369 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.374 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.374 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.483.316 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.01.483.316 I llama_perf_context_print:        load time =     775.45 ms
0.01.483.317 I llama_perf_context_print: prompt eval time =      48.95 ms /     7 tokens (    6.99 ms per token,   143.00 tokens per second)
0.01.483.321 I llama_perf_context_print:        eval time =     636.08 ms /    63 runs   (   10.10 ms per token,    99.04 tokens per second)
0.01.483.323 I llama_perf_context_print:       total time =     689.10 ms /    70 tokens
0.01.483.604 I ggml_metal_free: deallocating

real	0m1.507s
user	0m0.118s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.889 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.893 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.895 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.895 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.895 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.896 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.896 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.897 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.897 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.898 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.898 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.898 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.899 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.899 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.903 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.903 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.903 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.710 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.731 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.477 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.479 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.479 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.479 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.480 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.480 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.035.480 I llama_model_loader: - type  f32:  194 tensors
0.00.035.481 I llama_model_loader: - type q4_1:   97 tensors
0.00.035.481 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.481 I print_info: file format = GGUF V3 (latest)
0.00.035.482 I print_info: file type   = Q4_1
0.00.035.482 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.979 I load: special tokens cache size = 25
0.00.050.634 I load: token to piece cache size = 0.2984 MB
0.00.050.637 I print_info: arch             = gptneox
0.00.050.638 I print_info: vocab_only       = 0
0.00.050.638 I print_info: n_ctx_train      = 2048
0.00.050.638 I print_info: n_embd           = 2048
0.00.050.638 I print_info: n_layer          = 24
0.00.050.641 I print_info: n_head           = 16
0.00.050.642 I print_info: n_head_kv        = 16
0.00.050.642 I print_info: n_rot            = 32
0.00.050.642 I print_info: n_swa            = 0
0.00.050.642 I print_info: n_embd_head_k    = 128
0.00.050.642 I print_info: n_embd_head_v    = 128
0.00.050.643 I print_info: n_gqa            = 1
0.00.050.644 I print_info: n_embd_k_gqa     = 2048
0.00.050.644 I print_info: n_embd_v_gqa     = 2048
0.00.050.645 I print_info: f_norm_eps       = 1.0e-05
0.00.050.645 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.645 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.646 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.646 I print_info: f_logit_scale    = 0.0e+00
0.00.050.646 I print_info: n_ff             = 8192
0.00.050.646 I print_info: n_expert         = 0
0.00.050.647 I print_info: n_expert_used    = 0
0.00.050.647 I print_info: causal attn      = 1
0.00.050.647 I print_info: pooling type     = 0
0.00.050.648 I print_info: rope type        = 2
0.00.050.650 I print_info: rope scaling     = linear
0.00.050.651 I print_info: freq_base_train  = 10000.0
0.00.050.651 I print_info: freq_scale_train = 1
0.00.050.651 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.651 I print_info: rope_finetuned   = unknown
0.00.050.651 I print_info: ssm_d_conv       = 0
0.00.050.652 I print_info: ssm_d_inner      = 0
0.00.050.652 I print_info: ssm_d_state      = 0
0.00.050.652 I print_info: ssm_dt_rank      = 0
0.00.050.652 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.652 I print_info: model type       = 1.4B
0.00.050.653 I print_info: model params     = 1.41 B
0.00.050.654 I print_info: general.name     = 1.4B
0.00.050.654 I print_info: vocab type       = BPE
0.00.050.654 I print_info: n_vocab          = 50304
0.00.050.655 I print_info: n_merges         = 50009
0.00.050.655 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.655 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.655 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.655 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.656 I print_info: LF token         = 187 'Ċ'
0.00.050.656 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.656 I print_info: max token length = 1024
0.00.791.070 I load_tensors: offloading 24 repeating layers to GPU
0.00.791.082 I load_tensors: offloading output layer to GPU
0.00.791.082 I load_tensors: offloaded 25/25 layers to GPU
0.00.791.112 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.791.117 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.792.295 I llama_init_from_model: n_seq_max     = 1
0.00.792.307 I llama_init_from_model: n_ctx         = 2048
0.00.792.307 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.792.308 I llama_init_from_model: n_batch       = 2048
0.00.792.308 I llama_init_from_model: n_ubatch      = 512
0.00.792.308 I llama_init_from_model: flash_attn    = 0
0.00.792.310 I llama_init_from_model: freq_base     = 10000.0
0.00.792.310 I llama_init_from_model: freq_scale    = 1
0.00.792.316 I ggml_metal_init: allocating
0.00.792.403 I ggml_metal_init: found device: Apple M4
0.00.792.418 I ggml_metal_init: picking default device: Apple M4
0.00.794.252 I ggml_metal_init: using embedded metal library
0.00.799.591 I ggml_metal_init: GPU name:   Apple M4
0.00.799.597 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.799.598 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.799.599 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.799.599 I ggml_metal_init: simdgroup reduction   = true
0.00.799.599 I ggml_metal_init: simdgroup matrix mul. = true
0.00.799.599 I ggml_metal_init: has residency sets    = true
0.00.799.600 I ggml_metal_init: has bfloat            = true
0.00.799.600 I ggml_metal_init: use bfloat            = true
0.00.799.605 I ggml_metal_init: hasUnifiedMemory      = true
0.00.799.608 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.810.122 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.842.327 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.842.334 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.842.371 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.846.828 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.846.831 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.846.831 I llama_init_from_model: graph nodes  = 967
0.00.846.831 I llama_init_from_model: graph splits = 2
0.00.846.838 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.846.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.846.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.903.666 I main: llama threadpool init, n_threads = 4
0.00.903.700 I 
0.00.903.723 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.903.723 I 
0.00.903.870 I sampler seed: 1234
0.00.903.874 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.903.904 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.903.907 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.903.907 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.635.742 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51079.14 tokens per second)
0.01.635.743 I llama_perf_context_print:        load time =     893.87 ms
0.01.635.744 I llama_perf_context_print: prompt eval time =      49.35 ms /     7 tokens (    7.05 ms per token,   141.85 tokens per second)
0.01.635.745 I llama_perf_context_print:        eval time =     679.83 ms /    63 runs   (   10.79 ms per token,    92.67 tokens per second)
0.01.635.746 I llama_perf_context_print:       total time =     732.98 ms /    70 tokens
0.01.635.986 I ggml_metal_free: deallocating

real	0m1.655s
user	0m0.102s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.016.635 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.403 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.036.413 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.415 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.415 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.416 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.416 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.417 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.418 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.418 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.419 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.419 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.421 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.980 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.166 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.678 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.680 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.680 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.680 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.681 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.681 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.046.682 I llama_model_loader: - type  f32:  194 tensors
0.00.046.682 I llama_model_loader: - type q5_0:   97 tensors
0.00.046.682 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.683 I print_info: file format = GGUF V3 (latest)
0.00.046.683 I print_info: file type   = Q5_0
0.00.046.685 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.057.690 I load: special tokens cache size = 25
0.00.066.153 I load: token to piece cache size = 0.2984 MB
0.00.066.157 I print_info: arch             = gptneox
0.00.066.158 I print_info: vocab_only       = 0
0.00.066.158 I print_info: n_ctx_train      = 2048
0.00.066.158 I print_info: n_embd           = 2048
0.00.066.158 I print_info: n_layer          = 24
0.00.066.162 I print_info: n_head           = 16
0.00.066.163 I print_info: n_head_kv        = 16
0.00.066.166 I print_info: n_rot            = 32
0.00.066.166 I print_info: n_swa            = 0
0.00.066.167 I print_info: n_embd_head_k    = 128
0.00.066.167 I print_info: n_embd_head_v    = 128
0.00.066.168 I print_info: n_gqa            = 1
0.00.066.169 I print_info: n_embd_k_gqa     = 2048
0.00.066.175 I print_info: n_embd_v_gqa     = 2048
0.00.066.175 I print_info: f_norm_eps       = 1.0e-05
0.00.066.176 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.176 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.179 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.179 I print_info: f_logit_scale    = 0.0e+00
0.00.066.180 I print_info: n_ff             = 8192
0.00.066.180 I print_info: n_expert         = 0
0.00.066.180 I print_info: n_expert_used    = 0
0.00.066.180 I print_info: causal attn      = 1
0.00.066.181 I print_info: pooling type     = 0
0.00.066.181 I print_info: rope type        = 2
0.00.066.181 I print_info: rope scaling     = linear
0.00.066.182 I print_info: freq_base_train  = 10000.0
0.00.066.182 I print_info: freq_scale_train = 1
0.00.066.187 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.187 I print_info: rope_finetuned   = unknown
0.00.066.187 I print_info: ssm_d_conv       = 0
0.00.066.188 I print_info: ssm_d_inner      = 0
0.00.066.188 I print_info: ssm_d_state      = 0
0.00.066.188 I print_info: ssm_dt_rank      = 0
0.00.066.188 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.188 I print_info: model type       = 1.4B
0.00.066.189 I print_info: model params     = 1.41 B
0.00.066.189 I print_info: general.name     = 1.4B
0.00.066.190 I print_info: vocab type       = BPE
0.00.066.190 I print_info: n_vocab          = 50304
0.00.066.190 I print_info: n_merges         = 50009
0.00.066.191 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.193 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.193 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.193 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.194 I print_info: LF token         = 187 'Ċ'
0.00.066.194 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.194 I print_info: max token length = 1024
0.00.741.756 I load_tensors: offloading 24 repeating layers to GPU
0.00.741.772 I load_tensors: offloading output layer to GPU
0.00.741.772 I load_tensors: offloaded 25/25 layers to GPU
0.00.741.812 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.741.814 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.743.120 I llama_init_from_model: n_seq_max     = 1
0.00.743.125 I llama_init_from_model: n_ctx         = 2048
0.00.743.126 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.743.126 I llama_init_from_model: n_batch       = 2048
0.00.743.127 I llama_init_from_model: n_ubatch      = 512
0.00.743.127 I llama_init_from_model: flash_attn    = 0
0.00.743.130 I llama_init_from_model: freq_base     = 10000.0
0.00.743.130 I llama_init_from_model: freq_scale    = 1
0.00.743.133 I ggml_metal_init: allocating
0.00.743.212 I ggml_metal_init: found device: Apple M4
0.00.743.226 I ggml_metal_init: picking default device: Apple M4
0.00.745.230 I ggml_metal_init: using embedded metal library
0.00.751.830 I ggml_metal_init: GPU name:   Apple M4
0.00.751.835 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.751.836 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.751.837 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.751.837 I ggml_metal_init: simdgroup reduction   = true
0.00.751.837 I ggml_metal_init: simdgroup matrix mul. = true
0.00.751.838 I ggml_metal_init: has residency sets    = true
0.00.751.838 I ggml_metal_init: has bfloat            = true
0.00.751.838 I ggml_metal_init: use bfloat            = true
0.00.751.839 I ggml_metal_init: hasUnifiedMemory      = true
0.00.751.841 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.769.664 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.822.722 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.822.729 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.822.764 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.826.850 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.826.853 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.826.853 I llama_init_from_model: graph nodes  = 967
0.00.826.853 I llama_init_from_model: graph splits = 2
0.00.826.860 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.826.984 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.826.984 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.884.366 I main: llama threadpool init, n_threads = 4
0.00.884.409 I 
0.00.884.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.884.437 I 
0.00.884.585 I sampler seed: 1234
0.00.884.590 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.884.633 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.884.635 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.884.635 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.663.453 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53828.66 tokens per second)
0.01.663.453 I llama_perf_context_print:        load time =     866.73 ms
0.01.663.456 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.39 tokens per second)
0.01.663.457 I llama_perf_context_print:        eval time =     732.82 ms /    63 runs   (   11.63 ms per token,    85.97 tokens per second)
0.01.663.457 I llama_perf_context_print:       total time =     780.09 ms /    70 tokens
0.01.663.679 I ggml_metal_free: deallocating

real	0m1.687s
user	0m0.117s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.458 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.463 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.469 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.470 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.471 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.471 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.472 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.473 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.473 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.473 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.474 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.474 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.475 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.477 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.478 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.478 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.479 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.210 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.238 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.937 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.938 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.940 I llama_model_loader: - type  f32:  194 tensors
0.00.024.940 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.940 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.941 I print_info: file format = GGUF V3 (latest)
0.00.024.941 I print_info: file type   = Q5_1
0.00.024.942 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.762 I load: special tokens cache size = 25
0.00.038.659 I load: token to piece cache size = 0.2984 MB
0.00.038.662 I print_info: arch             = gptneox
0.00.038.662 I print_info: vocab_only       = 0
0.00.038.662 I print_info: n_ctx_train      = 2048
0.00.038.662 I print_info: n_embd           = 2048
0.00.038.663 I print_info: n_layer          = 24
0.00.038.665 I print_info: n_head           = 16
0.00.038.666 I print_info: n_head_kv        = 16
0.00.038.666 I print_info: n_rot            = 32
0.00.038.666 I print_info: n_swa            = 0
0.00.038.667 I print_info: n_embd_head_k    = 128
0.00.038.667 I print_info: n_embd_head_v    = 128
0.00.038.668 I print_info: n_gqa            = 1
0.00.038.669 I print_info: n_embd_k_gqa     = 2048
0.00.038.669 I print_info: n_embd_v_gqa     = 2048
0.00.038.670 I print_info: f_norm_eps       = 1.0e-05
0.00.038.670 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.670 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.671 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.671 I print_info: f_logit_scale    = 0.0e+00
0.00.038.671 I print_info: n_ff             = 8192
0.00.038.672 I print_info: n_expert         = 0
0.00.038.672 I print_info: n_expert_used    = 0
0.00.038.672 I print_info: causal attn      = 1
0.00.038.672 I print_info: pooling type     = 0
0.00.038.672 I print_info: rope type        = 2
0.00.038.672 I print_info: rope scaling     = linear
0.00.038.673 I print_info: freq_base_train  = 10000.0
0.00.038.673 I print_info: freq_scale_train = 1
0.00.038.673 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.674 I print_info: rope_finetuned   = unknown
0.00.038.674 I print_info: ssm_d_conv       = 0
0.00.038.674 I print_info: ssm_d_inner      = 0
0.00.038.674 I print_info: ssm_d_state      = 0
0.00.038.674 I print_info: ssm_dt_rank      = 0
0.00.038.674 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.675 I print_info: model type       = 1.4B
0.00.038.675 I print_info: model params     = 1.41 B
0.00.038.675 I print_info: general.name     = 1.4B
0.00.038.675 I print_info: vocab type       = BPE
0.00.038.676 I print_info: n_vocab          = 50304
0.00.038.676 I print_info: n_merges         = 50009
0.00.038.678 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.678 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.678 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.678 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.679 I print_info: LF token         = 187 'Ċ'
0.00.038.679 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.679 I print_info: max token length = 1024
0.00.648.598 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.603 I load_tensors: offloading output layer to GPU
0.00.648.605 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.631 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.648.633 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.649.824 I llama_init_from_model: n_seq_max     = 1
0.00.649.826 I llama_init_from_model: n_ctx         = 2048
0.00.649.826 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.649.827 I llama_init_from_model: n_batch       = 2048
0.00.649.827 I llama_init_from_model: n_ubatch      = 512
0.00.649.827 I llama_init_from_model: flash_attn    = 0
0.00.649.828 I llama_init_from_model: freq_base     = 10000.0
0.00.649.829 I llama_init_from_model: freq_scale    = 1
0.00.649.830 I ggml_metal_init: allocating
0.00.649.848 I ggml_metal_init: found device: Apple M4
0.00.649.857 I ggml_metal_init: picking default device: Apple M4
0.00.651.429 I ggml_metal_init: using embedded metal library
0.00.657.388 I ggml_metal_init: GPU name:   Apple M4
0.00.657.391 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.392 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.393 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.393 I ggml_metal_init: simdgroup reduction   = true
0.00.657.394 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.394 I ggml_metal_init: has residency sets    = true
0.00.657.394 I ggml_metal_init: has bfloat            = true
0.00.657.395 I ggml_metal_init: use bfloat            = true
0.00.657.395 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.397 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.510 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.727.712 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.727.719 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.727.799 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.731.891 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.731.893 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.731.893 I llama_init_from_model: graph nodes  = 967
0.00.731.893 I llama_init_from_model: graph splits = 2
0.00.731.896 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.732.030 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.732.031 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.619 I main: llama threadpool init, n_threads = 4
0.00.788.677 I 
0.00.788.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.702 I 
0.00.788.878 I sampler seed: 1234
0.00.788.882 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.788.902 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.788.902 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.788.902 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.619.330 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52592.59 tokens per second)
0.01.619.330 I llama_perf_context_print:        load time =     778.80 ms
0.01.619.331 I llama_perf_context_print: prompt eval time =      41.92 ms /     7 tokens (    5.99 ms per token,   167.00 tokens per second)
0.01.619.332 I llama_perf_context_print:        eval time =     785.60 ms /    63 runs   (   12.47 ms per token,    80.19 tokens per second)
0.01.619.332 I llama_perf_context_print:       total time =     831.64 ms /    70 tokens
0.01.619.611 I ggml_metal_free: deallocating

real	0m1.637s
user	0m0.107s
sys	0m0.237s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.037 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.522 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.528 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.529 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.530 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.531 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.533 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.536 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.275 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.075 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.076 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.076 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.077 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.077 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.077 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.078 I llama_model_loader: - type  f32:  194 tensors
0.00.025.078 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.078 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.079 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.079 I print_info: file format = GGUF V3 (latest)
0.00.025.080 I print_info: file type   = Q2_K - Medium
0.00.025.080 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.734 I load: special tokens cache size = 25
0.00.038.663 I load: token to piece cache size = 0.2984 MB
0.00.038.666 I print_info: arch             = gptneox
0.00.038.666 I print_info: vocab_only       = 0
0.00.038.666 I print_info: n_ctx_train      = 2048
0.00.038.667 I print_info: n_embd           = 2048
0.00.038.667 I print_info: n_layer          = 24
0.00.038.669 I print_info: n_head           = 16
0.00.038.670 I print_info: n_head_kv        = 16
0.00.038.670 I print_info: n_rot            = 32
0.00.038.670 I print_info: n_swa            = 0
0.00.038.670 I print_info: n_embd_head_k    = 128
0.00.038.671 I print_info: n_embd_head_v    = 128
0.00.038.671 I print_info: n_gqa            = 1
0.00.038.672 I print_info: n_embd_k_gqa     = 2048
0.00.038.673 I print_info: n_embd_v_gqa     = 2048
0.00.038.673 I print_info: f_norm_eps       = 1.0e-05
0.00.038.674 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.674 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.674 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.674 I print_info: f_logit_scale    = 0.0e+00
0.00.038.675 I print_info: n_ff             = 8192
0.00.038.675 I print_info: n_expert         = 0
0.00.038.675 I print_info: n_expert_used    = 0
0.00.038.675 I print_info: causal attn      = 1
0.00.038.675 I print_info: pooling type     = 0
0.00.038.676 I print_info: rope type        = 2
0.00.038.676 I print_info: rope scaling     = linear
0.00.038.676 I print_info: freq_base_train  = 10000.0
0.00.038.676 I print_info: freq_scale_train = 1
0.00.038.677 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.679 I print_info: rope_finetuned   = unknown
0.00.038.679 I print_info: ssm_d_conv       = 0
0.00.038.679 I print_info: ssm_d_inner      = 0
0.00.038.680 I print_info: ssm_d_state      = 0
0.00.038.680 I print_info: ssm_dt_rank      = 0
0.00.038.680 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.680 I print_info: model type       = 1.4B
0.00.038.682 I print_info: model params     = 1.41 B
0.00.038.682 I print_info: general.name     = 1.4B
0.00.038.683 I print_info: vocab type       = BPE
0.00.038.683 I print_info: n_vocab          = 50304
0.00.038.683 I print_info: n_merges         = 50009
0.00.038.683 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.683 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.684 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.684 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.684 I print_info: LF token         = 187 'Ċ'
0.00.038.684 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.684 I print_info: max token length = 1024
0.00.359.217 I load_tensors: offloading 24 repeating layers to GPU
0.00.359.228 I load_tensors: offloading output layer to GPU
0.00.359.229 I load_tensors: offloaded 25/25 layers to GPU
0.00.359.259 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.359.260 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.360.705 I llama_init_from_model: n_seq_max     = 1
0.00.360.711 I llama_init_from_model: n_ctx         = 2048
0.00.360.711 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.360.712 I llama_init_from_model: n_batch       = 2048
0.00.360.712 I llama_init_from_model: n_ubatch      = 512
0.00.360.712 I llama_init_from_model: flash_attn    = 0
0.00.360.713 I llama_init_from_model: freq_base     = 10000.0
0.00.360.714 I llama_init_from_model: freq_scale    = 1
0.00.360.716 I ggml_metal_init: allocating
0.00.360.758 I ggml_metal_init: found device: Apple M4
0.00.360.770 I ggml_metal_init: picking default device: Apple M4
0.00.362.569 I ggml_metal_init: using embedded metal library
0.00.368.239 I ggml_metal_init: GPU name:   Apple M4
0.00.368.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.368.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.368.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.368.253 I ggml_metal_init: simdgroup reduction   = true
0.00.368.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.368.254 I ggml_metal_init: has residency sets    = true
0.00.368.254 I ggml_metal_init: has bfloat            = true
0.00.368.254 I ggml_metal_init: use bfloat            = true
0.00.368.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.368.263 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.391.600 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.450.430 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.450.437 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.450.466 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.454.500 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.454.502 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.454.502 I llama_init_from_model: graph nodes  = 967
0.00.454.502 I llama_init_from_model: graph splits = 2
0.00.454.508 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.454.631 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.454.632 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.514.326 I main: llama threadpool init, n_threads = 4
0.00.514.371 I 
0.00.514.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.514.403 I 
0.00.514.578 I sampler seed: 1234
0.00.514.582 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.514.593 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.514.594 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.514.595 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.199.413 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.01.199.413 I llama_perf_context_print:        load time =     503.35 ms
0.01.199.414 I llama_perf_context_print: prompt eval time =      44.13 ms /     7 tokens (    6.30 ms per token,   158.63 tokens per second)
0.01.199.415 I llama_perf_context_print:        eval time =     637.88 ms /    63 runs   (   10.12 ms per token,    98.77 tokens per second)
0.01.199.416 I llama_perf_context_print:       total time =     686.02 ms /    70 tokens
0.01.199.644 I ggml_metal_free: deallocating

real	0m1.216s
user	0m0.111s
sys	0m0.178s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.707 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.146 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.153 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.153 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.154 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.154 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.154 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.155 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.156 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.156 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.156 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.157 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.157 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.158 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.161 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.161 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.161 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.868 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.889 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.590 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.592 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.592 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.592 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.593 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.593 I llama_model_loader: - type  f32:  194 tensors
0.00.024.594 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.594 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.594 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.594 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.595 I print_info: file format = GGUF V3 (latest)
0.00.024.595 I print_info: file type   = Q3_K - Medium
0.00.024.596 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.326 I load: special tokens cache size = 25
0.00.038.345 I load: token to piece cache size = 0.2984 MB
0.00.038.348 I print_info: arch             = gptneox
0.00.038.348 I print_info: vocab_only       = 0
0.00.038.348 I print_info: n_ctx_train      = 2048
0.00.038.348 I print_info: n_embd           = 2048
0.00.038.349 I print_info: n_layer          = 24
0.00.038.351 I print_info: n_head           = 16
0.00.038.352 I print_info: n_head_kv        = 16
0.00.038.352 I print_info: n_rot            = 32
0.00.038.352 I print_info: n_swa            = 0
0.00.038.352 I print_info: n_embd_head_k    = 128
0.00.038.353 I print_info: n_embd_head_v    = 128
0.00.038.354 I print_info: n_gqa            = 1
0.00.038.355 I print_info: n_embd_k_gqa     = 2048
0.00.038.357 I print_info: n_embd_v_gqa     = 2048
0.00.038.358 I print_info: f_norm_eps       = 1.0e-05
0.00.038.358 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.359 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.359 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.359 I print_info: f_logit_scale    = 0.0e+00
0.00.038.360 I print_info: n_ff             = 8192
0.00.038.361 I print_info: n_expert         = 0
0.00.038.361 I print_info: n_expert_used    = 0
0.00.038.362 I print_info: causal attn      = 1
0.00.038.363 I print_info: pooling type     = 0
0.00.038.363 I print_info: rope type        = 2
0.00.038.364 I print_info: rope scaling     = linear
0.00.038.364 I print_info: freq_base_train  = 10000.0
0.00.038.364 I print_info: freq_scale_train = 1
0.00.038.365 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.365 I print_info: rope_finetuned   = unknown
0.00.038.365 I print_info: ssm_d_conv       = 0
0.00.038.365 I print_info: ssm_d_inner      = 0
0.00.038.365 I print_info: ssm_d_state      = 0
0.00.038.365 I print_info: ssm_dt_rank      = 0
0.00.038.365 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.366 I print_info: model type       = 1.4B
0.00.038.366 I print_info: model params     = 1.41 B
0.00.038.366 I print_info: general.name     = 1.4B
0.00.038.373 I print_info: vocab type       = BPE
0.00.038.374 I print_info: n_vocab          = 50304
0.00.038.375 I print_info: n_merges         = 50009
0.00.038.375 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.375 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.376 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.376 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.376 I print_info: LF token         = 187 'Ċ'
0.00.038.376 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.376 I print_info: max token length = 1024
0.00.433.475 I load_tensors: offloading 24 repeating layers to GPU
0.00.433.486 I load_tensors: offloading output layer to GPU
0.00.433.486 I load_tensors: offloaded 25/25 layers to GPU
0.00.433.521 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.433.525 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.435.039 I llama_init_from_model: n_seq_max     = 1
0.00.435.048 I llama_init_from_model: n_ctx         = 2048
0.00.435.048 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.435.049 I llama_init_from_model: n_batch       = 2048
0.00.435.049 I llama_init_from_model: n_ubatch      = 512
0.00.435.050 I llama_init_from_model: flash_attn    = 0
0.00.435.056 I llama_init_from_model: freq_base     = 10000.0
0.00.435.056 I llama_init_from_model: freq_scale    = 1
0.00.435.059 I ggml_metal_init: allocating
0.00.435.113 I ggml_metal_init: found device: Apple M4
0.00.435.128 I ggml_metal_init: picking default device: Apple M4
0.00.437.433 I ggml_metal_init: using embedded metal library
0.00.443.803 I ggml_metal_init: GPU name:   Apple M4
0.00.443.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.443.810 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.443.811 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.443.811 I ggml_metal_init: simdgroup reduction   = true
0.00.443.812 I ggml_metal_init: simdgroup matrix mul. = true
0.00.443.812 I ggml_metal_init: has residency sets    = true
0.00.443.812 I ggml_metal_init: has bfloat            = true
0.00.443.812 I ggml_metal_init: use bfloat            = true
0.00.443.814 I ggml_metal_init: hasUnifiedMemory      = true
0.00.443.816 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.463.978 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.524.084 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.524.093 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.524.130 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.528.442 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.528.444 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.528.445 I llama_init_from_model: graph nodes  = 967
0.00.528.445 I llama_init_from_model: graph splits = 2
0.00.528.451 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.528.590 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.528.591 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.389 I main: llama threadpool init, n_threads = 4
0.00.584.430 I 
0.00.584.452 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.452 I 
0.00.584.623 I sampler seed: 1234
0.00.584.627 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.584.646 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.584.646 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.584.646 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.320.997 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50461.98 tokens per second)
0.01.320.998 I llama_perf_context_print:        load time =     574.77 ms
0.01.320.999 I llama_perf_context_print: prompt eval time =      40.18 ms /     7 tokens (    5.74 ms per token,   174.20 tokens per second)
0.01.321.000 I llama_perf_context_print:        eval time =     693.19 ms /    63 runs   (   11.00 ms per token,    90.88 tokens per second)
0.01.321.001 I llama_perf_context_print:       total time =     737.52 ms /    70 tokens
0.01.321.212 I ggml_metal_free: deallocating

real	0m1.338s
user	0m0.112s
sys	0m0.181s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.533 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.192 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.197 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.199 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.201 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.202 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.202 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.202 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.203 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.203 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.204 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.204 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.204 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.205 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.205 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.209 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.209 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.210 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.774 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.775 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.775 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.776 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.776 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.777 I llama_model_loader: - type  f32:  194 tensors
0.00.025.777 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.777 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.778 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.778 I print_info: file format = GGUF V3 (latest)
0.00.025.779 I print_info: file type   = Q4_K - Medium
0.00.025.780 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.507 I load: special tokens cache size = 25
0.00.039.465 I load: token to piece cache size = 0.2984 MB
0.00.039.468 I print_info: arch             = gptneox
0.00.039.468 I print_info: vocab_only       = 0
0.00.039.468 I print_info: n_ctx_train      = 2048
0.00.039.468 I print_info: n_embd           = 2048
0.00.039.468 I print_info: n_layer          = 24
0.00.039.471 I print_info: n_head           = 16
0.00.039.472 I print_info: n_head_kv        = 16
0.00.039.472 I print_info: n_rot            = 32
0.00.039.472 I print_info: n_swa            = 0
0.00.039.472 I print_info: n_embd_head_k    = 128
0.00.039.473 I print_info: n_embd_head_v    = 128
0.00.039.474 I print_info: n_gqa            = 1
0.00.039.475 I print_info: n_embd_k_gqa     = 2048
0.00.039.476 I print_info: n_embd_v_gqa     = 2048
0.00.039.476 I print_info: f_norm_eps       = 1.0e-05
0.00.039.477 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.477 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.477 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.477 I print_info: f_logit_scale    = 0.0e+00
0.00.039.478 I print_info: n_ff             = 8192
0.00.039.478 I print_info: n_expert         = 0
0.00.039.478 I print_info: n_expert_used    = 0
0.00.039.478 I print_info: causal attn      = 1
0.00.039.480 I print_info: pooling type     = 0
0.00.039.482 I print_info: rope type        = 2
0.00.039.482 I print_info: rope scaling     = linear
0.00.039.482 I print_info: freq_base_train  = 10000.0
0.00.039.483 I print_info: freq_scale_train = 1
0.00.039.483 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.483 I print_info: rope_finetuned   = unknown
0.00.039.483 I print_info: ssm_d_conv       = 0
0.00.039.483 I print_info: ssm_d_inner      = 0
0.00.039.483 I print_info: ssm_d_state      = 0
0.00.039.484 I print_info: ssm_dt_rank      = 0
0.00.039.484 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.484 I print_info: model type       = 1.4B
0.00.039.484 I print_info: model params     = 1.41 B
0.00.039.484 I print_info: general.name     = 1.4B
0.00.039.485 I print_info: vocab type       = BPE
0.00.039.485 I print_info: n_vocab          = 50304
0.00.039.485 I print_info: n_merges         = 50009
0.00.039.486 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: LF token         = 187 'Ċ'
0.00.039.490 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: max token length = 1024
0.00.523.390 I load_tensors: offloading 24 repeating layers to GPU
0.00.523.405 I load_tensors: offloading output layer to GPU
0.00.523.406 I load_tensors: offloaded 25/25 layers to GPU
0.00.523.437 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.523.438 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.524.983 I llama_init_from_model: n_seq_max     = 1
0.00.524.991 I llama_init_from_model: n_ctx         = 2048
0.00.524.992 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.524.992 I llama_init_from_model: n_batch       = 2048
0.00.524.993 I llama_init_from_model: n_ubatch      = 512
0.00.524.993 I llama_init_from_model: flash_attn    = 0
0.00.524.994 I llama_init_from_model: freq_base     = 10000.0
0.00.524.999 I llama_init_from_model: freq_scale    = 1
0.00.525.001 I ggml_metal_init: allocating
0.00.525.070 I ggml_metal_init: found device: Apple M4
0.00.525.085 I ggml_metal_init: picking default device: Apple M4
0.00.526.944 I ggml_metal_init: using embedded metal library
0.00.533.531 I ggml_metal_init: GPU name:   Apple M4
0.00.533.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.533.536 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.533.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.533.537 I ggml_metal_init: simdgroup reduction   = true
0.00.533.537 I ggml_metal_init: simdgroup matrix mul. = true
0.00.533.537 I ggml_metal_init: has residency sets    = true
0.00.533.538 I ggml_metal_init: has bfloat            = true
0.00.533.538 I ggml_metal_init: use bfloat            = true
0.00.533.539 I ggml_metal_init: hasUnifiedMemory      = true
0.00.533.540 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.550.721 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.611.193 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.611.199 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.611.231 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.615.372 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.615.374 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.615.375 I llama_init_from_model: graph nodes  = 967
0.00.615.375 I llama_init_from_model: graph splits = 2
0.00.615.381 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.615.505 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.615.505 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.142 I main: llama threadpool init, n_threads = 4
0.00.665.181 I 
0.00.665.203 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.203 I 
0.00.665.332 I sampler seed: 1234
0.00.665.337 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.665.371 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.665.372 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.665.392 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.438.010 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48999.31 tokens per second)
0.01.438.010 I llama_perf_context_print:        load time =     654.71 ms
0.01.438.012 I llama_perf_context_print: prompt eval time =      58.47 ms /     7 tokens (    8.35 ms per token,   119.73 tokens per second)
0.01.438.012 I llama_perf_context_print:        eval time =     711.08 ms /    63 runs   (   11.29 ms per token,    88.60 tokens per second)
0.01.438.014 I llama_perf_context_print:       total time =     773.77 ms /    70 tokens
0.01.438.264 I ggml_metal_free: deallocating

real	0m1.455s
user	0m0.109s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.011.358 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.634 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.636 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.636 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.637 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.637 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.638 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.638 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.639 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.640 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.640 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.388 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.351 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.062 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.063 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.063 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.064 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.064 I llama_model_loader: - type  f32:  194 tensors
0.00.027.064 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.065 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.065 I print_info: file format = GGUF V3 (latest)
0.00.027.066 I print_info: file type   = Q5_K - Medium
0.00.027.066 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.846 I load: special tokens cache size = 25
0.00.040.657 I load: token to piece cache size = 0.2984 MB
0.00.040.659 I print_info: arch             = gptneox
0.00.040.660 I print_info: vocab_only       = 0
0.00.040.660 I print_info: n_ctx_train      = 2048
0.00.040.660 I print_info: n_embd           = 2048
0.00.040.660 I print_info: n_layer          = 24
0.00.040.663 I print_info: n_head           = 16
0.00.040.663 I print_info: n_head_kv        = 16
0.00.040.664 I print_info: n_rot            = 32
0.00.040.664 I print_info: n_swa            = 0
0.00.040.665 I print_info: n_embd_head_k    = 128
0.00.040.666 I print_info: n_embd_head_v    = 128
0.00.040.667 I print_info: n_gqa            = 1
0.00.040.668 I print_info: n_embd_k_gqa     = 2048
0.00.040.668 I print_info: n_embd_v_gqa     = 2048
0.00.040.669 I print_info: f_norm_eps       = 1.0e-05
0.00.040.670 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.670 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.670 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.674 I print_info: f_logit_scale    = 0.0e+00
0.00.040.675 I print_info: n_ff             = 8192
0.00.040.675 I print_info: n_expert         = 0
0.00.040.676 I print_info: n_expert_used    = 0
0.00.040.676 I print_info: causal attn      = 1
0.00.040.676 I print_info: pooling type     = 0
0.00.040.677 I print_info: rope type        = 2
0.00.040.679 I print_info: rope scaling     = linear
0.00.040.679 I print_info: freq_base_train  = 10000.0
0.00.040.679 I print_info: freq_scale_train = 1
0.00.040.680 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.680 I print_info: rope_finetuned   = unknown
0.00.040.680 I print_info: ssm_d_conv       = 0
0.00.040.680 I print_info: ssm_d_inner      = 0
0.00.040.680 I print_info: ssm_d_state      = 0
0.00.040.680 I print_info: ssm_dt_rank      = 0
0.00.040.681 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.681 I print_info: model type       = 1.4B
0.00.040.681 I print_info: model params     = 1.41 B
0.00.040.681 I print_info: general.name     = 1.4B
0.00.040.682 I print_info: vocab type       = BPE
0.00.040.682 I print_info: n_vocab          = 50304
0.00.040.682 I print_info: n_merges         = 50009
0.00.040.683 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.683 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.683 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.683 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.683 I print_info: LF token         = 187 'Ċ'
0.00.040.684 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.685 I print_info: max token length = 1024
0.00.604.602 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.615 I load_tensors: offloading output layer to GPU
0.00.604.616 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.649 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.604.650 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.606.220 I llama_init_from_model: n_seq_max     = 1
0.00.606.226 I llama_init_from_model: n_ctx         = 2048
0.00.606.227 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.606.227 I llama_init_from_model: n_batch       = 2048
0.00.606.228 I llama_init_from_model: n_ubatch      = 512
0.00.606.228 I llama_init_from_model: flash_attn    = 0
0.00.606.230 I llama_init_from_model: freq_base     = 10000.0
0.00.606.230 I llama_init_from_model: freq_scale    = 1
0.00.606.242 I ggml_metal_init: allocating
0.00.606.315 I ggml_metal_init: found device: Apple M4
0.00.606.329 I ggml_metal_init: picking default device: Apple M4
0.00.608.332 I ggml_metal_init: using embedded metal library
0.00.614.888 I ggml_metal_init: GPU name:   Apple M4
0.00.614.893 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.894 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.895 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.895 I ggml_metal_init: simdgroup reduction   = true
0.00.614.895 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.896 I ggml_metal_init: has residency sets    = true
0.00.614.896 I ggml_metal_init: has bfloat            = true
0.00.614.896 I ggml_metal_init: use bfloat            = true
0.00.614.897 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.899 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.153 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.685.757 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.685.766 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.685.806 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.690.449 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.690.451 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.690.452 I llama_init_from_model: graph nodes  = 967
0.00.690.452 I llama_init_from_model: graph splits = 2
0.00.690.457 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.690.581 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.690.582 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.321 I main: llama threadpool init, n_threads = 4
0.00.754.365 I 
0.00.754.391 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.391 I 
0.00.754.561 I sampler seed: 1234
0.00.754.565 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.576 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.577 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.579 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.604.038 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.604.039 I llama_perf_context_print:        load time =     742.05 ms
0.01.604.039 I llama_perf_context_print: prompt eval time =      51.47 ms /     7 tokens (    7.35 ms per token,   136.00 tokens per second)
0.01.604.040 I llama_perf_context_print:        eval time =     795.34 ms /    63 runs   (   12.62 ms per token,    79.21 tokens per second)
0.01.604.040 I llama_perf_context_print:       total time =     850.63 ms /    70 tokens
0.01.604.327 I ggml_metal_free: deallocating

real	0m1.621s
user	0m0.109s
sys	0m0.219s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.404 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.214 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.225 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.226 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.226 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.230 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.231 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.234 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.235 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.236 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.237 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.237 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.972 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.147 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.989 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.991 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.991 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.992 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.992 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.993 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.993 I llama_model_loader: - type  f32:  194 tensors
0.00.026.994 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.994 I print_info: file format = GGUF V3 (latest)
0.00.026.995 I print_info: file type   = Q6_K
0.00.026.996 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.254 I load: special tokens cache size = 25
0.00.041.468 I load: token to piece cache size = 0.2984 MB
0.00.041.472 I print_info: arch             = gptneox
0.00.041.473 I print_info: vocab_only       = 0
0.00.041.473 I print_info: n_ctx_train      = 2048
0.00.041.475 I print_info: n_embd           = 2048
0.00.041.475 I print_info: n_layer          = 24
0.00.041.479 I print_info: n_head           = 16
0.00.041.479 I print_info: n_head_kv        = 16
0.00.041.480 I print_info: n_rot            = 32
0.00.041.480 I print_info: n_swa            = 0
0.00.041.480 I print_info: n_embd_head_k    = 128
0.00.041.480 I print_info: n_embd_head_v    = 128
0.00.041.482 I print_info: n_gqa            = 1
0.00.041.483 I print_info: n_embd_k_gqa     = 2048
0.00.041.483 I print_info: n_embd_v_gqa     = 2048
0.00.041.484 I print_info: f_norm_eps       = 1.0e-05
0.00.041.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.489 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.489 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.491 I print_info: f_logit_scale    = 0.0e+00
0.00.041.492 I print_info: n_ff             = 8192
0.00.041.493 I print_info: n_expert         = 0
0.00.041.493 I print_info: n_expert_used    = 0
0.00.041.493 I print_info: causal attn      = 1
0.00.041.493 I print_info: pooling type     = 0
0.00.041.493 I print_info: rope type        = 2
0.00.041.493 I print_info: rope scaling     = linear
0.00.041.494 I print_info: freq_base_train  = 10000.0
0.00.041.494 I print_info: freq_scale_train = 1
0.00.041.494 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.498 I print_info: rope_finetuned   = unknown
0.00.041.499 I print_info: ssm_d_conv       = 0
0.00.041.499 I print_info: ssm_d_inner      = 0
0.00.041.499 I print_info: ssm_d_state      = 0
0.00.041.499 I print_info: ssm_dt_rank      = 0
0.00.041.499 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.499 I print_info: model type       = 1.4B
0.00.041.500 I print_info: model params     = 1.41 B
0.00.041.500 I print_info: general.name     = 1.4B
0.00.041.500 I print_info: vocab type       = BPE
0.00.041.500 I print_info: n_vocab          = 50304
0.00.041.502 I print_info: n_merges         = 50009
0.00.041.502 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.502 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.502 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.503 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.503 I print_info: LF token         = 187 'Ċ'
0.00.041.503 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.503 I print_info: max token length = 1024
0.00.679.680 I load_tensors: offloading 24 repeating layers to GPU
0.00.679.698 I load_tensors: offloading output layer to GPU
0.00.679.699 I load_tensors: offloaded 25/25 layers to GPU
0.00.679.726 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.679.727 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.680.810 I llama_init_from_model: n_seq_max     = 1
0.00.680.814 I llama_init_from_model: n_ctx         = 2048
0.00.680.816 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.680.816 I llama_init_from_model: n_batch       = 2048
0.00.680.816 I llama_init_from_model: n_ubatch      = 512
0.00.680.817 I llama_init_from_model: flash_attn    = 0
0.00.680.818 I llama_init_from_model: freq_base     = 10000.0
0.00.680.819 I llama_init_from_model: freq_scale    = 1
0.00.680.820 I ggml_metal_init: allocating
0.00.680.856 I ggml_metal_init: found device: Apple M4
0.00.680.868 I ggml_metal_init: picking default device: Apple M4
0.00.682.103 I ggml_metal_init: using embedded metal library
0.00.686.697 I ggml_metal_init: GPU name:   Apple M4
0.00.686.704 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.686.704 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.686.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.686.705 I ggml_metal_init: simdgroup reduction   = true
0.00.686.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.686.706 I ggml_metal_init: has residency sets    = true
0.00.686.706 I ggml_metal_init: has bfloat            = true
0.00.686.707 I ggml_metal_init: use bfloat            = true
0.00.686.708 I ggml_metal_init: hasUnifiedMemory      = true
0.00.686.711 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.698.412 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.729.009 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.729.015 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.729.049 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.732.930 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.732.932 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.732.932 I llama_init_from_model: graph nodes  = 967
0.00.732.932 I llama_init_from_model: graph splits = 2
0.00.732.937 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.733.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.733.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.778 I main: llama threadpool init, n_threads = 4
0.00.788.815 I 
0.00.788.837 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.838 I 
0.00.788.957 I sampler seed: 1234
0.00.788.961 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.788.981 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.788.981 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.788.981 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.711.201 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53103.96 tokens per second)
0.01.711.201 I llama_perf_context_print:        load time =     778.46 ms
0.01.711.202 I llama_perf_context_print: prompt eval time =      54.11 ms /     7 tokens (    7.73 ms per token,   129.36 tokens per second)
0.01.711.203 I llama_perf_context_print:        eval time =     865.29 ms /    63 runs   (   13.73 ms per token,    72.81 tokens per second)
0.01.711.203 I llama_perf_context_print:       total time =     923.34 ms /    70 tokens
0.01.711.443 I ggml_metal_free: deallocating

real	0m1.728s
user	0m0.103s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.875 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.197 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.514 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.519 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.521 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.526 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.527 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.527 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.529 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.531 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.531 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.536 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.382 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.428 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.235 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.236 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.236 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.237 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.237 I llama_model_loader: - type  f32:  194 tensors
0.00.054.238 I llama_model_loader: - type  f16:   98 tensors
0.00.054.238 I print_info: file format = GGUF V3 (latest)
0.00.054.239 I print_info: file type   = all F32 (guessed)
0.00.054.241 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.194 I load: special tokens cache size = 25
0.00.073.813 I load: token to piece cache size = 0.2984 MB
0.00.073.817 I print_info: arch             = gptneox
0.00.073.817 I print_info: vocab_only       = 0
0.00.073.818 I print_info: n_ctx_train      = 2048
0.00.073.818 I print_info: n_embd           = 2048
0.00.073.818 I print_info: n_layer          = 24
0.00.073.822 I print_info: n_head           = 16
0.00.073.822 I print_info: n_head_kv        = 16
0.00.073.823 I print_info: n_rot            = 32
0.00.073.823 I print_info: n_swa            = 0
0.00.073.823 I print_info: n_embd_head_k    = 128
0.00.073.826 I print_info: n_embd_head_v    = 128
0.00.073.826 I print_info: n_gqa            = 1
0.00.073.827 I print_info: n_embd_k_gqa     = 2048
0.00.073.828 I print_info: n_embd_v_gqa     = 2048
0.00.073.829 I print_info: f_norm_eps       = 1.0e-05
0.00.073.829 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.829 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.830 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.831 I print_info: f_logit_scale    = 0.0e+00
0.00.073.832 I print_info: n_ff             = 8192
0.00.073.832 I print_info: n_expert         = 0
0.00.073.832 I print_info: n_expert_used    = 0
0.00.073.832 I print_info: causal attn      = 1
0.00.073.832 I print_info: pooling type     = 0
0.00.073.832 I print_info: rope type        = 2
0.00.073.833 I print_info: rope scaling     = linear
0.00.073.833 I print_info: freq_base_train  = 10000.0
0.00.073.833 I print_info: freq_scale_train = 1
0.00.073.834 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.834 I print_info: rope_finetuned   = unknown
0.00.073.834 I print_info: ssm_d_conv       = 0
0.00.073.835 I print_info: ssm_d_inner      = 0
0.00.073.835 I print_info: ssm_d_state      = 0
0.00.073.835 I print_info: ssm_dt_rank      = 0
0.00.073.835 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.839 I print_info: model type       = 1.4B
0.00.073.840 I print_info: model params     = 1.41 B
0.00.073.840 I print_info: general.name     = 1.4B
0.00.073.841 I print_info: vocab type       = BPE
0.00.073.841 I print_info: n_vocab          = 50304
0.00.073.841 I print_info: n_merges         = 50009
0.00.073.841 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.841 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.842 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.842 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.842 I print_info: LF token         = 187 'Ċ'
0.00.073.843 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.843 I print_info: max token length = 1024
0.01.474.978 I load_tensors: offloading 24 repeating layers to GPU
0.01.474.984 I load_tensors: offloading output layer to GPU
0.01.474.984 I load_tensors: offloaded 25/25 layers to GPU
0.01.475.009 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.475.011 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.475.795 I llama_init_from_model: n_seq_max     = 1
0.01.475.796 I llama_init_from_model: n_ctx         = 128
0.01.475.796 I llama_init_from_model: n_ctx_per_seq = 128
0.01.475.797 I llama_init_from_model: n_batch       = 128
0.01.475.797 I llama_init_from_model: n_ubatch      = 128
0.01.475.797 I llama_init_from_model: flash_attn    = 0
0.01.475.798 I llama_init_from_model: freq_base     = 10000.0
0.01.475.798 I llama_init_from_model: freq_scale    = 1
0.01.475.798 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.475.801 I ggml_metal_init: allocating
0.01.475.846 I ggml_metal_init: found device: Apple M4
0.01.475.853 I ggml_metal_init: picking default device: Apple M4
0.01.476.984 I ggml_metal_init: using embedded metal library
0.01.481.070 I ggml_metal_init: GPU name:   Apple M4
0.01.481.072 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.481.073 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.481.074 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.481.074 I ggml_metal_init: simdgroup reduction   = true
0.01.481.074 I ggml_metal_init: simdgroup matrix mul. = true
0.01.481.074 I ggml_metal_init: has residency sets    = true
0.01.481.074 I ggml_metal_init: has bfloat            = true
0.01.481.075 I ggml_metal_init: use bfloat            = true
0.01.481.075 I ggml_metal_init: hasUnifiedMemory      = true
0.01.481.076 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.492.956 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.494.864 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.494.866 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.494.892 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.496.614 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.496.615 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.496.616 I llama_init_from_model: graph nodes  = 967
0.01.496.616 I llama_init_from_model: graph splits = 2
0.01.496.617 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.496.617 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.532.804 I 
0.01.532.856 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.532.860 I perplexity: tokenizing the input ..
0.01.538.317 I perplexity: tokenization took 5.455 ms
0.01.538.322 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.660.519 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.661.873 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.661.909 I llama_perf_context_print:        load time =    1511.60 ms
0.01.661.910 I llama_perf_context_print: prompt eval time =     121.85 ms /   128 tokens (    0.95 ms per token,  1050.51 tokens per second)
0.01.661.911 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.661.911 I llama_perf_context_print:       total time =     129.11 ms /   129 tokens
0.01.662.316 I ggml_metal_free: deallocating

real	0m1.851s
user	0m0.099s
sys	0m0.296s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.296 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.184 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.367 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.373 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.380 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.381 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.381 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.381 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.381 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.383 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.383 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.383 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.384 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.384 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.384 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.384 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.386 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.387 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.387 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.144 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.417 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.198 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.199 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.200 I llama_model_loader: - type  f32:  194 tensors
0.00.027.200 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.201 I print_info: file format = GGUF V3 (latest)
0.00.027.202 I print_info: file type   = Q8_0
0.00.027.203 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.529 I load: special tokens cache size = 25
0.00.041.696 I load: token to piece cache size = 0.2984 MB
0.00.041.701 I print_info: arch             = gptneox
0.00.041.701 I print_info: vocab_only       = 0
0.00.041.701 I print_info: n_ctx_train      = 2048
0.00.041.701 I print_info: n_embd           = 2048
0.00.041.702 I print_info: n_layer          = 24
0.00.041.706 I print_info: n_head           = 16
0.00.041.707 I print_info: n_head_kv        = 16
0.00.041.707 I print_info: n_rot            = 32
0.00.041.708 I print_info: n_swa            = 0
0.00.041.708 I print_info: n_embd_head_k    = 128
0.00.041.710 I print_info: n_embd_head_v    = 128
0.00.041.710 I print_info: n_gqa            = 1
0.00.041.711 I print_info: n_embd_k_gqa     = 2048
0.00.041.712 I print_info: n_embd_v_gqa     = 2048
0.00.041.712 I print_info: f_norm_eps       = 1.0e-05
0.00.041.713 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.713 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.713 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.714 I print_info: f_logit_scale    = 0.0e+00
0.00.041.714 I print_info: n_ff             = 8192
0.00.041.715 I print_info: n_expert         = 0
0.00.041.715 I print_info: n_expert_used    = 0
0.00.041.715 I print_info: causal attn      = 1
0.00.041.715 I print_info: pooling type     = 0
0.00.041.715 I print_info: rope type        = 2
0.00.041.716 I print_info: rope scaling     = linear
0.00.041.716 I print_info: freq_base_train  = 10000.0
0.00.041.717 I print_info: freq_scale_train = 1
0.00.041.717 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.717 I print_info: rope_finetuned   = unknown
0.00.041.717 I print_info: ssm_d_conv       = 0
0.00.041.719 I print_info: ssm_d_inner      = 0
0.00.041.719 I print_info: ssm_d_state      = 0
0.00.041.720 I print_info: ssm_dt_rank      = 0
0.00.041.720 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.720 I print_info: model type       = 1.4B
0.00.041.720 I print_info: model params     = 1.41 B
0.00.041.721 I print_info: general.name     = 1.4B
0.00.041.721 I print_info: vocab type       = BPE
0.00.041.722 I print_info: n_vocab          = 50304
0.00.041.722 I print_info: n_merges         = 50009
0.00.041.722 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.723 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.723 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.723 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.724 I print_info: LF token         = 187 'Ċ'
0.00.041.724 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.725 I print_info: max token length = 1024
0.00.933.422 I load_tensors: offloading 24 repeating layers to GPU
0.00.933.428 I load_tensors: offloading output layer to GPU
0.00.933.429 I load_tensors: offloaded 25/25 layers to GPU
0.00.933.452 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.933.453 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.934.465 I llama_init_from_model: n_seq_max     = 1
0.00.934.467 I llama_init_from_model: n_ctx         = 128
0.00.934.468 I llama_init_from_model: n_ctx_per_seq = 128
0.00.934.468 I llama_init_from_model: n_batch       = 128
0.00.934.468 I llama_init_from_model: n_ubatch      = 128
0.00.934.469 I llama_init_from_model: flash_attn    = 0
0.00.934.469 I llama_init_from_model: freq_base     = 10000.0
0.00.934.470 I llama_init_from_model: freq_scale    = 1
0.00.934.471 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.934.472 I ggml_metal_init: allocating
0.00.934.490 I ggml_metal_init: found device: Apple M4
0.00.934.499 I ggml_metal_init: picking default device: Apple M4
0.00.935.917 I ggml_metal_init: using embedded metal library
0.00.941.138 I ggml_metal_init: GPU name:   Apple M4
0.00.941.141 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.941.141 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.941.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.941.143 I ggml_metal_init: simdgroup reduction   = true
0.00.941.143 I ggml_metal_init: simdgroup matrix mul. = true
0.00.941.143 I ggml_metal_init: has residency sets    = true
0.00.941.144 I ggml_metal_init: has bfloat            = true
0.00.941.144 I ggml_metal_init: use bfloat            = true
0.00.941.144 I ggml_metal_init: hasUnifiedMemory      = true
0.00.941.146 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.957.005 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.960.515 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.960.523 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.960.572 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.963.622 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.963.624 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.963.625 I llama_init_from_model: graph nodes  = 967
0.00.963.625 I llama_init_from_model: graph splits = 2
0.00.963.628 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.963.628 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.991.545 I 
0.00.991.609 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.991.616 I perplexity: tokenizing the input ..
0.00.998.660 I perplexity: tokenization took 7.04 ms
0.00.998.667 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.137.969 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.139.306 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.139.340 I llama_perf_context_print:        load time =     980.35 ms
0.01.139.342 I llama_perf_context_print: prompt eval time =     138.35 ms /   128 tokens (    1.08 ms per token,   925.16 tokens per second)
0.01.139.343 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.139.344 I llama_perf_context_print:       total time =     147.80 ms /   129 tokens
0.01.139.764 I ggml_metal_free: deallocating

real	0m1.156s
user	0m0.079s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.309 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.440 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.646 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.659 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.660 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.660 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.661 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.664 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.664 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.665 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.667 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.668 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.668 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.482 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.553 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.382 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.384 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.384 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.385 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.386 I llama_model_loader: - type  f32:  194 tensors
0.00.026.386 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.387 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.387 I print_info: file format = GGUF V3 (latest)
0.00.026.388 I print_info: file type   = Q4_0
0.00.026.389 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.496 I load: special tokens cache size = 25
0.00.040.436 I load: token to piece cache size = 0.2984 MB
0.00.040.439 I print_info: arch             = gptneox
0.00.040.440 I print_info: vocab_only       = 0
0.00.040.440 I print_info: n_ctx_train      = 2048
0.00.040.440 I print_info: n_embd           = 2048
0.00.040.440 I print_info: n_layer          = 24
0.00.040.444 I print_info: n_head           = 16
0.00.040.445 I print_info: n_head_kv        = 16
0.00.040.445 I print_info: n_rot            = 32
0.00.040.447 I print_info: n_swa            = 0
0.00.040.448 I print_info: n_embd_head_k    = 128
0.00.040.448 I print_info: n_embd_head_v    = 128
0.00.040.448 I print_info: n_gqa            = 1
0.00.040.449 I print_info: n_embd_k_gqa     = 2048
0.00.040.450 I print_info: n_embd_v_gqa     = 2048
0.00.040.451 I print_info: f_norm_eps       = 1.0e-05
0.00.040.451 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.451 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.451 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.452 I print_info: f_logit_scale    = 0.0e+00
0.00.040.452 I print_info: n_ff             = 8192
0.00.040.453 I print_info: n_expert         = 0
0.00.040.453 I print_info: n_expert_used    = 0
0.00.040.453 I print_info: causal attn      = 1
0.00.040.453 I print_info: pooling type     = 0
0.00.040.453 I print_info: rope type        = 2
0.00.040.453 I print_info: rope scaling     = linear
0.00.040.454 I print_info: freq_base_train  = 10000.0
0.00.040.454 I print_info: freq_scale_train = 1
0.00.040.454 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.454 I print_info: rope_finetuned   = unknown
0.00.040.455 I print_info: ssm_d_conv       = 0
0.00.040.456 I print_info: ssm_d_inner      = 0
0.00.040.456 I print_info: ssm_d_state      = 0
0.00.040.456 I print_info: ssm_dt_rank      = 0
0.00.040.456 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.456 I print_info: model type       = 1.4B
0.00.040.457 I print_info: model params     = 1.41 B
0.00.040.457 I print_info: general.name     = 1.4B
0.00.040.457 I print_info: vocab type       = BPE
0.00.040.458 I print_info: n_vocab          = 50304
0.00.040.458 I print_info: n_merges         = 50009
0.00.040.458 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.458 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.458 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.459 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.459 I print_info: LF token         = 187 'Ċ'
0.00.040.459 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.460 I print_info: max token length = 1024
0.00.593.046 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.067 I load_tensors: offloading output layer to GPU
0.00.593.067 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.102 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.593.103 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.594.353 I llama_init_from_model: n_seq_max     = 1
0.00.594.360 I llama_init_from_model: n_ctx         = 128
0.00.594.360 I llama_init_from_model: n_ctx_per_seq = 128
0.00.594.361 I llama_init_from_model: n_batch       = 128
0.00.594.362 I llama_init_from_model: n_ubatch      = 128
0.00.594.362 I llama_init_from_model: flash_attn    = 0
0.00.594.365 I llama_init_from_model: freq_base     = 10000.0
0.00.594.365 I llama_init_from_model: freq_scale    = 1
0.00.594.366 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.594.368 I ggml_metal_init: allocating
0.00.594.440 I ggml_metal_init: found device: Apple M4
0.00.594.454 I ggml_metal_init: picking default device: Apple M4
0.00.596.426 I ggml_metal_init: using embedded metal library
0.00.602.681 I ggml_metal_init: GPU name:   Apple M4
0.00.602.705 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.706 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.707 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.707 I ggml_metal_init: simdgroup reduction   = true
0.00.602.708 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.708 I ggml_metal_init: has residency sets    = true
0.00.602.708 I ggml_metal_init: has bfloat            = true
0.00.602.709 I ggml_metal_init: use bfloat            = true
0.00.602.712 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.716 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.737 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.626.492 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.626.499 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.626.561 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.629.855 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.629.857 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.629.857 I llama_init_from_model: graph nodes  = 967
0.00.629.858 I llama_init_from_model: graph splits = 2
0.00.629.861 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.629.861 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.538 I 
0.00.655.600 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.607 I perplexity: tokenizing the input ..
0.00.661.410 I perplexity: tokenization took 5.801 ms
0.00.661.419 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.251 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.798.589 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.798.612 I llama_perf_context_print:        load time =     645.09 ms
0.00.798.612 I llama_perf_context_print: prompt eval time =     135.58 ms /   128 tokens (    1.06 ms per token,   944.12 tokens per second)
0.00.798.613 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.613 I llama_perf_context_print:       total time =     143.08 ms /   129 tokens
0.00.799.015 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.079s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.217 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.461 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.475 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.478 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.480 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.480 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.480 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.481 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.481 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.482 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.316 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.119 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.120 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.121 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.122 I llama_model_loader: - type  f32:  194 tensors
0.00.025.122 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.122 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.123 I print_info: file format = GGUF V3 (latest)
0.00.025.123 I print_info: file type   = Q4_1
0.00.025.125 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.829 I load: special tokens cache size = 25
0.00.038.736 I load: token to piece cache size = 0.2984 MB
0.00.038.739 I print_info: arch             = gptneox
0.00.038.740 I print_info: vocab_only       = 0
0.00.038.740 I print_info: n_ctx_train      = 2048
0.00.038.740 I print_info: n_embd           = 2048
0.00.038.740 I print_info: n_layer          = 24
0.00.038.744 I print_info: n_head           = 16
0.00.038.745 I print_info: n_head_kv        = 16
0.00.038.745 I print_info: n_rot            = 32
0.00.038.748 I print_info: n_swa            = 0
0.00.038.748 I print_info: n_embd_head_k    = 128
0.00.038.748 I print_info: n_embd_head_v    = 128
0.00.038.749 I print_info: n_gqa            = 1
0.00.038.750 I print_info: n_embd_k_gqa     = 2048
0.00.038.750 I print_info: n_embd_v_gqa     = 2048
0.00.038.751 I print_info: f_norm_eps       = 1.0e-05
0.00.038.751 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.752 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.752 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.752 I print_info: f_logit_scale    = 0.0e+00
0.00.038.753 I print_info: n_ff             = 8192
0.00.038.753 I print_info: n_expert         = 0
0.00.038.753 I print_info: n_expert_used    = 0
0.00.038.753 I print_info: causal attn      = 1
0.00.038.753 I print_info: pooling type     = 0
0.00.038.753 I print_info: rope type        = 2
0.00.038.753 I print_info: rope scaling     = linear
0.00.038.755 I print_info: freq_base_train  = 10000.0
0.00.038.756 I print_info: freq_scale_train = 1
0.00.038.756 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.756 I print_info: rope_finetuned   = unknown
0.00.038.756 I print_info: ssm_d_conv       = 0
0.00.038.756 I print_info: ssm_d_inner      = 0
0.00.038.756 I print_info: ssm_d_state      = 0
0.00.038.756 I print_info: ssm_dt_rank      = 0
0.00.038.756 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.757 I print_info: model type       = 1.4B
0.00.038.757 I print_info: model params     = 1.41 B
0.00.038.757 I print_info: general.name     = 1.4B
0.00.038.758 I print_info: vocab type       = BPE
0.00.038.759 I print_info: n_vocab          = 50304
0.00.038.759 I print_info: n_merges         = 50009
0.00.038.760 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.761 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.761 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.761 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.761 I print_info: LF token         = 187 'Ċ'
0.00.038.761 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.762 I print_info: max token length = 1024
0.00.673.325 I load_tensors: offloading 24 repeating layers to GPU
0.00.673.346 I load_tensors: offloading output layer to GPU
0.00.673.346 I load_tensors: offloaded 25/25 layers to GPU
0.00.673.382 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.673.383 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.674.629 I llama_init_from_model: n_seq_max     = 1
0.00.674.636 I llama_init_from_model: n_ctx         = 128
0.00.674.636 I llama_init_from_model: n_ctx_per_seq = 128
0.00.674.637 I llama_init_from_model: n_batch       = 128
0.00.674.637 I llama_init_from_model: n_ubatch      = 128
0.00.674.638 I llama_init_from_model: flash_attn    = 0
0.00.674.640 I llama_init_from_model: freq_base     = 10000.0
0.00.674.640 I llama_init_from_model: freq_scale    = 1
0.00.674.641 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.674.644 I ggml_metal_init: allocating
0.00.674.725 I ggml_metal_init: found device: Apple M4
0.00.674.739 I ggml_metal_init: picking default device: Apple M4
0.00.676.657 I ggml_metal_init: using embedded metal library
0.00.682.923 I ggml_metal_init: GPU name:   Apple M4
0.00.682.930 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.682.930 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.682.931 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.682.932 I ggml_metal_init: simdgroup reduction   = true
0.00.682.933 I ggml_metal_init: simdgroup matrix mul. = true
0.00.682.933 I ggml_metal_init: has residency sets    = true
0.00.682.933 I ggml_metal_init: has bfloat            = true
0.00.682.933 I ggml_metal_init: use bfloat            = true
0.00.682.934 I ggml_metal_init: hasUnifiedMemory      = true
0.00.682.936 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.701.501 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.169 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.705.173 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.705.219 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.708.366 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.708.368 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.708.369 I llama_init_from_model: graph nodes  = 967
0.00.708.369 I llama_init_from_model: graph splits = 2
0.00.708.372 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.708.372 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.868 I 
0.00.736.930 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.935 I perplexity: tokenizing the input ..
0.00.743.867 I perplexity: tokenization took 6.928 ms
0.00.743.881 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.880.802 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.882.108 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.882.129 I llama_perf_context_print:        load time =     727.64 ms
0.00.882.130 I llama_perf_context_print: prompt eval time =     135.95 ms /   128 tokens (    1.06 ms per token,   941.52 tokens per second)
0.00.882.130 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.882.131 I llama_perf_context_print:       total time =     145.26 ms /   129 tokens
0.00.882.559 I ggml_metal_free: deallocating

real	0m0.897s
user	0m0.080s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.483 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.054 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.063 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.063 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.064 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.064 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.066 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.068 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.068 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.070 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.071 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.071 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.072 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.075 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.075 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.076 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.805 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.854 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.592 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.593 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.594 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.594 I llama_model_loader: - type  f32:  194 tensors
0.00.025.595 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.595 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.596 I print_info: file format = GGUF V3 (latest)
0.00.025.596 I print_info: file type   = Q5_0
0.00.025.597 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.536 I load: special tokens cache size = 25
0.00.039.336 I load: token to piece cache size = 0.2984 MB
0.00.039.339 I print_info: arch             = gptneox
0.00.039.340 I print_info: vocab_only       = 0
0.00.039.340 I print_info: n_ctx_train      = 2048
0.00.039.340 I print_info: n_embd           = 2048
0.00.039.340 I print_info: n_layer          = 24
0.00.039.344 I print_info: n_head           = 16
0.00.039.345 I print_info: n_head_kv        = 16
0.00.039.348 I print_info: n_rot            = 32
0.00.039.348 I print_info: n_swa            = 0
0.00.039.348 I print_info: n_embd_head_k    = 128
0.00.039.349 I print_info: n_embd_head_v    = 128
0.00.039.349 I print_info: n_gqa            = 1
0.00.039.350 I print_info: n_embd_k_gqa     = 2048
0.00.039.351 I print_info: n_embd_v_gqa     = 2048
0.00.039.351 I print_info: f_norm_eps       = 1.0e-05
0.00.039.352 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.352 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.352 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.352 I print_info: f_logit_scale    = 0.0e+00
0.00.039.353 I print_info: n_ff             = 8192
0.00.039.353 I print_info: n_expert         = 0
0.00.039.353 I print_info: n_expert_used    = 0
0.00.039.353 I print_info: causal attn      = 1
0.00.039.353 I print_info: pooling type     = 0
0.00.039.353 I print_info: rope type        = 2
0.00.039.355 I print_info: rope scaling     = linear
0.00.039.356 I print_info: freq_base_train  = 10000.0
0.00.039.356 I print_info: freq_scale_train = 1
0.00.039.356 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.356 I print_info: rope_finetuned   = unknown
0.00.039.356 I print_info: ssm_d_conv       = 0
0.00.039.356 I print_info: ssm_d_inner      = 0
0.00.039.357 I print_info: ssm_d_state      = 0
0.00.039.357 I print_info: ssm_dt_rank      = 0
0.00.039.357 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.357 I print_info: model type       = 1.4B
0.00.039.357 I print_info: model params     = 1.41 B
0.00.039.358 I print_info: general.name     = 1.4B
0.00.039.358 I print_info: vocab type       = BPE
0.00.039.358 I print_info: n_vocab          = 50304
0.00.039.362 I print_info: n_merges         = 50009
0.00.039.362 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.363 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.363 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.363 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.364 I print_info: LF token         = 187 'Ċ'
0.00.039.364 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.364 I print_info: max token length = 1024
0.00.703.997 I load_tensors: offloading 24 repeating layers to GPU
0.00.704.012 I load_tensors: offloading output layer to GPU
0.00.704.013 I load_tensors: offloaded 25/25 layers to GPU
0.00.704.043 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.704.044 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.705.257 I llama_init_from_model: n_seq_max     = 1
0.00.705.270 I llama_init_from_model: n_ctx         = 128
0.00.705.270 I llama_init_from_model: n_ctx_per_seq = 128
0.00.705.271 I llama_init_from_model: n_batch       = 128
0.00.705.271 I llama_init_from_model: n_ubatch      = 128
0.00.705.272 I llama_init_from_model: flash_attn    = 0
0.00.705.274 I llama_init_from_model: freq_base     = 10000.0
0.00.705.275 I llama_init_from_model: freq_scale    = 1
0.00.705.276 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.705.278 I ggml_metal_init: allocating
0.00.705.369 I ggml_metal_init: found device: Apple M4
0.00.705.385 I ggml_metal_init: picking default device: Apple M4
0.00.707.325 I ggml_metal_init: using embedded metal library
0.00.714.221 I ggml_metal_init: GPU name:   Apple M4
0.00.714.226 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.714.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.714.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.714.228 I ggml_metal_init: simdgroup reduction   = true
0.00.714.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.714.229 I ggml_metal_init: has residency sets    = true
0.00.714.229 I ggml_metal_init: has bfloat            = true
0.00.714.229 I ggml_metal_init: use bfloat            = true
0.00.714.230 I ggml_metal_init: hasUnifiedMemory      = true
0.00.714.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.732.461 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.190 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.736.196 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.736.249 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.739.601 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.739.603 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.739.603 I llama_init_from_model: graph nodes  = 967
0.00.739.604 I llama_init_from_model: graph splits = 2
0.00.739.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.739.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.914 I 
0.00.765.976 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.982 I perplexity: tokenizing the input ..
0.00.772.038 I perplexity: tokenization took 6.054 ms
0.00.772.043 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.906.707 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.908.046 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.908.071 I llama_perf_context_print:        load time =     755.42 ms
0.00.908.072 I llama_perf_context_print: prompt eval time =     134.35 ms /   128 tokens (    1.05 ms per token,   952.72 tokens per second)
0.00.908.073 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.908.073 I llama_perf_context_print:       total time =     142.16 ms /   129 tokens
0.00.908.480 I ggml_metal_free: deallocating

real	0m0.924s
user	0m0.078s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.322 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.100 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.106 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.107 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.112 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.112 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.113 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.113 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.114 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.115 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.115 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.115 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.116 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.116 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.117 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.118 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.119 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.119 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.883 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.874 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.724 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.726 I llama_model_loader: - type  f32:  194 tensors
0.00.024.726 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.726 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.727 I print_info: file format = GGUF V3 (latest)
0.00.024.728 I print_info: file type   = Q5_1
0.00.024.729 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.426 I load: special tokens cache size = 25
0.00.038.388 I load: token to piece cache size = 0.2984 MB
0.00.038.391 I print_info: arch             = gptneox
0.00.038.391 I print_info: vocab_only       = 0
0.00.038.391 I print_info: n_ctx_train      = 2048
0.00.038.391 I print_info: n_embd           = 2048
0.00.038.392 I print_info: n_layer          = 24
0.00.038.396 I print_info: n_head           = 16
0.00.038.396 I print_info: n_head_kv        = 16
0.00.038.396 I print_info: n_rot            = 32
0.00.038.397 I print_info: n_swa            = 0
0.00.038.399 I print_info: n_embd_head_k    = 128
0.00.038.400 I print_info: n_embd_head_v    = 128
0.00.038.400 I print_info: n_gqa            = 1
0.00.038.401 I print_info: n_embd_k_gqa     = 2048
0.00.038.402 I print_info: n_embd_v_gqa     = 2048
0.00.038.402 I print_info: f_norm_eps       = 1.0e-05
0.00.038.403 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.403 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.403 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.403 I print_info: f_logit_scale    = 0.0e+00
0.00.038.404 I print_info: n_ff             = 8192
0.00.038.404 I print_info: n_expert         = 0
0.00.038.406 I print_info: n_expert_used    = 0
0.00.038.406 I print_info: causal attn      = 1
0.00.038.406 I print_info: pooling type     = 0
0.00.038.406 I print_info: rope type        = 2
0.00.038.406 I print_info: rope scaling     = linear
0.00.038.407 I print_info: freq_base_train  = 10000.0
0.00.038.407 I print_info: freq_scale_train = 1
0.00.038.407 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.407 I print_info: rope_finetuned   = unknown
0.00.038.408 I print_info: ssm_d_conv       = 0
0.00.038.408 I print_info: ssm_d_inner      = 0
0.00.038.408 I print_info: ssm_d_state      = 0
0.00.038.408 I print_info: ssm_dt_rank      = 0
0.00.038.408 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.408 I print_info: model type       = 1.4B
0.00.038.409 I print_info: model params     = 1.41 B
0.00.038.409 I print_info: general.name     = 1.4B
0.00.038.413 I print_info: vocab type       = BPE
0.00.038.413 I print_info: n_vocab          = 50304
0.00.038.413 I print_info: n_merges         = 50009
0.00.038.414 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.414 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.414 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.414 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.414 I print_info: LF token         = 187 'Ċ'
0.00.038.415 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.415 I print_info: max token length = 1024
0.00.651.447 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.457 I load_tensors: offloading output layer to GPU
0.00.651.458 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.487 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.651.488 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.652.944 I llama_init_from_model: n_seq_max     = 1
0.00.652.947 I llama_init_from_model: n_ctx         = 128
0.00.652.948 I llama_init_from_model: n_ctx_per_seq = 128
0.00.652.948 I llama_init_from_model: n_batch       = 128
0.00.652.949 I llama_init_from_model: n_ubatch      = 128
0.00.652.949 I llama_init_from_model: flash_attn    = 0
0.00.652.950 I llama_init_from_model: freq_base     = 10000.0
0.00.652.951 I llama_init_from_model: freq_scale    = 1
0.00.652.951 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.652.953 I ggml_metal_init: allocating
0.00.652.970 I ggml_metal_init: found device: Apple M4
0.00.652.980 I ggml_metal_init: picking default device: Apple M4
0.00.654.398 I ggml_metal_init: using embedded metal library
0.00.660.497 I ggml_metal_init: GPU name:   Apple M4
0.00.660.501 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.660.501 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.660.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.660.503 I ggml_metal_init: simdgroup reduction   = true
0.00.660.503 I ggml_metal_init: simdgroup matrix mul. = true
0.00.660.503 I ggml_metal_init: has residency sets    = true
0.00.660.504 I ggml_metal_init: has bfloat            = true
0.00.660.504 I ggml_metal_init: use bfloat            = true
0.00.660.505 I ggml_metal_init: hasUnifiedMemory      = true
0.00.660.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.727 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.318 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.681.321 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.681.363 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.684.457 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.684.459 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.684.460 I llama_init_from_model: graph nodes  = 967
0.00.684.460 I llama_init_from_model: graph splits = 2
0.00.684.463 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.684.463 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.542 I 
0.00.710.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.620 I perplexity: tokenizing the input ..
0.00.717.347 I perplexity: tokenization took 6.725 ms
0.00.717.353 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.310 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.852.730 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.852.749 I llama_perf_context_print:        load time =     701.21 ms
0.00.852.750 I llama_perf_context_print: prompt eval time =     133.19 ms /   128 tokens (    1.04 ms per token,   961.00 tokens per second)
0.00.852.751 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.852.751 I llama_perf_context_print:       total time =     142.21 ms /   129 tokens
0.00.853.153 I ggml_metal_free: deallocating

real	0m0.868s
user	0m0.077s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.101 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.689 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.695 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.701 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.701 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.702 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.702 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.704 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.704 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.704 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.705 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.705 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.705 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.707 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.709 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.709 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.709 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.394 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.355 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.081 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.082 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.083 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.083 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.084 I llama_model_loader: - type  f32:  194 tensors
0.00.025.085 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.085 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.085 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.086 I print_info: file format = GGUF V3 (latest)
0.00.025.086 I print_info: file type   = Q2_K - Medium
0.00.025.088 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.908 I load: special tokens cache size = 25
0.00.038.657 I load: token to piece cache size = 0.2984 MB
0.00.038.660 I print_info: arch             = gptneox
0.00.038.661 I print_info: vocab_only       = 0
0.00.038.661 I print_info: n_ctx_train      = 2048
0.00.038.661 I print_info: n_embd           = 2048
0.00.038.661 I print_info: n_layer          = 24
0.00.038.665 I print_info: n_head           = 16
0.00.038.666 I print_info: n_head_kv        = 16
0.00.038.666 I print_info: n_rot            = 32
0.00.038.666 I print_info: n_swa            = 0
0.00.038.666 I print_info: n_embd_head_k    = 128
0.00.038.666 I print_info: n_embd_head_v    = 128
0.00.038.667 I print_info: n_gqa            = 1
0.00.038.668 I print_info: n_embd_k_gqa     = 2048
0.00.038.668 I print_info: n_embd_v_gqa     = 2048
0.00.038.669 I print_info: f_norm_eps       = 1.0e-05
0.00.038.669 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.669 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.670 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.670 I print_info: f_logit_scale    = 0.0e+00
0.00.038.670 I print_info: n_ff             = 8192
0.00.038.671 I print_info: n_expert         = 0
0.00.038.671 I print_info: n_expert_used    = 0
0.00.038.671 I print_info: causal attn      = 1
0.00.038.671 I print_info: pooling type     = 0
0.00.038.671 I print_info: rope type        = 2
0.00.038.672 I print_info: rope scaling     = linear
0.00.038.672 I print_info: freq_base_train  = 10000.0
0.00.038.672 I print_info: freq_scale_train = 1
0.00.038.672 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.672 I print_info: rope_finetuned   = unknown
0.00.038.673 I print_info: ssm_d_conv       = 0
0.00.038.673 I print_info: ssm_d_inner      = 0
0.00.038.673 I print_info: ssm_d_state      = 0
0.00.038.673 I print_info: ssm_dt_rank      = 0
0.00.038.673 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.674 I print_info: model type       = 1.4B
0.00.038.674 I print_info: model params     = 1.41 B
0.00.038.676 I print_info: general.name     = 1.4B
0.00.038.676 I print_info: vocab type       = BPE
0.00.038.677 I print_info: n_vocab          = 50304
0.00.038.677 I print_info: n_merges         = 50009
0.00.038.677 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.677 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.677 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.678 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.678 I print_info: LF token         = 187 'Ċ'
0.00.038.678 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.678 I print_info: max token length = 1024
0.00.337.116 I load_tensors: offloading 24 repeating layers to GPU
0.00.337.134 I load_tensors: offloading output layer to GPU
0.00.337.135 I load_tensors: offloaded 25/25 layers to GPU
0.00.337.164 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.337.166 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.338.745 I llama_init_from_model: n_seq_max     = 1
0.00.338.750 I llama_init_from_model: n_ctx         = 128
0.00.338.751 I llama_init_from_model: n_ctx_per_seq = 128
0.00.338.751 I llama_init_from_model: n_batch       = 128
0.00.338.752 I llama_init_from_model: n_ubatch      = 128
0.00.338.753 I llama_init_from_model: flash_attn    = 0
0.00.338.755 I llama_init_from_model: freq_base     = 10000.0
0.00.338.755 I llama_init_from_model: freq_scale    = 1
0.00.338.756 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.338.758 I ggml_metal_init: allocating
0.00.338.829 I ggml_metal_init: found device: Apple M4
0.00.338.843 I ggml_metal_init: picking default device: Apple M4
0.00.340.623 I ggml_metal_init: using embedded metal library
0.00.346.001 I ggml_metal_init: GPU name:   Apple M4
0.00.346.022 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.346.023 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.346.024 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.346.025 I ggml_metal_init: simdgroup reduction   = true
0.00.346.025 I ggml_metal_init: simdgroup matrix mul. = true
0.00.346.025 I ggml_metal_init: has residency sets    = true
0.00.346.026 I ggml_metal_init: has bfloat            = true
0.00.346.026 I ggml_metal_init: use bfloat            = true
0.00.346.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.346.032 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.367.284 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.371.091 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.371.098 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.371.141 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.374.554 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.374.556 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.374.557 I llama_init_from_model: graph nodes  = 967
0.00.374.557 I llama_init_from_model: graph splits = 2
0.00.374.560 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.374.560 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.404.837 I 
0.00.404.928 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.404.938 I perplexity: tokenizing the input ..
0.00.411.996 I perplexity: tokenization took 7.054 ms
0.00.412.006 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.555.830 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.557.169 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.557.190 I llama_perf_context_print:        load time =     394.73 ms
0.00.557.192 I llama_perf_context_print: prompt eval time =     142.88 ms /   128 tokens (    1.12 ms per token,   895.85 tokens per second)
0.00.557.193 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.557.193 I llama_perf_context_print:       total time =     152.36 ms /   129 tokens
0.00.557.574 I ggml_metal_free: deallocating

real	0m0.573s
user	0m0.080s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.400 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.383 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.394 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.395 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.395 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.396 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.396 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.397 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.397 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.398 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.398 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.398 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.399 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.399 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.401 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.401 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.401 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.144 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.938 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.940 I llama_model_loader: - type  f32:  194 tensors
0.00.024.940 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.940 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.940 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.941 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.941 I print_info: file format = GGUF V3 (latest)
0.00.024.942 I print_info: file type   = Q3_K - Medium
0.00.024.942 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.960 I load: special tokens cache size = 25
0.00.039.105 I load: token to piece cache size = 0.2984 MB
0.00.039.108 I print_info: arch             = gptneox
0.00.039.108 I print_info: vocab_only       = 0
0.00.039.108 I print_info: n_ctx_train      = 2048
0.00.039.109 I print_info: n_embd           = 2048
0.00.039.109 I print_info: n_layer          = 24
0.00.039.112 I print_info: n_head           = 16
0.00.039.113 I print_info: n_head_kv        = 16
0.00.039.113 I print_info: n_rot            = 32
0.00.039.114 I print_info: n_swa            = 0
0.00.039.114 I print_info: n_embd_head_k    = 128
0.00.039.114 I print_info: n_embd_head_v    = 128
0.00.039.115 I print_info: n_gqa            = 1
0.00.039.115 I print_info: n_embd_k_gqa     = 2048
0.00.039.116 I print_info: n_embd_v_gqa     = 2048
0.00.039.117 I print_info: f_norm_eps       = 1.0e-05
0.00.039.117 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.118 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.118 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.118 I print_info: f_logit_scale    = 0.0e+00
0.00.039.119 I print_info: n_ff             = 8192
0.00.039.119 I print_info: n_expert         = 0
0.00.039.119 I print_info: n_expert_used    = 0
0.00.039.119 I print_info: causal attn      = 1
0.00.039.119 I print_info: pooling type     = 0
0.00.039.119 I print_info: rope type        = 2
0.00.039.121 I print_info: rope scaling     = linear
0.00.039.122 I print_info: freq_base_train  = 10000.0
0.00.039.122 I print_info: freq_scale_train = 1
0.00.039.122 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.122 I print_info: rope_finetuned   = unknown
0.00.039.123 I print_info: ssm_d_conv       = 0
0.00.039.123 I print_info: ssm_d_inner      = 0
0.00.039.123 I print_info: ssm_d_state      = 0
0.00.039.123 I print_info: ssm_dt_rank      = 0
0.00.039.123 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.123 I print_info: model type       = 1.4B
0.00.039.124 I print_info: model params     = 1.41 B
0.00.039.124 I print_info: general.name     = 1.4B
0.00.039.124 I print_info: vocab type       = BPE
0.00.039.124 I print_info: n_vocab          = 50304
0.00.039.125 I print_info: n_merges         = 50009
0.00.039.125 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.126 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.128 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.128 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.128 I print_info: LF token         = 187 'Ċ'
0.00.039.128 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.129 I print_info: max token length = 1024
0.00.434.185 I load_tensors: offloading 24 repeating layers to GPU
0.00.434.197 I load_tensors: offloading output layer to GPU
0.00.434.198 I load_tensors: offloaded 25/25 layers to GPU
0.00.434.232 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.434.234 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.435.773 I llama_init_from_model: n_seq_max     = 1
0.00.435.780 I llama_init_from_model: n_ctx         = 128
0.00.435.780 I llama_init_from_model: n_ctx_per_seq = 128
0.00.435.781 I llama_init_from_model: n_batch       = 128
0.00.435.781 I llama_init_from_model: n_ubatch      = 128
0.00.435.782 I llama_init_from_model: flash_attn    = 0
0.00.435.784 I llama_init_from_model: freq_base     = 10000.0
0.00.435.784 I llama_init_from_model: freq_scale    = 1
0.00.435.785 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.435.800 I ggml_metal_init: allocating
0.00.435.872 I ggml_metal_init: found device: Apple M4
0.00.435.885 I ggml_metal_init: picking default device: Apple M4
0.00.437.735 I ggml_metal_init: using embedded metal library
0.00.443.424 I ggml_metal_init: GPU name:   Apple M4
0.00.443.429 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.443.430 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.443.431 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.443.431 I ggml_metal_init: simdgroup reduction   = true
0.00.443.432 I ggml_metal_init: simdgroup matrix mul. = true
0.00.443.432 I ggml_metal_init: has residency sets    = true
0.00.443.432 I ggml_metal_init: has bfloat            = true
0.00.443.433 I ggml_metal_init: use bfloat            = true
0.00.443.434 I ggml_metal_init: hasUnifiedMemory      = true
0.00.443.435 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.463.287 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.466.846 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.466.849 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.466.893 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.470.309 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.470.312 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.470.312 I llama_init_from_model: graph nodes  = 967
0.00.470.312 I llama_init_from_model: graph splits = 2
0.00.470.316 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.470.316 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.498.144 I 
0.00.498.228 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.498.235 I perplexity: tokenizing the input ..
0.00.505.138 I perplexity: tokenization took 6.901 ms
0.00.505.144 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.649.415 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.650.762 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.650.793 I llama_perf_context_print:        load time =     488.74 ms
0.00.650.794 I llama_perf_context_print: prompt eval time =     143.72 ms /   128 tokens (    1.12 ms per token,   890.63 tokens per second)
0.00.650.795 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.650.799 I llama_perf_context_print:       total time =     152.65 ms /   129 tokens
0.00.651.198 I ggml_metal_free: deallocating

real	0m0.664s
user	0m0.079s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.913 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.967 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.972 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.974 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.974 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.975 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.975 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.975 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.976 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.977 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.977 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.980 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.980 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.980 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.981 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.983 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.983 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.983 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.768 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.749 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.501 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.501 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.501 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.502 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.502 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.502 I llama_model_loader: - type  f32:  194 tensors
0.00.024.503 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.503 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.503 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.503 I print_info: file format = GGUF V3 (latest)
0.00.024.504 I print_info: file type   = Q4_K - Medium
0.00.024.505 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.182 I load: special tokens cache size = 25
0.00.038.153 I load: token to piece cache size = 0.2984 MB
0.00.038.156 I print_info: arch             = gptneox
0.00.038.156 I print_info: vocab_only       = 0
0.00.038.156 I print_info: n_ctx_train      = 2048
0.00.038.156 I print_info: n_embd           = 2048
0.00.038.157 I print_info: n_layer          = 24
0.00.038.159 I print_info: n_head           = 16
0.00.038.160 I print_info: n_head_kv        = 16
0.00.038.162 I print_info: n_rot            = 32
0.00.038.162 I print_info: n_swa            = 0
0.00.038.162 I print_info: n_embd_head_k    = 128
0.00.038.162 I print_info: n_embd_head_v    = 128
0.00.038.163 I print_info: n_gqa            = 1
0.00.038.164 I print_info: n_embd_k_gqa     = 2048
0.00.038.164 I print_info: n_embd_v_gqa     = 2048
0.00.038.165 I print_info: f_norm_eps       = 1.0e-05
0.00.038.165 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.165 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.166 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.166 I print_info: f_logit_scale    = 0.0e+00
0.00.038.166 I print_info: n_ff             = 8192
0.00.038.167 I print_info: n_expert         = 0
0.00.038.167 I print_info: n_expert_used    = 0
0.00.038.167 I print_info: causal attn      = 1
0.00.038.167 I print_info: pooling type     = 0
0.00.038.167 I print_info: rope type        = 2
0.00.038.167 I print_info: rope scaling     = linear
0.00.038.168 I print_info: freq_base_train  = 10000.0
0.00.038.168 I print_info: freq_scale_train = 1
0.00.038.168 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.169 I print_info: rope_finetuned   = unknown
0.00.038.169 I print_info: ssm_d_conv       = 0
0.00.038.169 I print_info: ssm_d_inner      = 0
0.00.038.169 I print_info: ssm_d_state      = 0
0.00.038.169 I print_info: ssm_dt_rank      = 0
0.00.038.169 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.170 I print_info: model type       = 1.4B
0.00.038.170 I print_info: model params     = 1.41 B
0.00.038.174 I print_info: general.name     = 1.4B
0.00.038.175 I print_info: vocab type       = BPE
0.00.038.175 I print_info: n_vocab          = 50304
0.00.038.175 I print_info: n_merges         = 50009
0.00.038.175 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.175 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.176 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.176 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.176 I print_info: LF token         = 187 'Ċ'
0.00.038.176 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.176 I print_info: max token length = 1024
0.00.520.868 I load_tensors: offloading 24 repeating layers to GPU
0.00.520.879 I load_tensors: offloading output layer to GPU
0.00.520.880 I load_tensors: offloaded 25/25 layers to GPU
0.00.520.910 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.520.912 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.522.339 I llama_init_from_model: n_seq_max     = 1
0.00.522.347 I llama_init_from_model: n_ctx         = 128
0.00.522.351 I llama_init_from_model: n_ctx_per_seq = 128
0.00.522.352 I llama_init_from_model: n_batch       = 128
0.00.522.352 I llama_init_from_model: n_ubatch      = 128
0.00.522.353 I llama_init_from_model: flash_attn    = 0
0.00.522.354 I llama_init_from_model: freq_base     = 10000.0
0.00.522.354 I llama_init_from_model: freq_scale    = 1
0.00.522.355 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.522.358 I ggml_metal_init: allocating
0.00.522.421 I ggml_metal_init: found device: Apple M4
0.00.522.435 I ggml_metal_init: picking default device: Apple M4
0.00.524.675 I ggml_metal_init: using embedded metal library
0.00.529.593 I ggml_metal_init: GPU name:   Apple M4
0.00.529.598 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.529.598 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.529.599 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.529.599 I ggml_metal_init: simdgroup reduction   = true
0.00.529.599 I ggml_metal_init: simdgroup matrix mul. = true
0.00.529.599 I ggml_metal_init: has residency sets    = true
0.00.529.599 I ggml_metal_init: has bfloat            = true
0.00.529.600 I ggml_metal_init: use bfloat            = true
0.00.529.600 I ggml_metal_init: hasUnifiedMemory      = true
0.00.529.602 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.540.280 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.542.146 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.542.149 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.542.177 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.543.908 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.543.909 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.543.909 I llama_init_from_model: graph nodes  = 967
0.00.543.909 I llama_init_from_model: graph splits = 2
0.00.543.911 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.543.911 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.567.671 I 
0.00.567.706 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.567.710 I perplexity: tokenizing the input ..
0.00.571.746 I perplexity: tokenization took 4.035 ms
0.00.571.753 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.705.271 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.706.695 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.706.717 I llama_perf_context_print:        load time =     558.75 ms
0.00.706.720 I llama_perf_context_print: prompt eval time =     133.28 ms /   128 tokens (    1.04 ms per token,   960.39 tokens per second)
0.00.706.720 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.706.721 I llama_perf_context_print:       total time =     139.05 ms /   129 tokens
0.00.707.105 I ggml_metal_free: deallocating

real	0m0.722s
user	0m0.065s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.835 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.883 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.888 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.890 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.891 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.891 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.891 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.892 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.893 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.893 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.894 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.899 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.665 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.404 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.406 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.407 I llama_model_loader: - type  f32:  194 tensors
0.00.026.407 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.407 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.408 I print_info: file format = GGUF V3 (latest)
0.00.026.408 I print_info: file type   = Q5_K - Medium
0.00.026.409 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.468 I load: special tokens cache size = 25
0.00.040.533 I load: token to piece cache size = 0.2984 MB
0.00.040.535 I print_info: arch             = gptneox
0.00.040.536 I print_info: vocab_only       = 0
0.00.040.536 I print_info: n_ctx_train      = 2048
0.00.040.536 I print_info: n_embd           = 2048
0.00.040.536 I print_info: n_layer          = 24
0.00.040.540 I print_info: n_head           = 16
0.00.040.540 I print_info: n_head_kv        = 16
0.00.040.541 I print_info: n_rot            = 32
0.00.040.541 I print_info: n_swa            = 0
0.00.040.541 I print_info: n_embd_head_k    = 128
0.00.040.541 I print_info: n_embd_head_v    = 128
0.00.040.543 I print_info: n_gqa            = 1
0.00.040.544 I print_info: n_embd_k_gqa     = 2048
0.00.040.544 I print_info: n_embd_v_gqa     = 2048
0.00.040.545 I print_info: f_norm_eps       = 1.0e-05
0.00.040.545 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.547 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.547 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.547 I print_info: f_logit_scale    = 0.0e+00
0.00.040.548 I print_info: n_ff             = 8192
0.00.040.548 I print_info: n_expert         = 0
0.00.040.549 I print_info: n_expert_used    = 0
0.00.040.549 I print_info: causal attn      = 1
0.00.040.549 I print_info: pooling type     = 0
0.00.040.549 I print_info: rope type        = 2
0.00.040.549 I print_info: rope scaling     = linear
0.00.040.550 I print_info: freq_base_train  = 10000.0
0.00.040.550 I print_info: freq_scale_train = 1
0.00.040.551 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.551 I print_info: rope_finetuned   = unknown
0.00.040.551 I print_info: ssm_d_conv       = 0
0.00.040.552 I print_info: ssm_d_inner      = 0
0.00.040.552 I print_info: ssm_d_state      = 0
0.00.040.552 I print_info: ssm_dt_rank      = 0
0.00.040.552 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.552 I print_info: model type       = 1.4B
0.00.040.553 I print_info: model params     = 1.41 B
0.00.040.553 I print_info: general.name     = 1.4B
0.00.040.553 I print_info: vocab type       = BPE
0.00.040.553 I print_info: n_vocab          = 50304
0.00.040.553 I print_info: n_merges         = 50009
0.00.040.554 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.554 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.554 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.555 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.555 I print_info: LF token         = 187 'Ċ'
0.00.040.556 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.556 I print_info: max token length = 1024
0.00.603.021 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.038 I load_tensors: offloading output layer to GPU
0.00.603.038 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.070 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.603.072 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.604.647 I llama_init_from_model: n_seq_max     = 1
0.00.604.653 I llama_init_from_model: n_ctx         = 128
0.00.604.654 I llama_init_from_model: n_ctx_per_seq = 128
0.00.604.654 I llama_init_from_model: n_batch       = 128
0.00.604.655 I llama_init_from_model: n_ubatch      = 128
0.00.604.656 I llama_init_from_model: flash_attn    = 0
0.00.604.658 I llama_init_from_model: freq_base     = 10000.0
0.00.604.659 I llama_init_from_model: freq_scale    = 1
0.00.604.659 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.604.662 I ggml_metal_init: allocating
0.00.604.737 I ggml_metal_init: found device: Apple M4
0.00.604.750 I ggml_metal_init: picking default device: Apple M4
0.00.606.350 I ggml_metal_init: using embedded metal library
0.00.612.746 I ggml_metal_init: GPU name:   Apple M4
0.00.612.750 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.751 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.752 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.752 I ggml_metal_init: simdgroup reduction   = true
0.00.612.753 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.753 I ggml_metal_init: has residency sets    = true
0.00.612.753 I ggml_metal_init: has bfloat            = true
0.00.612.754 I ggml_metal_init: use bfloat            = true
0.00.612.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.756 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.877 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.381 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.633.387 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.633.430 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.636.629 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.636.631 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.636.632 I llama_init_from_model: graph nodes  = 967
0.00.636.632 I llama_init_from_model: graph splits = 2
0.00.636.635 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.636.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.980 I 
0.00.666.066 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.092 I perplexity: tokenizing the input ..
0.00.672.831 I perplexity: tokenization took 6.735 ms
0.00.672.838 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.813.723 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.815.061 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.815.086 I llama_perf_context_print:        load time =     655.13 ms
0.00.815.087 I llama_perf_context_print: prompt eval time =     139.97 ms /   128 tokens (    1.09 ms per token,   914.50 tokens per second)
0.00.815.088 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.088 I llama_perf_context_print:       total time =     149.11 ms /   129 tokens
0.00.815.480 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.078s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.871 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.769 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.774 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.780 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.780 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.782 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.782 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.783 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.784 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.784 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.785 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.785 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.785 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.786 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.788 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.788 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.793 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.659 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.481 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.482 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.482 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.482 I llama_model_loader: - type  f32:  194 tensors
0.00.024.483 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.483 I print_info: file format = GGUF V3 (latest)
0.00.024.484 I print_info: file type   = Q6_K
0.00.024.484 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.209 I load: special tokens cache size = 25
0.00.038.258 I load: token to piece cache size = 0.2984 MB
0.00.038.261 I print_info: arch             = gptneox
0.00.038.262 I print_info: vocab_only       = 0
0.00.038.262 I print_info: n_ctx_train      = 2048
0.00.038.262 I print_info: n_embd           = 2048
0.00.038.262 I print_info: n_layer          = 24
0.00.038.266 I print_info: n_head           = 16
0.00.038.266 I print_info: n_head_kv        = 16
0.00.038.267 I print_info: n_rot            = 32
0.00.038.267 I print_info: n_swa            = 0
0.00.038.267 I print_info: n_embd_head_k    = 128
0.00.038.269 I print_info: n_embd_head_v    = 128
0.00.038.270 I print_info: n_gqa            = 1
0.00.038.271 I print_info: n_embd_k_gqa     = 2048
0.00.038.272 I print_info: n_embd_v_gqa     = 2048
0.00.038.272 I print_info: f_norm_eps       = 1.0e-05
0.00.038.274 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.274 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.274 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.274 I print_info: f_logit_scale    = 0.0e+00
0.00.038.275 I print_info: n_ff             = 8192
0.00.038.275 I print_info: n_expert         = 0
0.00.038.275 I print_info: n_expert_used    = 0
0.00.038.275 I print_info: causal attn      = 1
0.00.038.275 I print_info: pooling type     = 0
0.00.038.276 I print_info: rope type        = 2
0.00.038.276 I print_info: rope scaling     = linear
0.00.038.276 I print_info: freq_base_train  = 10000.0
0.00.038.276 I print_info: freq_scale_train = 1
0.00.038.277 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.277 I print_info: rope_finetuned   = unknown
0.00.038.277 I print_info: ssm_d_conv       = 0
0.00.038.277 I print_info: ssm_d_inner      = 0
0.00.038.277 I print_info: ssm_d_state      = 0
0.00.038.277 I print_info: ssm_dt_rank      = 0
0.00.038.277 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.278 I print_info: model type       = 1.4B
0.00.038.278 I print_info: model params     = 1.41 B
0.00.038.282 I print_info: general.name     = 1.4B
0.00.038.283 I print_info: vocab type       = BPE
0.00.038.283 I print_info: n_vocab          = 50304
0.00.038.283 I print_info: n_merges         = 50009
0.00.038.284 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.284 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.284 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.284 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.284 I print_info: LF token         = 187 'Ċ'
0.00.038.285 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.285 I print_info: max token length = 1024
0.00.621.232 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.236 I load_tensors: offloading output layer to GPU
0.00.621.237 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.264 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.621.269 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.622.391 I llama_init_from_model: n_seq_max     = 1
0.00.622.393 I llama_init_from_model: n_ctx         = 128
0.00.622.394 I llama_init_from_model: n_ctx_per_seq = 128
0.00.622.394 I llama_init_from_model: n_batch       = 128
0.00.622.398 I llama_init_from_model: n_ubatch      = 128
0.00.622.398 I llama_init_from_model: flash_attn    = 0
0.00.622.399 I llama_init_from_model: freq_base     = 10000.0
0.00.622.399 I llama_init_from_model: freq_scale    = 1
0.00.622.400 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.401 I ggml_metal_init: allocating
0.00.622.463 I ggml_metal_init: found device: Apple M4
0.00.622.473 I ggml_metal_init: picking default device: Apple M4
0.00.623.838 I ggml_metal_init: using embedded metal library
0.00.629.436 I ggml_metal_init: GPU name:   Apple M4
0.00.629.439 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.440 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.441 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.441 I ggml_metal_init: simdgroup reduction   = true
0.00.629.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.442 I ggml_metal_init: has residency sets    = true
0.00.629.442 I ggml_metal_init: has bfloat            = true
0.00.629.442 I ggml_metal_init: use bfloat            = true
0.00.629.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.645.269 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.648.753 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.648.757 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.648.798 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.651.968 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.651.970 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.651.970 I llama_init_from_model: graph nodes  = 967
0.00.651.971 I llama_init_from_model: graph splits = 2
0.00.651.973 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.651.974 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.934 I 
0.00.685.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.023 I perplexity: tokenizing the input ..
0.00.692.473 I perplexity: tokenization took 7.447 ms
0.00.692.484 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.917 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.835.349 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.835.370 I llama_perf_context_print:        load time =     676.05 ms
0.00.835.370 I llama_perf_context_print: prompt eval time =     140.56 ms /   128 tokens (    1.10 ms per token,   910.67 tokens per second)
0.00.835.371 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.371 I llama_perf_context_print:       total time =     150.44 ms /   129 tokens
0.00.835.732 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.076s
sys	0m0.137s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.284 I build: 4627 (b3451785) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.128 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.229 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.236 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.239 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.240 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.240 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.241 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.242 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.243 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.244 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.245 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.245 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.246 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.247 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.248 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.250 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.251 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.972 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.011 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.710 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.712 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.713 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.713 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.714 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.714 I llama_model_loader: - type  f32:  194 tensors
0.00.050.714 I llama_model_loader: - type  f16:   98 tensors
0.00.050.715 I print_info: file format = GGUF V3 (latest)
0.00.050.716 I print_info: file type   = all F32 (guessed)
0.00.050.717 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.673 I load: special tokens cache size = 25
0.00.070.327 I load: token to piece cache size = 0.2984 MB
0.00.070.330 I print_info: arch             = gptneox
0.00.070.330 I print_info: vocab_only       = 0
0.00.070.330 I print_info: n_ctx_train      = 2048
0.00.070.331 I print_info: n_embd           = 2048
0.00.070.331 I print_info: n_layer          = 24
0.00.070.333 I print_info: n_head           = 16
0.00.070.334 I print_info: n_head_kv        = 16
0.00.070.334 I print_info: n_rot            = 32
0.00.070.335 I print_info: n_swa            = 0
0.00.070.337 I print_info: n_embd_head_k    = 128
0.00.070.337 I print_info: n_embd_head_v    = 128
0.00.070.338 I print_info: n_gqa            = 1
0.00.070.338 I print_info: n_embd_k_gqa     = 2048
0.00.070.339 I print_info: n_embd_v_gqa     = 2048
0.00.070.344 I print_info: f_norm_eps       = 1.0e-05
0.00.070.344 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.344 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.345 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.345 I print_info: f_logit_scale    = 0.0e+00
0.00.070.346 I print_info: n_ff             = 8192
0.00.070.346 I print_info: n_expert         = 0
0.00.070.346 I print_info: n_expert_used    = 0
0.00.070.346 I print_info: causal attn      = 1
0.00.070.346 I print_info: pooling type     = 0
0.00.070.347 I print_info: rope type        = 2
0.00.070.348 I print_info: rope scaling     = linear
0.00.070.349 I print_info: freq_base_train  = 10000.0
0.00.070.349 I print_info: freq_scale_train = 1
0.00.070.349 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.349 I print_info: rope_finetuned   = unknown
0.00.070.349 I print_info: ssm_d_conv       = 0
0.00.070.349 I print_info: ssm_d_inner      = 0
0.00.070.350 I print_info: ssm_d_state      = 0
0.00.070.350 I print_info: ssm_dt_rank      = 0
0.00.070.350 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.350 I print_info: model type       = 1.4B
0.00.070.350 I print_info: model params     = 1.41 B
0.00.070.353 I print_info: general.name     = 1.4B
0.00.070.355 I print_info: vocab type       = BPE
0.00.070.355 I print_info: n_vocab          = 50304
0.00.070.355 I print_info: n_merges         = 50009
0.00.070.356 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.356 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.356 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.356 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.356 I print_info: LF token         = 187 'Ċ'
0.00.070.357 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.357 I print_info: max token length = 1024
0.01.333.526 I load_tensors: offloading 24 repeating layers to GPU
0.01.333.531 I load_tensors: offloading output layer to GPU
0.01.333.531 I load_tensors: offloaded 25/25 layers to GPU
0.01.333.563 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.333.565 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.334.382 I llama_init_from_model: n_seq_max     = 1
0.01.334.383 I llama_init_from_model: n_ctx         = 128
0.01.334.384 I llama_init_from_model: n_ctx_per_seq = 128
0.01.334.384 I llama_init_from_model: n_batch       = 128
0.01.334.384 I llama_init_from_model: n_ubatch      = 128
0.01.334.384 I llama_init_from_model: flash_attn    = 0
0.01.334.385 I llama_init_from_model: freq_base     = 10000.0
0.01.334.385 I llama_init_from_model: freq_scale    = 1
0.01.334.386 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.334.390 I ggml_metal_init: allocating
0.01.334.463 I ggml_metal_init: found device: Apple M4
0.01.334.470 I ggml_metal_init: picking default device: Apple M4
0.01.335.646 I ggml_metal_init: using embedded metal library
0.01.339.460 I ggml_metal_init: GPU name:   Apple M4
0.01.339.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.339.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.339.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.339.464 I ggml_metal_init: simdgroup reduction   = true
0.01.339.464 I ggml_metal_init: simdgroup matrix mul. = true
0.01.339.464 I ggml_metal_init: has residency sets    = true
0.01.339.464 I ggml_metal_init: has bfloat            = true
0.01.339.465 I ggml_metal_init: use bfloat            = true
0.01.339.465 I ggml_metal_init: hasUnifiedMemory      = true
0.01.339.466 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.350.091 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.351.746 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.351.748 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.351.774 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.353.307 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.353.308 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.353.309 I llama_init_from_model: graph nodes  = 967
0.01.353.309 I llama_init_from_model: graph splits = 2
0.01.353.310 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.353.311 I 
0.01.353.348 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.353.350 I compute_imatrix: tokenizing the input ..
0.01.357.458 I compute_imatrix: tokenization took 4.108 ms
0.01.357.460 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.630.089 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.632.513 I llama_perf_context_print:        load time =    1608.96 ms
0.01.632.514 I llama_perf_context_print: prompt eval time =     270.90 ms /   128 tokens (    2.12 ms per token,   472.50 tokens per second)
0.01.632.515 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.632.515 I llama_perf_context_print:       total time =    1611.38 ms /   129 tokens
0.01.633.127 I ggml_metal_free: deallocating

real	0m1.821s
user	0m0.122s
sys	0m0.263s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4627 (b3451785)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151d056d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151d05d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151d061b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151d06620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151d06a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151d06f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151d07370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151d07be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151d08080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151d08520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151d089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151d08c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151d09670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151d09e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151d0a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151d0ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151d0b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151d0bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151d0c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151d0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151d0d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151d0daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151d0e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151d0ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151d0f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151d0f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151d0fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151d106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151d10c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151d10ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151d11360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151d11620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151d11eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151d123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151d126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151d12b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151d12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151d13490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151d13930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151d13dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151d14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151d14710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151d14bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151d15050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151d15310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151d15920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151d15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151d16850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151d16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151d17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151d17a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151d18090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151d186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151d18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151d194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151d19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151d19de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151d1a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151d1a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151d1aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151d1b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151d1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151d1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151d1bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151d1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151d1c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151d1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151d1d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151d1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151d1db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151d1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151d1e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151d1e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151d1ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151d1f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151d1f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151d1fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151d20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151d208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151d20e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151d21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151d218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151d21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151d22350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151d228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151d22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151d23340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151d23890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151d23de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151d24330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151d24880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151d24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151d25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151d25870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151d25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151d26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151d26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151d16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151d26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151d27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151d279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151d27f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151d28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151d289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151d28f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151d29460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151d299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151d29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151d2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151d2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151d2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151d2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151d2b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151d2be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151d2c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151d2c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151d2cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151d2d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151d2d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151d2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151d2de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151d2e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151d2e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151d2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151d2f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151d2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151d2fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151d2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151d30390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151d30830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151d30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151d31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151d31610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151d31ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151d31f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151d323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151d32890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151d32d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151d331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151d33670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151d33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151d33fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x151d34450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151d348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151d34d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x151d35230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151d356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x151d35b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151d36010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151d364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151d36950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151d36df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151d37290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151d37730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151d37bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151d38070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151d38510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151d389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151d38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151d392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151d39790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151d39c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151d3a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151d3a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151d3aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151d3aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151d3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151d3b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151d3bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151d3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151d3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151d3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151d3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151d3d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151d3d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151d3dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151d3e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151d3e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151d3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151d3ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151d3f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151d3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151d3fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151d401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151d40690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151d40b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151d40fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151d41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151d41910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151d41db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151d42250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151d426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151d42b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151d430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151d43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x151d43b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151d440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151d44390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151d449a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151d44fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x151d455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151d45db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151d46250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x151d46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151d46b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151d47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151d47920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151d47dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151d48260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151d48700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151d48eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151d49400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151d49950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151d49ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151d4a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151d4a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151d4ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151d4b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151d4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151d4be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151d4c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151d4c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151d4ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151d4d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151d4d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151d4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151d4e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151d4e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151d4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151d4f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151d4f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151d4fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151d50390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151d508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151d50e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151d51380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151d518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151d51e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151d52370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151d528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151d52e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151d53360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151d538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151d53e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151d54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151d548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151d54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x151d55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151d55890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151d55de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x151d56330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151d56880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151d56dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x151d57320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151d57870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151d57dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151d58310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151d58860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x151d58db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151d59300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151d59850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151d59da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151d5a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151d5a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151d5ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151d5b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151d5b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151d5bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151d5c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151d5c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151d5cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151d5cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151d5d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151d5d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151d5dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151d5e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151d5e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151d5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151d5efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151d5f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151d5f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151d5fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151d602e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151d60a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151d61120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151d61840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151d61f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151d62220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151d62a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151d62cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151d632e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.731.121 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.731.126 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151d62f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151d44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151d44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151d45270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151d18350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151d17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151d0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151d161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151d16b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151d17120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151d15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151d155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151d17730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151d190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151d26f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151d624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151d11a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151d07770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151d45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151d0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151d0ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151d10290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151d63740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151d63a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151d63cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151d63f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151d64240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151d64500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151d647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151d64a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151d64d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151d65000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151d652c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151d65580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151d65840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151d65b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151d65dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151d66080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151d66340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151d66600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151d668c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151d66b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151d66e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151d67100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151d673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151d67680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151d67940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151d67c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151d67ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151d68180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151d68440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151d68700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151d689c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151d68c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151d68f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151d69200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151d694c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151d69780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151d69a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151d69d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151d69fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151d6a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151d6a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151d6a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151d6aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151d6ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151d6b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151d6b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151d6b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151d6b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151d6bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151d6be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151d6c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151d6c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151d6c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151d6c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151d6cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151d6ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151d6d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151d6d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151d6d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151d6d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151d6dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151d6df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151d6e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151d6e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151d6e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151d6ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151d6ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151d6ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151d6f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151d6f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151d6f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151d6fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151d6fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151d70000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151d702c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151d70580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151d70840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151d70b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151d70dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151d71080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151d71340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151d71600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151d718c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151d71b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151d71e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151d72100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151d723c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151d72680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151d72940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151d72c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151d72ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151d73180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151d73440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151d73700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151d739c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151d73c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151d73f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151d74200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151d744c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151d74780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151d74a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151d74d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151d74fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151d75280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151d75540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151d75800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151d75ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151d75d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151d76040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151d76300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151d765c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151d76880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151d76b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151d76e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151d770c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151d77380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151d77640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151d77900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151d77bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151d77e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x151d78140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151d78400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151d786c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x151d78980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151d78c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x151d78f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151d791c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151d79480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151d79740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151d79a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151d79cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151d79f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151d7a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151d7a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151d7a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151d7aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151d7ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151d7b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151d7b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151d7b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151d7b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151d7bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151d7bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151d7c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151d7c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151d7c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151d7c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151d7cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151d7ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151d7d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151d7d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151d7d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151d7d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151d7dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151d7dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151d7e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151d7e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151d7e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151d7e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151d7ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151d7ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151d7f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151d7f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151d7f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151d7fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151d7fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151d7ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151d80280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151d80540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151d80800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151d80ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151d80d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151d81040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x151d81300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151d815c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151d81880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151d81b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151d81e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x151d820c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151d82380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151d82640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x151d82900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151d82bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151d83100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151d833c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151d837c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151d83c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151d84100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151d848b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151d84b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151d84e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151d852a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151d85710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151d85b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151d85ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151d86460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151d868d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151d86d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151d871b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151d87620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151d87a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151d87f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151d88370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151d887e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151d88c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151d890c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151d89530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151d899a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151d89e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151d8a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151d8a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151d8ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151d8afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151d8b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151d8b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151d8bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151d8c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151d8c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151d8ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151d8cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151d8d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151d8d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151d8dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151d8e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151d8e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x151d8e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151d8edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151d8f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x151d8f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151d8fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151d8ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x151d90420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151d90890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151d90d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151d91170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151d915e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x151d91a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151d91ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151d92330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151d927a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151d92c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151d93080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151d934f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151d93960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151d93dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151d94240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151d946b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151d94b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151d94f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151d95400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151d95870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151d95ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151d96150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151d965c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151d96a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151d96ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151d97310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151d97780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151d97bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151d98060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151d984d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151d98f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151d99660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151d99d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151d9a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151d9a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151d9af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151d9b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151d9b820 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1368046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136804b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136804fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136805430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1368058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136805d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136806180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1368065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136806a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136806ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136807340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136807a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136808580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136808d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136809540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136809c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13680a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13680aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13680b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13680b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13680c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13680c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13680ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13680d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13680dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13680df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13680e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13680e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13680eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13680ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13680f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13680f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13680fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136810030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1368104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136810910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136810d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1368111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136811660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136811ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136811f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1368123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136812820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136812c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136813100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136813570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1368139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136813e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1368142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136814730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136814ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136815010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136815480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1368158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136815d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1368161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136816740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136816c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1368170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136817520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136817990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136817e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136818270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1368186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136818b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136818fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136819430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1368198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136819d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13681a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13681a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13681aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13681aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13681b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13681b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13681bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13681c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13681c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13681c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13681cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13681d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13681d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13681db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13681dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13681e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13681e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13681ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13681f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13681f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13681fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13681feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136820320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136820790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136820c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136821070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1368214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136821950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136821dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136822230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1368226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136822b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136822f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1368233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136823c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136823f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1368243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136824820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136824c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136825100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136825570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1368259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136825e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1368262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136826730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136826ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136827010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136827480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1368278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136827d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1368281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136828640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136828ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136828f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136829390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136829800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136829c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13682a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13682a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13682a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13682ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13682b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13682b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13682bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13682bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13682c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13682c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13682cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13682d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13682d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13682da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13682df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13682e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13682e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13682ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13682f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13682f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13682f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13682fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136830280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1368306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136830b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136830fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136831440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1368318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136831d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136832190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136832600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136832a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136832ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136833350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1368337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136833c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1368340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136834510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136834980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136834df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136835260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1368356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136835b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136835fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136836420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136836890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136836d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136837170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1368375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136837a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136837ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136838330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1368387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136838c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136839080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1368394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136839960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136839dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13683a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13683a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13683ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13683af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13683b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13683b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13683bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13683c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13683c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13683ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13683cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13683d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13683d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13683dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13683e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13683e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13683e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13683edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13683f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13683f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13683fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13683ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1368403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136840850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136840cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136841130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136841cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136841f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136842230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1368426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136842b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136842f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1368433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136843860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136843cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136844140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1368445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136844a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136844e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136845300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136845770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136845be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136846050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1368464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136846930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136846da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136847210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136847680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136847af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136847f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1368483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136848840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136848cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136849120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136849590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136849a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136849e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13684a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13684a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13684abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13684b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13684b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13684b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13684bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13684c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13684c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13684cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13684cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13684d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13684d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13684dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13684e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13684e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13684e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13684ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13684f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13684f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13684fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136850010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136850480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1368508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136850d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1368511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136851640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136851ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136851f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136852390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136852800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136852c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1368530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136853550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1368539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136853e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1368542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136854710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136854b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136854ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136855460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1368558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136856340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136856a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136857180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1368578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136857b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136857fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1368585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136858be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.785s
user	0m0.277s
sys	0m0.321s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4627 (b3451785)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f10a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f10ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f10b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f10b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f10bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f10c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f10c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f10cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f10d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f10d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f10dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f10e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f10ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f10f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f10fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f110420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f110b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f111260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f111980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f112150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f112870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f112f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f1136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f113f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f114670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f114930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f114f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f115bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f1160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f1163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f116850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f116b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f1173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f1178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f117ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f118040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f1184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f118980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f118e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f1192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f119760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f119c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f11a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f11a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f11a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f11ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f11b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f11bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f11c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f11c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f11cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f11d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f11db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f11e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f11e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f11ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f11f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f11f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f11fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f120390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f120650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f120af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f120f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f121430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f1218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f121d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f122210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f1226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f122b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f122ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f123490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f123930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f123dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f124320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f124870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f124dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f125310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f125860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f125db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f126300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f126850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f126da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f1272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f127840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f127d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f1282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f128830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f128d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f1292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f129820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f129d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f12a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f12a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f12ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f12b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f12b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f12bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f11ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f12c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f12c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f12cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f12d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f12d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f12deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f12e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f12e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f12eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f12f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f12f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f12fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f1303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f130930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f130e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f131320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f1317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f131c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f132100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f1325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f132a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f132ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f133380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f133820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f133cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f134160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f134600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f134aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f134f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f1353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f135880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f135d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f1361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f136660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f136b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f136fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f137440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f1378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f137d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f138220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f1386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f138b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f139000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f1394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f139940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f139de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f13a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f13a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f13abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f13b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f13b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f13b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f13be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f13c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f13c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f13cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f13d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f13d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f13da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f13dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f13e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f13e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f13ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f13f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f13f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f13fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f13ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f1403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f140840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f140ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f141180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f141620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f141ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f141f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f142400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f1428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f142d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f1431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f143680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f143b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f143fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f144460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f144900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f144da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f145240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f1456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f145b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f146020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f1464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f146960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f146e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f1472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f147740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f147be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f148080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f1485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f148b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f149070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f1495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f149880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f149e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f14a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f14aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f14b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f14b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f14ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f14c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f14c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f14ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f14d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f14d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f14dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f14e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f14e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f14ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f14f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f14f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f14fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f150380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f1508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f150e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f151370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f1518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f151e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f152360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f1528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f152e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f153350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f1538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f153df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f154340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f154890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f154de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f155330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f155880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f155dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f156320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f156870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f156dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f157310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f157860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f157db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f158300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f158850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f158da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f1592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f159840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f159d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f15a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f15a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f15ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f15b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f15b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f15bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f15c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f15c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f15cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f15d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f15d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f15dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f15e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f15e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f15ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f15f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f15f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f15fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f160280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f1607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f160d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f1611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f161660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f161b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f161fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f162440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f1628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f162d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f163220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f1636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f163b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f164000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f1644a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f164940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f164de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f165280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f1657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f165ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f166610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f166d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f167450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f167710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f167f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f1681c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f1687d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.550 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d6098f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d609d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d60a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d60a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d60aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d60af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d60b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d60b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d60bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d60c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d60c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d60cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d60d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d60df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d60e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d60eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d60f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d60fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d610400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d610bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d6112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d611a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d612130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d612850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d612f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d613230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d6134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d613960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d613dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d614240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d6146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d614be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d615050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d615310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d615780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d615bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d616060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d6164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d616940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d616db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d617220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d617f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d6183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d618850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d618cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d619130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d6195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d619a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d619e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d61a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d61a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d61abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d61b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d61b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d61ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d61bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d61c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d61c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d61cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d61d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d61d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d61d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d61de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d61e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d61e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d61eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d61eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d61f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d61f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d61fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d6201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d620620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d620a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d620f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d621370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d6217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d621c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d6220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d622530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d6229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d622e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d623280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d6236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d623b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d623fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d624440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d6248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d624d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d625190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d625600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d625a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d625ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d626350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d6267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d626c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d6270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d627510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d627980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d627df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d628260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d6286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d628b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d628fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d629420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d629890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d629d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d62a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d62a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d62aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d62aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d62b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d62b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d62bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d62c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d62c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d62c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d62cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d62d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d62d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d62db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d62df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d62e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d62e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d62ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d62f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d62f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d62fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d62fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d630310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d630780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d630bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d631060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d6314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d631940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d631db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d632220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d632690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d632b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d632f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d6333e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d633850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d633cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d634130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d6345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d634a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d634e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d6352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d635760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d635bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d636040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d6364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d636920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d637200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d637670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d637ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d637f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d6383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d638830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d638ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d639110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d639580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d6399f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d639e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d63aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d63ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d63b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d63b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d63b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d63bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d63c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d63c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d63cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d63cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d63d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d63d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d63dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d63e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d63e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d63e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d63ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d63f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d63f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d63fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d63fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d640460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d6408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d640d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d6411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d641620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d641a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d641f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d642370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d6427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d642c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d6430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d643530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d6439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d643e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d644280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d6447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d644cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d645160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d6455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d645a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d645eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d6463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d6468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d647450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d647710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d647cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d648290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d648850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d648e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d6493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d649990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d649f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d64a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d64aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d64b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d64b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d64bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d64c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d64c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d64cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d64d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d64d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d64de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d64e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d64ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d64efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d64f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d64fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d650110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d6506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d650c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d651250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d651810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d651dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d652950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d652f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d6534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d653a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d654050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d654610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d654bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d655190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d655750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d655d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d6562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d656890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d656e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d657410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d6579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d657f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d658550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d658b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d6590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d659690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d659c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d65a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d65a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d65ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d65b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d65b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d65be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d65c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d65c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d65cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d65d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d65d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d65dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d65e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d65e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d65eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d65f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d65f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d65fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d65ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d660410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d660e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d661540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d661c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d662380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d662640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d662e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d6630f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d663700 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d705290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d705700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d705b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d705fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d706450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d7068c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d706d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d7071a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d707610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d707a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d707ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d708590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d7090b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d709860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d70a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d70a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d70aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d70b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d70bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d70c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d70cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d70d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d70da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d70e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d70e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d70eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d70ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d70f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d70f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d70fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d70ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d7104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d710940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d710c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d711070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d7114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d711950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d711dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d712230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d7126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d712b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d712f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d7133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d713860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d713cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d714140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d7145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d714a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d714e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d715300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d715770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d715be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d716050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d7164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d716930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d716da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d717310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d717810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d717c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d7180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d718560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d7189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d718e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d7192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d719720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d719b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d71a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d71a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d71a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d71ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d71b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d71b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d71baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d71bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d71c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d71c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d71cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d71d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d71d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d71d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d71de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d71e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d71e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d71eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d71efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d71f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d71f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d71fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d7201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d720610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d720a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d720ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d721360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d7217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d721c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d7220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d722520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d722990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d722e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d723270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d7236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d723b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d723fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d724850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d724b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d724f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d7253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d725860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d725cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d726140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d7265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d726a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d726e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d727300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d727770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d727be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d728050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d7284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d728930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d728da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d729210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d729680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d729af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d729f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d72a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d72a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d72acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d72b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d72b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d72ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d72be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d72c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d72c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d72cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d72d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d72d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d72d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d72dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d72e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d72e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d72ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d72ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d72f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d72f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d72fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d730100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d730570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d7309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d730e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d7312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d731730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d731ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d732010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d732480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d7328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d732d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d7331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d733640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d733ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d733f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d734390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d734800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d734c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d7350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d735550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d7359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d735e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d7362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d736710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d736b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d736ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d737460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d7378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d737d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d7381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d738620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d738a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d738f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d739370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d7397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d739c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d73a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d73a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d73a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d73ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d73b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d73b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d73bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d73bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d73c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d73c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d73cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d73d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d73d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d73da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d73dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d73e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d73e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d73ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d73f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d73f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d73f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d73fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d740260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d7406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d740b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d740fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d741420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d741890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d741d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d742880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d742b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d742e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d743270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d7436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d743b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d743fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d744430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d7448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d744d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d745180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d7455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d745a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d745ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d746340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d7467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d746c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d747090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d747500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d747970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d747de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d748250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d7486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d748b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d748fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d749410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d749880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d749cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d74a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d74a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d74aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d74aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d74b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d74b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d74bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d74c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d74c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d74c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d74cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d74d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d74d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d74db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d74df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d74e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d74e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d74ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d74f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d74f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d74fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d74fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d750300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d750770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d750be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d751050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d7514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d751930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d751da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d752210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d752680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d752af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d752f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d7533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d753840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d753cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d754120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d754590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d754a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d754e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d7552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d755750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d755bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d756030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d7564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d756f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d757d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d758470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d758730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d758ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d7591a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d7597b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.960s
user	0m0.236s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
