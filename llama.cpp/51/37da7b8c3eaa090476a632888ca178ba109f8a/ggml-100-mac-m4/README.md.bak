### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.30 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.15 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.18 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.46 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.29 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.23 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.69 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.63 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.23 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.23 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.23 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   18.42 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.12 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.01 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.00 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  198.02 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.86 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.46 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 260.85 sec*proc (29 tests)

Total Test time (real) = 260.86 sec

real	4m20.956s
user	8m44.735s
sys	0m7.196s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.12 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.84 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.17 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.56 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   31.17 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.36 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.23 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.20 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  55.39 sec*proc (29 tests)

Total Test time (real) =  55.41 sec

real	0m55.422s
user	1m17.658s
sys	0m6.468s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.098 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.258 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.930 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.938 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.941 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.941 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.942 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.942 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.943 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.944 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.945 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.945 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.946 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.946 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.950 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.950 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.951 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.952 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.952 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.953 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.953 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.422 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.024.081 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.083 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.024.084 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.024.084 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.024.084 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.024.084 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.024.085 I llama_model_loader: - type  f32:  124 tensors
0.00.024.085 I llama_model_loader: - type  f16:   73 tensors
0.00.024.086 I print_info: file format = GGUF V3 (latest)
0.00.024.086 I print_info: file type   = F16
0.00.024.088 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.026.359 I load: special tokens cache size = 5
0.00.027.587 I load: token to piece cache size = 0.2032 MB
0.00.027.591 I print_info: arch             = bert
0.00.027.591 I print_info: vocab_only       = 0
0.00.027.591 I print_info: n_ctx_train      = 512
0.00.027.591 I print_info: n_embd           = 384
0.00.027.592 I print_info: n_layer          = 12
0.00.027.594 I print_info: n_head           = 12
0.00.027.595 I print_info: n_head_kv        = 12
0.00.027.595 I print_info: n_rot            = 32
0.00.027.595 I print_info: n_swa            = 0
0.00.027.595 I print_info: n_embd_head_k    = 32
0.00.027.595 I print_info: n_embd_head_v    = 32
0.00.027.597 I print_info: n_gqa            = 1
0.00.027.598 I print_info: n_embd_k_gqa     = 384
0.00.027.598 I print_info: n_embd_v_gqa     = 384
0.00.027.599 I print_info: f_norm_eps       = 1.0e-12
0.00.027.599 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.027.599 I print_info: f_clamp_kqv      = 0.0e+00
0.00.027.600 I print_info: f_max_alibi_bias = 0.0e+00
0.00.027.600 I print_info: f_logit_scale    = 0.0e+00
0.00.027.601 I print_info: n_ff             = 1536
0.00.027.601 I print_info: n_expert         = 0
0.00.027.601 I print_info: n_expert_used    = 0
0.00.027.601 I print_info: causal attn      = 0
0.00.027.601 I print_info: pooling type     = 2
0.00.027.601 I print_info: rope type        = 2
0.00.027.602 I print_info: rope scaling     = linear
0.00.027.602 I print_info: freq_base_train  = 10000.0
0.00.027.602 I print_info: freq_scale_train = 1
0.00.027.603 I print_info: n_ctx_orig_yarn  = 512
0.00.027.604 I print_info: rope_finetuned   = unknown
0.00.027.606 I print_info: ssm_d_conv       = 0
0.00.027.606 I print_info: ssm_d_inner      = 0
0.00.027.606 I print_info: ssm_d_state      = 0
0.00.027.606 I print_info: ssm_dt_rank      = 0
0.00.027.606 I print_info: ssm_dt_b_c_rms   = 0
0.00.027.606 I print_info: model type       = 33M
0.00.027.631 I print_info: model params     = 33.21 M
0.00.027.632 I print_info: general.name     = Bge Small
0.00.027.633 I print_info: vocab type       = WPM
0.00.027.633 I print_info: n_vocab          = 30522
0.00.027.633 I print_info: n_merges         = 0
0.00.027.633 I print_info: BOS token        = 101 '[CLS]'
0.00.027.634 I print_info: UNK token        = 100 '[UNK]'
0.00.027.634 I print_info: SEP token        = 102 '[SEP]'
0.00.027.634 I print_info: PAD token        = 0 '[PAD]'
0.00.027.634 I print_info: MASK token       = 103 '[MASK]'
0.00.027.634 I print_info: LF token         = 0 '[PAD]'
0.00.027.634 I print_info: max token length = 21
0.00.027.635 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.029.550 I load_tensors: offloading 12 repeating layers to GPU
0.00.029.551 I load_tensors: offloading output layer to GPU
0.00.029.551 I load_tensors: offloaded 13/13 layers to GPU
0.00.029.565 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.029.567 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.029.727 I llama_init_from_model: n_seq_max     = 1
0.00.029.728 I llama_init_from_model: n_ctx         = 512
0.00.029.728 I llama_init_from_model: n_ctx_per_seq = 512
0.00.029.728 I llama_init_from_model: n_batch       = 2048
0.00.029.728 I llama_init_from_model: n_ubatch      = 2048
0.00.029.729 I llama_init_from_model: flash_attn    = 0
0.00.029.729 I llama_init_from_model: freq_base     = 10000.0
0.00.029.729 I llama_init_from_model: freq_scale    = 1
0.00.029.730 I ggml_metal_init: allocating
0.00.029.733 I ggml_metal_init: found device: Apple M4
0.00.029.735 I ggml_metal_init: picking default device: Apple M4
0.00.030.244 I ggml_metal_init: using embedded metal library
0.00.032.743 I ggml_metal_init: GPU name:   Apple M4
0.00.032.744 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.032.745 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.032.745 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.032.745 I ggml_metal_init: simdgroup reduction   = true
0.00.032.746 I ggml_metal_init: simdgroup matrix mul. = true
0.00.032.746 I ggml_metal_init: has residency sets    = true
0.00.032.746 I ggml_metal_init: has bfloat            = true
0.00.032.746 I ggml_metal_init: use bfloat            = true
0.00.032.747 I ggml_metal_init: hasUnifiedMemory      = true
0.00.032.747 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.042.599 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.043.265 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.043.268 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.043.290 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.044.454 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.044.456 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.044.456 I llama_init_from_model: graph nodes  = 429
0.00.044.456 I llama_init_from_model: graph splits = 2
0.00.044.457 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.044.457 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.049.036 I 
0.00.049.058 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.049.619 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.054.021 I llama_perf_context_print:        load time =      32.77 ms
0.00.054.023 I llama_perf_context_print: prompt eval time =       4.28 ms /     9 tokens (    0.48 ms per token,  2100.84 tokens per second)
0.00.054.023 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.054.024 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.054.240 I ggml_metal_free: deallocating

real	0m0.236s
user	0m0.037s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.052 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.101 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.829 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.835 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.836 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.837 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.837 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.838 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.838 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.839 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.839 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.839 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.840 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.840 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.848 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.848 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.849 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.849 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.849 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.850 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.240 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.907 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.909 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.909 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.909 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.910 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.910 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.910 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.911 I llama_model_loader: - type  f32:  124 tensors
0.00.015.911 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.912 I print_info: file format = GGUF V3 (latest)
0.00.015.912 I print_info: file type   = Q8_0
0.00.015.914 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.561 I load: special tokens cache size = 5
0.00.019.870 I load: token to piece cache size = 0.2032 MB
0.00.019.876 I print_info: arch             = bert
0.00.019.877 I print_info: vocab_only       = 0
0.00.019.880 I print_info: n_ctx_train      = 512
0.00.019.880 I print_info: n_embd           = 384
0.00.019.880 I print_info: n_layer          = 12
0.00.019.884 I print_info: n_head           = 12
0.00.019.885 I print_info: n_head_kv        = 12
0.00.019.885 I print_info: n_rot            = 32
0.00.019.885 I print_info: n_swa            = 0
0.00.019.885 I print_info: n_embd_head_k    = 32
0.00.019.886 I print_info: n_embd_head_v    = 32
0.00.019.886 I print_info: n_gqa            = 1
0.00.019.887 I print_info: n_embd_k_gqa     = 384
0.00.019.887 I print_info: n_embd_v_gqa     = 384
0.00.019.888 I print_info: f_norm_eps       = 1.0e-12
0.00.019.888 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.888 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.888 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.889 I print_info: f_logit_scale    = 0.0e+00
0.00.019.889 I print_info: n_ff             = 1536
0.00.019.889 I print_info: n_expert         = 0
0.00.019.889 I print_info: n_expert_used    = 0
0.00.019.890 I print_info: causal attn      = 0
0.00.019.890 I print_info: pooling type     = 2
0.00.019.890 I print_info: rope type        = 2
0.00.019.890 I print_info: rope scaling     = linear
0.00.019.890 I print_info: freq_base_train  = 10000.0
0.00.019.891 I print_info: freq_scale_train = 1
0.00.019.891 I print_info: n_ctx_orig_yarn  = 512
0.00.019.891 I print_info: rope_finetuned   = unknown
0.00.019.891 I print_info: ssm_d_conv       = 0
0.00.019.891 I print_info: ssm_d_inner      = 0
0.00.019.891 I print_info: ssm_d_state      = 0
0.00.019.892 I print_info: ssm_dt_rank      = 0
0.00.019.892 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.892 I print_info: model type       = 33M
0.00.019.892 I print_info: model params     = 33.21 M
0.00.019.892 I print_info: general.name     = Bge Small
0.00.019.893 I print_info: vocab type       = WPM
0.00.019.893 I print_info: n_vocab          = 30522
0.00.019.893 I print_info: n_merges         = 0
0.00.019.893 I print_info: BOS token        = 101 '[CLS]'
0.00.019.894 I print_info: UNK token        = 100 '[UNK]'
0.00.019.894 I print_info: SEP token        = 102 '[SEP]'
0.00.019.894 I print_info: PAD token        = 0 '[PAD]'
0.00.019.894 I print_info: MASK token       = 103 '[MASK]'
0.00.019.894 I print_info: LF token         = 0 '[PAD]'
0.00.019.895 I print_info: max token length = 21
0.00.019.895 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.021.693 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.694 I load_tensors: offloading output layer to GPU
0.00.021.694 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.700 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.700 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.895 I llama_init_from_model: n_seq_max     = 1
0.00.021.896 I llama_init_from_model: n_ctx         = 512
0.00.021.896 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.896 I llama_init_from_model: n_batch       = 2048
0.00.021.896 I llama_init_from_model: n_ubatch      = 2048
0.00.021.897 I llama_init_from_model: flash_attn    = 0
0.00.021.897 I llama_init_from_model: freq_base     = 10000.0
0.00.021.897 I llama_init_from_model: freq_scale    = 1
0.00.021.898 I ggml_metal_init: allocating
0.00.021.905 I ggml_metal_init: found device: Apple M4
0.00.021.909 I ggml_metal_init: picking default device: Apple M4
0.00.022.490 I ggml_metal_init: using embedded metal library
0.00.025.140 I ggml_metal_init: GPU name:   Apple M4
0.00.025.143 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.025.143 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.025.144 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.025.144 I ggml_metal_init: simdgroup reduction   = true
0.00.025.144 I ggml_metal_init: simdgroup matrix mul. = true
0.00.025.144 I ggml_metal_init: has residency sets    = true
0.00.025.144 I ggml_metal_init: has bfloat            = true
0.00.025.145 I ggml_metal_init: use bfloat            = true
0.00.025.145 I ggml_metal_init: hasUnifiedMemory      = true
0.00.025.147 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.694 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.377 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.380 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.394 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.513 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.514 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.514 I llama_init_from_model: graph nodes  = 429
0.00.036.514 I llama_init_from_model: graph splits = 2
0.00.036.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.516 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.948 I 
0.00.040.973 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.580 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.046.192 I llama_perf_context_print:        load time =      30.84 ms
0.00.046.193 I llama_perf_context_print: prompt eval time =       4.50 ms /     9 tokens (    0.50 ms per token,  2001.78 tokens per second)
0.00.046.194 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.046.194 I llama_perf_context_print:       total time =       5.24 ms /    10 tokens
0.00.046.427 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.018s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.316 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.324 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.858 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.863 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.865 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.866 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.867 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.868 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.868 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.870 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.871 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.871 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.872 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.873 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.876 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.877 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.878 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.878 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.879 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.031 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.718 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.720 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.720 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.721 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.721 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.721 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.722 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.722 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.722 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.723 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.723 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.724 I llama_model_loader: - type  f32:   40 tensors
0.00.050.724 I llama_model_loader: - type  f16:   30 tensors
0.00.050.727 I print_info: file format = GGUF V3 (latest)
0.00.050.727 I print_info: file type   = F16
0.00.050.729 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.849 W load: empty token at index 5
0.00.060.007 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.555 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.590 I load: special tokens cache size = 5
0.00.327.711 I load: token to piece cache size = 1.5060 MB
0.00.327.718 I print_info: arch             = jina-bert-v2
0.00.327.718 I print_info: vocab_only       = 0
0.00.327.718 I print_info: n_ctx_train      = 8192
0.00.327.719 I print_info: n_embd           = 384
0.00.327.719 I print_info: n_layer          = 4
0.00.327.725 I print_info: n_head           = 12
0.00.327.726 I print_info: n_head_kv        = 12
0.00.327.726 I print_info: n_rot            = 32
0.00.327.726 I print_info: n_swa            = 0
0.00.327.728 I print_info: n_embd_head_k    = 32
0.00.327.728 I print_info: n_embd_head_v    = 32
0.00.327.735 I print_info: n_gqa            = 1
0.00.327.736 I print_info: n_embd_k_gqa     = 384
0.00.327.736 I print_info: n_embd_v_gqa     = 384
0.00.327.737 I print_info: f_norm_eps       = 1.0e-12
0.00.327.738 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.327.738 I print_info: f_clamp_kqv      = 0.0e+00
0.00.327.738 I print_info: f_max_alibi_bias = 8.0e+00
0.00.327.738 I print_info: f_logit_scale    = 0.0e+00
0.00.327.739 I print_info: n_ff             = 1536
0.00.327.739 I print_info: n_expert         = 0
0.00.327.747 I print_info: n_expert_used    = 0
0.00.327.749 I print_info: causal attn      = 0
0.00.327.749 I print_info: pooling type     = -1
0.00.327.749 I print_info: rope type        = -1
0.00.327.749 I print_info: rope scaling     = linear
0.00.327.750 I print_info: freq_base_train  = 10000.0
0.00.327.750 I print_info: freq_scale_train = 1
0.00.327.750 I print_info: n_ctx_orig_yarn  = 8192
0.00.327.751 I print_info: rope_finetuned   = unknown
0.00.327.751 I print_info: ssm_d_conv       = 0
0.00.327.751 I print_info: ssm_d_inner      = 0
0.00.327.751 I print_info: ssm_d_state      = 0
0.00.327.751 I print_info: ssm_dt_rank      = 0
0.00.327.751 I print_info: ssm_dt_b_c_rms   = 0
0.00.327.752 I print_info: model type       = 33M
0.00.327.752 I print_info: model params     = 32.90 M
0.00.327.755 I print_info: general.name     = Jina Bert Implementation
0.00.327.756 I print_info: vocab type       = BPE
0.00.327.756 I print_info: n_vocab          = 61056
0.00.327.757 I print_info: n_merges         = 39382
0.00.327.759 I print_info: BOS token        = 0 '<s>'
0.00.327.759 I print_info: EOS token        = 2 '</s>'
0.00.327.759 I print_info: UNK token        = 3 '<unk>'
0.00.327.759 I print_info: SEP token        = 2 '</s>'
0.00.327.760 I print_info: PAD token        = 1 '<pad>'
0.00.327.761 I print_info: MASK token       = 4 '<mask>'
0.00.327.761 I print_info: EOG token        = 2 '</s>'
0.00.327.761 I print_info: max token length = 45
0.00.327.762 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.329.756 I load_tensors: offloading 4 repeating layers to GPU
0.00.329.757 I load_tensors: offloading output layer to GPU
0.00.329.757 I load_tensors: offloaded 5/5 layers to GPU
0.00.329.779 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.329.780 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.330.056 I llama_init_from_model: n_seq_max     = 1
0.00.330.057 I llama_init_from_model: n_ctx         = 8192
0.00.330.057 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.330.057 I llama_init_from_model: n_batch       = 2048
0.00.330.057 I llama_init_from_model: n_ubatch      = 2048
0.00.330.058 I llama_init_from_model: flash_attn    = 0
0.00.330.058 I llama_init_from_model: freq_base     = 10000.0
0.00.330.058 I llama_init_from_model: freq_scale    = 1
0.00.330.059 I ggml_metal_init: allocating
0.00.330.063 I ggml_metal_init: found device: Apple M4
0.00.330.066 I ggml_metal_init: picking default device: Apple M4
0.00.330.835 I ggml_metal_init: using embedded metal library
0.00.333.687 I ggml_metal_init: GPU name:   Apple M4
0.00.333.689 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.333.689 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.333.690 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.333.690 I ggml_metal_init: simdgroup reduction   = true
0.00.333.690 I ggml_metal_init: simdgroup matrix mul. = true
0.00.333.690 I ggml_metal_init: has residency sets    = true
0.00.333.690 I ggml_metal_init: has bfloat            = true
0.00.333.690 I ggml_metal_init: use bfloat            = true
0.00.333.691 I ggml_metal_init: hasUnifiedMemory      = true
0.00.333.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.343.132 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.346.226 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.346.228 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.346.248 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.353.777 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.353.779 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.353.780 I llama_init_from_model: graph nodes  = 154
0.00.353.780 I llama_init_from_model: graph splits = 2
0.00.353.781 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.781 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.599 I 
0.00.360.615 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.777 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.360.778 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.360.781 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.360.781 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.360.788 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.360.789 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.358 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.842 I llama_perf_context_print:        load time =     336.24 ms
0.00.364.843 I llama_perf_context_print: prompt eval time =       3.48 ms /    62 tokens (    0.06 ms per token, 17836.59 tokens per second)
0.00.364.847 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.847 I llama_perf_context_print:       total time =       4.24 ms /    63 tokens
0.00.365.066 I ggml_metal_free: deallocating

real	0m1.075s
user	0m0.334s
sys	0m0.049s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.150 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.337 I main: llama backend init
0.00.000.343 I main: load the model and apply lora adapter, if any
0.00.030.682 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.043.012 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.025 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.030 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.031 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.032 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.034 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.035 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.035 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.036 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.037 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.038 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.038 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.043 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.044 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.044 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.264 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.041 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.658 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.061.662 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.663 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.663 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.664 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.665 I llama_model_loader: - type  f32:  194 tensors
0.00.061.665 I llama_model_loader: - type  f16:   98 tensors
0.00.061.667 I print_info: file format = GGUF V3 (latest)
0.00.061.669 I print_info: file type   = all F32 (guessed)
0.00.061.670 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.077.071 I load: special tokens cache size = 25
0.00.086.155 I load: token to piece cache size = 0.2984 MB
0.00.086.159 I print_info: arch             = gptneox
0.00.086.159 I print_info: vocab_only       = 0
0.00.086.159 I print_info: n_ctx_train      = 2048
0.00.086.160 I print_info: n_embd           = 2048
0.00.086.160 I print_info: n_layer          = 24
0.00.086.164 I print_info: n_head           = 16
0.00.086.165 I print_info: n_head_kv        = 16
0.00.086.165 I print_info: n_rot            = 32
0.00.086.165 I print_info: n_swa            = 0
0.00.086.166 I print_info: n_embd_head_k    = 128
0.00.086.166 I print_info: n_embd_head_v    = 128
0.00.086.167 I print_info: n_gqa            = 1
0.00.086.168 I print_info: n_embd_k_gqa     = 2048
0.00.086.168 I print_info: n_embd_v_gqa     = 2048
0.00.086.169 I print_info: f_norm_eps       = 1.0e-05
0.00.086.170 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.170 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.170 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.170 I print_info: f_logit_scale    = 0.0e+00
0.00.086.171 I print_info: n_ff             = 8192
0.00.086.171 I print_info: n_expert         = 0
0.00.086.172 I print_info: n_expert_used    = 0
0.00.086.172 I print_info: causal attn      = 1
0.00.086.172 I print_info: pooling type     = 0
0.00.086.172 I print_info: rope type        = 2
0.00.086.172 I print_info: rope scaling     = linear
0.00.086.173 I print_info: freq_base_train  = 10000.0
0.00.086.173 I print_info: freq_scale_train = 1
0.00.086.173 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.174 I print_info: rope_finetuned   = unknown
0.00.086.174 I print_info: ssm_d_conv       = 0
0.00.086.174 I print_info: ssm_d_inner      = 0
0.00.086.174 I print_info: ssm_d_state      = 0
0.00.086.174 I print_info: ssm_dt_rank      = 0
0.00.086.174 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.175 I print_info: model type       = 1.4B
0.00.086.175 I print_info: model params     = 1.41 B
0.00.086.175 I print_info: general.name     = 1.4B
0.00.086.176 I print_info: vocab type       = BPE
0.00.086.176 I print_info: n_vocab          = 50304
0.00.086.176 I print_info: n_merges         = 50009
0.00.086.177 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.177 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.177 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.177 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.177 I print_info: LF token         = 187 ''
0.00.086.178 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.178 I print_info: max token length = 1024
0.00.086.178 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.127.789 I load_tensors: offloading 24 repeating layers to GPU
0.00.127.794 I load_tensors: offloading output layer to GPU
0.00.127.794 I load_tensors: offloaded 25/25 layers to GPU
0.00.127.819 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.127.820 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.128.218 I llama_init_from_model: n_seq_max     = 1
0.00.128.219 I llama_init_from_model: n_ctx         = 2048
0.00.128.219 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.128.219 I llama_init_from_model: n_batch       = 2048
0.00.128.220 I llama_init_from_model: n_ubatch      = 512
0.00.128.220 I llama_init_from_model: flash_attn    = 0
0.00.128.221 I llama_init_from_model: freq_base     = 10000.0
0.00.128.221 I llama_init_from_model: freq_scale    = 1
0.00.128.222 I ggml_metal_init: allocating
0.00.128.253 I ggml_metal_init: found device: Apple M4
0.00.128.259 I ggml_metal_init: picking default device: Apple M4
0.00.128.907 I ggml_metal_init: using embedded metal library
0.00.137.903 I ggml_metal_init: GPU name:   Apple M4
0.00.137.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.137.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.137.906 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.137.906 I ggml_metal_init: simdgroup reduction   = true
0.00.137.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.137.906 I ggml_metal_init: has residency sets    = true
0.00.137.906 I ggml_metal_init: has bfloat            = true
0.00.137.906 I ggml_metal_init: use bfloat            = true
0.00.137.907 I ggml_metal_init: hasUnifiedMemory      = true
0.00.137.908 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.161.264 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.191.552 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.191.560 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.191.604 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.195.402 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.195.404 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.195.405 I llama_init_from_model: graph nodes  = 967
0.00.195.405 I llama_init_from_model: graph splits = 2
0.00.195.409 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.195.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.195.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.262.103 I main: llama threadpool init, n_threads = 4
0.00.262.147 I 
0.00.262.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.262.163 I 
0.00.262.208 I sampler seed: 1234
0.00.262.213 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.262.237 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.262.239 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.262.239 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.096.377 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.02.096.377 I llama_perf_context_print:        load time =     230.54 ms
0.02.096.378 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.49 tokens per second)
0.02.096.380 I llama_perf_context_print:        eval time =    1787.67 ms /    63 runs   (   28.38 ms per token,    35.24 tokens per second)
0.02.096.380 I llama_perf_context_print:       total time =    1835.13 ms /    70 tokens
0.02.096.651 I ggml_metal_free: deallocating

real	0m2.403s
user	0m0.131s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.663 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.577 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.804 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.810 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.819 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.827 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.830 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.833 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.833 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.835 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.836 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.838 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.839 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.839 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.840 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.841 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.849 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.849 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.850 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.091 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.880 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.402 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.404 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.404 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.405 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.405 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.406 I llama_model_loader: - type  f32:  194 tensors
0.00.053.406 I llama_model_loader: - type  f16:   98 tensors
0.00.053.407 I print_info: file format = GGUF V3 (latest)
0.00.053.407 I print_info: file type   = all F32 (guessed)
0.00.053.408 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.806 I load: special tokens cache size = 25
0.00.072.850 I load: token to piece cache size = 0.2984 MB
0.00.072.853 I print_info: arch             = gptneox
0.00.072.854 I print_info: vocab_only       = 0
0.00.072.854 I print_info: n_ctx_train      = 2048
0.00.072.854 I print_info: n_embd           = 2048
0.00.072.854 I print_info: n_layer          = 24
0.00.072.858 I print_info: n_head           = 16
0.00.072.859 I print_info: n_head_kv        = 16
0.00.072.859 I print_info: n_rot            = 32
0.00.072.859 I print_info: n_swa            = 0
0.00.072.859 I print_info: n_embd_head_k    = 128
0.00.072.859 I print_info: n_embd_head_v    = 128
0.00.072.860 I print_info: n_gqa            = 1
0.00.072.861 I print_info: n_embd_k_gqa     = 2048
0.00.072.861 I print_info: n_embd_v_gqa     = 2048
0.00.072.862 I print_info: f_norm_eps       = 1.0e-05
0.00.072.862 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.862 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.863 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.863 I print_info: f_logit_scale    = 0.0e+00
0.00.072.864 I print_info: n_ff             = 8192
0.00.072.864 I print_info: n_expert         = 0
0.00.072.864 I print_info: n_expert_used    = 0
0.00.072.864 I print_info: causal attn      = 1
0.00.072.864 I print_info: pooling type     = 0
0.00.072.869 I print_info: rope type        = 2
0.00.072.870 I print_info: rope scaling     = linear
0.00.072.870 I print_info: freq_base_train  = 10000.0
0.00.072.870 I print_info: freq_scale_train = 1
0.00.072.870 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.872 I print_info: rope_finetuned   = unknown
0.00.072.872 I print_info: ssm_d_conv       = 0
0.00.072.872 I print_info: ssm_d_inner      = 0
0.00.072.873 I print_info: ssm_d_state      = 0
0.00.072.873 I print_info: ssm_dt_rank      = 0
0.00.072.873 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.873 I print_info: model type       = 1.4B
0.00.072.874 I print_info: model params     = 1.41 B
0.00.072.874 I print_info: general.name     = 1.4B
0.00.072.874 I print_info: vocab type       = BPE
0.00.072.874 I print_info: n_vocab          = 50304
0.00.072.875 I print_info: n_merges         = 50009
0.00.072.875 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.875 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.876 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.877 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.877 I print_info: LF token         = 187 ''
0.00.072.877 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.877 I print_info: max token length = 1024
0.00.072.878 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.105.659 I load_tensors: offloading 24 repeating layers to GPU
0.01.105.664 I load_tensors: offloading output layer to GPU
0.01.105.665 I load_tensors: offloaded 25/25 layers to GPU
0.01.105.690 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.105.691 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.106.496 I llama_init_from_model: n_seq_max     = 1
0.01.106.497 I llama_init_from_model: n_ctx         = 128
0.01.106.498 I llama_init_from_model: n_ctx_per_seq = 128
0.01.106.498 I llama_init_from_model: n_batch       = 128
0.01.106.498 I llama_init_from_model: n_ubatch      = 128
0.01.106.498 I llama_init_from_model: flash_attn    = 0
0.01.106.499 I llama_init_from_model: freq_base     = 10000.0
0.01.106.499 I llama_init_from_model: freq_scale    = 1
0.01.106.500 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.106.501 I ggml_metal_init: allocating
0.01.106.539 I ggml_metal_init: found device: Apple M4
0.01.106.545 I ggml_metal_init: picking default device: Apple M4
0.01.107.619 I ggml_metal_init: using embedded metal library
0.01.111.450 I ggml_metal_init: GPU name:   Apple M4
0.01.111.452 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.111.452 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.111.453 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.111.453 I ggml_metal_init: simdgroup reduction   = true
0.01.111.454 I ggml_metal_init: simdgroup matrix mul. = true
0.01.111.454 I ggml_metal_init: has residency sets    = true
0.01.111.454 I ggml_metal_init: has bfloat            = true
0.01.111.454 I ggml_metal_init: use bfloat            = true
0.01.111.454 I ggml_metal_init: hasUnifiedMemory      = true
0.01.111.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.122.215 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.123.933 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.123.935 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.123.961 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.125.540 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.125.541 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.125.542 I llama_init_from_model: graph nodes  = 967
0.01.125.542 I llama_init_from_model: graph splits = 2
0.01.125.543 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.125.543 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.161.042 I 
0.01.161.076 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.161.081 I perplexity: tokenizing the input ..
0.01.166.132 I perplexity: tokenization took 5.05 ms
0.01.166.136 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.284.662 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.286.011 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.286.041 I llama_perf_context_print:        load time =    1138.45 ms
0.01.286.042 I llama_perf_context_print: prompt eval time =     118.22 ms /   128 tokens (    0.92 ms per token,  1082.73 tokens per second)
0.01.286.043 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.286.043 I llama_perf_context_print:       total time =     125.00 ms /   129 tokens
0.01.286.426 I ggml_metal_free: deallocating

real	0m1.473s
user	0m0.097s
sys	0m0.219s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.010.493 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.533 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.538 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.540 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.540 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.541 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.541 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.542 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.543 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.544 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.544 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.544 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.545 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.546 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.546 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.547 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.443 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.456 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.321 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.323 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.323 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.324 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.324 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.324 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.325 I llama_model_loader: - type  f32:  194 tensors
0.00.034.325 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.326 I print_info: file format = GGUF V3 (latest)
0.00.034.327 I print_info: file type   = Q8_0
0.00.034.327 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.637 I load: special tokens cache size = 25
0.00.048.599 I load: token to piece cache size = 0.2984 MB
0.00.048.605 I print_info: arch             = gptneox
0.00.048.605 I print_info: vocab_only       = 0
0.00.048.605 I print_info: n_ctx_train      = 2048
0.00.048.605 I print_info: n_embd           = 2048
0.00.048.606 I print_info: n_layer          = 24
0.00.048.612 I print_info: n_head           = 16
0.00.048.613 I print_info: n_head_kv        = 16
0.00.048.613 I print_info: n_rot            = 32
0.00.048.613 I print_info: n_swa            = 0
0.00.048.613 I print_info: n_embd_head_k    = 128
0.00.048.613 I print_info: n_embd_head_v    = 128
0.00.048.614 I print_info: n_gqa            = 1
0.00.048.615 I print_info: n_embd_k_gqa     = 2048
0.00.048.615 I print_info: n_embd_v_gqa     = 2048
0.00.048.616 I print_info: f_norm_eps       = 1.0e-05
0.00.048.620 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.620 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.620 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.620 I print_info: f_logit_scale    = 0.0e+00
0.00.048.621 I print_info: n_ff             = 8192
0.00.048.621 I print_info: n_expert         = 0
0.00.048.621 I print_info: n_expert_used    = 0
0.00.048.621 I print_info: causal attn      = 1
0.00.048.621 I print_info: pooling type     = 0
0.00.048.622 I print_info: rope type        = 2
0.00.048.622 I print_info: rope scaling     = linear
0.00.048.623 I print_info: freq_base_train  = 10000.0
0.00.048.623 I print_info: freq_scale_train = 1
0.00.048.623 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.623 I print_info: rope_finetuned   = unknown
0.00.048.624 I print_info: ssm_d_conv       = 0
0.00.048.625 I print_info: ssm_d_inner      = 0
0.00.048.625 I print_info: ssm_d_state      = 0
0.00.048.625 I print_info: ssm_dt_rank      = 0
0.00.048.625 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.625 I print_info: model type       = 1.4B
0.00.048.625 I print_info: model params     = 1.41 B
0.00.048.626 I print_info: general.name     = 1.4B
0.00.048.626 I print_info: vocab type       = BPE
0.00.048.627 I print_info: n_vocab          = 50304
0.00.048.627 I print_info: n_merges         = 50009
0.00.048.627 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.627 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.627 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.627 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.628 I print_info: LF token         = 187 ''
0.00.048.628 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.628 I print_info: max token length = 1024
0.00.048.628 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.225.541 I load_tensors: offloading 24 repeating layers to GPU
0.01.225.547 I load_tensors: offloading output layer to GPU
0.01.225.549 I load_tensors: offloaded 25/25 layers to GPU
0.01.225.573 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.225.576 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.226.330 I llama_init_from_model: n_seq_max     = 1
0.01.226.332 I llama_init_from_model: n_ctx         = 2048
0.01.226.333 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.226.333 I llama_init_from_model: n_batch       = 2048
0.01.226.333 I llama_init_from_model: n_ubatch      = 512
0.01.226.334 I llama_init_from_model: flash_attn    = 0
0.01.226.335 I llama_init_from_model: freq_base     = 10000.0
0.01.226.335 I llama_init_from_model: freq_scale    = 1
0.01.226.336 I ggml_metal_init: allocating
0.01.226.349 I ggml_metal_init: found device: Apple M4
0.01.226.357 I ggml_metal_init: picking default device: Apple M4
0.01.227.664 I ggml_metal_init: using embedded metal library
0.01.233.437 I ggml_metal_init: GPU name:   Apple M4
0.01.233.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.233.442 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.233.443 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.233.443 I ggml_metal_init: simdgroup reduction   = true
0.01.233.443 I ggml_metal_init: simdgroup matrix mul. = true
0.01.233.444 I ggml_metal_init: has residency sets    = true
0.01.233.444 I ggml_metal_init: has bfloat            = true
0.01.233.444 I ggml_metal_init: use bfloat            = true
0.01.233.445 I ggml_metal_init: hasUnifiedMemory      = true
0.01.233.446 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.255.406 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.311.503 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.311.510 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.311.545 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.316.147 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.316.150 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.316.150 I llama_init_from_model: graph nodes  = 967
0.01.316.150 I llama_init_from_model: graph splits = 2
0.01.316.154 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.316.281 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.316.282 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.369.768 I main: llama threadpool init, n_threads = 4
0.01.369.823 I 
0.01.369.840 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.369.840 I 
0.01.369.992 I sampler seed: 1234
0.01.369.997 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.370.018 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.370.018 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.370.018 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.459.150 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52090.98 tokens per second)
0.02.459.151 I llama_perf_context_print:        load time =    1358.53 ms
0.02.459.152 I llama_perf_context_print: prompt eval time =      39.64 ms /     7 tokens (    5.66 ms per token,   176.58 tokens per second)
0.02.459.155 I llama_perf_context_print:        eval time =    1046.51 ms /    63 runs   (   16.61 ms per token,    60.20 tokens per second)
0.02.459.156 I llama_perf_context_print:       total time =    1090.12 ms /    70 tokens
0.02.459.427 I ggml_metal_free: deallocating

real	0m2.478s
user	0m0.110s
sys	0m0.258s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.239 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.385 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.391 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.393 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.399 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.399 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.399 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.400 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.401 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.401 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.401 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.402 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.402 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.402 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.403 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.404 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.405 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.405 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.424 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.388 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.392 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.392 I llama_model_loader: - type  f32:  194 tensors
0.00.025.393 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.393 I print_info: file format = GGUF V3 (latest)
0.00.025.394 I print_info: file type   = Q8_0
0.00.025.395 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.916 I load: special tokens cache size = 25
0.00.040.185 I load: token to piece cache size = 0.2984 MB
0.00.040.189 I print_info: arch             = gptneox
0.00.040.189 I print_info: vocab_only       = 0
0.00.040.189 I print_info: n_ctx_train      = 2048
0.00.040.190 I print_info: n_embd           = 2048
0.00.040.190 I print_info: n_layer          = 24
0.00.040.194 I print_info: n_head           = 16
0.00.040.195 I print_info: n_head_kv        = 16
0.00.040.195 I print_info: n_rot            = 32
0.00.040.195 I print_info: n_swa            = 0
0.00.040.196 I print_info: n_embd_head_k    = 128
0.00.040.196 I print_info: n_embd_head_v    = 128
0.00.040.196 I print_info: n_gqa            = 1
0.00.040.197 I print_info: n_embd_k_gqa     = 2048
0.00.040.198 I print_info: n_embd_v_gqa     = 2048
0.00.040.200 I print_info: f_norm_eps       = 1.0e-05
0.00.040.201 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.201 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.201 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.201 I print_info: f_logit_scale    = 0.0e+00
0.00.040.202 I print_info: n_ff             = 8192
0.00.040.204 I print_info: n_expert         = 0
0.00.040.204 I print_info: n_expert_used    = 0
0.00.040.204 I print_info: causal attn      = 1
0.00.040.204 I print_info: pooling type     = 0
0.00.040.204 I print_info: rope type        = 2
0.00.040.204 I print_info: rope scaling     = linear
0.00.040.205 I print_info: freq_base_train  = 10000.0
0.00.040.205 I print_info: freq_scale_train = 1
0.00.040.205 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.205 I print_info: rope_finetuned   = unknown
0.00.040.206 I print_info: ssm_d_conv       = 0
0.00.040.206 I print_info: ssm_d_inner      = 0
0.00.040.206 I print_info: ssm_d_state      = 0
0.00.040.206 I print_info: ssm_dt_rank      = 0
0.00.040.207 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.208 I print_info: model type       = 1.4B
0.00.040.208 I print_info: model params     = 1.41 B
0.00.040.208 I print_info: general.name     = 1.4B
0.00.040.209 I print_info: vocab type       = BPE
0.00.040.209 I print_info: n_vocab          = 50304
0.00.040.209 I print_info: n_merges         = 50009
0.00.040.209 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.209 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.209 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.209 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.210 I print_info: LF token         = 187 ''
0.00.040.210 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.210 I print_info: max token length = 1024
0.00.040.211 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.853.659 I load_tensors: offloading 24 repeating layers to GPU
0.00.853.666 I load_tensors: offloading output layer to GPU
0.00.853.666 I load_tensors: offloaded 25/25 layers to GPU
0.00.853.701 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.853.703 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.854.794 I llama_init_from_model: n_seq_max     = 1
0.00.854.797 I llama_init_from_model: n_ctx         = 128
0.00.854.797 I llama_init_from_model: n_ctx_per_seq = 128
0.00.854.797 I llama_init_from_model: n_batch       = 128
0.00.854.798 I llama_init_from_model: n_ubatch      = 128
0.00.854.798 I llama_init_from_model: flash_attn    = 0
0.00.854.799 I llama_init_from_model: freq_base     = 10000.0
0.00.854.799 I llama_init_from_model: freq_scale    = 1
0.00.854.800 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.854.801 I ggml_metal_init: allocating
0.00.854.864 I ggml_metal_init: found device: Apple M4
0.00.854.873 I ggml_metal_init: picking default device: Apple M4
0.00.856.160 I ggml_metal_init: using embedded metal library
0.00.861.263 I ggml_metal_init: GPU name:   Apple M4
0.00.861.267 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.861.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.861.268 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.861.269 I ggml_metal_init: simdgroup reduction   = true
0.00.861.269 I ggml_metal_init: simdgroup matrix mul. = true
0.00.861.269 I ggml_metal_init: has residency sets    = true
0.00.861.269 I ggml_metal_init: has bfloat            = true
0.00.861.270 I ggml_metal_init: use bfloat            = true
0.00.861.270 I ggml_metal_init: hasUnifiedMemory      = true
0.00.861.271 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.876.202 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.879.388 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.879.391 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.879.430 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.882.323 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.882.324 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.882.325 I llama_init_from_model: graph nodes  = 967
0.00.882.325 I llama_init_from_model: graph splits = 2
0.00.882.327 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.882.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.910.157 I 
0.00.910.204 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.910.211 I perplexity: tokenizing the input ..
0.00.916.588 I perplexity: tokenization took 6.375 ms
0.00.916.593 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.055.920 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.057.454 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.057.480 I llama_perf_context_print:        load time =     900.90 ms
0.01.057.480 I llama_perf_context_print: prompt eval time =     138.93 ms /   128 tokens (    1.09 ms per token,   921.31 tokens per second)
0.01.057.481 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.057.481 I llama_perf_context_print:       total time =     147.33 ms /   129 tokens
0.01.057.883 I ggml_metal_free: deallocating

real	0m1.073s
user	0m0.075s
sys	0m0.159s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.016.712 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.364 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.371 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.377 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.378 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.378 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.379 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.379 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.381 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.382 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.382 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.382 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.383 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.383 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.383 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.385 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.385 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.385 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.649 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.075 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.076 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.076 I llama_model_loader: - type  f32:  194 tensors
0.00.035.076 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.077 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.078 I print_info: file format = GGUF V3 (latest)
0.00.035.078 I print_info: file type   = Q4_0
0.00.035.079 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.680 I load: special tokens cache size = 25
0.00.052.450 I load: token to piece cache size = 0.2984 MB
0.00.052.454 I print_info: arch             = gptneox
0.00.052.454 I print_info: vocab_only       = 0
0.00.052.455 I print_info: n_ctx_train      = 2048
0.00.052.455 I print_info: n_embd           = 2048
0.00.052.455 I print_info: n_layer          = 24
0.00.052.459 I print_info: n_head           = 16
0.00.052.460 I print_info: n_head_kv        = 16
0.00.052.460 I print_info: n_rot            = 32
0.00.052.460 I print_info: n_swa            = 0
0.00.052.461 I print_info: n_embd_head_k    = 128
0.00.052.461 I print_info: n_embd_head_v    = 128
0.00.052.462 I print_info: n_gqa            = 1
0.00.052.462 I print_info: n_embd_k_gqa     = 2048
0.00.052.463 I print_info: n_embd_v_gqa     = 2048
0.00.052.464 I print_info: f_norm_eps       = 1.0e-05
0.00.052.464 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.464 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.464 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.465 I print_info: f_logit_scale    = 0.0e+00
0.00.052.465 I print_info: n_ff             = 8192
0.00.052.466 I print_info: n_expert         = 0
0.00.052.466 I print_info: n_expert_used    = 0
0.00.052.466 I print_info: causal attn      = 1
0.00.052.466 I print_info: pooling type     = 0
0.00.052.467 I print_info: rope type        = 2
0.00.052.468 I print_info: rope scaling     = linear
0.00.052.468 I print_info: freq_base_train  = 10000.0
0.00.052.469 I print_info: freq_scale_train = 1
0.00.052.469 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.469 I print_info: rope_finetuned   = unknown
0.00.052.469 I print_info: ssm_d_conv       = 0
0.00.052.470 I print_info: ssm_d_inner      = 0
0.00.052.470 I print_info: ssm_d_state      = 0
0.00.052.470 I print_info: ssm_dt_rank      = 0
0.00.052.470 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.470 I print_info: model type       = 1.4B
0.00.052.471 I print_info: model params     = 1.41 B
0.00.052.471 I print_info: general.name     = 1.4B
0.00.052.472 I print_info: vocab type       = BPE
0.00.052.472 I print_info: n_vocab          = 50304
0.00.052.472 I print_info: n_merges         = 50009
0.00.052.472 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.473 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.473 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.473 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.474 I print_info: LF token         = 187 ''
0.00.052.474 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.474 I print_info: max token length = 1024
0.00.052.474 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.629.367 I load_tensors: offloading 24 repeating layers to GPU
0.00.629.381 I load_tensors: offloading output layer to GPU
0.00.629.382 I load_tensors: offloaded 25/25 layers to GPU
0.00.629.417 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.629.419 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.630.910 I llama_init_from_model: n_seq_max     = 1
0.00.630.913 I llama_init_from_model: n_ctx         = 2048
0.00.630.914 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.630.915 I llama_init_from_model: n_batch       = 2048
0.00.630.916 I llama_init_from_model: n_ubatch      = 512
0.00.630.916 I llama_init_from_model: flash_attn    = 0
0.00.630.918 I llama_init_from_model: freq_base     = 10000.0
0.00.630.919 I llama_init_from_model: freq_scale    = 1
0.00.630.923 I ggml_metal_init: allocating
0.00.630.999 I ggml_metal_init: found device: Apple M4
0.00.631.013 I ggml_metal_init: picking default device: Apple M4
0.00.632.957 I ggml_metal_init: using embedded metal library
0.00.638.633 I ggml_metal_init: GPU name:   Apple M4
0.00.638.638 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.639 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.640 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.641 I ggml_metal_init: simdgroup reduction   = true
0.00.638.641 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.642 I ggml_metal_init: has residency sets    = true
0.00.638.642 I ggml_metal_init: has bfloat            = true
0.00.638.642 I ggml_metal_init: use bfloat            = true
0.00.638.644 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.061 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.829 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.718.836 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.718.872 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.723.486 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.723.488 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.723.489 I llama_init_from_model: graph nodes  = 967
0.00.723.489 I llama_init_from_model: graph splits = 2
0.00.723.496 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.723.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.723.625 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.630 I main: llama threadpool init, n_threads = 4
0.00.778.714 I 
0.00.778.735 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.736 I 
0.00.778.888 I sampler seed: 1234
0.00.778.893 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.904 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.904 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.905 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.463.479 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.01.463.480 I llama_perf_context_print:        load time =     761.20 ms
0.01.463.481 I llama_perf_context_print: prompt eval time =      49.01 ms /     7 tokens (    7.00 ms per token,   142.82 tokens per second)
0.01.463.481 I llama_perf_context_print:        eval time =     632.70 ms /    63 runs   (   10.04 ms per token,    99.57 tokens per second)
0.01.463.482 I llama_perf_context_print:       total time =     685.55 ms /    70 tokens
0.01.463.704 I ggml_metal_free: deallocating

real	0m1.488s
user	0m0.116s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.577 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.019 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.024 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.026 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.028 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.029 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.029 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.029 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.030 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.030 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.031 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.031 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.034 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.939 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.986 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.987 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.988 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.988 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.988 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.989 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.989 I llama_model_loader: - type  f32:  194 tensors
0.00.025.990 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.990 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.991 I print_info: file format = GGUF V3 (latest)
0.00.025.991 I print_info: file type   = Q4_0
0.00.025.992 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.183 I load: special tokens cache size = 25
0.00.040.211 I load: token to piece cache size = 0.2984 MB
0.00.040.215 I print_info: arch             = gptneox
0.00.040.216 I print_info: vocab_only       = 0
0.00.040.216 I print_info: n_ctx_train      = 2048
0.00.040.216 I print_info: n_embd           = 2048
0.00.040.216 I print_info: n_layer          = 24
0.00.040.220 I print_info: n_head           = 16
0.00.040.221 I print_info: n_head_kv        = 16
0.00.040.221 I print_info: n_rot            = 32
0.00.040.222 I print_info: n_swa            = 0
0.00.040.222 I print_info: n_embd_head_k    = 128
0.00.040.222 I print_info: n_embd_head_v    = 128
0.00.040.223 I print_info: n_gqa            = 1
0.00.040.223 I print_info: n_embd_k_gqa     = 2048
0.00.040.224 I print_info: n_embd_v_gqa     = 2048
0.00.040.225 I print_info: f_norm_eps       = 1.0e-05
0.00.040.225 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.226 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.227 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.227 I print_info: f_logit_scale    = 0.0e+00
0.00.040.227 I print_info: n_ff             = 8192
0.00.040.228 I print_info: n_expert         = 0
0.00.040.228 I print_info: n_expert_used    = 0
0.00.040.228 I print_info: causal attn      = 1
0.00.040.228 I print_info: pooling type     = 0
0.00.040.228 I print_info: rope type        = 2
0.00.040.230 I print_info: rope scaling     = linear
0.00.040.231 I print_info: freq_base_train  = 10000.0
0.00.040.231 I print_info: freq_scale_train = 1
0.00.040.231 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.231 I print_info: rope_finetuned   = unknown
0.00.040.231 I print_info: ssm_d_conv       = 0
0.00.040.232 I print_info: ssm_d_inner      = 0
0.00.040.233 I print_info: ssm_d_state      = 0
0.00.040.233 I print_info: ssm_dt_rank      = 0
0.00.040.233 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.233 I print_info: model type       = 1.4B
0.00.040.233 I print_info: model params     = 1.41 B
0.00.040.235 I print_info: general.name     = 1.4B
0.00.040.235 I print_info: vocab type       = BPE
0.00.040.235 I print_info: n_vocab          = 50304
0.00.040.235 I print_info: n_merges         = 50009
0.00.040.236 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.236 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.236 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.236 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.236 I print_info: LF token         = 187 ''
0.00.040.236 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.237 I print_info: max token length = 1024
0.00.040.237 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.607.072 I load_tensors: offloading 24 repeating layers to GPU
0.00.607.087 I load_tensors: offloading output layer to GPU
0.00.607.088 I load_tensors: offloaded 25/25 layers to GPU
0.00.607.125 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.607.126 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.608.821 I llama_init_from_model: n_seq_max     = 1
0.00.608.823 I llama_init_from_model: n_ctx         = 128
0.00.608.824 I llama_init_from_model: n_ctx_per_seq = 128
0.00.608.824 I llama_init_from_model: n_batch       = 128
0.00.608.824 I llama_init_from_model: n_ubatch      = 128
0.00.608.825 I llama_init_from_model: flash_attn    = 0
0.00.608.827 I llama_init_from_model: freq_base     = 10000.0
0.00.608.828 I llama_init_from_model: freq_scale    = 1
0.00.608.828 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.608.831 I ggml_metal_init: allocating
0.00.608.913 I ggml_metal_init: found device: Apple M4
0.00.608.932 I ggml_metal_init: picking default device: Apple M4
0.00.610.760 I ggml_metal_init: using embedded metal library
0.00.617.071 I ggml_metal_init: GPU name:   Apple M4
0.00.617.080 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.080 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.081 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.082 I ggml_metal_init: simdgroup reduction   = true
0.00.617.082 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.083 I ggml_metal_init: has residency sets    = true
0.00.617.083 I ggml_metal_init: has bfloat            = true
0.00.617.083 I ggml_metal_init: use bfloat            = true
0.00.617.085 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.097 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.584 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.639.172 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.639.176 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.639.218 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.642.576 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.642.578 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.642.579 I llama_init_from_model: graph nodes  = 967
0.00.642.579 I llama_init_from_model: graph splits = 2
0.00.642.583 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.642.583 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.119 I 
0.00.672.179 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.186 I perplexity: tokenizing the input ..
0.00.679.296 I perplexity: tokenization took 7.109 ms
0.00.679.302 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.636 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.811.971 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.811.993 I llama_perf_context_print:        load time =     662.53 ms
0.00.811.993 I llama_perf_context_print: prompt eval time =     131.05 ms /   128 tokens (    1.02 ms per token,   976.76 tokens per second)
0.00.811.994 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.994 I llama_perf_context_print:       total time =     139.88 ms /   129 tokens
0.00.812.361 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.079s
sys	0m0.132s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.755 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.591 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.602 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.604 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.604 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.605 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.605 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.605 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.606 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.607 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.607 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.607 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.608 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.609 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.610 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.611 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.611 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.437 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.459 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.294 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.295 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.296 I llama_model_loader: - type  f32:  194 tensors
0.00.032.297 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.297 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.297 I print_info: file format = GGUF V3 (latest)
0.00.032.298 I print_info: file type   = Q4_1
0.00.032.299 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.040.977 I load: special tokens cache size = 25
0.00.047.857 I load: token to piece cache size = 0.2984 MB
0.00.047.860 I print_info: arch             = gptneox
0.00.047.860 I print_info: vocab_only       = 0
0.00.047.860 I print_info: n_ctx_train      = 2048
0.00.047.860 I print_info: n_embd           = 2048
0.00.047.860 I print_info: n_layer          = 24
0.00.047.863 I print_info: n_head           = 16
0.00.047.864 I print_info: n_head_kv        = 16
0.00.047.864 I print_info: n_rot            = 32
0.00.047.864 I print_info: n_swa            = 0
0.00.047.864 I print_info: n_embd_head_k    = 128
0.00.047.864 I print_info: n_embd_head_v    = 128
0.00.047.865 I print_info: n_gqa            = 1
0.00.047.866 I print_info: n_embd_k_gqa     = 2048
0.00.047.866 I print_info: n_embd_v_gqa     = 2048
0.00.047.867 I print_info: f_norm_eps       = 1.0e-05
0.00.047.867 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.869 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.869 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.869 I print_info: f_logit_scale    = 0.0e+00
0.00.047.870 I print_info: n_ff             = 8192
0.00.047.870 I print_info: n_expert         = 0
0.00.047.871 I print_info: n_expert_used    = 0
0.00.047.871 I print_info: causal attn      = 1
0.00.047.871 I print_info: pooling type     = 0
0.00.047.871 I print_info: rope type        = 2
0.00.047.871 I print_info: rope scaling     = linear
0.00.047.872 I print_info: freq_base_train  = 10000.0
0.00.047.872 I print_info: freq_scale_train = 1
0.00.047.872 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.872 I print_info: rope_finetuned   = unknown
0.00.047.873 I print_info: ssm_d_conv       = 0
0.00.047.873 I print_info: ssm_d_inner      = 0
0.00.047.873 I print_info: ssm_d_state      = 0
0.00.047.873 I print_info: ssm_dt_rank      = 0
0.00.047.873 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.873 I print_info: model type       = 1.4B
0.00.047.874 I print_info: model params     = 1.41 B
0.00.047.874 I print_info: general.name     = 1.4B
0.00.047.874 I print_info: vocab type       = BPE
0.00.047.875 I print_info: n_vocab          = 50304
0.00.047.875 I print_info: n_merges         = 50009
0.00.047.875 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.875 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.875 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.875 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.876 I print_info: LF token         = 187 ''
0.00.047.876 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.876 I print_info: max token length = 1024
0.00.047.876 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.692.843 I load_tensors: offloading 24 repeating layers to GPU
0.00.692.856 I load_tensors: offloading output layer to GPU
0.00.692.857 I load_tensors: offloaded 25/25 layers to GPU
0.00.692.892 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.692.894 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.694.390 I llama_init_from_model: n_seq_max     = 1
0.00.694.392 I llama_init_from_model: n_ctx         = 2048
0.00.694.393 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.694.394 I llama_init_from_model: n_batch       = 2048
0.00.694.394 I llama_init_from_model: n_ubatch      = 512
0.00.694.395 I llama_init_from_model: flash_attn    = 0
0.00.694.396 I llama_init_from_model: freq_base     = 10000.0
0.00.694.397 I llama_init_from_model: freq_scale    = 1
0.00.694.399 I ggml_metal_init: allocating
0.00.694.473 I ggml_metal_init: found device: Apple M4
0.00.694.488 I ggml_metal_init: picking default device: Apple M4
0.00.696.445 I ggml_metal_init: using embedded metal library
0.00.703.045 I ggml_metal_init: GPU name:   Apple M4
0.00.703.048 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.703.049 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.703.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.703.051 I ggml_metal_init: simdgroup reduction   = true
0.00.703.051 I ggml_metal_init: simdgroup matrix mul. = true
0.00.703.051 I ggml_metal_init: has residency sets    = true
0.00.703.051 I ggml_metal_init: has bfloat            = true
0.00.703.052 I ggml_metal_init: use bfloat            = true
0.00.703.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.703.054 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.720.848 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.775.019 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.775.026 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.775.073 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.779.398 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.779.400 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.779.400 I llama_init_from_model: graph nodes  = 967
0.00.779.401 I llama_init_from_model: graph splits = 2
0.00.779.407 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.779.535 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.779.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.833.305 I main: llama threadpool init, n_threads = 4
0.00.833.357 I 
0.00.833.373 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.833.373 I 
0.00.833.526 I sampler seed: 1234
0.00.833.531 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.833.551 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.833.552 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.833.552 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.562.402 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.562.403 I llama_perf_context_print:        load time =     821.83 ms
0.01.562.403 I llama_perf_context_print: prompt eval time =      48.90 ms /     7 tokens (    6.99 ms per token,   143.14 tokens per second)
0.01.562.404 I llama_perf_context_print:        eval time =     677.13 ms /    63 runs   (   10.75 ms per token,    93.04 tokens per second)
0.01.562.404 I llama_perf_context_print:       total time =     729.81 ms /    70 tokens
0.01.562.699 I ggml_metal_free: deallocating

real	0m1.581s
user	0m0.111s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.064 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.079 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.085 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.087 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.092 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.093 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.093 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.093 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.094 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.095 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.095 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.097 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.097 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.098 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.098 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.959 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.956 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.749 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.749 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.750 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.750 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.750 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.751 I llama_model_loader: - type  f32:  194 tensors
0.00.024.751 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.752 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.753 I print_info: file format = GGUF V3 (latest)
0.00.024.753 I print_info: file type   = Q4_1
0.00.024.755 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.879 I load: special tokens cache size = 25
0.00.038.976 I load: token to piece cache size = 0.2984 MB
0.00.038.980 I print_info: arch             = gptneox
0.00.038.980 I print_info: vocab_only       = 0
0.00.038.980 I print_info: n_ctx_train      = 2048
0.00.038.980 I print_info: n_embd           = 2048
0.00.038.981 I print_info: n_layer          = 24
0.00.038.985 I print_info: n_head           = 16
0.00.038.986 I print_info: n_head_kv        = 16
0.00.038.986 I print_info: n_rot            = 32
0.00.038.986 I print_info: n_swa            = 0
0.00.038.986 I print_info: n_embd_head_k    = 128
0.00.038.986 I print_info: n_embd_head_v    = 128
0.00.038.987 I print_info: n_gqa            = 1
0.00.038.988 I print_info: n_embd_k_gqa     = 2048
0.00.038.988 I print_info: n_embd_v_gqa     = 2048
0.00.038.989 I print_info: f_norm_eps       = 1.0e-05
0.00.038.990 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.990 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.990 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.990 I print_info: f_logit_scale    = 0.0e+00
0.00.038.991 I print_info: n_ff             = 8192
0.00.038.991 I print_info: n_expert         = 0
0.00.038.991 I print_info: n_expert_used    = 0
0.00.038.991 I print_info: causal attn      = 1
0.00.038.991 I print_info: pooling type     = 0
0.00.038.991 I print_info: rope type        = 2
0.00.038.992 I print_info: rope scaling     = linear
0.00.038.992 I print_info: freq_base_train  = 10000.0
0.00.038.992 I print_info: freq_scale_train = 1
0.00.038.992 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.993 I print_info: rope_finetuned   = unknown
0.00.038.993 I print_info: ssm_d_conv       = 0
0.00.038.993 I print_info: ssm_d_inner      = 0
0.00.038.993 I print_info: ssm_d_state      = 0
0.00.038.993 I print_info: ssm_dt_rank      = 0
0.00.038.993 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.993 I print_info: model type       = 1.4B
0.00.038.994 I print_info: model params     = 1.41 B
0.00.038.994 I print_info: general.name     = 1.4B
0.00.038.994 I print_info: vocab type       = BPE
0.00.038.994 I print_info: n_vocab          = 50304
0.00.038.995 I print_info: n_merges         = 50009
0.00.038.995 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.996 I print_info: LF token         = 187 ''
0.00.038.998 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.999 I print_info: max token length = 1024
0.00.038.999 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.648.350 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.364 I load_tensors: offloading output layer to GPU
0.00.648.364 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.400 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.648.402 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.650.144 I llama_init_from_model: n_seq_max     = 1
0.00.650.147 I llama_init_from_model: n_ctx         = 128
0.00.650.148 I llama_init_from_model: n_ctx_per_seq = 128
0.00.650.148 I llama_init_from_model: n_batch       = 128
0.00.650.148 I llama_init_from_model: n_ubatch      = 128
0.00.650.149 I llama_init_from_model: flash_attn    = 0
0.00.650.151 I llama_init_from_model: freq_base     = 10000.0
0.00.650.152 I llama_init_from_model: freq_scale    = 1
0.00.650.152 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.650.155 I ggml_metal_init: allocating
0.00.650.220 I ggml_metal_init: found device: Apple M4
0.00.650.234 I ggml_metal_init: picking default device: Apple M4
0.00.652.006 I ggml_metal_init: using embedded metal library
0.00.658.857 I ggml_metal_init: GPU name:   Apple M4
0.00.658.864 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.865 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.866 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.867 I ggml_metal_init: simdgroup reduction   = true
0.00.658.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.867 I ggml_metal_init: has residency sets    = true
0.00.658.868 I ggml_metal_init: has bfloat            = true
0.00.658.868 I ggml_metal_init: use bfloat            = true
0.00.658.869 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.880 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.089 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.649 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.680.653 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.680.702 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.683.890 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.683.891 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.683.892 I llama_init_from_model: graph nodes  = 967
0.00.683.892 I llama_init_from_model: graph splits = 2
0.00.683.895 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.683.895 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.031 I 
0.00.712.093 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.099 I perplexity: tokenizing the input ..
0.00.718.932 I perplexity: tokenization took 6.832 ms
0.00.718.936 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.319 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.850.656 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.850.681 I llama_perf_context_print:        load time =     702.95 ms
0.00.850.682 I llama_perf_context_print: prompt eval time =     130.15 ms /   128 tokens (    1.02 ms per token,   983.47 tokens per second)
0.00.850.683 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.683 I llama_perf_context_print:       total time =     138.65 ms /   129 tokens
0.00.851.078 I ggml_metal_free: deallocating

real	0m0.865s
user	0m0.079s
sys	0m0.132s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.822 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.851 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.855 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.857 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.858 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.858 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.862 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.863 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.863 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.864 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.864 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.865 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.865 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.865 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.866 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.869 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.869 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.869 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.761 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.758 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.545 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.547 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.547 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.547 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.548 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.548 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.548 I llama_model_loader: - type  f32:  194 tensors
0.00.025.549 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.549 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.550 I print_info: file format = GGUF V3 (latest)
0.00.025.550 I print_info: file type   = Q5_0
0.00.025.554 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.496 I load: special tokens cache size = 25
0.00.039.497 I load: token to piece cache size = 0.2984 MB
0.00.039.500 I print_info: arch             = gptneox
0.00.039.501 I print_info: vocab_only       = 0
0.00.039.501 I print_info: n_ctx_train      = 2048
0.00.039.501 I print_info: n_embd           = 2048
0.00.039.501 I print_info: n_layer          = 24
0.00.039.504 I print_info: n_head           = 16
0.00.039.505 I print_info: n_head_kv        = 16
0.00.039.505 I print_info: n_rot            = 32
0.00.039.505 I print_info: n_swa            = 0
0.00.039.507 I print_info: n_embd_head_k    = 128
0.00.039.507 I print_info: n_embd_head_v    = 128
0.00.039.508 I print_info: n_gqa            = 1
0.00.039.509 I print_info: n_embd_k_gqa     = 2048
0.00.039.509 I print_info: n_embd_v_gqa     = 2048
0.00.039.515 I print_info: f_norm_eps       = 1.0e-05
0.00.039.515 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.516 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.516 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.520 I print_info: f_logit_scale    = 0.0e+00
0.00.039.520 I print_info: n_ff             = 8192
0.00.039.521 I print_info: n_expert         = 0
0.00.039.521 I print_info: n_expert_used    = 0
0.00.039.521 I print_info: causal attn      = 1
0.00.039.521 I print_info: pooling type     = 0
0.00.039.523 I print_info: rope type        = 2
0.00.039.524 I print_info: rope scaling     = linear
0.00.039.524 I print_info: freq_base_train  = 10000.0
0.00.039.524 I print_info: freq_scale_train = 1
0.00.039.525 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.525 I print_info: rope_finetuned   = unknown
0.00.039.525 I print_info: ssm_d_conv       = 0
0.00.039.525 I print_info: ssm_d_inner      = 0
0.00.039.525 I print_info: ssm_d_state      = 0
0.00.039.525 I print_info: ssm_dt_rank      = 0
0.00.039.525 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.525 I print_info: model type       = 1.4B
0.00.039.526 I print_info: model params     = 1.41 B
0.00.039.526 I print_info: general.name     = 1.4B
0.00.039.526 I print_info: vocab type       = BPE
0.00.039.527 I print_info: n_vocab          = 50304
0.00.039.527 I print_info: n_merges         = 50009
0.00.039.529 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.529 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.530 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.530 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.530 I print_info: LF token         = 187 ''
0.00.039.530 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.530 I print_info: max token length = 1024
0.00.039.531 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.688.134 I load_tensors: offloading 24 repeating layers to GPU
0.00.688.150 I load_tensors: offloading output layer to GPU
0.00.688.151 I load_tensors: offloaded 25/25 layers to GPU
0.00.688.182 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.688.183 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.689.509 I llama_init_from_model: n_seq_max     = 1
0.00.689.519 I llama_init_from_model: n_ctx         = 2048
0.00.689.520 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.689.520 I llama_init_from_model: n_batch       = 2048
0.00.689.521 I llama_init_from_model: n_ubatch      = 512
0.00.689.521 I llama_init_from_model: flash_attn    = 0
0.00.689.522 I llama_init_from_model: freq_base     = 10000.0
0.00.689.522 I llama_init_from_model: freq_scale    = 1
0.00.689.525 I ggml_metal_init: allocating
0.00.689.580 I ggml_metal_init: found device: Apple M4
0.00.689.596 I ggml_metal_init: picking default device: Apple M4
0.00.691.362 I ggml_metal_init: using embedded metal library
0.00.696.476 I ggml_metal_init: GPU name:   Apple M4
0.00.696.480 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.481 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.482 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.482 I ggml_metal_init: simdgroup reduction   = true
0.00.696.482 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.482 I ggml_metal_init: has residency sets    = true
0.00.696.483 I ggml_metal_init: has bfloat            = true
0.00.696.483 I ggml_metal_init: use bfloat            = true
0.00.696.483 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.485 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.709.806 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.584 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.744.591 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.744.625 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.734 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.748.736 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.748.736 I llama_init_from_model: graph nodes  = 967
0.00.748.737 I llama_init_from_model: graph splits = 2
0.00.748.743 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.871 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.509 I main: llama threadpool init, n_threads = 4
0.00.806.551 I 
0.00.806.567 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.806.568 I 
0.00.806.712 I sampler seed: 1234
0.00.806.716 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.806.727 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.806.727 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.806.727 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.595.405 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48965.52 tokens per second)
0.01.595.405 I llama_perf_context_print:        load time =     796.97 ms
0.01.595.406 I llama_perf_context_print: prompt eval time =      42.86 ms /     7 tokens (    6.12 ms per token,   163.33 tokens per second)
0.01.595.408 I llama_perf_context_print:        eval time =     743.31 ms /    63 runs   (   11.80 ms per token,    84.76 tokens per second)
0.01.595.408 I llama_perf_context_print:       total time =     789.60 ms /    70 tokens
0.01.595.660 I ggml_metal_free: deallocating

real	0m1.611s
user	0m0.104s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.374 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.907 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.913 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.915 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.915 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.915 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.916 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.916 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.917 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.920 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.921 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.921 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.921 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.922 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.923 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.924 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.924 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.860 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.904 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.823 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.824 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.825 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.825 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.825 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.826 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.826 I llama_model_loader: - type  f32:  194 tensors
0.00.026.827 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.827 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.828 I print_info: file format = GGUF V3 (latest)
0.00.026.828 I print_info: file type   = Q5_0
0.00.026.829 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.266 I load: special tokens cache size = 25
0.00.041.384 I load: token to piece cache size = 0.2984 MB
0.00.041.388 I print_info: arch             = gptneox
0.00.041.388 I print_info: vocab_only       = 0
0.00.041.389 I print_info: n_ctx_train      = 2048
0.00.041.389 I print_info: n_embd           = 2048
0.00.041.389 I print_info: n_layer          = 24
0.00.041.393 I print_info: n_head           = 16
0.00.041.394 I print_info: n_head_kv        = 16
0.00.041.394 I print_info: n_rot            = 32
0.00.041.394 I print_info: n_swa            = 0
0.00.041.396 I print_info: n_embd_head_k    = 128
0.00.041.398 I print_info: n_embd_head_v    = 128
0.00.041.399 I print_info: n_gqa            = 1
0.00.041.399 I print_info: n_embd_k_gqa     = 2048
0.00.041.400 I print_info: n_embd_v_gqa     = 2048
0.00.041.400 I print_info: f_norm_eps       = 1.0e-05
0.00.041.402 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.402 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.402 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.402 I print_info: f_logit_scale    = 0.0e+00
0.00.041.403 I print_info: n_ff             = 8192
0.00.041.403 I print_info: n_expert         = 0
0.00.041.403 I print_info: n_expert_used    = 0
0.00.041.403 I print_info: causal attn      = 1
0.00.041.403 I print_info: pooling type     = 0
0.00.041.404 I print_info: rope type        = 2
0.00.041.404 I print_info: rope scaling     = linear
0.00.041.404 I print_info: freq_base_train  = 10000.0
0.00.041.404 I print_info: freq_scale_train = 1
0.00.041.406 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.406 I print_info: rope_finetuned   = unknown
0.00.041.406 I print_info: ssm_d_conv       = 0
0.00.041.406 I print_info: ssm_d_inner      = 0
0.00.041.406 I print_info: ssm_d_state      = 0
0.00.041.406 I print_info: ssm_dt_rank      = 0
0.00.041.406 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.407 I print_info: model type       = 1.4B
0.00.041.408 I print_info: model params     = 1.41 B
0.00.041.408 I print_info: general.name     = 1.4B
0.00.041.440 I print_info: vocab type       = BPE
0.00.041.442 I print_info: n_vocab          = 50304
0.00.041.442 I print_info: n_merges         = 50009
0.00.041.442 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.443 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.443 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.443 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.443 I print_info: LF token         = 187 ''
0.00.041.444 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.444 I print_info: max token length = 1024
0.00.041.444 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.702.437 I load_tensors: offloading 24 repeating layers to GPU
0.00.702.448 I load_tensors: offloading output layer to GPU
0.00.702.449 I load_tensors: offloaded 25/25 layers to GPU
0.00.702.477 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.702.478 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.703.999 I llama_init_from_model: n_seq_max     = 1
0.00.704.005 I llama_init_from_model: n_ctx         = 128
0.00.704.006 I llama_init_from_model: n_ctx_per_seq = 128
0.00.704.006 I llama_init_from_model: n_batch       = 128
0.00.704.006 I llama_init_from_model: n_ubatch      = 128
0.00.704.007 I llama_init_from_model: flash_attn    = 0
0.00.704.008 I llama_init_from_model: freq_base     = 10000.0
0.00.704.009 I llama_init_from_model: freq_scale    = 1
0.00.704.009 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.704.011 I ggml_metal_init: allocating
0.00.704.074 I ggml_metal_init: found device: Apple M4
0.00.704.089 I ggml_metal_init: picking default device: Apple M4
0.00.705.857 I ggml_metal_init: using embedded metal library
0.00.712.102 I ggml_metal_init: GPU name:   Apple M4
0.00.712.109 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.712.110 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.712.111 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.712.112 I ggml_metal_init: simdgroup reduction   = true
0.00.712.112 I ggml_metal_init: simdgroup matrix mul. = true
0.00.712.113 I ggml_metal_init: has residency sets    = true
0.00.712.113 I ggml_metal_init: has bfloat            = true
0.00.712.113 I ggml_metal_init: use bfloat            = true
0.00.712.115 I ggml_metal_init: hasUnifiedMemory      = true
0.00.712.125 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.730.745 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.734.307 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.734.310 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.734.366 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.737.736 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.737.738 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.737.738 I llama_init_from_model: graph nodes  = 967
0.00.737.738 I llama_init_from_model: graph splits = 2
0.00.737.742 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.737.742 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.437 I 
0.00.768.497 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.504 I perplexity: tokenizing the input ..
0.00.775.397 I perplexity: tokenization took 6.892 ms
0.00.775.404 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.910.310 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.911.645 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.911.668 I llama_perf_context_print:        load time =     758.05 ms
0.00.911.669 I llama_perf_context_print: prompt eval time =     134.49 ms /   128 tokens (    1.05 ms per token,   951.76 tokens per second)
0.00.911.671 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.911.671 I llama_perf_context_print:       total time =     143.23 ms /   129 tokens
0.00.912.073 I ggml_metal_free: deallocating

real	0m0.927s
user	0m0.079s
sys	0m0.141s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.016.516 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.745 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.758 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.758 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.648 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.776 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.702 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.703 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.703 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.704 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.704 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.704 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.033.705 I llama_model_loader: - type  f32:  194 tensors
0.00.033.705 I llama_model_loader: - type q5_1:   97 tensors
0.00.033.705 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.706 I print_info: file format = GGUF V3 (latest)
0.00.033.706 I print_info: file type   = Q5_1
0.00.033.707 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.041.967 I load: special tokens cache size = 25
0.00.048.155 I load: token to piece cache size = 0.2984 MB
0.00.048.159 I print_info: arch             = gptneox
0.00.048.160 I print_info: vocab_only       = 0
0.00.048.160 I print_info: n_ctx_train      = 2048
0.00.048.160 I print_info: n_embd           = 2048
0.00.048.160 I print_info: n_layer          = 24
0.00.048.163 I print_info: n_head           = 16
0.00.048.164 I print_info: n_head_kv        = 16
0.00.048.164 I print_info: n_rot            = 32
0.00.048.164 I print_info: n_swa            = 0
0.00.048.164 I print_info: n_embd_head_k    = 128
0.00.048.164 I print_info: n_embd_head_v    = 128
0.00.048.166 I print_info: n_gqa            = 1
0.00.048.167 I print_info: n_embd_k_gqa     = 2048
0.00.048.169 I print_info: n_embd_v_gqa     = 2048
0.00.048.170 I print_info: f_norm_eps       = 1.0e-05
0.00.048.170 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.170 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.171 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.172 I print_info: f_logit_scale    = 0.0e+00
0.00.048.174 I print_info: n_ff             = 8192
0.00.048.174 I print_info: n_expert         = 0
0.00.048.174 I print_info: n_expert_used    = 0
0.00.048.174 I print_info: causal attn      = 1
0.00.048.174 I print_info: pooling type     = 0
0.00.048.174 I print_info: rope type        = 2
0.00.048.175 I print_info: rope scaling     = linear
0.00.048.175 I print_info: freq_base_train  = 10000.0
0.00.048.175 I print_info: freq_scale_train = 1
0.00.048.175 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.176 I print_info: rope_finetuned   = unknown
0.00.048.176 I print_info: ssm_d_conv       = 0
0.00.048.176 I print_info: ssm_d_inner      = 0
0.00.048.176 I print_info: ssm_d_state      = 0
0.00.048.176 I print_info: ssm_dt_rank      = 0
0.00.048.176 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.176 I print_info: model type       = 1.4B
0.00.048.177 I print_info: model params     = 1.41 B
0.00.048.177 I print_info: general.name     = 1.4B
0.00.048.177 I print_info: vocab type       = BPE
0.00.048.177 I print_info: n_vocab          = 50304
0.00.048.177 I print_info: n_merges         = 50009
0.00.048.178 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.178 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.178 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.180 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.181 I print_info: LF token         = 187 ''
0.00.048.181 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.181 I print_info: max token length = 1024
0.00.048.181 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.040.912 I load_tensors: offloading 24 repeating layers to GPU
0.01.040.928 I load_tensors: offloading output layer to GPU
0.01.040.929 I load_tensors: offloaded 25/25 layers to GPU
0.01.040.962 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.01.040.964 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.01.042.670 I llama_init_from_model: n_seq_max     = 1
0.01.042.673 I llama_init_from_model: n_ctx         = 2048
0.01.042.673 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.042.674 I llama_init_from_model: n_batch       = 2048
0.01.042.674 I llama_init_from_model: n_ubatch      = 512
0.01.042.674 I llama_init_from_model: flash_attn    = 0
0.01.042.677 I llama_init_from_model: freq_base     = 10000.0
0.01.042.678 I llama_init_from_model: freq_scale    = 1
0.01.042.680 I ggml_metal_init: allocating
0.01.042.747 I ggml_metal_init: found device: Apple M4
0.01.042.760 I ggml_metal_init: picking default device: Apple M4
0.01.044.370 I ggml_metal_init: using embedded metal library
0.01.050.596 I ggml_metal_init: GPU name:   Apple M4
0.01.050.600 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.050.601 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.050.601 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.050.602 I ggml_metal_init: simdgroup reduction   = true
0.01.050.602 I ggml_metal_init: simdgroup matrix mul. = true
0.01.050.602 I ggml_metal_init: has residency sets    = true
0.01.050.603 I ggml_metal_init: has bfloat            = true
0.01.050.603 I ggml_metal_init: use bfloat            = true
0.01.050.604 I ggml_metal_init: hasUnifiedMemory      = true
0.01.050.607 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.066.998 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.120.300 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.120.307 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.120.386 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.125.496 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.125.499 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.125.499 I llama_init_from_model: graph nodes  = 967
0.01.125.500 I llama_init_from_model: graph splits = 2
0.01.125.505 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.125.625 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.125.626 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.186.646 I main: llama threadpool init, n_threads = 4
0.01.186.693 I 
0.01.186.709 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.186.709 I 
0.01.186.891 I sampler seed: 1234
0.01.186.896 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.186.908 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.186.908 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.186.909 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.02.031.843 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51486.58 tokens per second)
0.02.031.843 I llama_perf_context_print:        load time =    1169.41 ms
0.02.031.844 I llama_perf_context_print: prompt eval time =      51.93 ms /     7 tokens (    7.42 ms per token,   134.80 tokens per second)
0.02.031.845 I llama_perf_context_print:        eval time =     790.05 ms /    63 runs   (   12.54 ms per token,    79.74 tokens per second)
0.02.031.845 I llama_perf_context_print:       total time =     845.91 ms /    70 tokens
0.02.032.069 I ggml_metal_free: deallocating

real	0m2.066s
user	0m0.109s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.039 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.202 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.208 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.215 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.216 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.216 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.217 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.217 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.219 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.219 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.220 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.222 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.222 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.224 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.004 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.062 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.875 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.877 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.877 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.877 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.878 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.878 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.879 I llama_model_loader: - type  f32:  194 tensors
0.00.024.879 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.879 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.880 I print_info: file format = GGUF V3 (latest)
0.00.024.881 I print_info: file type   = Q5_1
0.00.024.882 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.060 I load: special tokens cache size = 25
0.00.039.194 I load: token to piece cache size = 0.2984 MB
0.00.039.198 I print_info: arch             = gptneox
0.00.039.198 I print_info: vocab_only       = 0
0.00.039.199 I print_info: n_ctx_train      = 2048
0.00.039.199 I print_info: n_embd           = 2048
0.00.039.199 I print_info: n_layer          = 24
0.00.039.204 I print_info: n_head           = 16
0.00.039.204 I print_info: n_head_kv        = 16
0.00.039.204 I print_info: n_rot            = 32
0.00.039.205 I print_info: n_swa            = 0
0.00.039.205 I print_info: n_embd_head_k    = 128
0.00.039.205 I print_info: n_embd_head_v    = 128
0.00.039.206 I print_info: n_gqa            = 1
0.00.039.207 I print_info: n_embd_k_gqa     = 2048
0.00.039.207 I print_info: n_embd_v_gqa     = 2048
0.00.039.208 I print_info: f_norm_eps       = 1.0e-05
0.00.039.208 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.208 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.209 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.209 I print_info: f_logit_scale    = 0.0e+00
0.00.039.209 I print_info: n_ff             = 8192
0.00.039.210 I print_info: n_expert         = 0
0.00.039.210 I print_info: n_expert_used    = 0
0.00.039.210 I print_info: causal attn      = 1
0.00.039.210 I print_info: pooling type     = 0
0.00.039.210 I print_info: rope type        = 2
0.00.039.210 I print_info: rope scaling     = linear
0.00.039.211 I print_info: freq_base_train  = 10000.0
0.00.039.211 I print_info: freq_scale_train = 1
0.00.039.211 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.211 I print_info: rope_finetuned   = unknown
0.00.039.211 I print_info: ssm_d_conv       = 0
0.00.039.212 I print_info: ssm_d_inner      = 0
0.00.039.212 I print_info: ssm_d_state      = 0
0.00.039.214 I print_info: ssm_dt_rank      = 0
0.00.039.214 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.215 I print_info: model type       = 1.4B
0.00.039.215 I print_info: model params     = 1.41 B
0.00.039.215 I print_info: general.name     = 1.4B
0.00.039.216 I print_info: vocab type       = BPE
0.00.039.216 I print_info: n_vocab          = 50304
0.00.039.216 I print_info: n_merges         = 50009
0.00.039.216 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.216 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.218 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.218 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.218 I print_info: LF token         = 187 ''
0.00.039.218 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.218 I print_info: max token length = 1024
0.00.039.219 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.626.559 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.573 I load_tensors: offloading output layer to GPU
0.00.626.574 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.608 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.626.610 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.628.357 I llama_init_from_model: n_seq_max     = 1
0.00.628.359 I llama_init_from_model: n_ctx         = 128
0.00.628.360 I llama_init_from_model: n_ctx_per_seq = 128
0.00.628.361 I llama_init_from_model: n_batch       = 128
0.00.628.361 I llama_init_from_model: n_ubatch      = 128
0.00.628.362 I llama_init_from_model: flash_attn    = 0
0.00.628.365 I llama_init_from_model: freq_base     = 10000.0
0.00.628.365 I llama_init_from_model: freq_scale    = 1
0.00.628.366 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.628.368 I ggml_metal_init: allocating
0.00.628.447 I ggml_metal_init: found device: Apple M4
0.00.628.461 I ggml_metal_init: picking default device: Apple M4
0.00.630.049 I ggml_metal_init: using embedded metal library
0.00.636.412 I ggml_metal_init: GPU name:   Apple M4
0.00.636.418 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.419 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.419 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.420 I ggml_metal_init: simdgroup reduction   = true
0.00.636.420 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.420 I ggml_metal_init: has residency sets    = true
0.00.636.421 I ggml_metal_init: has bfloat            = true
0.00.636.421 I ggml_metal_init: use bfloat            = true
0.00.636.422 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.653.267 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.656.841 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.656.846 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.656.889 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.660.067 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.660.069 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.660.069 I llama_init_from_model: graph nodes  = 967
0.00.660.070 I llama_init_from_model: graph splits = 2
0.00.660.073 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.660.073 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.705 I 
0.00.689.762 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.773 I perplexity: tokenizing the input ..
0.00.697.614 I perplexity: tokenization took 7.838 ms
0.00.697.622 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.967 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.847.295 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.847.319 I llama_perf_context_print:        load time =     680.65 ms
0.00.847.320 I llama_perf_context_print: prompt eval time =     147.41 ms /   128 tokens (    1.15 ms per token,   868.31 tokens per second)
0.00.847.321 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.321 I llama_perf_context_print:       total time =     157.62 ms /   129 tokens
0.00.847.677 I ggml_metal_free: deallocating

real	0m0.862s
user	0m0.080s
sys	0m0.136s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.012.951 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.125 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.029.130 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.134 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.135 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.135 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.136 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.136 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.137 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.138 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.139 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.034 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.836 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.838 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.838 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.838 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.839 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.839 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.039.839 I llama_model_loader: - type  f32:  194 tensors
0.00.039.840 I llama_model_loader: - type q2_K:   49 tensors
0.00.039.840 I llama_model_loader: - type q3_K:   48 tensors
0.00.039.840 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.841 I print_info: file format = GGUF V3 (latest)
0.00.039.841 I print_info: file type   = Q2_K - Medium
0.00.039.842 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.050.711 I load: special tokens cache size = 25
0.00.060.187 I load: token to piece cache size = 0.2984 MB
0.00.060.191 I print_info: arch             = gptneox
0.00.060.191 I print_info: vocab_only       = 0
0.00.060.191 I print_info: n_ctx_train      = 2048
0.00.060.192 I print_info: n_embd           = 2048
0.00.060.192 I print_info: n_layer          = 24
0.00.060.195 I print_info: n_head           = 16
0.00.060.196 I print_info: n_head_kv        = 16
0.00.060.197 I print_info: n_rot            = 32
0.00.060.197 I print_info: n_swa            = 0
0.00.060.197 I print_info: n_embd_head_k    = 128
0.00.060.199 I print_info: n_embd_head_v    = 128
0.00.060.200 I print_info: n_gqa            = 1
0.00.060.201 I print_info: n_embd_k_gqa     = 2048
0.00.060.202 I print_info: n_embd_v_gqa     = 2048
0.00.060.203 I print_info: f_norm_eps       = 1.0e-05
0.00.060.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.203 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.204 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.204 I print_info: f_logit_scale    = 0.0e+00
0.00.060.205 I print_info: n_ff             = 8192
0.00.060.205 I print_info: n_expert         = 0
0.00.060.205 I print_info: n_expert_used    = 0
0.00.060.205 I print_info: causal attn      = 1
0.00.060.206 I print_info: pooling type     = 0
0.00.060.206 I print_info: rope type        = 2
0.00.060.206 I print_info: rope scaling     = linear
0.00.060.207 I print_info: freq_base_train  = 10000.0
0.00.060.207 I print_info: freq_scale_train = 1
0.00.060.244 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.246 I print_info: rope_finetuned   = unknown
0.00.060.247 I print_info: ssm_d_conv       = 0
0.00.060.247 I print_info: ssm_d_inner      = 0
0.00.060.247 I print_info: ssm_d_state      = 0
0.00.060.247 I print_info: ssm_dt_rank      = 0
0.00.060.247 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.248 I print_info: model type       = 1.4B
0.00.060.249 I print_info: model params     = 1.41 B
0.00.060.249 I print_info: general.name     = 1.4B
0.00.060.249 I print_info: vocab type       = BPE
0.00.060.257 I print_info: n_vocab          = 50304
0.00.060.258 I print_info: n_merges         = 50009
0.00.060.259 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.259 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.259 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.259 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.260 I print_info: LF token         = 187 ''
0.00.060.260 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.261 I print_info: max token length = 1024
0.00.060.261 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.392.789 I load_tensors: offloading 24 repeating layers to GPU
0.00.392.802 I load_tensors: offloading output layer to GPU
0.00.392.803 I load_tensors: offloaded 25/25 layers to GPU
0.00.392.832 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.392.833 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.394.434 I llama_init_from_model: n_seq_max     = 1
0.00.394.440 I llama_init_from_model: n_ctx         = 2048
0.00.394.441 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.394.441 I llama_init_from_model: n_batch       = 2048
0.00.394.441 I llama_init_from_model: n_ubatch      = 512
0.00.394.442 I llama_init_from_model: flash_attn    = 0
0.00.394.443 I llama_init_from_model: freq_base     = 10000.0
0.00.394.443 I llama_init_from_model: freq_scale    = 1
0.00.394.446 I ggml_metal_init: allocating
0.00.394.491 I ggml_metal_init: found device: Apple M4
0.00.394.512 I ggml_metal_init: picking default device: Apple M4
0.00.396.248 I ggml_metal_init: using embedded metal library
0.00.401.977 I ggml_metal_init: GPU name:   Apple M4
0.00.401.989 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.401.989 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.401.990 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.401.991 I ggml_metal_init: simdgroup reduction   = true
0.00.401.991 I ggml_metal_init: simdgroup matrix mul. = true
0.00.401.991 I ggml_metal_init: has residency sets    = true
0.00.401.992 I ggml_metal_init: has bfloat            = true
0.00.401.992 I ggml_metal_init: use bfloat            = true
0.00.401.994 I ggml_metal_init: hasUnifiedMemory      = true
0.00.401.997 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.423.362 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.478.423 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.478.428 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.478.504 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.482.702 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.482.704 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.482.704 I llama_init_from_model: graph nodes  = 967
0.00.482.704 I llama_init_from_model: graph splits = 2
0.00.482.710 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.482.834 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.482.834 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.156 I main: llama threadpool init, n_threads = 4
0.00.543.199 I 
0.00.543.214 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.214 I 
0.00.543.393 I sampler seed: 1234
0.00.543.397 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.543.431 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.543.435 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.543.435 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.230.356 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.230.356 I llama_perf_context_print:        load time =     529.47 ms
0.01.230.358 I llama_perf_context_print: prompt eval time =      44.45 ms /     7 tokens (    6.35 ms per token,   157.50 tokens per second)
0.01.230.359 I llama_perf_context_print:        eval time =     639.62 ms /    63 runs   (   10.15 ms per token,    98.50 tokens per second)
0.01.230.360 I llama_perf_context_print:       total time =     687.92 ms /    70 tokens
0.01.230.566 I ggml_metal_free: deallocating

real	0m1.260s
user	0m0.122s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.144 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.150 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.156 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.158 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.159 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.159 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.160 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.161 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.161 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.162 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.162 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.163 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.163 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.166 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.097 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.977 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.978 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.979 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.979 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.979 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.980 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.981 I llama_model_loader: - type  f32:  194 tensors
0.00.025.981 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.981 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.981 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.982 I print_info: file format = GGUF V3 (latest)
0.00.025.983 I print_info: file type   = Q2_K - Medium
0.00.025.985 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.132 I load: special tokens cache size = 25
0.00.040.344 I load: token to piece cache size = 0.2984 MB
0.00.040.348 I print_info: arch             = gptneox
0.00.040.348 I print_info: vocab_only       = 0
0.00.040.348 I print_info: n_ctx_train      = 2048
0.00.040.348 I print_info: n_embd           = 2048
0.00.040.349 I print_info: n_layer          = 24
0.00.040.353 I print_info: n_head           = 16
0.00.040.353 I print_info: n_head_kv        = 16
0.00.040.354 I print_info: n_rot            = 32
0.00.040.354 I print_info: n_swa            = 0
0.00.040.356 I print_info: n_embd_head_k    = 128
0.00.040.357 I print_info: n_embd_head_v    = 128
0.00.040.357 I print_info: n_gqa            = 1
0.00.040.358 I print_info: n_embd_k_gqa     = 2048
0.00.040.359 I print_info: n_embd_v_gqa     = 2048
0.00.040.359 I print_info: f_norm_eps       = 1.0e-05
0.00.040.360 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.360 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.360 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.360 I print_info: f_logit_scale    = 0.0e+00
0.00.040.361 I print_info: n_ff             = 8192
0.00.040.361 I print_info: n_expert         = 0
0.00.040.361 I print_info: n_expert_used    = 0
0.00.040.361 I print_info: causal attn      = 1
0.00.040.363 I print_info: pooling type     = 0
0.00.040.363 I print_info: rope type        = 2
0.00.040.363 I print_info: rope scaling     = linear
0.00.040.363 I print_info: freq_base_train  = 10000.0
0.00.040.364 I print_info: freq_scale_train = 1
0.00.040.366 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.366 I print_info: rope_finetuned   = unknown
0.00.040.366 I print_info: ssm_d_conv       = 0
0.00.040.366 I print_info: ssm_d_inner      = 0
0.00.040.366 I print_info: ssm_d_state      = 0
0.00.040.366 I print_info: ssm_dt_rank      = 0
0.00.040.366 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.367 I print_info: model type       = 1.4B
0.00.040.367 I print_info: model params     = 1.41 B
0.00.040.367 I print_info: general.name     = 1.4B
0.00.040.368 I print_info: vocab type       = BPE
0.00.040.368 I print_info: n_vocab          = 50304
0.00.040.368 I print_info: n_merges         = 50009
0.00.040.368 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: LF token         = 187 ''
0.00.040.369 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.370 I print_info: max token length = 1024
0.00.040.370 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.356.705 I load_tensors: offloading 24 repeating layers to GPU
0.00.356.716 I load_tensors: offloading output layer to GPU
0.00.356.717 I load_tensors: offloaded 25/25 layers to GPU
0.00.356.750 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.356.754 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.358.249 I llama_init_from_model: n_seq_max     = 1
0.00.358.252 I llama_init_from_model: n_ctx         = 128
0.00.358.253 I llama_init_from_model: n_ctx_per_seq = 128
0.00.358.253 I llama_init_from_model: n_batch       = 128
0.00.358.253 I llama_init_from_model: n_ubatch      = 128
0.00.358.254 I llama_init_from_model: flash_attn    = 0
0.00.358.256 I llama_init_from_model: freq_base     = 10000.0
0.00.358.257 I llama_init_from_model: freq_scale    = 1
0.00.358.257 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.358.260 I ggml_metal_init: allocating
0.00.358.335 I ggml_metal_init: found device: Apple M4
0.00.358.349 I ggml_metal_init: picking default device: Apple M4
0.00.360.128 I ggml_metal_init: using embedded metal library
0.00.365.541 I ggml_metal_init: GPU name:   Apple M4
0.00.365.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.365.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.365.556 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.365.557 I ggml_metal_init: simdgroup reduction   = true
0.00.365.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.365.558 I ggml_metal_init: has residency sets    = true
0.00.365.558 I ggml_metal_init: has bfloat            = true
0.00.365.558 I ggml_metal_init: use bfloat            = true
0.00.365.560 I ggml_metal_init: hasUnifiedMemory      = true
0.00.365.566 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.387.008 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.390.651 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.390.659 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.390.731 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.394.031 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.394.033 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.394.033 I llama_init_from_model: graph nodes  = 967
0.00.394.034 I llama_init_from_model: graph splits = 2
0.00.394.037 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.394.037 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.426.477 I 
0.00.426.552 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.426.559 I perplexity: tokenizing the input ..
0.00.433.607 I perplexity: tokenization took 7.044 ms
0.00.433.614 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.578.146 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.579.507 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.579.529 I llama_perf_context_print:        load time =     416.32 ms
0.00.579.530 I llama_perf_context_print: prompt eval time =     143.59 ms /   128 tokens (    1.12 ms per token,   891.43 tokens per second)
0.00.579.531 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.579.531 I llama_perf_context_print:       total time =     153.06 ms /   129 tokens
0.00.579.903 I ggml_metal_free: deallocating

real	0m0.595s
user	0m0.082s
sys	0m0.094s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.801 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.059 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.065 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.066 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.071 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.072 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.072 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.073 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.074 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.074 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.074 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.075 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.076 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.077 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.077 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.024 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.874 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.875 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.875 I llama_model_loader: - type  f32:  194 tensors
0.00.026.876 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.876 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.876 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.876 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.877 I print_info: file format = GGUF V3 (latest)
0.00.026.878 I print_info: file type   = Q3_K - Medium
0.00.026.879 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.835 I load: special tokens cache size = 25
0.00.040.864 I load: token to piece cache size = 0.2984 MB
0.00.040.867 I print_info: arch             = gptneox
0.00.040.867 I print_info: vocab_only       = 0
0.00.040.868 I print_info: n_ctx_train      = 2048
0.00.040.868 I print_info: n_embd           = 2048
0.00.040.868 I print_info: n_layer          = 24
0.00.040.870 I print_info: n_head           = 16
0.00.040.871 I print_info: n_head_kv        = 16
0.00.040.871 I print_info: n_rot            = 32
0.00.040.871 I print_info: n_swa            = 0
0.00.040.872 I print_info: n_embd_head_k    = 128
0.00.040.872 I print_info: n_embd_head_v    = 128
0.00.040.872 I print_info: n_gqa            = 1
0.00.040.873 I print_info: n_embd_k_gqa     = 2048
0.00.040.876 I print_info: n_embd_v_gqa     = 2048
0.00.040.876 I print_info: f_norm_eps       = 1.0e-05
0.00.040.877 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.877 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.877 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.877 I print_info: f_logit_scale    = 0.0e+00
0.00.040.878 I print_info: n_ff             = 8192
0.00.040.878 I print_info: n_expert         = 0
0.00.040.878 I print_info: n_expert_used    = 0
0.00.040.880 I print_info: causal attn      = 1
0.00.040.880 I print_info: pooling type     = 0
0.00.040.880 I print_info: rope type        = 2
0.00.040.880 I print_info: rope scaling     = linear
0.00.040.881 I print_info: freq_base_train  = 10000.0
0.00.040.881 I print_info: freq_scale_train = 1
0.00.040.881 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.882 I print_info: rope_finetuned   = unknown
0.00.040.882 I print_info: ssm_d_conv       = 0
0.00.040.882 I print_info: ssm_d_inner      = 0
0.00.040.882 I print_info: ssm_d_state      = 0
0.00.040.882 I print_info: ssm_dt_rank      = 0
0.00.040.882 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.883 I print_info: model type       = 1.4B
0.00.040.883 I print_info: model params     = 1.41 B
0.00.040.883 I print_info: general.name     = 1.4B
0.00.040.884 I print_info: vocab type       = BPE
0.00.040.884 I print_info: n_vocab          = 50304
0.00.040.884 I print_info: n_merges         = 50009
0.00.040.884 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.884 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.884 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.885 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.885 I print_info: LF token         = 187 ''
0.00.040.885 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.885 I print_info: max token length = 1024
0.00.040.886 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.459.942 I load_tensors: offloading 24 repeating layers to GPU
0.00.459.957 I load_tensors: offloading output layer to GPU
0.00.459.957 I load_tensors: offloaded 25/25 layers to GPU
0.00.459.991 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.459.992 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.461.704 I llama_init_from_model: n_seq_max     = 1
0.00.461.707 I llama_init_from_model: n_ctx         = 2048
0.00.461.708 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.461.708 I llama_init_from_model: n_batch       = 2048
0.00.461.709 I llama_init_from_model: n_ubatch      = 512
0.00.461.709 I llama_init_from_model: flash_attn    = 0
0.00.461.711 I llama_init_from_model: freq_base     = 10000.0
0.00.461.712 I llama_init_from_model: freq_scale    = 1
0.00.461.714 I ggml_metal_init: allocating
0.00.461.790 I ggml_metal_init: found device: Apple M4
0.00.461.807 I ggml_metal_init: picking default device: Apple M4
0.00.463.703 I ggml_metal_init: using embedded metal library
0.00.469.279 I ggml_metal_init: GPU name:   Apple M4
0.00.469.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.469.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.469.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.469.292 I ggml_metal_init: simdgroup reduction   = true
0.00.469.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.469.293 I ggml_metal_init: has residency sets    = true
0.00.469.293 I ggml_metal_init: has bfloat            = true
0.00.469.293 I ggml_metal_init: use bfloat            = true
0.00.469.303 I ggml_metal_init: hasUnifiedMemory      = true
0.00.469.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.490.169 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.545.460 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.545.467 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.545.502 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.550.376 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.550.378 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.550.379 I llama_init_from_model: graph nodes  = 967
0.00.550.379 I llama_init_from_model: graph splits = 2
0.00.550.385 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.550.501 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.550.501 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.651 I main: llama threadpool init, n_threads = 4
0.00.608.695 I 
0.00.608.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.708 I 
0.00.608.876 I sampler seed: 1234
0.00.608.881 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.608.892 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.608.892 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.608.892 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.363.785 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50000.00 tokens per second)
0.01.363.785 I llama_perf_context_print:        load time =     599.13 ms
0.01.363.786 I llama_perf_context_print: prompt eval time =      49.85 ms /     7 tokens (    7.12 ms per token,   140.43 tokens per second)
0.01.363.787 I llama_perf_context_print:        eval time =     702.13 ms /    63 runs   (   11.14 ms per token,    89.73 tokens per second)
0.01.363.787 I llama_perf_context_print:       total time =     755.85 ms /    70 tokens
0.01.364.000 I ggml_metal_free: deallocating

real	0m1.381s
user	0m0.112s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.927 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.118 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.124 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.134 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.135 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.135 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.135 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.136 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.138 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.138 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.155 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.064 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.065 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.066 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.066 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.066 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.067 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.068 I llama_model_loader: - type  f32:  194 tensors
0.00.025.068 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.068 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.068 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.069 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.070 I print_info: file format = GGUF V3 (latest)
0.00.025.070 I print_info: file type   = Q3_K - Medium
0.00.025.071 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.117 I load: special tokens cache size = 25
0.00.039.125 I load: token to piece cache size = 0.2984 MB
0.00.039.128 I print_info: arch             = gptneox
0.00.039.129 I print_info: vocab_only       = 0
0.00.039.129 I print_info: n_ctx_train      = 2048
0.00.039.129 I print_info: n_embd           = 2048
0.00.039.129 I print_info: n_layer          = 24
0.00.039.133 I print_info: n_head           = 16
0.00.039.134 I print_info: n_head_kv        = 16
0.00.039.136 I print_info: n_rot            = 32
0.00.039.136 I print_info: n_swa            = 0
0.00.039.136 I print_info: n_embd_head_k    = 128
0.00.039.136 I print_info: n_embd_head_v    = 128
0.00.039.137 I print_info: n_gqa            = 1
0.00.039.137 I print_info: n_embd_k_gqa     = 2048
0.00.039.138 I print_info: n_embd_v_gqa     = 2048
0.00.039.139 I print_info: f_norm_eps       = 1.0e-05
0.00.039.139 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.139 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.139 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.139 I print_info: f_logit_scale    = 0.0e+00
0.00.039.140 I print_info: n_ff             = 8192
0.00.039.140 I print_info: n_expert         = 0
0.00.039.140 I print_info: n_expert_used    = 0
0.00.039.141 I print_info: causal attn      = 1
0.00.039.141 I print_info: pooling type     = 0
0.00.039.141 I print_info: rope type        = 2
0.00.039.143 I print_info: rope scaling     = linear
0.00.039.144 I print_info: freq_base_train  = 10000.0
0.00.039.144 I print_info: freq_scale_train = 1
0.00.039.144 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.144 I print_info: rope_finetuned   = unknown
0.00.039.145 I print_info: ssm_d_conv       = 0
0.00.039.145 I print_info: ssm_d_inner      = 0
0.00.039.145 I print_info: ssm_d_state      = 0
0.00.039.145 I print_info: ssm_dt_rank      = 0
0.00.039.145 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.145 I print_info: model type       = 1.4B
0.00.039.146 I print_info: model params     = 1.41 B
0.00.039.146 I print_info: general.name     = 1.4B
0.00.039.147 I print_info: vocab type       = BPE
0.00.039.147 I print_info: n_vocab          = 50304
0.00.039.147 I print_info: n_merges         = 50009
0.00.039.147 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.147 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.148 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.148 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.149 I print_info: LF token         = 187 ''
0.00.039.152 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.152 I print_info: max token length = 1024
0.00.039.153 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.468.242 I load_tensors: offloading 24 repeating layers to GPU
0.00.468.253 I load_tensors: offloading output layer to GPU
0.00.468.254 I load_tensors: offloaded 25/25 layers to GPU
0.00.468.288 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.468.289 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.469.996 I llama_init_from_model: n_seq_max     = 1
0.00.469.999 I llama_init_from_model: n_ctx         = 128
0.00.470.000 I llama_init_from_model: n_ctx_per_seq = 128
0.00.470.000 I llama_init_from_model: n_batch       = 128
0.00.470.001 I llama_init_from_model: n_ubatch      = 128
0.00.470.001 I llama_init_from_model: flash_attn    = 0
0.00.470.002 I llama_init_from_model: freq_base     = 10000.0
0.00.470.003 I llama_init_from_model: freq_scale    = 1
0.00.470.003 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.470.006 I ggml_metal_init: allocating
0.00.470.087 I ggml_metal_init: found device: Apple M4
0.00.470.103 I ggml_metal_init: picking default device: Apple M4
0.00.472.484 I ggml_metal_init: using embedded metal library
0.00.478.448 I ggml_metal_init: GPU name:   Apple M4
0.00.478.465 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.478.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.478.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.478.467 I ggml_metal_init: simdgroup reduction   = true
0.00.478.468 I ggml_metal_init: simdgroup matrix mul. = true
0.00.478.468 I ggml_metal_init: has residency sets    = true
0.00.478.468 I ggml_metal_init: has bfloat            = true
0.00.478.469 I ggml_metal_init: use bfloat            = true
0.00.478.470 I ggml_metal_init: hasUnifiedMemory      = true
0.00.478.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.498.810 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.502.451 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.502.455 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.502.505 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.505.934 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.505.936 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.505.937 I llama_init_from_model: graph nodes  = 967
0.00.505.937 I llama_init_from_model: graph splits = 2
0.00.505.940 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.505.940 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.533.440 I 
0.00.533.503 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.533.512 I perplexity: tokenizing the input ..
0.00.540.822 I perplexity: tokenization took 7.308 ms
0.00.540.831 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.672.891 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.674.321 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.674.343 I llama_perf_context_print:        load time =     524.50 ms
0.00.674.344 I llama_perf_context_print: prompt eval time =     131.49 ms /   128 tokens (    1.03 ms per token,   973.45 tokens per second)
0.00.674.344 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.674.345 I llama_perf_context_print:       total time =     140.91 ms /   129 tokens
0.00.674.746 I ggml_metal_free: deallocating

real	0m0.688s
user	0m0.080s
sys	0m0.114s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.160 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.813 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.824 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.826 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.826 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.826 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.827 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.827 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.828 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.829 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.830 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.830 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.676 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.412 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.413 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.414 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.414 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.414 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.415 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.415 I llama_model_loader: - type  f32:  194 tensors
0.00.026.416 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.416 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.416 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.417 I print_info: file format = GGUF V3 (latest)
0.00.026.417 I print_info: file type   = Q4_K - Medium
0.00.026.418 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.626 I load: special tokens cache size = 25
0.00.040.742 I load: token to piece cache size = 0.2984 MB
0.00.040.745 I print_info: arch             = gptneox
0.00.040.745 I print_info: vocab_only       = 0
0.00.040.745 I print_info: n_ctx_train      = 2048
0.00.040.746 I print_info: n_embd           = 2048
0.00.040.746 I print_info: n_layer          = 24
0.00.040.749 I print_info: n_head           = 16
0.00.040.750 I print_info: n_head_kv        = 16
0.00.040.750 I print_info: n_rot            = 32
0.00.040.750 I print_info: n_swa            = 0
0.00.040.750 I print_info: n_embd_head_k    = 128
0.00.040.751 I print_info: n_embd_head_v    = 128
0.00.040.751 I print_info: n_gqa            = 1
0.00.040.752 I print_info: n_embd_k_gqa     = 2048
0.00.040.755 I print_info: n_embd_v_gqa     = 2048
0.00.040.755 I print_info: f_norm_eps       = 1.0e-05
0.00.040.756 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.756 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.756 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.756 I print_info: f_logit_scale    = 0.0e+00
0.00.040.757 I print_info: n_ff             = 8192
0.00.040.763 I print_info: n_expert         = 0
0.00.040.765 I print_info: n_expert_used    = 0
0.00.040.766 I print_info: causal attn      = 1
0.00.040.766 I print_info: pooling type     = 0
0.00.040.766 I print_info: rope type        = 2
0.00.040.766 I print_info: rope scaling     = linear
0.00.040.767 I print_info: freq_base_train  = 10000.0
0.00.040.767 I print_info: freq_scale_train = 1
0.00.040.767 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.768 I print_info: rope_finetuned   = unknown
0.00.040.768 I print_info: ssm_d_conv       = 0
0.00.040.768 I print_info: ssm_d_inner      = 0
0.00.040.768 I print_info: ssm_d_state      = 0
0.00.040.768 I print_info: ssm_dt_rank      = 0
0.00.040.768 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.769 I print_info: model type       = 1.4B
0.00.040.769 I print_info: model params     = 1.41 B
0.00.040.769 I print_info: general.name     = 1.4B
0.00.040.769 I print_info: vocab type       = BPE
0.00.040.770 I print_info: n_vocab          = 50304
0.00.040.770 I print_info: n_merges         = 50009
0.00.040.770 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.770 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.770 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.770 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.771 I print_info: LF token         = 187 ''
0.00.040.772 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.773 I print_info: max token length = 1024
0.00.040.773 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.521.405 I load_tensors: offloading 24 repeating layers to GPU
0.00.521.420 I load_tensors: offloading output layer to GPU
0.00.521.421 I load_tensors: offloaded 25/25 layers to GPU
0.00.521.455 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.521.457 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.523.025 I llama_init_from_model: n_seq_max     = 1
0.00.523.028 I llama_init_from_model: n_ctx         = 2048
0.00.523.029 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.523.029 I llama_init_from_model: n_batch       = 2048
0.00.523.029 I llama_init_from_model: n_ubatch      = 512
0.00.523.030 I llama_init_from_model: flash_attn    = 0
0.00.523.032 I llama_init_from_model: freq_base     = 10000.0
0.00.523.032 I llama_init_from_model: freq_scale    = 1
0.00.523.052 I ggml_metal_init: allocating
0.00.523.136 I ggml_metal_init: found device: Apple M4
0.00.523.177 I ggml_metal_init: picking default device: Apple M4
0.00.525.133 I ggml_metal_init: using embedded metal library
0.00.531.589 I ggml_metal_init: GPU name:   Apple M4
0.00.531.594 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.531.595 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.531.596 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.531.596 I ggml_metal_init: simdgroup reduction   = true
0.00.531.597 I ggml_metal_init: simdgroup matrix mul. = true
0.00.531.597 I ggml_metal_init: has residency sets    = true
0.00.531.597 I ggml_metal_init: has bfloat            = true
0.00.531.598 I ggml_metal_init: use bfloat            = true
0.00.531.599 I ggml_metal_init: hasUnifiedMemory      = true
0.00.531.603 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.885 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.605.598 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.605.605 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.605.641 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.610.491 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.610.492 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.610.493 I llama_init_from_model: graph nodes  = 967
0.00.610.493 I llama_init_from_model: graph splits = 2
0.00.610.499 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.610.640 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.610.641 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.137 I main: llama threadpool init, n_threads = 4
0.00.665.184 I 
0.00.665.200 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.200 I 
0.00.665.353 I sampler seed: 1234
0.00.665.358 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.665.376 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.665.376 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.665.376 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.427.925 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.427.927 I llama_perf_context_print:        load time =     654.25 ms
0.01.427.928 I llama_perf_context_print: prompt eval time =      47.18 ms /     7 tokens (    6.74 ms per token,   148.37 tokens per second)
0.01.427.928 I llama_perf_context_print:        eval time =     712.46 ms /    63 runs   (   11.31 ms per token,    88.43 tokens per second)
0.01.427.928 I llama_perf_context_print:       total time =     763.51 ms /    70 tokens
0.01.428.213 I ggml_metal_free: deallocating

real	0m1.447s
user	0m0.110s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.616 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.008 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.014 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.021 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.022 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.022 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.023 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.024 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.024 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.024 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.025 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.025 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.025 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.026 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.027 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.028 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.959 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.018 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.983 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.985 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.985 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.986 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.986 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.986 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.987 I llama_model_loader: - type  f32:  194 tensors
0.00.026.987 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.988 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.988 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.993 I print_info: file format = GGUF V3 (latest)
0.00.026.993 I print_info: file type   = Q4_K - Medium
0.00.026.994 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.143 I load: special tokens cache size = 25
0.00.041.307 I load: token to piece cache size = 0.2984 MB
0.00.041.312 I print_info: arch             = gptneox
0.00.041.312 I print_info: vocab_only       = 0
0.00.041.312 I print_info: n_ctx_train      = 2048
0.00.041.312 I print_info: n_embd           = 2048
0.00.041.312 I print_info: n_layer          = 24
0.00.041.316 I print_info: n_head           = 16
0.00.041.317 I print_info: n_head_kv        = 16
0.00.041.317 I print_info: n_rot            = 32
0.00.041.317 I print_info: n_swa            = 0
0.00.041.319 I print_info: n_embd_head_k    = 128
0.00.041.319 I print_info: n_embd_head_v    = 128
0.00.041.319 I print_info: n_gqa            = 1
0.00.041.320 I print_info: n_embd_k_gqa     = 2048
0.00.041.321 I print_info: n_embd_v_gqa     = 2048
0.00.041.321 I print_info: f_norm_eps       = 1.0e-05
0.00.041.322 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.322 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.324 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.324 I print_info: f_logit_scale    = 0.0e+00
0.00.041.324 I print_info: n_ff             = 8192
0.00.041.324 I print_info: n_expert         = 0
0.00.041.325 I print_info: n_expert_used    = 0
0.00.041.325 I print_info: causal attn      = 1
0.00.041.325 I print_info: pooling type     = 0
0.00.041.325 I print_info: rope type        = 2
0.00.041.325 I print_info: rope scaling     = linear
0.00.041.326 I print_info: freq_base_train  = 10000.0
0.00.041.327 I print_info: freq_scale_train = 1
0.00.041.327 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.327 I print_info: rope_finetuned   = unknown
0.00.041.327 I print_info: ssm_d_conv       = 0
0.00.041.327 I print_info: ssm_d_inner      = 0
0.00.041.328 I print_info: ssm_d_state      = 0
0.00.041.328 I print_info: ssm_dt_rank      = 0
0.00.041.329 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.329 I print_info: model type       = 1.4B
0.00.041.329 I print_info: model params     = 1.41 B
0.00.041.330 I print_info: general.name     = 1.4B
0.00.041.330 I print_info: vocab type       = BPE
0.00.041.330 I print_info: n_vocab          = 50304
0.00.041.330 I print_info: n_merges         = 50009
0.00.041.331 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.331 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.331 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.331 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.331 I print_info: LF token         = 187 ''
0.00.041.333 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.333 I print_info: max token length = 1024
0.00.041.333 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.517.948 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.961 I load_tensors: offloading output layer to GPU
0.00.517.962 I load_tensors: offloaded 25/25 layers to GPU
0.00.518.002 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.518.003 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.519.641 I llama_init_from_model: n_seq_max     = 1
0.00.519.643 I llama_init_from_model: n_ctx         = 128
0.00.519.644 I llama_init_from_model: n_ctx_per_seq = 128
0.00.519.644 I llama_init_from_model: n_batch       = 128
0.00.519.645 I llama_init_from_model: n_ubatch      = 128
0.00.519.645 I llama_init_from_model: flash_attn    = 0
0.00.519.647 I llama_init_from_model: freq_base     = 10000.0
0.00.519.648 I llama_init_from_model: freq_scale    = 1
0.00.519.648 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.519.651 I ggml_metal_init: allocating
0.00.519.741 I ggml_metal_init: found device: Apple M4
0.00.519.756 I ggml_metal_init: picking default device: Apple M4
0.00.521.767 I ggml_metal_init: using embedded metal library
0.00.528.556 I ggml_metal_init: GPU name:   Apple M4
0.00.528.564 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.528.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.528.566 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.528.566 I ggml_metal_init: simdgroup reduction   = true
0.00.528.566 I ggml_metal_init: simdgroup matrix mul. = true
0.00.528.567 I ggml_metal_init: has residency sets    = true
0.00.528.567 I ggml_metal_init: has bfloat            = true
0.00.528.567 I ggml_metal_init: use bfloat            = true
0.00.528.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.528.572 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.546.586 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.550.129 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.550.138 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.550.203 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.553.351 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.553.354 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.553.354 I llama_init_from_model: graph nodes  = 967
0.00.553.354 I llama_init_from_model: graph splits = 2
0.00.553.358 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.553.358 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.581 I 
0.00.584.647 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.655 I perplexity: tokenizing the input ..
0.00.590.261 I perplexity: tokenization took 5.605 ms
0.00.590.265 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.724.717 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.726.060 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.726.081 I llama_perf_context_print:        load time =     573.95 ms
0.00.726.082 I llama_perf_context_print: prompt eval time =     134.22 ms /   128 tokens (    1.05 ms per token,   953.64 tokens per second)
0.00.726.083 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.726.084 I llama_perf_context_print:       total time =     141.50 ms /   129 tokens
0.00.726.474 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.078s
sys	0m0.122s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.567 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.569 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.571 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.571 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.572 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.573 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.574 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.574 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.575 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.575 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.579 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.580 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.580 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.285 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.286 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.286 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.287 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.287 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.287 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.288 I llama_model_loader: - type  f32:  194 tensors
0.00.025.288 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.288 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.289 I print_info: file format = GGUF V3 (latest)
0.00.025.289 I print_info: file type   = Q5_K - Medium
0.00.025.290 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.190 I load: special tokens cache size = 25
0.00.039.183 I load: token to piece cache size = 0.2984 MB
0.00.039.185 I print_info: arch             = gptneox
0.00.039.185 I print_info: vocab_only       = 0
0.00.039.186 I print_info: n_ctx_train      = 2048
0.00.039.186 I print_info: n_embd           = 2048
0.00.039.186 I print_info: n_layer          = 24
0.00.039.189 I print_info: n_head           = 16
0.00.039.190 I print_info: n_head_kv        = 16
0.00.039.190 I print_info: n_rot            = 32
0.00.039.190 I print_info: n_swa            = 0
0.00.039.190 I print_info: n_embd_head_k    = 128
0.00.039.191 I print_info: n_embd_head_v    = 128
0.00.039.192 I print_info: n_gqa            = 1
0.00.039.192 I print_info: n_embd_k_gqa     = 2048
0.00.039.193 I print_info: n_embd_v_gqa     = 2048
0.00.039.194 I print_info: f_norm_eps       = 1.0e-05
0.00.039.194 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.194 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.194 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.195 I print_info: f_logit_scale    = 0.0e+00
0.00.039.195 I print_info: n_ff             = 8192
0.00.039.195 I print_info: n_expert         = 0
0.00.039.196 I print_info: n_expert_used    = 0
0.00.039.196 I print_info: causal attn      = 1
0.00.039.196 I print_info: pooling type     = 0
0.00.039.197 I print_info: rope type        = 2
0.00.039.199 I print_info: rope scaling     = linear
0.00.039.200 I print_info: freq_base_train  = 10000.0
0.00.039.200 I print_info: freq_scale_train = 1
0.00.039.200 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.201 I print_info: rope_finetuned   = unknown
0.00.039.201 I print_info: ssm_d_conv       = 0
0.00.039.201 I print_info: ssm_d_inner      = 0
0.00.039.201 I print_info: ssm_d_state      = 0
0.00.039.201 I print_info: ssm_dt_rank      = 0
0.00.039.201 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.201 I print_info: model type       = 1.4B
0.00.039.202 I print_info: model params     = 1.41 B
0.00.039.202 I print_info: general.name     = 1.4B
0.00.039.202 I print_info: vocab type       = BPE
0.00.039.203 I print_info: n_vocab          = 50304
0.00.039.203 I print_info: n_merges         = 50009
0.00.039.203 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.203 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.203 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.204 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.204 I print_info: LF token         = 187 ''
0.00.039.205 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.206 I print_info: max token length = 1024
0.00.039.206 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.609.407 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.424 I load_tensors: offloading output layer to GPU
0.00.609.425 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.460 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.609.462 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.611.084 I llama_init_from_model: n_seq_max     = 1
0.00.611.087 I llama_init_from_model: n_ctx         = 2048
0.00.611.087 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.611.087 I llama_init_from_model: n_batch       = 2048
0.00.611.088 I llama_init_from_model: n_ubatch      = 512
0.00.611.088 I llama_init_from_model: flash_attn    = 0
0.00.611.089 I llama_init_from_model: freq_base     = 10000.0
0.00.611.090 I llama_init_from_model: freq_scale    = 1
0.00.611.091 I ggml_metal_init: allocating
0.00.611.111 I ggml_metal_init: found device: Apple M4
0.00.611.122 I ggml_metal_init: picking default device: Apple M4
0.00.612.688 I ggml_metal_init: using embedded metal library
0.00.618.969 I ggml_metal_init: GPU name:   Apple M4
0.00.618.973 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.974 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.975 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.975 I ggml_metal_init: simdgroup reduction   = true
0.00.618.976 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.976 I ggml_metal_init: has residency sets    = true
0.00.618.976 I ggml_metal_init: has bfloat            = true
0.00.618.976 I ggml_metal_init: use bfloat            = true
0.00.618.977 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.982 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.506 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.691.710 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.691.718 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.691.758 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.726 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.696.728 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.696.728 I llama_init_from_model: graph nodes  = 967
0.00.696.729 I llama_init_from_model: graph splits = 2
0.00.696.734 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.696.854 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.696.855 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.884 I main: llama threadpool init, n_threads = 4
0.00.757.933 I 
0.00.757.949 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.949 I 
0.00.758.109 I sampler seed: 1234
0.00.758.114 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.133 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.134 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.134 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.605.964 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51673.94 tokens per second)
0.01.605.965 I llama_perf_context_print:        load time =     748.30 ms
0.01.605.966 I llama_perf_context_print: prompt eval time =      52.58 ms /     7 tokens (    7.51 ms per token,   133.13 tokens per second)
0.01.605.966 I llama_perf_context_print:        eval time =     792.29 ms /    63 runs   (   12.58 ms per token,    79.52 tokens per second)
0.01.605.967 I llama_perf_context_print:       total time =     848.80 ms /    70 tokens
0.01.606.235 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.107s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.919 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.269 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.276 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.281 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.282 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.282 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.284 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.285 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.285 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.285 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.287 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.287 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.288 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.160 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.240 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.024 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.027 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.027 I llama_model_loader: - type  f32:  194 tensors
0.00.025.028 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.028 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.029 I print_info: file format = GGUF V3 (latest)
0.00.025.033 I print_info: file type   = Q5_K - Medium
0.00.025.034 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.150 I load: special tokens cache size = 25
0.00.039.230 I load: token to piece cache size = 0.2984 MB
0.00.039.234 I print_info: arch             = gptneox
0.00.039.234 I print_info: vocab_only       = 0
0.00.039.235 I print_info: n_ctx_train      = 2048
0.00.039.235 I print_info: n_embd           = 2048
0.00.039.235 I print_info: n_layer          = 24
0.00.039.239 I print_info: n_head           = 16
0.00.039.239 I print_info: n_head_kv        = 16
0.00.039.240 I print_info: n_rot            = 32
0.00.039.241 I print_info: n_swa            = 0
0.00.039.241 I print_info: n_embd_head_k    = 128
0.00.039.241 I print_info: n_embd_head_v    = 128
0.00.039.242 I print_info: n_gqa            = 1
0.00.039.243 I print_info: n_embd_k_gqa     = 2048
0.00.039.244 I print_info: n_embd_v_gqa     = 2048
0.00.039.244 I print_info: f_norm_eps       = 1.0e-05
0.00.039.244 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.245 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.245 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.247 I print_info: f_logit_scale    = 0.0e+00
0.00.039.248 I print_info: n_ff             = 8192
0.00.039.248 I print_info: n_expert         = 0
0.00.039.248 I print_info: n_expert_used    = 0
0.00.039.248 I print_info: causal attn      = 1
0.00.039.248 I print_info: pooling type     = 0
0.00.039.248 I print_info: rope type        = 2
0.00.039.249 I print_info: rope scaling     = linear
0.00.039.249 I print_info: freq_base_train  = 10000.0
0.00.039.249 I print_info: freq_scale_train = 1
0.00.039.249 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.251 I print_info: rope_finetuned   = unknown
0.00.039.251 I print_info: ssm_d_conv       = 0
0.00.039.251 I print_info: ssm_d_inner      = 0
0.00.039.251 I print_info: ssm_d_state      = 0
0.00.039.251 I print_info: ssm_dt_rank      = 0
0.00.039.251 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.275 I print_info: model type       = 1.4B
0.00.039.277 I print_info: model params     = 1.41 B
0.00.039.277 I print_info: general.name     = 1.4B
0.00.039.277 I print_info: vocab type       = BPE
0.00.039.278 I print_info: n_vocab          = 50304
0.00.039.278 I print_info: n_merges         = 50009
0.00.039.278 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.280 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.280 I print_info: LF token         = 187 ''
0.00.039.283 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.284 I print_info: max token length = 1024
0.00.039.285 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.605.263 I load_tensors: offloading 24 repeating layers to GPU
0.00.605.279 I load_tensors: offloading output layer to GPU
0.00.605.280 I load_tensors: offloaded 25/25 layers to GPU
0.00.605.322 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.605.325 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.607.145 I llama_init_from_model: n_seq_max     = 1
0.00.607.147 I llama_init_from_model: n_ctx         = 128
0.00.607.148 I llama_init_from_model: n_ctx_per_seq = 128
0.00.607.148 I llama_init_from_model: n_batch       = 128
0.00.607.148 I llama_init_from_model: n_ubatch      = 128
0.00.607.149 I llama_init_from_model: flash_attn    = 0
0.00.607.151 I llama_init_from_model: freq_base     = 10000.0
0.00.607.152 I llama_init_from_model: freq_scale    = 1
0.00.607.152 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.607.154 I ggml_metal_init: allocating
0.00.607.230 I ggml_metal_init: found device: Apple M4
0.00.607.242 I ggml_metal_init: picking default device: Apple M4
0.00.609.142 I ggml_metal_init: using embedded metal library
0.00.615.904 I ggml_metal_init: GPU name:   Apple M4
0.00.615.909 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.615.910 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.615.910 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.615.914 I ggml_metal_init: simdgroup reduction   = true
0.00.615.914 I ggml_metal_init: simdgroup matrix mul. = true
0.00.615.915 I ggml_metal_init: has residency sets    = true
0.00.615.915 I ggml_metal_init: has bfloat            = true
0.00.615.915 I ggml_metal_init: use bfloat            = true
0.00.615.916 I ggml_metal_init: hasUnifiedMemory      = true
0.00.615.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.520 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.637.137 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.637.146 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.637.195 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.640.492 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.640.494 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.640.494 I llama_init_from_model: graph nodes  = 967
0.00.640.494 I llama_init_from_model: graph splits = 2
0.00.640.497 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.640.498 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.150 I 
0.00.672.212 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.220 I perplexity: tokenizing the input ..
0.00.679.286 I perplexity: tokenization took 7.065 ms
0.00.679.291 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.815.627 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.816.966 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.816.987 I llama_perf_context_print:        load time =     663.22 ms
0.00.816.988 I llama_perf_context_print: prompt eval time =     136.10 ms /   128 tokens (    1.06 ms per token,   940.46 tokens per second)
0.00.816.988 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.816.989 I llama_perf_context_print:       total time =     144.84 ms /   129 tokens
0.00.817.361 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.079s
sys	0m0.146s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.452 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.183 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.190 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.190 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.191 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.191 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.192 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.193 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.193 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.193 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.194 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.194 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.036 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.092 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.921 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.922 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.922 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.923 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.923 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.923 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.924 I llama_model_loader: - type  f32:  194 tensors
0.00.026.924 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.925 I print_info: file format = GGUF V3 (latest)
0.00.026.925 I print_info: file type   = Q6_K
0.00.026.926 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.859 I load: special tokens cache size = 25
0.00.041.116 I load: token to piece cache size = 0.2984 MB
0.00.041.119 I print_info: arch             = gptneox
0.00.041.119 I print_info: vocab_only       = 0
0.00.041.120 I print_info: n_ctx_train      = 2048
0.00.041.120 I print_info: n_embd           = 2048
0.00.041.120 I print_info: n_layer          = 24
0.00.041.122 I print_info: n_head           = 16
0.00.041.123 I print_info: n_head_kv        = 16
0.00.041.126 I print_info: n_rot            = 32
0.00.041.126 I print_info: n_swa            = 0
0.00.041.126 I print_info: n_embd_head_k    = 128
0.00.041.126 I print_info: n_embd_head_v    = 128
0.00.041.127 I print_info: n_gqa            = 1
0.00.041.128 I print_info: n_embd_k_gqa     = 2048
0.00.041.129 I print_info: n_embd_v_gqa     = 2048
0.00.041.136 I print_info: f_norm_eps       = 1.0e-05
0.00.041.137 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.137 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.137 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.137 I print_info: f_logit_scale    = 0.0e+00
0.00.041.145 I print_info: n_ff             = 8192
0.00.041.145 I print_info: n_expert         = 0
0.00.041.145 I print_info: n_expert_used    = 0
0.00.041.145 I print_info: causal attn      = 1
0.00.041.145 I print_info: pooling type     = 0
0.00.041.146 I print_info: rope type        = 2
0.00.041.147 I print_info: rope scaling     = linear
0.00.041.149 I print_info: freq_base_train  = 10000.0
0.00.041.149 I print_info: freq_scale_train = 1
0.00.041.149 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.151 I print_info: rope_finetuned   = unknown
0.00.041.151 I print_info: ssm_d_conv       = 0
0.00.041.151 I print_info: ssm_d_inner      = 0
0.00.041.151 I print_info: ssm_d_state      = 0
0.00.041.151 I print_info: ssm_dt_rank      = 0
0.00.041.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.153 I print_info: model type       = 1.4B
0.00.041.153 I print_info: model params     = 1.41 B
0.00.041.153 I print_info: general.name     = 1.4B
0.00.041.154 I print_info: vocab type       = BPE
0.00.041.154 I print_info: n_vocab          = 50304
0.00.041.154 I print_info: n_merges         = 50009
0.00.041.154 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.156 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.156 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.156 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.156 I print_info: LF token         = 187 ''
0.00.041.156 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.157 I print_info: max token length = 1024
0.00.041.158 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.649.447 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.452 I load_tensors: offloading output layer to GPU
0.00.649.453 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.476 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.649.477 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.650.933 I llama_init_from_model: n_seq_max     = 1
0.00.650.935 I llama_init_from_model: n_ctx         = 2048
0.00.650.935 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.935 I llama_init_from_model: n_batch       = 2048
0.00.650.936 I llama_init_from_model: n_ubatch      = 512
0.00.650.936 I llama_init_from_model: flash_attn    = 0
0.00.650.937 I llama_init_from_model: freq_base     = 10000.0
0.00.650.937 I llama_init_from_model: freq_scale    = 1
0.00.650.939 I ggml_metal_init: allocating
0.00.650.955 I ggml_metal_init: found device: Apple M4
0.00.650.965 I ggml_metal_init: picking default device: Apple M4
0.00.652.356 I ggml_metal_init: using embedded metal library
0.00.658.275 I ggml_metal_init: GPU name:   Apple M4
0.00.658.280 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.281 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.283 I ggml_metal_init: simdgroup reduction   = true
0.00.658.283 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.283 I ggml_metal_init: has residency sets    = true
0.00.658.283 I ggml_metal_init: has bfloat            = true
0.00.658.284 I ggml_metal_init: use bfloat            = true
0.00.658.292 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.570 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.734.128 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.734.134 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.734.170 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.144 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.738.146 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.738.147 I llama_init_from_model: graph nodes  = 967
0.00.738.147 I llama_init_from_model: graph splits = 2
0.00.738.153 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.738.281 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.282 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.801.634 I main: llama threadpool init, n_threads = 4
0.00.801.677 I 
0.00.801.692 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.801.692 I 
0.00.801.858 I sampler seed: 1234
0.00.801.863 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.801.883 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.801.883 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.801.884 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.681.694 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.681.694 I llama_perf_context_print:        load time =     790.45 ms
0.01.681.695 I llama_perf_context_print: prompt eval time =      57.54 ms /     7 tokens (    8.22 ms per token,   121.66 tokens per second)
0.01.681.699 I llama_perf_context_print:        eval time =     819.34 ms /    63 runs   (   13.01 ms per token,    76.89 tokens per second)
0.01.681.700 I llama_perf_context_print:       total time =     880.79 ms /    70 tokens
0.01.681.957 I ggml_metal_free: deallocating

real	0m1.702s
user	0m0.109s
sys	0m0.229s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4737 (5137da7b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.031 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.146 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.158 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.158 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.164 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.164 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.115 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.186 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.086 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.088 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.088 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.088 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.088 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.089 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.089 I llama_model_loader: - type  f32:  194 tensors
0.00.026.090 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.090 I print_info: file format = GGUF V3 (latest)
0.00.026.091 I print_info: file type   = Q6_K
0.00.026.092 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.388 I load: special tokens cache size = 25
0.00.040.276 I load: token to piece cache size = 0.2984 MB
0.00.040.280 I print_info: arch             = gptneox
0.00.040.281 I print_info: vocab_only       = 0
0.00.040.281 I print_info: n_ctx_train      = 2048
0.00.040.281 I print_info: n_embd           = 2048
0.00.040.281 I print_info: n_layer          = 24
0.00.040.286 I print_info: n_head           = 16
0.00.040.287 I print_info: n_head_kv        = 16
0.00.040.287 I print_info: n_rot            = 32
0.00.040.287 I print_info: n_swa            = 0
0.00.040.288 I print_info: n_embd_head_k    = 128
0.00.040.288 I print_info: n_embd_head_v    = 128
0.00.040.288 I print_info: n_gqa            = 1
0.00.040.289 I print_info: n_embd_k_gqa     = 2048
0.00.040.290 I print_info: n_embd_v_gqa     = 2048
0.00.040.291 I print_info: f_norm_eps       = 1.0e-05
0.00.040.291 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.291 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.291 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.291 I print_info: f_logit_scale    = 0.0e+00
0.00.040.292 I print_info: n_ff             = 8192
0.00.040.292 I print_info: n_expert         = 0
0.00.040.292 I print_info: n_expert_used    = 0
0.00.040.293 I print_info: causal attn      = 1
0.00.040.293 I print_info: pooling type     = 0
0.00.040.293 I print_info: rope type        = 2
0.00.040.296 I print_info: rope scaling     = linear
0.00.040.296 I print_info: freq_base_train  = 10000.0
0.00.040.296 I print_info: freq_scale_train = 1
0.00.040.296 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.297 I print_info: rope_finetuned   = unknown
0.00.040.297 I print_info: ssm_d_conv       = 0
0.00.040.297 I print_info: ssm_d_inner      = 0
0.00.040.297 I print_info: ssm_d_state      = 0
0.00.040.297 I print_info: ssm_dt_rank      = 0
0.00.040.297 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.298 I print_info: model type       = 1.4B
0.00.040.298 I print_info: model params     = 1.41 B
0.00.040.299 I print_info: general.name     = 1.4B
0.00.040.300 I print_info: vocab type       = BPE
0.00.040.300 I print_info: n_vocab          = 50304
0.00.040.300 I print_info: n_merges         = 50009
0.00.040.300 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.301 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.301 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.302 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.302 I print_info: LF token         = 187 ''
0.00.040.302 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.302 I print_info: max token length = 1024
0.00.040.303 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.330.741 I load_tensors: offloading 24 repeating layers to GPU
0.00.330.745 I load_tensors: offloading output layer to GPU
0.00.330.746 I load_tensors: offloaded 25/25 layers to GPU
0.00.330.772 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.330.774 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.332.095 I llama_init_from_model: n_seq_max     = 1
0.00.332.097 I llama_init_from_model: n_ctx         = 128
0.00.332.098 I llama_init_from_model: n_ctx_per_seq = 128
0.00.332.098 I llama_init_from_model: n_batch       = 128
0.00.332.098 I llama_init_from_model: n_ubatch      = 128
0.00.332.099 I llama_init_from_model: flash_attn    = 0
0.00.332.100 I llama_init_from_model: freq_base     = 10000.0
0.00.332.101 I llama_init_from_model: freq_scale    = 1
0.00.332.101 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.332.104 I ggml_metal_init: allocating
0.00.332.156 I ggml_metal_init: found device: Apple M4
0.00.332.169 I ggml_metal_init: picking default device: Apple M4
0.00.333.653 I ggml_metal_init: using embedded metal library
0.00.339.767 I ggml_metal_init: GPU name:   Apple M4
0.00.339.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.771 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.772 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.773 I ggml_metal_init: simdgroup reduction   = true
0.00.339.773 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.774 I ggml_metal_init: has residency sets    = true
0.00.339.774 I ggml_metal_init: has bfloat            = true
0.00.339.774 I ggml_metal_init: use bfloat            = true
0.00.339.775 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.777 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.356.373 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.359.890 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.359.900 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.359.947 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.363.181 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.363.183 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.363.183 I llama_init_from_model: graph nodes  = 967
0.00.363.184 I llama_init_from_model: graph splits = 2
0.00.363.186 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.363.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.401.634 I 
0.00.401.691 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.401.700 I perplexity: tokenizing the input ..
0.00.408.758 I perplexity: tokenization took 7.056 ms
0.00.408.766 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.541.130 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.542.540 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.542.564 I llama_perf_context_print:        load time =     391.59 ms
0.00.542.565 I llama_perf_context_print: prompt eval time =     131.39 ms /   128 tokens (    1.03 ms per token,   974.19 tokens per second)
0.00.542.566 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.542.566 I llama_perf_context_print:       total time =     140.93 ms /   129 tokens
0.00.542.939 I ggml_metal_free: deallocating

real	0m0.559s
user	0m0.078s
sys	0m0.102s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4737 (5137da7b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151d09360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151d09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151d0a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151d0a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151d0ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151d0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151d0b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151d0bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151d0c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151d0c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151d0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151d0d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151d0dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151d0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151d0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151d0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151d0fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151d10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151d108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151d11070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151d11790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151d11eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151d125d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151d12e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151d13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151d13850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151d13e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151d14ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151d15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151d152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151d15770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151d15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151d162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151d16800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151d16ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151d16f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151d17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151d178a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151d17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151d181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151d18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151d18b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151d18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151d19460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151d19720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151d19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151d1a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151d1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151d1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151d1b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151d1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151d1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151d1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151d1d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151d1d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151d1dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151d1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151d1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151d1eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151d1f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151d1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151d1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151d1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151d20350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151d207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151d20c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151d21130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151d215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151d21a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151d21f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151d223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151d22850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151d22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151d23240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151d23790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151d23ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151d24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151d24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151d24cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151d25220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151d25770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151d25cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151d26210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151d26760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151d26cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151d27200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151d27750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151d27ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151d281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151d28740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151d28c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151d291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151d29730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151d29c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151d2a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151d2a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151d2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151d1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151d2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151d2b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151d2bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151d2c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151d2c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151d2cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151d2d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151d2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151d2ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151d2e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151d2e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151d2edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151d2f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151d2f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151d2fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151d30240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151d306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151d30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151d31020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151d314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151d31960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151d31e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151d322a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151d32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151d32be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151d33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151d33520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151d339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151d33e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151d34300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151d347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151d34c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151d350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151d35580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151d35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151d35ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151d36360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151d36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151d36ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151d37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151d375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151d37a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151d37f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151d383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x151d38860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151d38d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151d391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x151d39640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151d39ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x151d39f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151d3a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151d3a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151d3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151d3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151d3b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151d3bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151d3bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151d3c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151d3c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151d3cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151d3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151d3d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151d3dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151d3e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151d3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151d3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151d3ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151d3f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151d3f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151d3fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151d400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151d40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151d409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151d40e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151d41320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151d417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151d41c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151d42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151d425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151d42a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151d42ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151d43380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151d43820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151d43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151d44160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151d44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151d44aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151d44f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151d453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151d45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151d45d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151d461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151d46660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151d46b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151d46fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151d474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151d47a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x151d47f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151d484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151d487a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151d48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151d493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x151d499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151d4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151d4a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x151d4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151d4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151d4b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151d4bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151d4c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151d4c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151d4cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151d4d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151d4d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151d4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151d4e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151d4e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151d4ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151d4f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151d4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151d4fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151d50290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151d507e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151d50d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151d51280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151d517d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151d51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151d52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151d527c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151d52d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151d53260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151d537b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151d53d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151d54250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151d547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151d54cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151d55240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151d55790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151d55ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151d56230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151d56780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151d56cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151d57220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151d57770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151d57cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151d58210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151d58760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151d58cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151d59200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x151d59750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151d59ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151d5a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x151d5a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151d5ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151d5b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x151d5b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151d5bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151d5c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151d5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151d5cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x151d5d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151d5d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151d5dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151d5e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151d5e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151d5ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151d5f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151d5f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151d5fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151d600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151d60580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151d60a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151d60ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151d61360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151d61800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151d61ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151d62140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151d625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151d62a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151d62f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151d633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151d63860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151d63d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151d641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151d646f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151d64e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151d65530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151d65c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151d66370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151d66630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151d66e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151d670e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151d676f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.732.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.732.664 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151d673a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151d49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151d48a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151d49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151d1c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151d1c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151d1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151d4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151d13b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151d1a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151d1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151d1b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151d199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151d1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151d12b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151d08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151d1d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151d1ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151d2b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151d668f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151d15cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151d15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151d4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151d49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151d14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151d143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151d146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151d67b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151d67e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151d680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151d68390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151d68650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151d68910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151d68bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151d68e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151d69150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151d69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151d696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151d69990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151d69c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151d69f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151d6a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151d6a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151d6a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151d6aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151d6acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151d6af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151d6b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151d6b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151d6b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151d6ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151d6bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151d6c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151d6c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151d6c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151d6c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151d6cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151d6cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151d6d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151d6d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151d6d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151d6d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151d6db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151d6de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151d6e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151d6e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151d6e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151b06eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151b09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151b095c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151b09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151b09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151b0a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151b0a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151b0ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151b0b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151b0b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151b0be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151b0c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151b0c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151b0ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151b0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151b0d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151b0de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151b0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151b0e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151b0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151b0f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151b0f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151b0fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151b10340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151b10890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151b10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151b11330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151b11880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151b11dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151b12320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151b12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151b12dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151b13310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151b13860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151b13db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151b14300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151b14850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151b14da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151b152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151b15840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151b15d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151b162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151b16830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151b16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151b172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151b17820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151b17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151b18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151b18600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151b18aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151b18f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151b193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151b19880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151b19d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151b1a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151b1a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151b1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151b1afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151b1b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151b1b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151b1bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151b1c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151b1c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151b1cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151b1d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151b1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151b1d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151b1dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151b1e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151b1e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151b1ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151b1f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151b1f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151b1f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151b1fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x151b202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151b20780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151b20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x151b210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151b21560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x151b21a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151b21ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151b22340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151b227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151b22c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151b23120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151b235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151b23a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151b23f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151b243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151b24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151b24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151b25180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151b25620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151b25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151b25f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151b26400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151b268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151b26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151b27450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151b27710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151b27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151b28110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151b28610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151b28b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151b29010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151b29510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151b29a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151b29f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151b2a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151b2a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151b2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151b2b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151b2b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151b2bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151b2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151b2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151b2cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151b2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151b2d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151b2db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151b2e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151b2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151b2ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151b2ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151b2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151b2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151b2fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x151b30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151b30a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151b30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151b315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151b31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x151b32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151b329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151b32e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x151b33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151b33760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151b33d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151b34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151b34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151b34ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151b35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151b35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151b36040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151b36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151b36ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151b37030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151b37580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151b37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151b38020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151b38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151b38ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151b39010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151b39560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151b39ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151b3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151b3a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151b3aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151b3aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151b3b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151b3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151b3bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151b3c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151b3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151b3cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151b3d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151b3da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151b3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151b3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151b3ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151b3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151b3f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151b3fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151b3ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151b404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151b40a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151b40f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151b414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151b41a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x151b41f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151b424d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151b42a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x151b42f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151b434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151b43a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x151b43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151b444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151b44a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151b44f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151b454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x151b459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151b45f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151b46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151b469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151b46f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151b47480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151b479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151b47f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151b48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151b48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151b48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151b49250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151b496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151b49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151b4a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151b4a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151b4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151b4ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151b4b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151b4b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151b4bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151b4c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151b4c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151b4c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151b4cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151b4d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151b4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151b4e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151b4eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151b4ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151b4f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151b4f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151b4ff20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a7044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a7056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a7063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a7078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a7083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a70a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a70a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a70b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a70b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a70bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a70c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a70cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a70d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a70db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a70de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a70e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a70e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a70e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a70ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a70f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a70f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a70fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a70ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a7107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a7110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a7119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a7138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a7141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a7157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a7160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a7185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a71a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a71a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a71a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a71adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a71b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a71b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a71bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a71bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a71c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a71c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a71ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a71d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a71d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a71da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a71de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a71e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a71e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a71ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a71f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a71f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a71f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a71fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a7213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a7229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a7232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a7240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a724370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a7247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a724c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a7250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a725530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a7259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a725e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a726280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a7266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a726b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a726fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a727440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a7278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a727d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a728190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a728600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a728a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a728ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a729350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a7297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a729c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a72a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a72a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a72a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a72adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a72b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a72b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a72bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a72bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a72c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a72c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a72cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a72d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a72d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a72da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a72dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a72e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a72e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a72ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a72f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a72f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a72f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a72fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a730240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a7306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a730b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a730f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a731400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a731870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a731ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a732150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a7325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a732a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a732ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a733310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a733780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a733bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a734060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a7344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a734940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a734db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a735220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a735690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a735b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a735f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a7363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a736850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a736cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a737130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a7375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a737a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a737e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a7382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a738760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a738bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a739040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a7394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a739920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a739d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a73a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a73a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a73aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a73af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a73b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a73b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a73bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a73c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a73c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a73c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a73ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a73d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a73d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a73dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a73e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a73e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a73e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a73ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a73f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a73f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a73fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a73ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a7403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a740930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a740da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a741210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a741d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a742020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a7422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a742750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a742bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a743030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a7434a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a743910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a743d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a7441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a744660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a744ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a744f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a7453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a745820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a745c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a746100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a746570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a7469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a746e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a7472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a747730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a747ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a748010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a748480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a7488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a748d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a7491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a749640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a749ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a749f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a74a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a74a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a74ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a74b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a74b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a74b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a74be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a74c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a74c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a74cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a74cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a74d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a74d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a74dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a74e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a74e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a74ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a74ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a74f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a74f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a74fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a7500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a750530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a7509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a750e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a751280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a7516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a751b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a751fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a752440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a7528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a752d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a753190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a753600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a753a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a753ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a754350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a7547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a754c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a7550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a755510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a755980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a7563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a756b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a757230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a757950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a757c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a758080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a758680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a758c90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.763s
user	0m0.276s
sys	0m0.324s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4737 (5137da7b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141710790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141710e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141711440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1417119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141711fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141712550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141712b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1417130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141713660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141713b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141714060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141714560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141715080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141715830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141716040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141716760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141716e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1417175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141717cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141718490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141718bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1417192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1417199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14171a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14171a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14171ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14171b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14171bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14171c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14171c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14171cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14171ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14171d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14171dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14171dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14171e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14171e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14171ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14171f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14171f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14171faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14171ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1417203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141720880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141720b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141721150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141721760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141722080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141722690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141722ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1417232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1417238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141723ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1417244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141724cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141725170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141725610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1417258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141725ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1417266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141726990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141726e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1417272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141727770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141727c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1417280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141728550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1417289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141728e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141729330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1417297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141729c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14172a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14172a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14172abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14172b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14172b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14172bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14172c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14172c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14172cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14172d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14172d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14172db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14172e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14172e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14172eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14172f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14172f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14172fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1417300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141730600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141730b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1417310a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1417315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141731b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141732090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141721d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141732500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141732cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141733200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141733750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141733ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1417341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141734740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141734c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1417351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141735730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141735c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1417361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141736720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141736c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1417371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141737660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141737b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141737fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141738440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1417388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141738d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141739220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1417396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141739b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14173a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14173a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14173a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14173ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14173b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14173b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14173bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14173c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14173c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14173c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14173ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14173d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14173d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14173dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14173e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14173e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14173ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14173eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14173f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14173f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14173fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141740120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1417405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141740a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141740f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1417413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141741840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141741ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141742180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141742620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141742ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141742f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141743400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1417438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141743d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1417441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141744680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141744b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141744fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141745460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141745900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141745da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141746240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1417466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141746b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141747020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1417474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141747960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141747e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1417482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141748740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141748be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141749080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141749520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1417499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141749e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14174a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14174a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14174ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14174b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14174b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14174ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14174bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14174c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14174c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14174cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14174d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14174d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14174da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14174df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14174e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14174e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14174ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14174f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14174f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14174fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1417501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1417507e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141750df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1417515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141751a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141751d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141752350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141752960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141753150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1417535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141753a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141753f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1417546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141754c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141755180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1417556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141755c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141756170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1417566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141756c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141757160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1417576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141757c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141758150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1417586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141758bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141759140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141759690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141759be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14175a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14175a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14175abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14175b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14175b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14175bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14175c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14175c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14175cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14175d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14175d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14175dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14175e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14175e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14175eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14175f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14175f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14175fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1417600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141760620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141760b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1417610c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141761610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141761b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1417620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141762600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141762b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1417630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1417635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141763b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141764090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1417645e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141764b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141765080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1417655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141765b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141766070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1417665c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141766b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141767060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141767500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1417679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141767e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1417682e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141768780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141768c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1417690c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141769560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141769a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141769ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14176a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14176a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14176ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14176b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14176b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14176bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14176c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14176c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14176d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14176d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14176da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14176e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14176e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14176eb10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141605bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141606020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141606490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141606900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141606d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1416071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141607650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141607ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141607f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1416083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141608810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141608e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141609990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14160a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14160a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14160b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14160b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14160beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14160c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14160cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14160d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14160dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14160e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14160ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14160f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14160f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14160f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14160fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14160ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141610410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141610db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141611220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1416114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141611950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141611dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141612230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1416126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141612b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141612f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1416133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141613860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141613cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141614140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1416145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141614a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141614e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141615300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141615770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141615be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141616050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1416164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141616930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141616da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141617210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141617680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1416180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141618560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1416189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141618e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1416192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141619720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141619b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14161a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14161a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14161a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14161ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14161b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14161b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14161baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14161bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14161c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14161c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14161cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14161d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14161d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14161d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14161de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14161e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14161e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14161eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14161efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14161f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14161f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14161fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1416201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141620610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141620a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141620ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141621360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1416217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141621c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1416220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141622520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141622990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141622e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141623270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1416236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141623b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141623fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141624430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1416248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141624d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141625180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1416255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141625a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141626340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1416267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141626c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141627090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141627500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141627970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141627de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1416286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141628b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141628fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141629410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141629880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141629cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14162a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14162a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14162aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14162aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14162b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14162b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14162bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14162c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14162c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14162c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14162cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14162d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14162d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14162db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14162df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14162e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14162e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14162ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14162f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14162f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14162fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14162fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141630300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141630770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141630be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141631050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1416314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141631930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141631da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141632210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141632680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141632af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141632f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1416333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141633840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141633cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141634120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141634590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141634a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141634e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1416352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141635750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141635bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141636030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1416367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141636aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141636f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141637380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1416377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141637c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1416380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141638540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1416389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141638e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141639700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141639b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141639fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14163a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14163a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14163ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14163b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14163b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14163ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14163bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14163c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14163c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14163cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14163d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14163d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14163d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14163de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14163e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14163e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14163eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14163efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14163f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14163f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14163fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141640180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1416405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141640a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141640ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141641340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1416417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141641c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141642090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141642500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1416430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141643370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141643630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141643aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141643f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141644380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1416447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141644c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1416450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141645540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1416459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141645e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141646700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141646b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141646fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141647450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1416478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141647d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1416481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141648610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141648a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141648ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141649360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1416497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141649c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14164a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14164a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14164a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14164ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14164b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14164b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14164bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14164bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14164c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14164c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14164cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14164d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14164d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14164da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14164ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14164e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14164e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14164ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14164f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14164f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14164f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14164fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141650250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1416506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141650fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141651410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141651880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141651cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141652160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1416525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141652a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141652eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141653320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141653790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141653c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141654070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1416544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141654950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141654dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141655230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1416556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141655b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141655f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1416563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141656860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141656cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141657740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141657e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141658580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141658ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141658f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1416593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1416599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141659fe0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1478044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1478056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1478063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147807870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147808390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147808b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147809350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147809a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14780a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14780a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14780afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14780b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14780be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14780c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14780cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14780d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14780daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14780dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14780e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14780e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14780e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14780ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14780f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14780f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14780fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14780fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1478102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147810720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147810b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147811000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147811470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1478118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147811d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1478121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147812630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147812aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147812f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147813380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1478137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147813c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1478140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147814540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1478149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147814e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147815290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147815700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147815b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147815fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147816550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147816a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147816ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147817330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1478177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147817c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147818080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1478184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147818960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147818dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147819240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1478196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147819b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147819f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14781a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14781a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14781ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14781b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14781b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14781ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14781bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14781c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14781c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14781cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14781d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14781d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14781d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14781ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14781e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14781e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14781eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14781ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14781f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14781f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14781fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147820130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1478205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147820a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147820e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1478212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147821760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147821bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147822040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1478224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147822920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147822d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147823200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147823670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147823fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1478242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147824710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147824b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147824ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147825460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1478258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147825d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1478261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147826620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147826a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147826f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147827370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1478277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147827c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1478280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147828530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1478289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147828e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147829280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1478296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147829b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147829fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14782a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14782a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14782ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14782b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14782b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14782ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14782bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14782c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14782c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14782cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14782d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14782d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14782d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14782ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14782e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14782e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14782eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14782efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14782f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14782f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14782fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147830170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1478305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147830a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147830ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147831330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1478317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147831c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147832080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1478324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147832960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147832dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147833240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1478336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147833b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147833f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147834400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147834870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147834ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147835150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1478355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147835a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147835ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147836310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147836780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147836bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147837060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1478374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147837940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147837db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147838220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147838690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147838b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147838f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1478393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147839850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147839cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14783a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14783a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14783aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14783ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14783b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14783b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14783bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14783c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14783c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14783c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14783cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14783d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14783d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14783dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14783df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14783e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14783e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14783eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14783f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14783f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14783f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14783fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1478402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147840860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147840cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147841140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147841c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147841f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147842210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147842680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147842af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147842f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1478433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147843840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147843cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147844120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147844590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147844a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147844e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1478452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147845750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147845bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147846030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1478464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147846910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147846d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1478471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147847660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147847ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147847f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1478483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147848820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147848c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147849100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147849570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1478499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147849e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14784a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14784a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14784aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14784b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14784b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14784b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14784bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14784c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14784c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14784cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14784cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14784d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14784d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14784dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14784e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14784e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14784e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14784ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14784f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14784f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14784fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14784fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147850460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1478508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147850d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1478511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147851620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147851a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147851f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147852370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1478527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147852c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1478530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147853530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1478539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147853e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147854280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1478546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147854b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147854fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147855440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1478558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147856320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147856a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147857160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147857880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147857b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147857fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1478585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147858bc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.958s
user	0m0.232s
sys	0m0.187s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.42 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.78 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.20 sec*proc (2 tests)

Total Test time (real) =   2.21 sec
        2.23 real         0.51 user         0.27 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.13 user         0.09 sys
```
