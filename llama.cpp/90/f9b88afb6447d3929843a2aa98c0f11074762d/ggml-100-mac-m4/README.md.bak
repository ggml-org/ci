### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.33 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.09 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.11 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.85 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.74 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  189.96 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.89 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.85 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.24 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 250.15 sec*proc (29 tests)

Total Test time (real) = 250.16 sec

real	4m10.191s
user	8m26.822s
sys	0m7.206s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.20 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.80 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.46 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.36 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.62 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.20 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.45 sec*proc (29 tests)

Total Test time (real) =  54.48 sec

real	0m54.491s
user	1m16.194s
sys	0m6.315s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.150 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.164 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.983 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.994 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.997 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.998 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.998 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.999 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.999 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.027.001 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.027.001 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.027.002 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.027.003 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.027.003 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.027.007 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.007 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.008 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.027.008 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.027.009 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.027.009 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.027.010 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.663 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.031.395 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.397 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.031.398 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.031.398 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.031.398 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.031.398 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.031.399 I llama_model_loader: - type  f32:  124 tensors
0.00.031.399 I llama_model_loader: - type  f16:   73 tensors
0.00.031.400 I print_info: file format = GGUF V3 (latest)
0.00.031.402 I print_info: file type   = F16
0.00.031.403 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.034.346 I load: special tokens cache size = 5
0.00.035.704 I load: token to piece cache size = 0.2032 MB
0.00.035.708 I print_info: arch             = bert
0.00.035.709 I print_info: vocab_only       = 0
0.00.035.709 I print_info: n_ctx_train      = 512
0.00.035.711 I print_info: n_embd           = 384
0.00.035.711 I print_info: n_layer          = 12
0.00.035.714 I print_info: n_head           = 12
0.00.035.714 I print_info: n_head_kv        = 12
0.00.035.715 I print_info: n_rot            = 32
0.00.035.715 I print_info: n_swa            = 0
0.00.035.715 I print_info: n_embd_head_k    = 32
0.00.035.715 I print_info: n_embd_head_v    = 32
0.00.035.717 I print_info: n_gqa            = 1
0.00.035.717 I print_info: n_embd_k_gqa     = 384
0.00.035.718 I print_info: n_embd_v_gqa     = 384
0.00.035.718 I print_info: f_norm_eps       = 1.0e-12
0.00.035.723 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.035.723 I print_info: f_clamp_kqv      = 0.0e+00
0.00.035.723 I print_info: f_max_alibi_bias = 0.0e+00
0.00.035.723 I print_info: f_logit_scale    = 0.0e+00
0.00.035.724 I print_info: n_ff             = 1536
0.00.035.724 I print_info: n_expert         = 0
0.00.035.724 I print_info: n_expert_used    = 0
0.00.035.724 I print_info: causal attn      = 0
0.00.035.725 I print_info: pooling type     = 2
0.00.035.725 I print_info: rope type        = 2
0.00.035.725 I print_info: rope scaling     = linear
0.00.035.725 I print_info: freq_base_train  = 10000.0
0.00.035.726 I print_info: freq_scale_train = 1
0.00.035.728 I print_info: n_ctx_orig_yarn  = 512
0.00.035.728 I print_info: rope_finetuned   = unknown
0.00.035.728 I print_info: ssm_d_conv       = 0
0.00.035.728 I print_info: ssm_d_inner      = 0
0.00.035.728 I print_info: ssm_d_state      = 0
0.00.035.728 I print_info: ssm_dt_rank      = 0
0.00.035.729 I print_info: ssm_dt_b_c_rms   = 0
0.00.035.729 I print_info: model type       = 33M
0.00.035.729 I print_info: model params     = 33.21 M
0.00.035.729 I print_info: general.name     = Bge Small
0.00.035.730 I print_info: vocab type       = WPM
0.00.035.730 I print_info: n_vocab          = 30522
0.00.035.730 I print_info: n_merges         = 0
0.00.035.730 I print_info: BOS token        = 101 '[CLS]'
0.00.035.731 I print_info: UNK token        = 100 '[UNK]'
0.00.035.731 I print_info: SEP token        = 102 '[SEP]'
0.00.035.731 I print_info: PAD token        = 0 '[PAD]'
0.00.035.731 I print_info: MASK token       = 103 '[MASK]'
0.00.035.731 I print_info: LF token         = 0 '[PAD]'
0.00.035.732 I print_info: max token length = 21
0.00.037.956 I load_tensors: offloading 12 repeating layers to GPU
0.00.037.957 I load_tensors: offloading output layer to GPU
0.00.037.957 I load_tensors: offloaded 13/13 layers to GPU
0.00.037.976 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.977 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.038.142 I llama_init_from_model: n_seq_max     = 1
0.00.038.143 I llama_init_from_model: n_ctx         = 512
0.00.038.143 I llama_init_from_model: n_ctx_per_seq = 512
0.00.038.143 I llama_init_from_model: n_batch       = 2048
0.00.038.144 I llama_init_from_model: n_ubatch      = 2048
0.00.038.144 I llama_init_from_model: flash_attn    = 0
0.00.038.144 I llama_init_from_model: freq_base     = 10000.0
0.00.038.144 I llama_init_from_model: freq_scale    = 1
0.00.038.145 I ggml_metal_init: allocating
0.00.038.148 I ggml_metal_init: found device: Apple M4
0.00.038.152 I ggml_metal_init: picking default device: Apple M4
0.00.038.717 I ggml_metal_init: using embedded metal library
0.00.041.456 I ggml_metal_init: GPU name:   Apple M4
0.00.041.458 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.041.458 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.041.459 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.041.459 I ggml_metal_init: simdgroup reduction   = true
0.00.041.459 I ggml_metal_init: simdgroup matrix mul. = true
0.00.041.459 I ggml_metal_init: has residency sets    = true
0.00.041.459 I ggml_metal_init: has bfloat            = true
0.00.041.459 I ggml_metal_init: use bfloat            = true
0.00.041.460 I ggml_metal_init: hasUnifiedMemory      = true
0.00.041.461 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.507 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.052.110 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.052.112 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.052.135 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.053.137 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.053.138 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.053.138 I llama_init_from_model: graph nodes  = 429
0.00.053.138 I llama_init_from_model: graph splits = 2
0.00.053.139 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.057.549 I 
0.00.057.583 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.058.111 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.062.441 I llama_perf_context_print:        load time =      35.38 ms
0.00.062.442 I llama_perf_context_print: prompt eval time =       4.20 ms /     9 tokens (    0.47 ms per token,  2140.82 tokens per second)
0.00.062.443 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.062.443 I llama_perf_context_print:       total time =       4.89 ms /    10 tokens
0.00.062.649 I ggml_metal_free: deallocating

real	0m0.240s
user	0m0.043s
sys	0m0.030s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.041 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.947 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.576 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.582 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.583 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.583 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.583 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.584 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.585 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.585 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.585 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.587 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.589 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.590 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.590 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.591 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.591 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.593 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.832 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.433 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.434 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.434 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.434 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.435 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.435 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.435 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.436 I llama_model_loader: - type  f32:  124 tensors
0.00.014.436 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.437 I print_info: file format = GGUF V3 (latest)
0.00.014.437 I print_info: file type   = Q8_0
0.00.014.439 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.682 I load: special tokens cache size = 5
0.00.017.846 I load: token to piece cache size = 0.2032 MB
0.00.017.848 I print_info: arch             = bert
0.00.017.848 I print_info: vocab_only       = 0
0.00.017.849 I print_info: n_ctx_train      = 512
0.00.017.849 I print_info: n_embd           = 384
0.00.017.849 I print_info: n_layer          = 12
0.00.017.852 I print_info: n_head           = 12
0.00.017.852 I print_info: n_head_kv        = 12
0.00.017.853 I print_info: n_rot            = 32
0.00.017.853 I print_info: n_swa            = 0
0.00.017.853 I print_info: n_embd_head_k    = 32
0.00.017.853 I print_info: n_embd_head_v    = 32
0.00.017.854 I print_info: n_gqa            = 1
0.00.017.854 I print_info: n_embd_k_gqa     = 384
0.00.017.855 I print_info: n_embd_v_gqa     = 384
0.00.017.856 I print_info: f_norm_eps       = 1.0e-12
0.00.017.856 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.856 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.856 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.857 I print_info: f_logit_scale    = 0.0e+00
0.00.017.857 I print_info: n_ff             = 1536
0.00.017.857 I print_info: n_expert         = 0
0.00.017.858 I print_info: n_expert_used    = 0
0.00.017.859 I print_info: causal attn      = 0
0.00.017.859 I print_info: pooling type     = 2
0.00.017.859 I print_info: rope type        = 2
0.00.017.861 I print_info: rope scaling     = linear
0.00.017.861 I print_info: freq_base_train  = 10000.0
0.00.017.862 I print_info: freq_scale_train = 1
0.00.017.862 I print_info: n_ctx_orig_yarn  = 512
0.00.017.862 I print_info: rope_finetuned   = unknown
0.00.017.862 I print_info: ssm_d_conv       = 0
0.00.017.863 I print_info: ssm_d_inner      = 0
0.00.017.863 I print_info: ssm_d_state      = 0
0.00.017.863 I print_info: ssm_dt_rank      = 0
0.00.017.863 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.863 I print_info: model type       = 33M
0.00.017.864 I print_info: model params     = 33.21 M
0.00.017.864 I print_info: general.name     = Bge Small
0.00.017.864 I print_info: vocab type       = WPM
0.00.017.865 I print_info: n_vocab          = 30522
0.00.017.865 I print_info: n_merges         = 0
0.00.017.865 I print_info: BOS token        = 101 '[CLS]'
0.00.017.865 I print_info: UNK token        = 100 '[UNK]'
0.00.017.866 I print_info: SEP token        = 102 '[SEP]'
0.00.017.866 I print_info: PAD token        = 0 '[PAD]'
0.00.017.866 I print_info: MASK token       = 103 '[MASK]'
0.00.017.866 I print_info: LF token         = 0 '[PAD]'
0.00.017.866 I print_info: max token length = 21
0.00.019.503 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.504 I load_tensors: offloading output layer to GPU
0.00.019.504 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.510 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.510 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.019.653 I llama_init_from_model: n_seq_max     = 1
0.00.019.654 I llama_init_from_model: n_ctx         = 512
0.00.019.654 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.654 I llama_init_from_model: n_batch       = 2048
0.00.019.654 I llama_init_from_model: n_ubatch      = 2048
0.00.019.654 I llama_init_from_model: flash_attn    = 0
0.00.019.655 I llama_init_from_model: freq_base     = 10000.0
0.00.019.655 I llama_init_from_model: freq_scale    = 1
0.00.019.656 I ggml_metal_init: allocating
0.00.019.659 I ggml_metal_init: found device: Apple M4
0.00.019.663 I ggml_metal_init: picking default device: Apple M4
0.00.020.142 I ggml_metal_init: using embedded metal library
0.00.022.594 I ggml_metal_init: GPU name:   Apple M4
0.00.022.596 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.596 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.596 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.597 I ggml_metal_init: simdgroup reduction   = true
0.00.022.597 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.597 I ggml_metal_init: has residency sets    = true
0.00.022.597 I ggml_metal_init: has bfloat            = true
0.00.022.597 I ggml_metal_init: use bfloat            = true
0.00.022.597 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.598 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.136 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.752 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.754 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.768 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.737 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.738 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.739 I llama_init_from_model: graph nodes  = 429
0.00.034.739 I llama_init_from_model: graph splits = 2
0.00.034.740 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.740 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.849 I 
0.00.038.874 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.420 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.868 I llama_perf_context_print:        load time =      29.90 ms
0.00.043.869 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2076.60 tokens per second)
0.00.043.870 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.870 I llama_perf_context_print:       total time =       5.02 ms /    10 tokens
0.00.044.037 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.028s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.368 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.982 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.636 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.642 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.644 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.645 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.647 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.648 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.649 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.650 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.651 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.651 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.652 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.652 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.659 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.660 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.661 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.662 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.058 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.590 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.592 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.593 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.593 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.594 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.594 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.594 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.595 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.595 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.596 I llama_model_loader: - type  f32:   40 tensors
0.00.049.596 I llama_model_loader: - type  f16:   30 tensors
0.00.049.597 I print_info: file format = GGUF V3 (latest)
0.00.049.597 I print_info: file type   = F16
0.00.049.599 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.053.824 W load: empty token at index 5
0.00.059.087 W load: model vocab missing newline token, using special_pad_id instead
0.00.060.683 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.060.717 I load: special tokens cache size = 5
0.00.326.879 I load: token to piece cache size = 1.5060 MB
0.00.326.892 I print_info: arch             = jina-bert-v2
0.00.326.892 I print_info: vocab_only       = 0
0.00.326.892 I print_info: n_ctx_train      = 8192
0.00.326.893 I print_info: n_embd           = 384
0.00.326.897 I print_info: n_layer          = 4
0.00.326.904 I print_info: n_head           = 12
0.00.326.905 I print_info: n_head_kv        = 12
0.00.326.905 I print_info: n_rot            = 32
0.00.326.905 I print_info: n_swa            = 0
0.00.326.905 I print_info: n_embd_head_k    = 32
0.00.326.905 I print_info: n_embd_head_v    = 32
0.00.326.906 I print_info: n_gqa            = 1
0.00.326.907 I print_info: n_embd_k_gqa     = 384
0.00.326.918 I print_info: n_embd_v_gqa     = 384
0.00.326.922 I print_info: f_norm_eps       = 1.0e-12
0.00.326.923 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.326.923 I print_info: f_clamp_kqv      = 0.0e+00
0.00.326.923 I print_info: f_max_alibi_bias = 8.0e+00
0.00.326.924 I print_info: f_logit_scale    = 0.0e+00
0.00.326.931 I print_info: n_ff             = 1536
0.00.326.931 I print_info: n_expert         = 0
0.00.326.932 I print_info: n_expert_used    = 0
0.00.326.932 I print_info: causal attn      = 0
0.00.326.932 I print_info: pooling type     = -1
0.00.326.932 I print_info: rope type        = -1
0.00.326.933 I print_info: rope scaling     = linear
0.00.326.934 I print_info: freq_base_train  = 10000.0
0.00.326.934 I print_info: freq_scale_train = 1
0.00.326.934 I print_info: n_ctx_orig_yarn  = 8192
0.00.326.935 I print_info: rope_finetuned   = unknown
0.00.326.935 I print_info: ssm_d_conv       = 0
0.00.326.935 I print_info: ssm_d_inner      = 0
0.00.326.936 I print_info: ssm_d_state      = 0
0.00.326.936 I print_info: ssm_dt_rank      = 0
0.00.326.936 I print_info: ssm_dt_b_c_rms   = 0
0.00.326.936 I print_info: model type       = 33M
0.00.326.937 I print_info: model params     = 32.90 M
0.00.326.937 I print_info: general.name     = Jina Bert Implementation
0.00.326.938 I print_info: vocab type       = BPE
0.00.326.938 I print_info: n_vocab          = 61056
0.00.326.938 I print_info: n_merges         = 39382
0.00.326.940 I print_info: BOS token        = 0 '<s>'
0.00.326.940 I print_info: EOS token        = 2 '</s>'
0.00.326.940 I print_info: UNK token        = 3 '<unk>'
0.00.326.940 I print_info: SEP token        = 2 '</s>'
0.00.326.940 I print_info: PAD token        = 1 '<pad>'
0.00.326.941 I print_info: MASK token       = 4 '<mask>'
0.00.326.942 I print_info: EOG token        = 2 '</s>'
0.00.326.943 I print_info: max token length = 45
0.00.329.171 I load_tensors: offloading 4 repeating layers to GPU
0.00.329.172 I load_tensors: offloading output layer to GPU
0.00.329.173 I load_tensors: offloaded 5/5 layers to GPU
0.00.329.195 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.329.196 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.329.397 I llama_init_from_model: n_seq_max     = 1
0.00.329.398 I llama_init_from_model: n_ctx         = 8192
0.00.329.398 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.329.398 I llama_init_from_model: n_batch       = 2048
0.00.329.398 I llama_init_from_model: n_ubatch      = 2048
0.00.329.398 I llama_init_from_model: flash_attn    = 0
0.00.329.399 I llama_init_from_model: freq_base     = 10000.0
0.00.329.399 I llama_init_from_model: freq_scale    = 1
0.00.329.399 I ggml_metal_init: allocating
0.00.329.404 I ggml_metal_init: found device: Apple M4
0.00.329.407 I ggml_metal_init: picking default device: Apple M4
0.00.330.045 I ggml_metal_init: using embedded metal library
0.00.332.921 I ggml_metal_init: GPU name:   Apple M4
0.00.332.922 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.332.923 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.332.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.332.924 I ggml_metal_init: simdgroup reduction   = true
0.00.332.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.332.924 I ggml_metal_init: has residency sets    = true
0.00.332.924 I ggml_metal_init: has bfloat            = true
0.00.332.924 I ggml_metal_init: use bfloat            = true
0.00.332.925 I ggml_metal_init: hasUnifiedMemory      = true
0.00.332.925 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.342.460 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.345.683 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.345.685 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.345.708 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.352.525 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.352.527 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.352.527 I llama_init_from_model: graph nodes  = 154
0.00.352.527 I llama_init_from_model: graph splits = 2
0.00.352.528 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.352.528 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.654 I 
0.00.360.683 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.781 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.360.782 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.360.785 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.360.788 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.360.792 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.360.792 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.335 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.803 I llama_perf_context_print:        load time =     337.67 ms
0.00.364.804 I llama_perf_context_print: prompt eval time =       3.46 ms /    62 tokens (    0.06 ms per token, 17924.26 tokens per second)
0.00.364.805 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.805 I llama_perf_context_print:       total time =       4.15 ms /    63 tokens
0.00.365.091 I ggml_metal_free: deallocating

real	0m1.081s
user	0m0.334s
sys	0m0.050s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.153 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.312 I main: llama backend init
0.00.000.321 I main: load the model and apply lora adapter, if any
0.00.051.687 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.064.467 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.064.485 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.064.489 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.064.489 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.064.496 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.064.496 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.064.497 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.064.500 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.064.501 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.064.502 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.064.503 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.064.503 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.064.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.064.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.064.511 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.064.511 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.064.512 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.073.164 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.075.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.083.640 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.083.643 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.083.644 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.083.644 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.083.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.083.646 I llama_model_loader: - type  f32:  194 tensors
0.00.083.646 I llama_model_loader: - type  f16:   98 tensors
0.00.083.649 I print_info: file format = GGUF V3 (latest)
0.00.083.651 I print_info: file type   = all F32 (guessed)
0.00.083.654 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.098.633 I load: special tokens cache size = 25
0.00.107.448 I load: token to piece cache size = 0.2984 MB
0.00.107.452 I print_info: arch             = gptneox
0.00.107.452 I print_info: vocab_only       = 0
0.00.107.452 I print_info: n_ctx_train      = 2048
0.00.107.452 I print_info: n_embd           = 2048
0.00.107.453 I print_info: n_layer          = 24
0.00.107.456 I print_info: n_head           = 16
0.00.107.457 I print_info: n_head_kv        = 16
0.00.107.457 I print_info: n_rot            = 32
0.00.107.457 I print_info: n_swa            = 0
0.00.107.458 I print_info: n_embd_head_k    = 128
0.00.107.458 I print_info: n_embd_head_v    = 128
0.00.107.459 I print_info: n_gqa            = 1
0.00.107.459 I print_info: n_embd_k_gqa     = 2048
0.00.107.460 I print_info: n_embd_v_gqa     = 2048
0.00.107.461 I print_info: f_norm_eps       = 1.0e-05
0.00.107.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.107.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.107.463 I print_info: f_max_alibi_bias = 0.0e+00
0.00.107.463 I print_info: f_logit_scale    = 0.0e+00
0.00.107.464 I print_info: n_ff             = 8192
0.00.107.464 I print_info: n_expert         = 0
0.00.107.465 I print_info: n_expert_used    = 0
0.00.107.465 I print_info: causal attn      = 1
0.00.107.465 I print_info: pooling type     = 0
0.00.107.465 I print_info: rope type        = 2
0.00.107.465 I print_info: rope scaling     = linear
0.00.107.467 I print_info: freq_base_train  = 10000.0
0.00.107.468 I print_info: freq_scale_train = 1
0.00.107.468 I print_info: n_ctx_orig_yarn  = 2048
0.00.107.468 I print_info: rope_finetuned   = unknown
0.00.107.468 I print_info: ssm_d_conv       = 0
0.00.107.468 I print_info: ssm_d_inner      = 0
0.00.107.468 I print_info: ssm_d_state      = 0
0.00.107.469 I print_info: ssm_dt_rank      = 0
0.00.107.469 I print_info: ssm_dt_b_c_rms   = 0
0.00.107.469 I print_info: model type       = 1.4B
0.00.107.469 I print_info: model params     = 1.41 B
0.00.107.470 I print_info: general.name     = 1.4B
0.00.107.470 I print_info: vocab type       = BPE
0.00.107.470 I print_info: n_vocab          = 50304
0.00.107.470 I print_info: n_merges         = 50009
0.00.107.471 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.107.473 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.107.473 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.107.473 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.107.474 I print_info: LF token         = 187 'Ċ'
0.00.107.474 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.107.474 I print_info: max token length = 1024
0.00.147.461 I load_tensors: offloading 24 repeating layers to GPU
0.00.147.465 I load_tensors: offloading output layer to GPU
0.00.147.465 I load_tensors: offloaded 25/25 layers to GPU
0.00.147.491 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.147.492 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.147.914 I llama_init_from_model: n_seq_max     = 1
0.00.147.914 I llama_init_from_model: n_ctx         = 2048
0.00.147.915 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.147.915 I llama_init_from_model: n_batch       = 2048
0.00.147.915 I llama_init_from_model: n_ubatch      = 512
0.00.147.915 I llama_init_from_model: flash_attn    = 0
0.00.147.915 I llama_init_from_model: freq_base     = 10000.0
0.00.147.916 I llama_init_from_model: freq_scale    = 1
0.00.147.918 I ggml_metal_init: allocating
0.00.147.954 I ggml_metal_init: found device: Apple M4
0.00.147.961 I ggml_metal_init: picking default device: Apple M4
0.00.148.553 I ggml_metal_init: using embedded metal library
0.00.157.668 I ggml_metal_init: GPU name:   Apple M4
0.00.157.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.157.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.157.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.157.671 I ggml_metal_init: simdgroup reduction   = true
0.00.157.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.157.671 I ggml_metal_init: has residency sets    = true
0.00.157.671 I ggml_metal_init: has bfloat            = true
0.00.157.671 I ggml_metal_init: use bfloat            = true
0.00.157.672 I ggml_metal_init: hasUnifiedMemory      = true
0.00.157.673 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.181.561 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.211.598 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.211.604 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.211.644 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.216.512 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.216.515 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.216.515 I llama_init_from_model: graph nodes  = 967
0.00.216.515 I llama_init_from_model: graph splits = 2
0.00.216.522 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.216.644 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.216.645 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.282.895 I main: llama threadpool init, n_threads = 4
0.00.282.936 I 
0.00.282.971 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.282.972 I 
0.00.283.159 I sampler seed: 1234
0.00.283.164 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.283.189 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.283.190 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.283.190 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.118.477 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59865.09 tokens per second)
0.02.118.477 I llama_perf_context_print:        load time =     230.12 ms
0.02.118.478 I llama_perf_context_print: prompt eval time =      53.58 ms /     7 tokens (    7.65 ms per token,   130.65 tokens per second)
0.02.118.478 I llama_perf_context_print:        eval time =    1778.84 ms /    63 runs   (   28.24 ms per token,    35.42 tokens per second)
0.02.118.479 I llama_perf_context_print:       total time =    1836.66 ms /    70 tokens
0.02.118.713 I ggml_metal_free: deallocating

real	0m2.706s
user	0m0.133s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.780 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.400 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.227 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.236 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.240 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.240 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.241 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.242 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.242 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.246 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.246 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.249 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.251 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.254 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.254 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.255 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.053 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.921 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.212 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.215 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.216 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.217 I llama_model_loader: - type  f32:  194 tensors
0.00.055.217 I llama_model_loader: - type  f16:   98 tensors
0.00.055.218 I print_info: file format = GGUF V3 (latest)
0.00.055.219 I print_info: file type   = all F32 (guessed)
0.00.055.222 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.715 I load: special tokens cache size = 25
0.00.075.113 I load: token to piece cache size = 0.2984 MB
0.00.075.116 I print_info: arch             = gptneox
0.00.075.117 I print_info: vocab_only       = 0
0.00.075.117 I print_info: n_ctx_train      = 2048
0.00.075.117 I print_info: n_embd           = 2048
0.00.075.117 I print_info: n_layer          = 24
0.00.075.120 I print_info: n_head           = 16
0.00.075.121 I print_info: n_head_kv        = 16
0.00.075.121 I print_info: n_rot            = 32
0.00.075.122 I print_info: n_swa            = 0
0.00.075.122 I print_info: n_embd_head_k    = 128
0.00.075.122 I print_info: n_embd_head_v    = 128
0.00.075.123 I print_info: n_gqa            = 1
0.00.075.123 I print_info: n_embd_k_gqa     = 2048
0.00.075.124 I print_info: n_embd_v_gqa     = 2048
0.00.075.125 I print_info: f_norm_eps       = 1.0e-05
0.00.075.127 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.128 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.128 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.128 I print_info: f_logit_scale    = 0.0e+00
0.00.075.129 I print_info: n_ff             = 8192
0.00.075.129 I print_info: n_expert         = 0
0.00.075.129 I print_info: n_expert_used    = 0
0.00.075.130 I print_info: causal attn      = 1
0.00.075.130 I print_info: pooling type     = 0
0.00.075.130 I print_info: rope type        = 2
0.00.075.131 I print_info: rope scaling     = linear
0.00.075.131 I print_info: freq_base_train  = 10000.0
0.00.075.131 I print_info: freq_scale_train = 1
0.00.075.132 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.132 I print_info: rope_finetuned   = unknown
0.00.075.132 I print_info: ssm_d_conv       = 0
0.00.075.132 I print_info: ssm_d_inner      = 0
0.00.075.132 I print_info: ssm_d_state      = 0
0.00.075.132 I print_info: ssm_dt_rank      = 0
0.00.075.132 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.133 I print_info: model type       = 1.4B
0.00.075.133 I print_info: model params     = 1.41 B
0.00.075.133 I print_info: general.name     = 1.4B
0.00.075.134 I print_info: vocab type       = BPE
0.00.075.134 I print_info: n_vocab          = 50304
0.00.075.134 I print_info: n_merges         = 50009
0.00.075.135 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.135 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.135 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.135 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.135 I print_info: LF token         = 187 'Ċ'
0.00.075.136 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.136 I print_info: max token length = 1024
0.01.434.560 I load_tensors: offloading 24 repeating layers to GPU
0.01.434.566 I load_tensors: offloading output layer to GPU
0.01.434.566 I load_tensors: offloaded 25/25 layers to GPU
0.01.434.590 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.434.592 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.435.616 I llama_init_from_model: n_seq_max     = 1
0.01.435.617 I llama_init_from_model: n_ctx         = 128
0.01.435.617 I llama_init_from_model: n_ctx_per_seq = 128
0.01.435.618 I llama_init_from_model: n_batch       = 128
0.01.435.618 I llama_init_from_model: n_ubatch      = 128
0.01.435.621 I llama_init_from_model: flash_attn    = 0
0.01.435.623 I llama_init_from_model: freq_base     = 10000.0
0.01.435.624 I llama_init_from_model: freq_scale    = 1
0.01.435.624 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.435.625 I ggml_metal_init: allocating
0.01.435.666 I ggml_metal_init: found device: Apple M4
0.01.435.673 I ggml_metal_init: picking default device: Apple M4
0.01.436.745 I ggml_metal_init: using embedded metal library
0.01.441.527 I ggml_metal_init: GPU name:   Apple M4
0.01.441.529 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.441.530 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.441.530 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.441.530 I ggml_metal_init: simdgroup reduction   = true
0.01.441.531 I ggml_metal_init: simdgroup matrix mul. = true
0.01.441.531 I ggml_metal_init: has residency sets    = true
0.01.441.531 I ggml_metal_init: has bfloat            = true
0.01.441.531 I ggml_metal_init: use bfloat            = true
0.01.441.532 I ggml_metal_init: hasUnifiedMemory      = true
0.01.441.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.452.835 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.454.638 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.454.640 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.454.665 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.456.455 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.456.456 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.456.456 I llama_init_from_model: graph nodes  = 967
0.01.456.457 I llama_init_from_model: graph splits = 2
0.01.456.458 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.456.458 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.492.901 I 
0.01.492.937 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.492.941 I perplexity: tokenizing the input ..
0.01.498.240 I perplexity: tokenization took 5.298 ms
0.01.498.244 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.617.894 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.619.394 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.619.428 I llama_perf_context_print:        load time =    1468.49 ms
0.01.619.429 I llama_perf_context_print: prompt eval time =     119.33 ms /   128 tokens (    0.93 ms per token,  1072.66 tokens per second)
0.01.619.429 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.619.430 I llama_perf_context_print:       total time =     126.53 ms /   129 tokens
0.01.619.842 I ggml_metal_free: deallocating

real	0m1.839s
user	0m0.098s
sys	0m0.255s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.939 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.552 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.558 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.560 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.561 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.563 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.563 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.565 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.566 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.566 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.568 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.568 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.335 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.351 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.178 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.179 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.180 I llama_model_loader: - type  f32:  194 tensors
0.00.033.180 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.181 I print_info: file format = GGUF V3 (latest)
0.00.033.182 I print_info: file type   = Q8_0
0.00.033.183 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.041.314 I load: special tokens cache size = 25
0.00.047.043 I load: token to piece cache size = 0.2984 MB
0.00.047.047 I print_info: arch             = gptneox
0.00.047.048 I print_info: vocab_only       = 0
0.00.047.048 I print_info: n_ctx_train      = 2048
0.00.047.048 I print_info: n_embd           = 2048
0.00.047.048 I print_info: n_layer          = 24
0.00.047.054 I print_info: n_head           = 16
0.00.047.055 I print_info: n_head_kv        = 16
0.00.047.057 I print_info: n_rot            = 32
0.00.047.057 I print_info: n_swa            = 0
0.00.047.057 I print_info: n_embd_head_k    = 128
0.00.047.057 I print_info: n_embd_head_v    = 128
0.00.047.058 I print_info: n_gqa            = 1
0.00.047.059 I print_info: n_embd_k_gqa     = 2048
0.00.047.060 I print_info: n_embd_v_gqa     = 2048
0.00.047.061 I print_info: f_norm_eps       = 1.0e-05
0.00.047.061 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.061 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.061 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.062 I print_info: f_logit_scale    = 0.0e+00
0.00.047.062 I print_info: n_ff             = 8192
0.00.047.063 I print_info: n_expert         = 0
0.00.047.063 I print_info: n_expert_used    = 0
0.00.047.063 I print_info: causal attn      = 1
0.00.047.063 I print_info: pooling type     = 0
0.00.047.063 I print_info: rope type        = 2
0.00.047.064 I print_info: rope scaling     = linear
0.00.047.064 I print_info: freq_base_train  = 10000.0
0.00.047.064 I print_info: freq_scale_train = 1
0.00.047.064 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.065 I print_info: rope_finetuned   = unknown
0.00.047.065 I print_info: ssm_d_conv       = 0
0.00.047.065 I print_info: ssm_d_inner      = 0
0.00.047.065 I print_info: ssm_d_state      = 0
0.00.047.065 I print_info: ssm_dt_rank      = 0
0.00.047.065 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.066 I print_info: model type       = 1.4B
0.00.047.066 I print_info: model params     = 1.41 B
0.00.047.066 I print_info: general.name     = 1.4B
0.00.047.070 I print_info: vocab type       = BPE
0.00.047.070 I print_info: n_vocab          = 50304
0.00.047.070 I print_info: n_merges         = 50009
0.00.047.070 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.070 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.070 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.071 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.075 I print_info: LF token         = 187 'Ċ'
0.00.047.075 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.075 I print_info: max token length = 1024
0.01.121.225 I load_tensors: offloading 24 repeating layers to GPU
0.01.121.230 I load_tensors: offloading output layer to GPU
0.01.121.232 I load_tensors: offloaded 25/25 layers to GPU
0.01.121.257 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.121.259 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.121.949 I llama_init_from_model: n_seq_max     = 1
0.01.121.951 I llama_init_from_model: n_ctx         = 2048
0.01.121.951 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.121.952 I llama_init_from_model: n_batch       = 2048
0.01.121.952 I llama_init_from_model: n_ubatch      = 512
0.01.121.953 I llama_init_from_model: flash_attn    = 0
0.01.121.954 I llama_init_from_model: freq_base     = 10000.0
0.01.121.954 I llama_init_from_model: freq_scale    = 1
0.01.121.955 I ggml_metal_init: allocating
0.01.121.973 I ggml_metal_init: found device: Apple M4
0.01.121.981 I ggml_metal_init: picking default device: Apple M4
0.01.123.245 I ggml_metal_init: using embedded metal library
0.01.128.558 I ggml_metal_init: GPU name:   Apple M4
0.01.128.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.128.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.128.563 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.128.563 I ggml_metal_init: simdgroup reduction   = true
0.01.128.564 I ggml_metal_init: simdgroup matrix mul. = true
0.01.128.564 I ggml_metal_init: has residency sets    = true
0.01.128.564 I ggml_metal_init: has bfloat            = true
0.01.128.564 I ggml_metal_init: use bfloat            = true
0.01.128.565 I ggml_metal_init: hasUnifiedMemory      = true
0.01.128.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.143.946 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.200.134 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.200.140 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.200.172 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.204.595 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.204.597 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.204.597 I llama_init_from_model: graph nodes  = 967
0.01.204.597 I llama_init_from_model: graph splits = 2
0.01.204.602 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.204.736 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.204.737 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.260.177 I main: llama threadpool init, n_threads = 4
0.01.260.222 I 
0.01.260.246 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.260.247 I 
0.01.260.399 I sampler seed: 1234
0.01.260.403 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.260.444 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.260.445 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.260.447 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.339.065 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54198.47 tokens per second)
0.02.339.065 I llama_perf_context_print:        load time =    1249.31 ms
0.02.339.066 I llama_perf_context_print: prompt eval time =      47.15 ms /     7 tokens (    6.74 ms per token,   148.47 tokens per second)
0.02.339.067 I llama_perf_context_print:        eval time =    1028.57 ms /    63 runs   (   16.33 ms per token,    61.25 tokens per second)
0.02.339.067 I llama_perf_context_print:       total time =    1079.81 ms /    70 tokens
0.02.339.307 I ggml_metal_free: deallocating

real	0m2.359s
user	0m0.107s
sys	0m0.272s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.207 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.526 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.533 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.540 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.541 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.541 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.541 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.542 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.543 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.544 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.544 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.545 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.545 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.545 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.546 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.548 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.360 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.327 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.102 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.104 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.105 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.106 I llama_model_loader: - type  f32:  194 tensors
0.00.025.107 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.107 I print_info: file format = GGUF V3 (latest)
0.00.025.108 I print_info: file type   = Q8_0
0.00.025.109 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.045 I load: special tokens cache size = 25
0.00.039.167 I load: token to piece cache size = 0.2984 MB
0.00.039.171 I print_info: arch             = gptneox
0.00.039.171 I print_info: vocab_only       = 0
0.00.039.171 I print_info: n_ctx_train      = 2048
0.00.039.172 I print_info: n_embd           = 2048
0.00.039.172 I print_info: n_layer          = 24
0.00.039.176 I print_info: n_head           = 16
0.00.039.177 I print_info: n_head_kv        = 16
0.00.039.177 I print_info: n_rot            = 32
0.00.039.177 I print_info: n_swa            = 0
0.00.039.177 I print_info: n_embd_head_k    = 128
0.00.039.177 I print_info: n_embd_head_v    = 128
0.00.039.178 I print_info: n_gqa            = 1
0.00.039.179 I print_info: n_embd_k_gqa     = 2048
0.00.039.179 I print_info: n_embd_v_gqa     = 2048
0.00.039.180 I print_info: f_norm_eps       = 1.0e-05
0.00.039.180 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.181 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.181 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.184 I print_info: f_logit_scale    = 0.0e+00
0.00.039.185 I print_info: n_ff             = 8192
0.00.039.185 I print_info: n_expert         = 0
0.00.039.185 I print_info: n_expert_used    = 0
0.00.039.185 I print_info: causal attn      = 1
0.00.039.185 I print_info: pooling type     = 0
0.00.039.186 I print_info: rope type        = 2
0.00.039.186 I print_info: rope scaling     = linear
0.00.039.186 I print_info: freq_base_train  = 10000.0
0.00.039.187 I print_info: freq_scale_train = 1
0.00.039.187 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.187 I print_info: rope_finetuned   = unknown
0.00.039.187 I print_info: ssm_d_conv       = 0
0.00.039.187 I print_info: ssm_d_inner      = 0
0.00.039.188 I print_info: ssm_d_state      = 0
0.00.039.188 I print_info: ssm_dt_rank      = 0
0.00.039.188 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.189 I print_info: model type       = 1.4B
0.00.039.189 I print_info: model params     = 1.41 B
0.00.039.189 I print_info: general.name     = 1.4B
0.00.039.190 I print_info: vocab type       = BPE
0.00.039.190 I print_info: n_vocab          = 50304
0.00.039.190 I print_info: n_merges         = 50009
0.00.039.190 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.190 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.191 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.191 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.191 I print_info: LF token         = 187 'Ċ'
0.00.039.191 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.192 I print_info: max token length = 1024
0.00.861.883 I load_tensors: offloading 24 repeating layers to GPU
0.00.861.889 I load_tensors: offloading output layer to GPU
0.00.861.890 I load_tensors: offloaded 25/25 layers to GPU
0.00.861.918 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.861.920 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.863.184 I llama_init_from_model: n_seq_max     = 1
0.00.863.186 I llama_init_from_model: n_ctx         = 128
0.00.863.187 I llama_init_from_model: n_ctx_per_seq = 128
0.00.863.187 I llama_init_from_model: n_batch       = 128
0.00.863.187 I llama_init_from_model: n_ubatch      = 128
0.00.863.188 I llama_init_from_model: flash_attn    = 0
0.00.863.189 I llama_init_from_model: freq_base     = 10000.0
0.00.863.189 I llama_init_from_model: freq_scale    = 1
0.00.863.190 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.863.191 I ggml_metal_init: allocating
0.00.863.241 I ggml_metal_init: found device: Apple M4
0.00.863.252 I ggml_metal_init: picking default device: Apple M4
0.00.864.489 I ggml_metal_init: using embedded metal library
0.00.869.937 I ggml_metal_init: GPU name:   Apple M4
0.00.869.942 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.869.942 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.869.943 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.869.944 I ggml_metal_init: simdgroup reduction   = true
0.00.869.944 I ggml_metal_init: simdgroup matrix mul. = true
0.00.869.944 I ggml_metal_init: has residency sets    = true
0.00.869.945 I ggml_metal_init: has bfloat            = true
0.00.869.945 I ggml_metal_init: use bfloat            = true
0.00.869.946 I ggml_metal_init: hasUnifiedMemory      = true
0.00.869.949 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.884.572 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.886.984 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.886.987 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.887.019 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.889.170 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.889.171 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.889.172 I llama_init_from_model: graph nodes  = 967
0.00.889.172 I llama_init_from_model: graph splits = 2
0.00.889.174 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.889.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.915.346 I 
0.00.915.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.915.393 I perplexity: tokenizing the input ..
0.00.920.970 I perplexity: tokenization took 5.575 ms
0.00.920.973 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.058.262 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.059.607 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.059.631 I llama_perf_context_print:        load time =     906.13 ms
0.01.059.632 I llama_perf_context_print: prompt eval time =     137.06 ms /   128 tokens (    1.07 ms per token,   933.88 tokens per second)
0.01.059.632 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.059.633 I llama_perf_context_print:       total time =     144.29 ms /   129 tokens
0.01.060.039 I ggml_metal_free: deallocating

real	0m1.074s
user	0m0.073s
sys	0m0.159s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.470 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.336 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.344 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.344 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.345 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.345 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.345 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.346 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.347 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.347 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.347 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.348 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.350 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.352 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.352 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.353 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.097 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.134 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.907 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.908 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.909 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.909 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.909 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.910 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.910 I llama_model_loader: - type  f32:  194 tensors
0.00.027.911 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.911 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.912 I print_info: file format = GGUF V3 (latest)
0.00.027.912 I print_info: file type   = Q4_0
0.00.027.913 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.902 I load: special tokens cache size = 25
0.00.042.035 I load: token to piece cache size = 0.2984 MB
0.00.042.038 I print_info: arch             = gptneox
0.00.042.038 I print_info: vocab_only       = 0
0.00.042.038 I print_info: n_ctx_train      = 2048
0.00.042.038 I print_info: n_embd           = 2048
0.00.042.039 I print_info: n_layer          = 24
0.00.042.043 I print_info: n_head           = 16
0.00.042.044 I print_info: n_head_kv        = 16
0.00.042.044 I print_info: n_rot            = 32
0.00.042.044 I print_info: n_swa            = 0
0.00.042.044 I print_info: n_embd_head_k    = 128
0.00.042.045 I print_info: n_embd_head_v    = 128
0.00.042.045 I print_info: n_gqa            = 1
0.00.042.048 I print_info: n_embd_k_gqa     = 2048
0.00.042.049 I print_info: n_embd_v_gqa     = 2048
0.00.042.049 I print_info: f_norm_eps       = 1.0e-05
0.00.042.049 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.050 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.050 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.050 I print_info: f_logit_scale    = 0.0e+00
0.00.042.052 I print_info: n_ff             = 8192
0.00.042.052 I print_info: n_expert         = 0
0.00.042.052 I print_info: n_expert_used    = 0
0.00.042.053 I print_info: causal attn      = 1
0.00.042.053 I print_info: pooling type     = 0
0.00.042.053 I print_info: rope type        = 2
0.00.042.053 I print_info: rope scaling     = linear
0.00.042.054 I print_info: freq_base_train  = 10000.0
0.00.042.055 I print_info: freq_scale_train = 1
0.00.042.055 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.055 I print_info: rope_finetuned   = unknown
0.00.042.055 I print_info: ssm_d_conv       = 0
0.00.042.056 I print_info: ssm_d_inner      = 0
0.00.042.056 I print_info: ssm_d_state      = 0
0.00.042.056 I print_info: ssm_dt_rank      = 0
0.00.042.056 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.056 I print_info: model type       = 1.4B
0.00.042.057 I print_info: model params     = 1.41 B
0.00.042.057 I print_info: general.name     = 1.4B
0.00.042.057 I print_info: vocab type       = BPE
0.00.042.061 I print_info: n_vocab          = 50304
0.00.042.062 I print_info: n_merges         = 50009
0.00.042.062 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.062 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.062 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.062 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.066 I print_info: LF token         = 187 'Ċ'
0.00.042.066 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.066 I print_info: max token length = 1024
0.00.602.241 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.257 I load_tensors: offloading output layer to GPU
0.00.602.258 I load_tensors: offloaded 25/25 layers to GPU
0.00.602.291 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.602.292 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.603.477 I llama_init_from_model: n_seq_max     = 1
0.00.603.490 I llama_init_from_model: n_ctx         = 2048
0.00.603.491 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.603.491 I llama_init_from_model: n_batch       = 2048
0.00.603.491 I llama_init_from_model: n_ubatch      = 512
0.00.603.492 I llama_init_from_model: flash_attn    = 0
0.00.603.493 I llama_init_from_model: freq_base     = 10000.0
0.00.603.494 I llama_init_from_model: freq_scale    = 1
0.00.603.496 I ggml_metal_init: allocating
0.00.603.582 I ggml_metal_init: found device: Apple M4
0.00.603.596 I ggml_metal_init: picking default device: Apple M4
0.00.605.171 I ggml_metal_init: using embedded metal library
0.00.609.805 I ggml_metal_init: GPU name:   Apple M4
0.00.609.813 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.814 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.814 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.815 I ggml_metal_init: simdgroup reduction   = true
0.00.609.815 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.815 I ggml_metal_init: has residency sets    = true
0.00.609.816 I ggml_metal_init: has bfloat            = true
0.00.609.816 I ggml_metal_init: use bfloat            = true
0.00.609.817 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.820 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.509 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.476 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.651.482 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.651.516 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.655.578 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.655.580 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.655.581 I llama_init_from_model: graph nodes  = 967
0.00.655.581 I llama_init_from_model: graph splits = 2
0.00.655.585 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.655.707 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.655.708 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.005 I main: llama threadpool init, n_threads = 4
0.00.712.052 I 
0.00.712.074 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.076 I 
0.00.712.250 I sampler seed: 1234
0.00.712.254 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.265 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.267 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.267 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.388.179 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50750.54 tokens per second)
0.01.388.179 I llama_perf_context_print:        load time =     699.62 ms
0.01.388.180 I llama_perf_context_print: prompt eval time =      47.70 ms /     7 tokens (    6.81 ms per token,   146.74 tokens per second)
0.01.388.181 I llama_perf_context_print:        eval time =     625.30 ms /    63 runs   (    9.93 ms per token,   100.75 tokens per second)
0.01.388.181 I llama_perf_context_print:       total time =     677.08 ms /    70 tokens
0.01.388.442 I ggml_metal_free: deallocating

real	0m1.407s
user	0m0.100s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.785 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.821 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.828 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.834 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.835 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.835 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.835 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.836 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.837 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.837 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.838 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.838 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.838 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.839 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.843 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.844 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.657 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.639 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.411 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.413 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.413 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.413 I llama_model_loader: - type  f32:  194 tensors
0.00.026.414 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.414 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.415 I print_info: file format = GGUF V3 (latest)
0.00.026.416 I print_info: file type   = Q4_0
0.00.026.417 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.803 I load: special tokens cache size = 25
0.00.040.812 I load: token to piece cache size = 0.2984 MB
0.00.040.816 I print_info: arch             = gptneox
0.00.040.816 I print_info: vocab_only       = 0
0.00.040.816 I print_info: n_ctx_train      = 2048
0.00.040.816 I print_info: n_embd           = 2048
0.00.040.817 I print_info: n_layer          = 24
0.00.040.820 I print_info: n_head           = 16
0.00.040.821 I print_info: n_head_kv        = 16
0.00.040.821 I print_info: n_rot            = 32
0.00.040.821 I print_info: n_swa            = 0
0.00.040.824 I print_info: n_embd_head_k    = 128
0.00.040.824 I print_info: n_embd_head_v    = 128
0.00.040.825 I print_info: n_gqa            = 1
0.00.040.825 I print_info: n_embd_k_gqa     = 2048
0.00.040.826 I print_info: n_embd_v_gqa     = 2048
0.00.040.827 I print_info: f_norm_eps       = 1.0e-05
0.00.040.827 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.827 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.828 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.828 I print_info: f_logit_scale    = 0.0e+00
0.00.040.828 I print_info: n_ff             = 8192
0.00.040.829 I print_info: n_expert         = 0
0.00.040.829 I print_info: n_expert_used    = 0
0.00.040.829 I print_info: causal attn      = 1
0.00.040.829 I print_info: pooling type     = 0
0.00.040.830 I print_info: rope type        = 2
0.00.040.830 I print_info: rope scaling     = linear
0.00.040.830 I print_info: freq_base_train  = 10000.0
0.00.040.831 I print_info: freq_scale_train = 1
0.00.040.831 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.831 I print_info: rope_finetuned   = unknown
0.00.040.831 I print_info: ssm_d_conv       = 0
0.00.040.832 I print_info: ssm_d_inner      = 0
0.00.040.832 I print_info: ssm_d_state      = 0
0.00.040.832 I print_info: ssm_dt_rank      = 0
0.00.040.832 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.832 I print_info: model type       = 1.4B
0.00.040.833 I print_info: model params     = 1.41 B
0.00.040.833 I print_info: general.name     = 1.4B
0.00.040.833 I print_info: vocab type       = BPE
0.00.040.835 I print_info: n_vocab          = 50304
0.00.040.835 I print_info: n_merges         = 50009
0.00.040.835 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.835 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.836 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.836 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.836 I print_info: LF token         = 187 'Ċ'
0.00.040.836 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.837 I print_info: max token length = 1024
0.00.586.666 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.682 I load_tensors: offloading output layer to GPU
0.00.586.682 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.713 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.586.715 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.588.238 I llama_init_from_model: n_seq_max     = 1
0.00.588.244 I llama_init_from_model: n_ctx         = 128
0.00.588.245 I llama_init_from_model: n_ctx_per_seq = 128
0.00.588.245 I llama_init_from_model: n_batch       = 128
0.00.588.246 I llama_init_from_model: n_ubatch      = 128
0.00.588.246 I llama_init_from_model: flash_attn    = 0
0.00.588.248 I llama_init_from_model: freq_base     = 10000.0
0.00.588.249 I llama_init_from_model: freq_scale    = 1
0.00.588.249 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.588.252 I ggml_metal_init: allocating
0.00.588.326 I ggml_metal_init: found device: Apple M4
0.00.588.340 I ggml_metal_init: picking default device: Apple M4
0.00.590.049 I ggml_metal_init: using embedded metal library
0.00.596.722 I ggml_metal_init: GPU name:   Apple M4
0.00.596.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.596.748 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.596.749 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.596.750 I ggml_metal_init: simdgroup reduction   = true
0.00.596.751 I ggml_metal_init: simdgroup matrix mul. = true
0.00.596.751 I ggml_metal_init: has residency sets    = true
0.00.596.751 I ggml_metal_init: has bfloat            = true
0.00.596.751 I ggml_metal_init: use bfloat            = true
0.00.596.753 I ggml_metal_init: hasUnifiedMemory      = true
0.00.596.755 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.918 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.337 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.347 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.412 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.674 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.676 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.676 I llama_init_from_model: graph nodes  = 967
0.00.621.677 I llama_init_from_model: graph splits = 2
0.00.621.679 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.679 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.186 I 
0.00.647.269 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.275 I perplexity: tokenizing the input ..
0.00.654.644 I perplexity: tokenization took 7.366 ms
0.00.654.651 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.763 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.791.195 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.791.219 I llama_perf_context_print:        load time =     636.39 ms
0.00.791.219 I llama_perf_context_print: prompt eval time =     134.57 ms /   128 tokens (    1.05 ms per token,   951.21 tokens per second)
0.00.791.220 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.220 I llama_perf_context_print:       total time =     144.04 ms /   129 tokens
0.00.791.593 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.079s
sys	0m0.120s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.865 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.684 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.688 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.695 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.696 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.696 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.696 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.697 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.698 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.698 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.698 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.700 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.701 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.701 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.703 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.703 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.703 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.540 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.569 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.355 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.356 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.357 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.357 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.357 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.358 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.358 I llama_model_loader: - type  f32:  194 tensors
0.00.025.358 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.359 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.359 I print_info: file format = GGUF V3 (latest)
0.00.025.360 I print_info: file type   = Q4_1
0.00.025.360 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.394 I load: special tokens cache size = 25
0.00.039.426 I load: token to piece cache size = 0.2984 MB
0.00.039.429 I print_info: arch             = gptneox
0.00.039.429 I print_info: vocab_only       = 0
0.00.039.429 I print_info: n_ctx_train      = 2048
0.00.039.430 I print_info: n_embd           = 2048
0.00.039.430 I print_info: n_layer          = 24
0.00.039.432 I print_info: n_head           = 16
0.00.039.433 I print_info: n_head_kv        = 16
0.00.039.433 I print_info: n_rot            = 32
0.00.039.435 I print_info: n_swa            = 0
0.00.039.435 I print_info: n_embd_head_k    = 128
0.00.039.435 I print_info: n_embd_head_v    = 128
0.00.039.436 I print_info: n_gqa            = 1
0.00.039.437 I print_info: n_embd_k_gqa     = 2048
0.00.039.442 I print_info: n_embd_v_gqa     = 2048
0.00.039.444 I print_info: f_norm_eps       = 1.0e-05
0.00.039.444 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.444 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.444 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.445 I print_info: f_logit_scale    = 0.0e+00
0.00.039.445 I print_info: n_ff             = 8192
0.00.039.446 I print_info: n_expert         = 0
0.00.039.446 I print_info: n_expert_used    = 0
0.00.039.446 I print_info: causal attn      = 1
0.00.039.446 I print_info: pooling type     = 0
0.00.039.446 I print_info: rope type        = 2
0.00.039.446 I print_info: rope scaling     = linear
0.00.039.450 I print_info: freq_base_train  = 10000.0
0.00.039.450 I print_info: freq_scale_train = 1
0.00.039.450 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.451 I print_info: rope_finetuned   = unknown
0.00.039.451 I print_info: ssm_d_conv       = 0
0.00.039.451 I print_info: ssm_d_inner      = 0
0.00.039.451 I print_info: ssm_d_state      = 0
0.00.039.451 I print_info: ssm_dt_rank      = 0
0.00.039.451 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.452 I print_info: model type       = 1.4B
0.00.039.453 I print_info: model params     = 1.41 B
0.00.039.453 I print_info: general.name     = 1.4B
0.00.039.453 I print_info: vocab type       = BPE
0.00.039.453 I print_info: n_vocab          = 50304
0.00.039.453 I print_info: n_merges         = 50009
0.00.039.457 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.459 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.459 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.459 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.459 I print_info: LF token         = 187 'Ċ'
0.00.039.460 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.460 I print_info: max token length = 1024
0.00.656.928 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.944 I load_tensors: offloading output layer to GPU
0.00.656.945 I load_tensors: offloaded 25/25 layers to GPU
0.00.656.980 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.656.981 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.658.357 I llama_init_from_model: n_seq_max     = 1
0.00.658.363 I llama_init_from_model: n_ctx         = 2048
0.00.658.363 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.658.363 I llama_init_from_model: n_batch       = 2048
0.00.658.364 I llama_init_from_model: n_ubatch      = 512
0.00.658.364 I llama_init_from_model: flash_attn    = 0
0.00.658.367 I llama_init_from_model: freq_base     = 10000.0
0.00.658.367 I llama_init_from_model: freq_scale    = 1
0.00.658.369 I ggml_metal_init: allocating
0.00.658.450 I ggml_metal_init: found device: Apple M4
0.00.658.463 I ggml_metal_init: picking default device: Apple M4
0.00.660.250 I ggml_metal_init: using embedded metal library
0.00.666.892 I ggml_metal_init: GPU name:   Apple M4
0.00.666.897 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.898 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.899 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.900 I ggml_metal_init: simdgroup reduction   = true
0.00.666.900 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.900 I ggml_metal_init: has residency sets    = true
0.00.666.901 I ggml_metal_init: has bfloat            = true
0.00.666.901 I ggml_metal_init: use bfloat            = true
0.00.666.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.904 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.299 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.742.454 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.742.466 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.742.499 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.746.859 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.746.860 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.746.861 I llama_init_from_model: graph nodes  = 967
0.00.746.861 I llama_init_from_model: graph splits = 2
0.00.746.870 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.747.002 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.747.003 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.378 I main: llama threadpool init, n_threads = 4
0.00.804.421 I 
0.00.804.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.448 I 
0.00.804.599 I sampler seed: 1234
0.00.804.603 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.804.663 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.804.666 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.804.667 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.528.920 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.528.921 I llama_perf_context_print:        load time =     794.56 ms
0.01.528.922 I llama_perf_context_print: prompt eval time =      48.31 ms /     7 tokens (    6.90 ms per token,   144.91 tokens per second)
0.01.528.922 I llama_perf_context_print:        eval time =     673.04 ms /    63 runs   (   10.68 ms per token,    93.60 tokens per second)
0.01.528.923 I llama_perf_context_print:       total time =     725.49 ms /    70 tokens
0.01.529.157 I ggml_metal_free: deallocating

real	0m1.545s
user	0m0.110s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.343 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.616 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.622 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.624 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.626 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.626 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.627 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.629 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.632 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.633 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.510 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.649 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.464 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.468 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.469 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.469 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.470 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.470 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.473 I llama_model_loader: - type  f32:  194 tensors
0.00.025.474 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.474 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.475 I print_info: file format = GGUF V3 (latest)
0.00.025.475 I print_info: file type   = Q4_1
0.00.025.479 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.101 I load: special tokens cache size = 25
0.00.039.016 I load: token to piece cache size = 0.2984 MB
0.00.039.024 I print_info: arch             = gptneox
0.00.039.024 I print_info: vocab_only       = 0
0.00.039.024 I print_info: n_ctx_train      = 2048
0.00.039.024 I print_info: n_embd           = 2048
0.00.039.025 I print_info: n_layer          = 24
0.00.039.029 I print_info: n_head           = 16
0.00.039.030 I print_info: n_head_kv        = 16
0.00.039.030 I print_info: n_rot            = 32
0.00.039.030 I print_info: n_swa            = 0
0.00.039.030 I print_info: n_embd_head_k    = 128
0.00.039.030 I print_info: n_embd_head_v    = 128
0.00.039.031 I print_info: n_gqa            = 1
0.00.039.031 I print_info: n_embd_k_gqa     = 2048
0.00.039.032 I print_info: n_embd_v_gqa     = 2048
0.00.039.032 I print_info: f_norm_eps       = 1.0e-05
0.00.039.033 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.033 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.033 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.033 I print_info: f_logit_scale    = 0.0e+00
0.00.039.036 I print_info: n_ff             = 8192
0.00.039.037 I print_info: n_expert         = 0
0.00.039.037 I print_info: n_expert_used    = 0
0.00.039.037 I print_info: causal attn      = 1
0.00.039.038 I print_info: pooling type     = 0
0.00.039.038 I print_info: rope type        = 2
0.00.039.038 I print_info: rope scaling     = linear
0.00.039.042 I print_info: freq_base_train  = 10000.0
0.00.039.043 I print_info: freq_scale_train = 1
0.00.039.043 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.043 I print_info: rope_finetuned   = unknown
0.00.039.043 I print_info: ssm_d_conv       = 0
0.00.039.043 I print_info: ssm_d_inner      = 0
0.00.039.043 I print_info: ssm_d_state      = 0
0.00.039.044 I print_info: ssm_dt_rank      = 0
0.00.039.044 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.044 I print_info: model type       = 1.4B
0.00.039.044 I print_info: model params     = 1.41 B
0.00.039.044 I print_info: general.name     = 1.4B
0.00.039.045 I print_info: vocab type       = BPE
0.00.039.045 I print_info: n_vocab          = 50304
0.00.039.045 I print_info: n_merges         = 50009
0.00.039.046 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.046 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.046 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.046 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.046 I print_info: LF token         = 187 'Ċ'
0.00.039.047 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.048 I print_info: max token length = 1024
0.00.649.221 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.233 I load_tensors: offloading output layer to GPU
0.00.649.234 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.266 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.649.268 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.650.641 I llama_init_from_model: n_seq_max     = 1
0.00.650.646 I llama_init_from_model: n_ctx         = 128
0.00.650.647 I llama_init_from_model: n_ctx_per_seq = 128
0.00.650.652 I llama_init_from_model: n_batch       = 128
0.00.650.653 I llama_init_from_model: n_ubatch      = 128
0.00.650.653 I llama_init_from_model: flash_attn    = 0
0.00.650.656 I llama_init_from_model: freq_base     = 10000.0
0.00.650.656 I llama_init_from_model: freq_scale    = 1
0.00.650.657 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.650.659 I ggml_metal_init: allocating
0.00.650.741 I ggml_metal_init: found device: Apple M4
0.00.650.756 I ggml_metal_init: picking default device: Apple M4
0.00.652.559 I ggml_metal_init: using embedded metal library
0.00.659.218 I ggml_metal_init: GPU name:   Apple M4
0.00.659.226 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.228 I ggml_metal_init: simdgroup reduction   = true
0.00.659.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.229 I ggml_metal_init: has residency sets    = true
0.00.659.229 I ggml_metal_init: has bfloat            = true
0.00.659.229 I ggml_metal_init: use bfloat            = true
0.00.659.230 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.233 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.896 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.372 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.681.376 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.681.419 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.684.785 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.684.787 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.684.787 I llama_init_from_model: graph nodes  = 967
0.00.684.788 I llama_init_from_model: graph splits = 2
0.00.684.790 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.684.790 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.791 I 
0.00.709.869 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.877 I perplexity: tokenizing the input ..
0.00.716.903 I perplexity: tokenization took 7.023 ms
0.00.716.910 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.946 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.842.280 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.842.306 I llama_perf_context_print:        load time =     700.44 ms
0.00.842.307 I llama_perf_context_print: prompt eval time =     123.10 ms /   128 tokens (    0.96 ms per token,  1039.78 tokens per second)
0.00.842.308 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.842.308 I llama_perf_context_print:       total time =     132.52 ms /   129 tokens
0.00.842.663 I ggml_metal_free: deallocating

real	0m0.856s
user	0m0.080s
sys	0m0.141s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.805 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.229 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.233 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.235 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.235 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.240 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.240 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.241 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.241 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.242 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.242 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.243 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.243 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.243 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.244 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.247 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.248 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.248 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.794 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.774 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.353 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.354 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.355 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.355 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.355 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.356 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.356 I llama_model_loader: - type  f32:  194 tensors
0.00.023.357 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.357 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.357 I print_info: file format = GGUF V3 (latest)
0.00.023.358 I print_info: file type   = Q5_0
0.00.023.359 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.031.071 I load: special tokens cache size = 25
0.00.037.028 I load: token to piece cache size = 0.2984 MB
0.00.037.031 I print_info: arch             = gptneox
0.00.037.031 I print_info: vocab_only       = 0
0.00.037.031 I print_info: n_ctx_train      = 2048
0.00.037.032 I print_info: n_embd           = 2048
0.00.037.032 I print_info: n_layer          = 24
0.00.037.034 I print_info: n_head           = 16
0.00.037.035 I print_info: n_head_kv        = 16
0.00.037.035 I print_info: n_rot            = 32
0.00.037.035 I print_info: n_swa            = 0
0.00.037.036 I print_info: n_embd_head_k    = 128
0.00.037.038 I print_info: n_embd_head_v    = 128
0.00.037.038 I print_info: n_gqa            = 1
0.00.037.039 I print_info: n_embd_k_gqa     = 2048
0.00.037.040 I print_info: n_embd_v_gqa     = 2048
0.00.037.040 I print_info: f_norm_eps       = 1.0e-05
0.00.037.041 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.041 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.041 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.041 I print_info: f_logit_scale    = 0.0e+00
0.00.037.042 I print_info: n_ff             = 8192
0.00.037.042 I print_info: n_expert         = 0
0.00.037.042 I print_info: n_expert_used    = 0
0.00.037.042 I print_info: causal attn      = 1
0.00.037.042 I print_info: pooling type     = 0
0.00.037.043 I print_info: rope type        = 2
0.00.037.043 I print_info: rope scaling     = linear
0.00.037.048 I print_info: freq_base_train  = 10000.0
0.00.037.050 I print_info: freq_scale_train = 1
0.00.037.051 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.051 I print_info: rope_finetuned   = unknown
0.00.037.051 I print_info: ssm_d_conv       = 0
0.00.037.051 I print_info: ssm_d_inner      = 0
0.00.037.051 I print_info: ssm_d_state      = 0
0.00.037.052 I print_info: ssm_dt_rank      = 0
0.00.037.052 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.052 I print_info: model type       = 1.4B
0.00.037.053 I print_info: model params     = 1.41 B
0.00.037.053 I print_info: general.name     = 1.4B
0.00.037.053 I print_info: vocab type       = BPE
0.00.037.053 I print_info: n_vocab          = 50304
0.00.037.054 I print_info: n_merges         = 50009
0.00.037.054 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.054 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.055 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.055 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.057 I print_info: LF token         = 187 'Ċ'
0.00.037.057 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.057 I print_info: max token length = 1024
0.00.755.465 I load_tensors: offloading 24 repeating layers to GPU
0.00.755.479 I load_tensors: offloading output layer to GPU
0.00.755.480 I load_tensors: offloaded 25/25 layers to GPU
0.00.755.517 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.755.518 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.756.970 I llama_init_from_model: n_seq_max     = 1
0.00.756.975 I llama_init_from_model: n_ctx         = 2048
0.00.756.976 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.756.976 I llama_init_from_model: n_batch       = 2048
0.00.756.977 I llama_init_from_model: n_ubatch      = 512
0.00.756.977 I llama_init_from_model: flash_attn    = 0
0.00.756.979 I llama_init_from_model: freq_base     = 10000.0
0.00.756.980 I llama_init_from_model: freq_scale    = 1
0.00.756.986 I ggml_metal_init: allocating
0.00.757.091 I ggml_metal_init: found device: Apple M4
0.00.757.105 I ggml_metal_init: picking default device: Apple M4
0.00.758.917 I ggml_metal_init: using embedded metal library
0.00.765.362 I ggml_metal_init: GPU name:   Apple M4
0.00.765.368 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.765.369 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.765.370 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.765.370 I ggml_metal_init: simdgroup reduction   = true
0.00.765.371 I ggml_metal_init: simdgroup matrix mul. = true
0.00.765.371 I ggml_metal_init: has residency sets    = true
0.00.765.371 I ggml_metal_init: has bfloat            = true
0.00.765.371 I ggml_metal_init: use bfloat            = true
0.00.765.372 I ggml_metal_init: hasUnifiedMemory      = true
0.00.765.376 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.783.094 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.836.357 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.836.365 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.836.404 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.840.537 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.840.539 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.840.539 I llama_init_from_model: graph nodes  = 967
0.00.840.540 I llama_init_from_model: graph splits = 2
0.00.840.545 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.840.670 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.840.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.898.804 I main: llama threadpool init, n_threads = 4
0.00.898.841 I 
0.00.898.866 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.898.866 I 
0.00.899.042 I sampler seed: 1234
0.00.899.047 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.899.058 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.899.058 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.899.058 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.686.902 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53183.52 tokens per second)
0.01.686.903 I llama_perf_context_print:        load time =     889.07 ms
0.01.686.904 I llama_perf_context_print: prompt eval time =      52.84 ms /     7 tokens (    7.55 ms per token,   132.49 tokens per second)
0.01.686.904 I llama_perf_context_print:        eval time =     732.11 ms /    63 runs   (   11.62 ms per token,    86.05 tokens per second)
0.01.686.905 I llama_perf_context_print:       total time =     789.03 ms /    70 tokens
0.01.687.164 I ggml_metal_free: deallocating

real	0m1.705s
user	0m0.108s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.812 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.139 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.145 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.147 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.148 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.148 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.150 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.150 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.150 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.151 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.151 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.153 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.153 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.157 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.873 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.914 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.709 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.709 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.710 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.710 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.711 I llama_model_loader: - type  f32:  194 tensors
0.00.024.711 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.711 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.712 I print_info: file format = GGUF V3 (latest)
0.00.024.713 I print_info: file type   = Q5_0
0.00.024.714 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.699 I load: special tokens cache size = 25
0.00.038.659 I load: token to piece cache size = 0.2984 MB
0.00.038.664 I print_info: arch             = gptneox
0.00.038.664 I print_info: vocab_only       = 0
0.00.038.664 I print_info: n_ctx_train      = 2048
0.00.038.664 I print_info: n_embd           = 2048
0.00.038.664 I print_info: n_layer          = 24
0.00.038.668 I print_info: n_head           = 16
0.00.038.669 I print_info: n_head_kv        = 16
0.00.038.669 I print_info: n_rot            = 32
0.00.038.669 I print_info: n_swa            = 0
0.00.038.669 I print_info: n_embd_head_k    = 128
0.00.038.671 I print_info: n_embd_head_v    = 128
0.00.038.672 I print_info: n_gqa            = 1
0.00.038.673 I print_info: n_embd_k_gqa     = 2048
0.00.038.673 I print_info: n_embd_v_gqa     = 2048
0.00.038.674 I print_info: f_norm_eps       = 1.0e-05
0.00.038.674 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.674 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.675 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.675 I print_info: f_logit_scale    = 0.0e+00
0.00.038.675 I print_info: n_ff             = 8192
0.00.038.676 I print_info: n_expert         = 0
0.00.038.676 I print_info: n_expert_used    = 0
0.00.038.676 I print_info: causal attn      = 1
0.00.038.676 I print_info: pooling type     = 0
0.00.038.676 I print_info: rope type        = 2
0.00.038.677 I print_info: rope scaling     = linear
0.00.038.677 I print_info: freq_base_train  = 10000.0
0.00.038.677 I print_info: freq_scale_train = 1
0.00.038.678 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.678 I print_info: rope_finetuned   = unknown
0.00.038.678 I print_info: ssm_d_conv       = 0
0.00.038.680 I print_info: ssm_d_inner      = 0
0.00.038.680 I print_info: ssm_d_state      = 0
0.00.038.680 I print_info: ssm_dt_rank      = 0
0.00.038.680 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.680 I print_info: model type       = 1.4B
0.00.038.681 I print_info: model params     = 1.41 B
0.00.038.681 I print_info: general.name     = 1.4B
0.00.038.682 I print_info: vocab type       = BPE
0.00.038.682 I print_info: n_vocab          = 50304
0.00.038.682 I print_info: n_merges         = 50009
0.00.038.682 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.682 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.683 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.683 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.683 I print_info: LF token         = 187 'Ċ'
0.00.038.683 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.684 I print_info: max token length = 1024
0.00.684.318 I load_tensors: offloading 24 repeating layers to GPU
0.00.684.331 I load_tensors: offloading output layer to GPU
0.00.684.332 I load_tensors: offloaded 25/25 layers to GPU
0.00.684.364 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.684.365 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.685.932 I llama_init_from_model: n_seq_max     = 1
0.00.685.937 I llama_init_from_model: n_ctx         = 128
0.00.685.937 I llama_init_from_model: n_ctx_per_seq = 128
0.00.685.938 I llama_init_from_model: n_batch       = 128
0.00.685.938 I llama_init_from_model: n_ubatch      = 128
0.00.685.939 I llama_init_from_model: flash_attn    = 0
0.00.685.941 I llama_init_from_model: freq_base     = 10000.0
0.00.685.942 I llama_init_from_model: freq_scale    = 1
0.00.685.942 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.685.950 I ggml_metal_init: allocating
0.00.686.027 I ggml_metal_init: found device: Apple M4
0.00.686.042 I ggml_metal_init: picking default device: Apple M4
0.00.687.734 I ggml_metal_init: using embedded metal library
0.00.694.573 I ggml_metal_init: GPU name:   Apple M4
0.00.694.579 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.694.580 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.694.580 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.694.581 I ggml_metal_init: simdgroup reduction   = true
0.00.694.581 I ggml_metal_init: simdgroup matrix mul. = true
0.00.694.581 I ggml_metal_init: has residency sets    = true
0.00.694.582 I ggml_metal_init: has bfloat            = true
0.00.694.582 I ggml_metal_init: use bfloat            = true
0.00.694.583 I ggml_metal_init: hasUnifiedMemory      = true
0.00.694.585 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.713.481 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.717.052 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.717.059 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.717.110 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.370 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.720.371 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.720.372 I llama_init_from_model: graph nodes  = 967
0.00.720.372 I llama_init_from_model: graph splits = 2
0.00.720.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.720.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.475 I 
0.00.750.560 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.567 I perplexity: tokenizing the input ..
0.00.758.089 I perplexity: tokenization took 7.519 ms
0.00.758.098 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.907.717 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.909.044 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.909.067 I llama_perf_context_print:        load time =     741.65 ms
0.00.909.069 I llama_perf_context_print: prompt eval time =     148.75 ms /   128 tokens (    1.16 ms per token,   860.48 tokens per second)
0.00.909.069 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.909.070 I llama_perf_context_print:       total time =     158.60 ms /   129 tokens
0.00.909.388 I ggml_metal_free: deallocating

real	0m0.923s
user	0m0.081s
sys	0m0.130s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.745 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.244 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.249 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.251 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.252 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.252 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.254 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.254 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.254 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.255 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.256 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.256 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.257 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.258 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.260 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.261 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.961 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.995 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.729 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.729 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.730 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.730 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.731 I llama_model_loader: - type  f32:  194 tensors
0.00.025.731 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.731 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.732 I print_info: file format = GGUF V3 (latest)
0.00.025.733 I print_info: file type   = Q5_1
0.00.025.738 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.523 I load: special tokens cache size = 25
0.00.039.424 I load: token to piece cache size = 0.2984 MB
0.00.039.426 I print_info: arch             = gptneox
0.00.039.426 I print_info: vocab_only       = 0
0.00.039.426 I print_info: n_ctx_train      = 2048
0.00.039.427 I print_info: n_embd           = 2048
0.00.039.427 I print_info: n_layer          = 24
0.00.039.430 I print_info: n_head           = 16
0.00.039.430 I print_info: n_head_kv        = 16
0.00.039.430 I print_info: n_rot            = 32
0.00.039.431 I print_info: n_swa            = 0
0.00.039.431 I print_info: n_embd_head_k    = 128
0.00.039.431 I print_info: n_embd_head_v    = 128
0.00.039.432 I print_info: n_gqa            = 1
0.00.039.433 I print_info: n_embd_k_gqa     = 2048
0.00.039.433 I print_info: n_embd_v_gqa     = 2048
0.00.039.434 I print_info: f_norm_eps       = 1.0e-05
0.00.039.436 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.436 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.436 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.437 I print_info: f_logit_scale    = 0.0e+00
0.00.039.437 I print_info: n_ff             = 8192
0.00.039.438 I print_info: n_expert         = 0
0.00.039.438 I print_info: n_expert_used    = 0
0.00.039.438 I print_info: causal attn      = 1
0.00.039.438 I print_info: pooling type     = 0
0.00.039.438 I print_info: rope type        = 2
0.00.039.439 I print_info: rope scaling     = linear
0.00.039.440 I print_info: freq_base_train  = 10000.0
0.00.039.440 I print_info: freq_scale_train = 1
0.00.039.441 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.441 I print_info: rope_finetuned   = unknown
0.00.039.441 I print_info: ssm_d_conv       = 0
0.00.039.441 I print_info: ssm_d_inner      = 0
0.00.039.441 I print_info: ssm_d_state      = 0
0.00.039.441 I print_info: ssm_dt_rank      = 0
0.00.039.441 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.442 I print_info: model type       = 1.4B
0.00.039.442 I print_info: model params     = 1.41 B
0.00.039.442 I print_info: general.name     = 1.4B
0.00.039.443 I print_info: vocab type       = BPE
0.00.039.443 I print_info: n_vocab          = 50304
0.00.039.443 I print_info: n_merges         = 50009
0.00.039.443 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.443 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.443 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: LF token         = 187 'Ċ'
0.00.039.444 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: max token length = 1024
0.00.643.501 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.513 I load_tensors: offloading output layer to GPU
0.00.643.513 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.547 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.643.548 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.645.116 I llama_init_from_model: n_seq_max     = 1
0.00.645.122 I llama_init_from_model: n_ctx         = 2048
0.00.645.123 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.645.123 I llama_init_from_model: n_batch       = 2048
0.00.645.124 I llama_init_from_model: n_ubatch      = 512
0.00.645.124 I llama_init_from_model: flash_attn    = 0
0.00.645.126 I llama_init_from_model: freq_base     = 10000.0
0.00.645.127 I llama_init_from_model: freq_scale    = 1
0.00.645.129 I ggml_metal_init: allocating
0.00.645.240 I ggml_metal_init: found device: Apple M4
0.00.645.254 I ggml_metal_init: picking default device: Apple M4
0.00.647.138 I ggml_metal_init: using embedded metal library
0.00.653.559 I ggml_metal_init: GPU name:   Apple M4
0.00.653.563 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.564 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.565 I ggml_metal_init: simdgroup reduction   = true
0.00.653.565 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.566 I ggml_metal_init: has residency sets    = true
0.00.653.566 I ggml_metal_init: has bfloat            = true
0.00.653.566 I ggml_metal_init: use bfloat            = true
0.00.653.567 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.649 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.729.618 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.729.624 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.729.658 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.293 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.734.295 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.734.295 I llama_init_from_model: graph nodes  = 967
0.00.734.296 I llama_init_from_model: graph splits = 2
0.00.734.301 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.734.435 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.734.436 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.052 I main: llama threadpool init, n_threads = 4
0.00.792.098 I 
0.00.792.131 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.131 I 
0.00.792.311 I sampler seed: 1234
0.00.792.315 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.792.335 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.792.335 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.792.335 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.618.815 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50105.86 tokens per second)
0.01.618.816 I llama_perf_context_print:        load time =     780.29 ms
0.01.618.817 I llama_perf_context_print: prompt eval time =      41.94 ms /     7 tokens (    5.99 ms per token,   166.91 tokens per second)
0.01.618.817 I llama_perf_context_print:        eval time =     781.51 ms /    63 runs   (   12.40 ms per token,    80.61 tokens per second)
0.01.618.818 I llama_perf_context_print:       total time =     827.78 ms /    70 tokens
0.01.619.040 I ggml_metal_free: deallocating

real	0m1.637s
user	0m0.108s
sys	0m0.232s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.695 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.730 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.735 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.742 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.743 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.743 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.743 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.746 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.746 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.746 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.747 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.747 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.748 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.749 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.752 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.753 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.753 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.420 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.419 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.207 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.207 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.208 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.208 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.209 I llama_model_loader: - type  f32:  194 tensors
0.00.026.209 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.210 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.210 I print_info: file format = GGUF V3 (latest)
0.00.026.211 I print_info: file type   = Q5_1
0.00.026.212 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.836 I load: special tokens cache size = 25
0.00.039.867 I load: token to piece cache size = 0.2984 MB
0.00.039.869 I print_info: arch             = gptneox
0.00.039.869 I print_info: vocab_only       = 0
0.00.039.870 I print_info: n_ctx_train      = 2048
0.00.039.870 I print_info: n_embd           = 2048
0.00.039.870 I print_info: n_layer          = 24
0.00.039.873 I print_info: n_head           = 16
0.00.039.873 I print_info: n_head_kv        = 16
0.00.039.873 I print_info: n_rot            = 32
0.00.039.874 I print_info: n_swa            = 0
0.00.039.874 I print_info: n_embd_head_k    = 128
0.00.039.874 I print_info: n_embd_head_v    = 128
0.00.039.875 I print_info: n_gqa            = 1
0.00.039.878 I print_info: n_embd_k_gqa     = 2048
0.00.039.879 I print_info: n_embd_v_gqa     = 2048
0.00.039.879 I print_info: f_norm_eps       = 1.0e-05
0.00.039.880 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.880 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.880 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.880 I print_info: f_logit_scale    = 0.0e+00
0.00.039.881 I print_info: n_ff             = 8192
0.00.039.881 I print_info: n_expert         = 0
0.00.039.881 I print_info: n_expert_used    = 0
0.00.039.881 I print_info: causal attn      = 1
0.00.039.881 I print_info: pooling type     = 0
0.00.039.882 I print_info: rope type        = 2
0.00.039.882 I print_info: rope scaling     = linear
0.00.039.882 I print_info: freq_base_train  = 10000.0
0.00.039.883 I print_info: freq_scale_train = 1
0.00.039.883 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.883 I print_info: rope_finetuned   = unknown
0.00.039.883 I print_info: ssm_d_conv       = 0
0.00.039.883 I print_info: ssm_d_inner      = 0
0.00.039.883 I print_info: ssm_d_state      = 0
0.00.039.884 I print_info: ssm_dt_rank      = 0
0.00.039.886 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.886 I print_info: model type       = 1.4B
0.00.039.886 I print_info: model params     = 1.41 B
0.00.039.886 I print_info: general.name     = 1.4B
0.00.039.887 I print_info: vocab type       = BPE
0.00.039.887 I print_info: n_vocab          = 50304
0.00.039.887 I print_info: n_merges         = 50009
0.00.039.889 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.889 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.890 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.890 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.890 I print_info: LF token         = 187 'Ċ'
0.00.039.890 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.891 I print_info: max token length = 1024
0.00.634.397 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.412 I load_tensors: offloading output layer to GPU
0.00.634.412 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.451 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.634.452 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.635.993 I llama_init_from_model: n_seq_max     = 1
0.00.635.996 I llama_init_from_model: n_ctx         = 128
0.00.635.997 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.997 I llama_init_from_model: n_batch       = 128
0.00.635.997 I llama_init_from_model: n_ubatch      = 128
0.00.635.998 I llama_init_from_model: flash_attn    = 0
0.00.635.999 I llama_init_from_model: freq_base     = 10000.0
0.00.635.999 I llama_init_from_model: freq_scale    = 1
0.00.636.000 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.636.001 I ggml_metal_init: allocating
0.00.636.058 I ggml_metal_init: found device: Apple M4
0.00.636.068 I ggml_metal_init: picking default device: Apple M4
0.00.637.559 I ggml_metal_init: using embedded metal library
0.00.644.374 I ggml_metal_init: GPU name:   Apple M4
0.00.644.378 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.379 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.380 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.380 I ggml_metal_init: simdgroup reduction   = true
0.00.644.380 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.381 I ggml_metal_init: has residency sets    = true
0.00.644.381 I ggml_metal_init: has bfloat            = true
0.00.644.381 I ggml_metal_init: use bfloat            = true
0.00.644.382 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.383 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.899 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.665.457 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.665.460 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.665.499 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.668.673 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.668.675 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.668.676 I llama_init_from_model: graph nodes  = 967
0.00.668.676 I llama_init_from_model: graph splits = 2
0.00.668.679 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.668.679 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.965 I 
0.00.696.035 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.048 I perplexity: tokenizing the input ..
0.00.703.759 I perplexity: tokenization took 7.708 ms
0.00.703.766 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.839.458 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.840.882 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.840.904 I llama_perf_context_print:        load time =     685.26 ms
0.00.840.907 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.80 tokens per second)
0.00.840.908 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.840.908 I llama_perf_context_print:       total time =     144.95 ms /   129 tokens
0.00.841.259 I ggml_metal_free: deallocating

real	0m0.856s
user	0m0.078s
sys	0m0.148s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.682 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.168 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.175 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.176 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.176 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.177 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.177 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.178 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.178 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.178 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.181 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.181 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.183 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.823 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.787 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.451 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.452 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.452 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.453 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.453 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.453 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.454 I llama_model_loader: - type  f32:  194 tensors
0.00.023.454 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.454 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.454 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.455 I print_info: file format = GGUF V3 (latest)
0.00.023.455 I print_info: file type   = Q2_K - Medium
0.00.023.456 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.160 I load: special tokens cache size = 25
0.00.037.076 I load: token to piece cache size = 0.2984 MB
0.00.037.079 I print_info: arch             = gptneox
0.00.037.079 I print_info: vocab_only       = 0
0.00.037.079 I print_info: n_ctx_train      = 2048
0.00.037.080 I print_info: n_embd           = 2048
0.00.037.080 I print_info: n_layer          = 24
0.00.037.082 I print_info: n_head           = 16
0.00.037.083 I print_info: n_head_kv        = 16
0.00.037.083 I print_info: n_rot            = 32
0.00.037.083 I print_info: n_swa            = 0
0.00.037.083 I print_info: n_embd_head_k    = 128
0.00.037.084 I print_info: n_embd_head_v    = 128
0.00.037.084 I print_info: n_gqa            = 1
0.00.037.085 I print_info: n_embd_k_gqa     = 2048
0.00.037.086 I print_info: n_embd_v_gqa     = 2048
0.00.037.086 I print_info: f_norm_eps       = 1.0e-05
0.00.037.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.087 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.087 I print_info: f_logit_scale    = 0.0e+00
0.00.037.088 I print_info: n_ff             = 8192
0.00.037.088 I print_info: n_expert         = 0
0.00.037.088 I print_info: n_expert_used    = 0
0.00.037.088 I print_info: causal attn      = 1
0.00.037.089 I print_info: pooling type     = 0
0.00.037.089 I print_info: rope type        = 2
0.00.037.089 I print_info: rope scaling     = linear
0.00.037.089 I print_info: freq_base_train  = 10000.0
0.00.037.090 I print_info: freq_scale_train = 1
0.00.037.090 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.090 I print_info: rope_finetuned   = unknown
0.00.037.090 I print_info: ssm_d_conv       = 0
0.00.037.092 I print_info: ssm_d_inner      = 0
0.00.037.092 I print_info: ssm_d_state      = 0
0.00.037.092 I print_info: ssm_dt_rank      = 0
0.00.037.093 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.093 I print_info: model type       = 1.4B
0.00.037.093 I print_info: model params     = 1.41 B
0.00.037.093 I print_info: general.name     = 1.4B
0.00.037.094 I print_info: vocab type       = BPE
0.00.037.095 I print_info: n_vocab          = 50304
0.00.037.095 I print_info: n_merges         = 50009
0.00.037.096 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.096 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.096 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.096 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.096 I print_info: LF token         = 187 'Ċ'
0.00.037.097 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.097 I print_info: max token length = 1024
0.00.351.910 I load_tensors: offloading 24 repeating layers to GPU
0.00.351.923 I load_tensors: offloading output layer to GPU
0.00.351.924 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.954 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.955 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.353.455 I llama_init_from_model: n_seq_max     = 1
0.00.353.467 I llama_init_from_model: n_ctx         = 2048
0.00.353.467 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.353.468 I llama_init_from_model: n_batch       = 2048
0.00.353.468 I llama_init_from_model: n_ubatch      = 512
0.00.353.468 I llama_init_from_model: flash_attn    = 0
0.00.353.470 I llama_init_from_model: freq_base     = 10000.0
0.00.353.473 I llama_init_from_model: freq_scale    = 1
0.00.353.476 I ggml_metal_init: allocating
0.00.353.560 I ggml_metal_init: found device: Apple M4
0.00.353.574 I ggml_metal_init: picking default device: Apple M4
0.00.355.374 I ggml_metal_init: using embedded metal library
0.00.361.061 I ggml_metal_init: GPU name:   Apple M4
0.00.361.074 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.361.075 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.361.076 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.361.076 I ggml_metal_init: simdgroup reduction   = true
0.00.361.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.361.077 I ggml_metal_init: has residency sets    = true
0.00.361.077 I ggml_metal_init: has bfloat            = true
0.00.361.077 I ggml_metal_init: use bfloat            = true
0.00.361.082 I ggml_metal_init: hasUnifiedMemory      = true
0.00.361.090 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.382.517 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.445.812 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.445.820 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.445.858 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.450.656 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.450.658 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.450.658 I llama_init_from_model: graph nodes  = 967
0.00.450.659 I llama_init_from_model: graph splits = 2
0.00.450.665 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.450.790 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.450.790 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.902 I main: llama threadpool init, n_threads = 4
0.00.508.946 I 
0.00.508.970 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.970 I 
0.00.509.143 I sampler seed: 1234
0.00.509.148 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.509.192 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.509.204 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.509.204 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.179.453 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.179.454 I llama_perf_context_print:        load time =     499.30 ms
0.01.179.455 I llama_perf_context_print: prompt eval time =      35.51 ms /     7 tokens (    5.07 ms per token,   197.16 tokens per second)
0.01.179.455 I llama_perf_context_print:        eval time =     631.80 ms /    63 runs   (   10.03 ms per token,    99.72 tokens per second)
0.01.179.456 I llama_perf_context_print:       total time =     671.48 ms /    70 tokens
0.01.179.689 I ggml_metal_free: deallocating

real	0m1.195s
user	0m0.110s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.695 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.597 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.602 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.604 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.605 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.605 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.605 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.606 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.607 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.607 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.607 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.608 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.610 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.611 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.612 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.612 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.395 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.408 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.409 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.409 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.410 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.410 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.410 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.411 I llama_model_loader: - type  f32:  194 tensors
0.00.024.411 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.411 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.412 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.412 I print_info: file format = GGUF V3 (latest)
0.00.024.413 I print_info: file type   = Q2_K - Medium
0.00.024.414 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.305 I load: special tokens cache size = 25
0.00.038.394 I load: token to piece cache size = 0.2984 MB
0.00.038.397 I print_info: arch             = gptneox
0.00.038.397 I print_info: vocab_only       = 0
0.00.038.398 I print_info: n_ctx_train      = 2048
0.00.038.398 I print_info: n_embd           = 2048
0.00.038.398 I print_info: n_layer          = 24
0.00.038.401 I print_info: n_head           = 16
0.00.038.402 I print_info: n_head_kv        = 16
0.00.038.403 I print_info: n_rot            = 32
0.00.038.403 I print_info: n_swa            = 0
0.00.038.404 I print_info: n_embd_head_k    = 128
0.00.038.404 I print_info: n_embd_head_v    = 128
0.00.038.404 I print_info: n_gqa            = 1
0.00.038.405 I print_info: n_embd_k_gqa     = 2048
0.00.038.406 I print_info: n_embd_v_gqa     = 2048
0.00.038.407 I print_info: f_norm_eps       = 1.0e-05
0.00.038.408 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.409 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.409 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.409 I print_info: f_logit_scale    = 0.0e+00
0.00.038.409 I print_info: n_ff             = 8192
0.00.038.411 I print_info: n_expert         = 0
0.00.038.411 I print_info: n_expert_used    = 0
0.00.038.411 I print_info: causal attn      = 1
0.00.038.411 I print_info: pooling type     = 0
0.00.038.411 I print_info: rope type        = 2
0.00.038.411 I print_info: rope scaling     = linear
0.00.038.412 I print_info: freq_base_train  = 10000.0
0.00.038.413 I print_info: freq_scale_train = 1
0.00.038.413 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.413 I print_info: rope_finetuned   = unknown
0.00.038.413 I print_info: ssm_d_conv       = 0
0.00.038.414 I print_info: ssm_d_inner      = 0
0.00.038.414 I print_info: ssm_d_state      = 0
0.00.038.414 I print_info: ssm_dt_rank      = 0
0.00.038.414 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.414 I print_info: model type       = 1.4B
0.00.038.415 I print_info: model params     = 1.41 B
0.00.038.415 I print_info: general.name     = 1.4B
0.00.038.415 I print_info: vocab type       = BPE
0.00.038.415 I print_info: n_vocab          = 50304
0.00.038.416 I print_info: n_merges         = 50009
0.00.038.416 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.416 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.419 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.420 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.420 I print_info: LF token         = 187 'Ċ'
0.00.038.420 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.420 I print_info: max token length = 1024
0.00.363.479 I load_tensors: offloading 24 repeating layers to GPU
0.00.363.483 I load_tensors: offloading output layer to GPU
0.00.363.484 I load_tensors: offloaded 25/25 layers to GPU
0.00.363.508 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.363.510 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.364.441 I llama_init_from_model: n_seq_max     = 1
0.00.364.444 I llama_init_from_model: n_ctx         = 128
0.00.364.444 I llama_init_from_model: n_ctx_per_seq = 128
0.00.364.445 I llama_init_from_model: n_batch       = 128
0.00.364.445 I llama_init_from_model: n_ubatch      = 128
0.00.364.445 I llama_init_from_model: flash_attn    = 0
0.00.364.446 I llama_init_from_model: freq_base     = 10000.0
0.00.364.447 I llama_init_from_model: freq_scale    = 1
0.00.364.448 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.364.451 I ggml_metal_init: allocating
0.00.364.504 I ggml_metal_init: found device: Apple M4
0.00.364.516 I ggml_metal_init: picking default device: Apple M4
0.00.365.544 I ggml_metal_init: using embedded metal library
0.00.370.448 I ggml_metal_init: GPU name:   Apple M4
0.00.370.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.370.455 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.370.456 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.370.456 I ggml_metal_init: simdgroup reduction   = true
0.00.370.456 I ggml_metal_init: simdgroup matrix mul. = true
0.00.370.457 I ggml_metal_init: has residency sets    = true
0.00.370.457 I ggml_metal_init: has bfloat            = true
0.00.370.457 I ggml_metal_init: use bfloat            = true
0.00.370.459 I ggml_metal_init: hasUnifiedMemory      = true
0.00.370.461 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.387.309 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.388.933 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.388.935 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.388.963 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.390.593 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.390.594 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.390.595 I llama_init_from_model: graph nodes  = 967
0.00.390.595 I llama_init_from_model: graph splits = 2
0.00.390.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.390.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.417.395 I 
0.00.417.430 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.417.433 I perplexity: tokenizing the input ..
0.00.421.304 I perplexity: tokenization took 3.869 ms
0.00.421.307 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.566.489 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.568.080 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.568.104 I llama_perf_context_print:        load time =     408.70 ms
0.00.568.105 I llama_perf_context_print: prompt eval time =     144.95 ms /   128 tokens (    1.13 ms per token,   883.04 tokens per second)
0.00.568.106 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.568.106 I llama_perf_context_print:       total time =     150.71 ms /   129 tokens
0.00.568.445 I ggml_metal_free: deallocating

real	0m0.584s
user	0m0.072s
sys	0m0.068s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.057 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.354 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.359 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.360 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.360 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.361 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.361 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.361 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.362 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.363 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.363 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.363 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.364 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.364 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.364 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.366 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.366 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.366 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.098 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.080 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.862 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.863 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.864 I llama_model_loader: - type  f32:  194 tensors
0.00.023.864 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.864 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.864 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.865 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.865 I print_info: file format = GGUF V3 (latest)
0.00.023.865 I print_info: file type   = Q3_K - Medium
0.00.023.866 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.589 I load: special tokens cache size = 25
0.00.037.575 I load: token to piece cache size = 0.2984 MB
0.00.037.578 I print_info: arch             = gptneox
0.00.037.578 I print_info: vocab_only       = 0
0.00.037.579 I print_info: n_ctx_train      = 2048
0.00.037.579 I print_info: n_embd           = 2048
0.00.037.579 I print_info: n_layer          = 24
0.00.037.582 I print_info: n_head           = 16
0.00.037.582 I print_info: n_head_kv        = 16
0.00.037.582 I print_info: n_rot            = 32
0.00.037.583 I print_info: n_swa            = 0
0.00.037.589 I print_info: n_embd_head_k    = 128
0.00.037.591 I print_info: n_embd_head_v    = 128
0.00.037.593 I print_info: n_gqa            = 1
0.00.037.594 I print_info: n_embd_k_gqa     = 2048
0.00.037.595 I print_info: n_embd_v_gqa     = 2048
0.00.037.596 I print_info: f_norm_eps       = 1.0e-05
0.00.037.596 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.598 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.598 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.598 I print_info: f_logit_scale    = 0.0e+00
0.00.037.599 I print_info: n_ff             = 8192
0.00.037.599 I print_info: n_expert         = 0
0.00.037.600 I print_info: n_expert_used    = 0
0.00.037.600 I print_info: causal attn      = 1
0.00.037.601 I print_info: pooling type     = 0
0.00.037.601 I print_info: rope type        = 2
0.00.037.601 I print_info: rope scaling     = linear
0.00.037.601 I print_info: freq_base_train  = 10000.0
0.00.037.602 I print_info: freq_scale_train = 1
0.00.037.602 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.602 I print_info: rope_finetuned   = unknown
0.00.037.602 I print_info: ssm_d_conv       = 0
0.00.037.602 I print_info: ssm_d_inner      = 0
0.00.037.602 I print_info: ssm_d_state      = 0
0.00.037.603 I print_info: ssm_dt_rank      = 0
0.00.037.603 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.603 I print_info: model type       = 1.4B
0.00.037.603 I print_info: model params     = 1.41 B
0.00.037.603 I print_info: general.name     = 1.4B
0.00.037.605 I print_info: vocab type       = BPE
0.00.037.605 I print_info: n_vocab          = 50304
0.00.037.605 I print_info: n_merges         = 50009
0.00.037.606 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.606 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.606 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.606 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.606 I print_info: LF token         = 187 'Ċ'
0.00.037.607 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.607 I print_info: max token length = 1024
0.00.435.571 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.588 I load_tensors: offloading output layer to GPU
0.00.435.589 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.623 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.624 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.437.139 I llama_init_from_model: n_seq_max     = 1
0.00.437.149 I llama_init_from_model: n_ctx         = 2048
0.00.437.149 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.437.150 I llama_init_from_model: n_batch       = 2048
0.00.437.150 I llama_init_from_model: n_ubatch      = 512
0.00.437.150 I llama_init_from_model: flash_attn    = 0
0.00.437.152 I llama_init_from_model: freq_base     = 10000.0
0.00.437.153 I llama_init_from_model: freq_scale    = 1
0.00.437.155 I ggml_metal_init: allocating
0.00.437.264 I ggml_metal_init: found device: Apple M4
0.00.437.278 I ggml_metal_init: picking default device: Apple M4
0.00.439.123 I ggml_metal_init: using embedded metal library
0.00.444.672 I ggml_metal_init: GPU name:   Apple M4
0.00.444.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.444.678 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.444.679 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.444.680 I ggml_metal_init: simdgroup reduction   = true
0.00.444.680 I ggml_metal_init: simdgroup matrix mul. = true
0.00.444.680 I ggml_metal_init: has residency sets    = true
0.00.444.681 I ggml_metal_init: has bfloat            = true
0.00.444.681 I ggml_metal_init: use bfloat            = true
0.00.444.682 I ggml_metal_init: hasUnifiedMemory      = true
0.00.444.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.241 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.524.601 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.524.611 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.524.649 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.529.665 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.529.666 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.529.667 I llama_init_from_model: graph nodes  = 967
0.00.529.667 I llama_init_from_model: graph splits = 2
0.00.529.672 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.529.786 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.529.786 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.580.929 I main: llama threadpool init, n_threads = 4
0.00.580.967 I 
0.00.580.991 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.580.991 I 
0.00.581.122 I sampler seed: 1234
0.00.581.126 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.581.147 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.581.147 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.581.148 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.334.194 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48464.16 tokens per second)
0.01.334.194 I llama_perf_context_print:        load time =     570.86 ms
0.01.334.195 I llama_perf_context_print: prompt eval time =      50.09 ms /     7 tokens (    7.16 ms per token,   139.74 tokens per second)
0.01.334.196 I llama_perf_context_print:        eval time =     700.46 ms /    63 runs   (   11.12 ms per token,    89.94 tokens per second)
0.01.334.196 I llama_perf_context_print:       total time =     754.28 ms /    70 tokens
0.01.334.437 I ggml_metal_free: deallocating

real	0m1.353s
user	0m0.109s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.769 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.801 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.808 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.809 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.810 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.810 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.811 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.811 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.812 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.812 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.813 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.814 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.814 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.815 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.817 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.818 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.818 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.461 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.508 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.445 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.447 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.448 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.448 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.449 I llama_model_loader: - type  f32:  194 tensors
0.00.024.449 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.449 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.450 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.450 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.450 I print_info: file format = GGUF V3 (latest)
0.00.024.451 I print_info: file type   = Q3_K - Medium
0.00.024.452 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.277 I load: special tokens cache size = 25
0.00.038.284 I load: token to piece cache size = 0.2984 MB
0.00.038.288 I print_info: arch             = gptneox
0.00.038.288 I print_info: vocab_only       = 0
0.00.038.289 I print_info: n_ctx_train      = 2048
0.00.038.289 I print_info: n_embd           = 2048
0.00.038.289 I print_info: n_layer          = 24
0.00.038.293 I print_info: n_head           = 16
0.00.038.293 I print_info: n_head_kv        = 16
0.00.038.294 I print_info: n_rot            = 32
0.00.038.294 I print_info: n_swa            = 0
0.00.038.294 I print_info: n_embd_head_k    = 128
0.00.038.294 I print_info: n_embd_head_v    = 128
0.00.038.295 I print_info: n_gqa            = 1
0.00.038.296 I print_info: n_embd_k_gqa     = 2048
0.00.038.296 I print_info: n_embd_v_gqa     = 2048
0.00.038.297 I print_info: f_norm_eps       = 1.0e-05
0.00.038.297 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.298 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.299 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.299 I print_info: f_logit_scale    = 0.0e+00
0.00.038.300 I print_info: n_ff             = 8192
0.00.038.300 I print_info: n_expert         = 0
0.00.038.300 I print_info: n_expert_used    = 0
0.00.038.300 I print_info: causal attn      = 1
0.00.038.302 I print_info: pooling type     = 0
0.00.038.302 I print_info: rope type        = 2
0.00.038.303 I print_info: rope scaling     = linear
0.00.038.303 I print_info: freq_base_train  = 10000.0
0.00.038.303 I print_info: freq_scale_train = 1
0.00.038.303 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.304 I print_info: rope_finetuned   = unknown
0.00.038.304 I print_info: ssm_d_conv       = 0
0.00.038.304 I print_info: ssm_d_inner      = 0
0.00.038.304 I print_info: ssm_d_state      = 0
0.00.038.304 I print_info: ssm_dt_rank      = 0
0.00.038.304 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.304 I print_info: model type       = 1.4B
0.00.038.305 I print_info: model params     = 1.41 B
0.00.038.305 I print_info: general.name     = 1.4B
0.00.038.305 I print_info: vocab type       = BPE
0.00.038.306 I print_info: n_vocab          = 50304
0.00.038.306 I print_info: n_merges         = 50009
0.00.038.306 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.306 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.306 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.306 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.306 I print_info: LF token         = 187 'Ċ'
0.00.038.307 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.307 I print_info: max token length = 1024
0.00.475.783 I load_tensors: offloading 24 repeating layers to GPU
0.00.475.790 I load_tensors: offloading output layer to GPU
0.00.475.791 I load_tensors: offloaded 25/25 layers to GPU
0.00.475.809 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.475.810 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.476.482 I llama_init_from_model: n_seq_max     = 1
0.00.476.487 I llama_init_from_model: n_ctx         = 128
0.00.476.487 I llama_init_from_model: n_ctx_per_seq = 128
0.00.476.488 I llama_init_from_model: n_batch       = 128
0.00.476.488 I llama_init_from_model: n_ubatch      = 128
0.00.476.488 I llama_init_from_model: flash_attn    = 0
0.00.476.489 I llama_init_from_model: freq_base     = 10000.0
0.00.476.490 I llama_init_from_model: freq_scale    = 1
0.00.476.493 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.476.494 I ggml_metal_init: allocating
0.00.476.534 I ggml_metal_init: found device: Apple M4
0.00.476.546 I ggml_metal_init: picking default device: Apple M4
0.00.477.551 I ggml_metal_init: using embedded metal library
0.00.483.227 I ggml_metal_init: GPU name:   Apple M4
0.00.483.233 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.483.234 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.483.235 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.483.235 I ggml_metal_init: simdgroup reduction   = true
0.00.483.236 I ggml_metal_init: simdgroup matrix mul. = true
0.00.483.236 I ggml_metal_init: has residency sets    = true
0.00.483.236 I ggml_metal_init: has bfloat            = true
0.00.483.236 I ggml_metal_init: use bfloat            = true
0.00.483.238 I ggml_metal_init: hasUnifiedMemory      = true
0.00.483.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.504.804 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.507.909 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.507.919 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.507.983 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.510.097 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.510.100 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.510.101 I llama_init_from_model: graph nodes  = 967
0.00.510.101 I llama_init_from_model: graph splits = 2
0.00.510.102 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.510.103 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.534.652 I 
0.00.534.685 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.534.689 I perplexity: tokenizing the input ..
0.00.538.733 I perplexity: tokenization took 4.043 ms
0.00.538.736 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.669.942 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.671.424 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.671.446 I llama_perf_context_print:        load time =     525.88 ms
0.00.671.447 I llama_perf_context_print: prompt eval time =     130.95 ms /   128 tokens (    1.02 ms per token,   977.47 tokens per second)
0.00.671.448 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.671.448 I llama_perf_context_print:       total time =     136.80 ms /   129 tokens
0.00.671.776 I ggml_metal_free: deallocating

real	0m0.686s
user	0m0.072s
sys	0m0.094s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.731 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.119 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.123 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.128 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.850 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.837 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.611 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.612 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.612 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.613 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.613 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.613 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.614 I llama_model_loader: - type  f32:  194 tensors
0.00.025.614 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.614 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.615 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.615 I print_info: file format = GGUF V3 (latest)
0.00.025.616 I print_info: file type   = Q4_K - Medium
0.00.025.617 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.477 I load: special tokens cache size = 25
0.00.039.374 I load: token to piece cache size = 0.2984 MB
0.00.039.377 I print_info: arch             = gptneox
0.00.039.377 I print_info: vocab_only       = 0
0.00.039.377 I print_info: n_ctx_train      = 2048
0.00.039.377 I print_info: n_embd           = 2048
0.00.039.378 I print_info: n_layer          = 24
0.00.039.380 I print_info: n_head           = 16
0.00.039.381 I print_info: n_head_kv        = 16
0.00.039.381 I print_info: n_rot            = 32
0.00.039.381 I print_info: n_swa            = 0
0.00.039.381 I print_info: n_embd_head_k    = 128
0.00.039.383 I print_info: n_embd_head_v    = 128
0.00.039.384 I print_info: n_gqa            = 1
0.00.039.385 I print_info: n_embd_k_gqa     = 2048
0.00.039.385 I print_info: n_embd_v_gqa     = 2048
0.00.039.390 I print_info: f_norm_eps       = 1.0e-05
0.00.039.390 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.391 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.392 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.393 I print_info: f_logit_scale    = 0.0e+00
0.00.039.393 I print_info: n_ff             = 8192
0.00.039.394 I print_info: n_expert         = 0
0.00.039.394 I print_info: n_expert_used    = 0
0.00.039.394 I print_info: causal attn      = 1
0.00.039.394 I print_info: pooling type     = 0
0.00.039.394 I print_info: rope type        = 2
0.00.039.394 I print_info: rope scaling     = linear
0.00.039.395 I print_info: freq_base_train  = 10000.0
0.00.039.395 I print_info: freq_scale_train = 1
0.00.039.395 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.395 I print_info: rope_finetuned   = unknown
0.00.039.396 I print_info: ssm_d_conv       = 0
0.00.039.396 I print_info: ssm_d_inner      = 0
0.00.039.396 I print_info: ssm_d_state      = 0
0.00.039.396 I print_info: ssm_dt_rank      = 0
0.00.039.396 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.396 I print_info: model type       = 1.4B
0.00.039.397 I print_info: model params     = 1.41 B
0.00.039.397 I print_info: general.name     = 1.4B
0.00.039.397 I print_info: vocab type       = BPE
0.00.039.398 I print_info: n_vocab          = 50304
0.00.039.398 I print_info: n_merges         = 50009
0.00.039.398 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.398 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.398 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.399 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.399 I print_info: LF token         = 187 'Ċ'
0.00.039.399 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.399 I print_info: max token length = 1024
0.00.519.958 I load_tensors: offloading 24 repeating layers to GPU
0.00.519.974 I load_tensors: offloading output layer to GPU
0.00.519.974 I load_tensors: offloaded 25/25 layers to GPU
0.00.520.008 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.520.010 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.521.647 I llama_init_from_model: n_seq_max     = 1
0.00.521.652 I llama_init_from_model: n_ctx         = 2048
0.00.521.652 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.521.653 I llama_init_from_model: n_batch       = 2048
0.00.521.653 I llama_init_from_model: n_ubatch      = 512
0.00.521.654 I llama_init_from_model: flash_attn    = 0
0.00.521.656 I llama_init_from_model: freq_base     = 10000.0
0.00.521.657 I llama_init_from_model: freq_scale    = 1
0.00.521.659 I ggml_metal_init: allocating
0.00.521.750 I ggml_metal_init: found device: Apple M4
0.00.521.765 I ggml_metal_init: picking default device: Apple M4
0.00.523.558 I ggml_metal_init: using embedded metal library
0.00.530.229 I ggml_metal_init: GPU name:   Apple M4
0.00.530.234 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.530.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.530.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.530.236 I ggml_metal_init: simdgroup reduction   = true
0.00.530.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.530.237 I ggml_metal_init: has residency sets    = true
0.00.530.237 I ggml_metal_init: has bfloat            = true
0.00.530.238 I ggml_metal_init: use bfloat            = true
0.00.530.239 I ggml_metal_init: hasUnifiedMemory      = true
0.00.530.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.547.985 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.609.989 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.609.996 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.610.033 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.614.458 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.614.461 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.614.461 I llama_init_from_model: graph nodes  = 967
0.00.614.461 I llama_init_from_model: graph splits = 2
0.00.614.467 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.614.591 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.614.592 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.230 I main: llama threadpool init, n_threads = 4
0.00.670.274 I 
0.00.670.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.300 I 
0.00.670.477 I sampler seed: 1234
0.00.670.482 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.670.507 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.670.508 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.670.508 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.435.632 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51005.75 tokens per second)
0.01.435.634 I llama_perf_context_print:        load time =     658.59 ms
0.01.435.635 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.58 tokens per second)
0.01.435.636 I llama_perf_context_print:        eval time =     714.95 ms /    63 runs   (   11.35 ms per token,    88.12 tokens per second)
0.01.435.637 I llama_perf_context_print:       total time =     766.31 ms /    70 tokens
0.01.435.843 I ggml_metal_free: deallocating

real	0m1.454s
user	0m0.110s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.671 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.048 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.054 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.055 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.055 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.056 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.057 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.058 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.671 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.673 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.673 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.673 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.674 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.674 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.674 I llama_model_loader: - type  f32:  194 tensors
0.00.026.675 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.675 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.675 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.676 I print_info: file format = GGUF V3 (latest)
0.00.026.676 I print_info: file type   = Q4_K - Medium
0.00.026.677 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.847 I load: special tokens cache size = 25
0.00.040.825 I load: token to piece cache size = 0.2984 MB
0.00.040.829 I print_info: arch             = gptneox
0.00.040.829 I print_info: vocab_only       = 0
0.00.040.829 I print_info: n_ctx_train      = 2048
0.00.040.829 I print_info: n_embd           = 2048
0.00.040.830 I print_info: n_layer          = 24
0.00.040.833 I print_info: n_head           = 16
0.00.040.833 I print_info: n_head_kv        = 16
0.00.040.834 I print_info: n_rot            = 32
0.00.040.834 I print_info: n_swa            = 0
0.00.040.835 I print_info: n_embd_head_k    = 128
0.00.040.835 I print_info: n_embd_head_v    = 128
0.00.040.836 I print_info: n_gqa            = 1
0.00.040.837 I print_info: n_embd_k_gqa     = 2048
0.00.040.839 I print_info: n_embd_v_gqa     = 2048
0.00.040.840 I print_info: f_norm_eps       = 1.0e-05
0.00.040.840 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.840 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.840 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.840 I print_info: f_logit_scale    = 0.0e+00
0.00.040.841 I print_info: n_ff             = 8192
0.00.040.841 I print_info: n_expert         = 0
0.00.040.841 I print_info: n_expert_used    = 0
0.00.040.843 I print_info: causal attn      = 1
0.00.040.843 I print_info: pooling type     = 0
0.00.040.843 I print_info: rope type        = 2
0.00.040.843 I print_info: rope scaling     = linear
0.00.040.844 I print_info: freq_base_train  = 10000.0
0.00.040.844 I print_info: freq_scale_train = 1
0.00.040.847 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.848 I print_info: rope_finetuned   = unknown
0.00.040.848 I print_info: ssm_d_conv       = 0
0.00.040.849 I print_info: ssm_d_inner      = 0
0.00.040.849 I print_info: ssm_d_state      = 0
0.00.040.849 I print_info: ssm_dt_rank      = 0
0.00.040.849 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.849 I print_info: model type       = 1.4B
0.00.040.850 I print_info: model params     = 1.41 B
0.00.040.851 I print_info: general.name     = 1.4B
0.00.040.851 I print_info: vocab type       = BPE
0.00.040.852 I print_info: n_vocab          = 50304
0.00.040.852 I print_info: n_merges         = 50009
0.00.040.852 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.852 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.852 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.852 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.853 I print_info: LF token         = 187 'Ċ'
0.00.040.853 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.853 I print_info: max token length = 1024
0.00.533.511 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.529 I load_tensors: offloading output layer to GPU
0.00.533.530 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.565 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.567 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.535.200 I llama_init_from_model: n_seq_max     = 1
0.00.535.204 I llama_init_from_model: n_ctx         = 128
0.00.535.204 I llama_init_from_model: n_ctx_per_seq = 128
0.00.535.205 I llama_init_from_model: n_batch       = 128
0.00.535.205 I llama_init_from_model: n_ubatch      = 128
0.00.535.205 I llama_init_from_model: flash_attn    = 0
0.00.535.208 I llama_init_from_model: freq_base     = 10000.0
0.00.535.208 I llama_init_from_model: freq_scale    = 1
0.00.535.209 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.535.211 I ggml_metal_init: allocating
0.00.535.293 I ggml_metal_init: found device: Apple M4
0.00.535.307 I ggml_metal_init: picking default device: Apple M4
0.00.536.989 I ggml_metal_init: using embedded metal library
0.00.543.690 I ggml_metal_init: GPU name:   Apple M4
0.00.543.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.701 I ggml_metal_init: simdgroup reduction   = true
0.00.543.701 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.701 I ggml_metal_init: has residency sets    = true
0.00.543.702 I ggml_metal_init: has bfloat            = true
0.00.543.702 I ggml_metal_init: use bfloat            = true
0.00.543.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.711 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.565.269 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.565.274 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.565.320 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.568.629 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.568.631 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.568.631 I llama_init_from_model: graph nodes  = 967
0.00.568.631 I llama_init_from_model: graph splits = 2
0.00.568.634 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.568.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.560 I 
0.00.594.655 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.594.662 I perplexity: tokenizing the input ..
0.00.600.898 I perplexity: tokenization took 6.233 ms
0.00.600.902 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.734.865 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.736.200 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.736.224 I llama_perf_context_print:        load time =     584.88 ms
0.00.736.224 I llama_perf_context_print: prompt eval time =     133.67 ms /   128 tokens (    1.04 ms per token,   957.57 tokens per second)
0.00.736.225 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.736.226 I llama_perf_context_print:       total time =     141.67 ms /   129 tokens
0.00.736.606 I ggml_metal_free: deallocating

real	0m0.756s
user	0m0.078s
sys	0m0.131s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.642 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.292 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.297 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.299 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.299 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.300 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.300 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.300 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.301 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.301 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.302 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.302 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.303 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.303 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.306 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.307 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.307 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.308 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.084 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.862 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.863 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.863 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.864 I llama_model_loader: - type  f32:  194 tensors
0.00.024.864 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.864 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.865 I print_info: file format = GGUF V3 (latest)
0.00.024.865 I print_info: file type   = Q5_K - Medium
0.00.024.866 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.984 I load: special tokens cache size = 25
0.00.038.819 I load: token to piece cache size = 0.2984 MB
0.00.038.822 I print_info: arch             = gptneox
0.00.038.822 I print_info: vocab_only       = 0
0.00.038.822 I print_info: n_ctx_train      = 2048
0.00.038.822 I print_info: n_embd           = 2048
0.00.038.822 I print_info: n_layer          = 24
0.00.038.825 I print_info: n_head           = 16
0.00.038.826 I print_info: n_head_kv        = 16
0.00.038.826 I print_info: n_rot            = 32
0.00.038.828 I print_info: n_swa            = 0
0.00.038.829 I print_info: n_embd_head_k    = 128
0.00.038.829 I print_info: n_embd_head_v    = 128
0.00.038.829 I print_info: n_gqa            = 1
0.00.038.830 I print_info: n_embd_k_gqa     = 2048
0.00.038.831 I print_info: n_embd_v_gqa     = 2048
0.00.038.832 I print_info: f_norm_eps       = 1.0e-05
0.00.038.832 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.832 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.832 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.832 I print_info: f_logit_scale    = 0.0e+00
0.00.038.833 I print_info: n_ff             = 8192
0.00.038.833 I print_info: n_expert         = 0
0.00.038.834 I print_info: n_expert_used    = 0
0.00.038.834 I print_info: causal attn      = 1
0.00.038.834 I print_info: pooling type     = 0
0.00.038.834 I print_info: rope type        = 2
0.00.038.834 I print_info: rope scaling     = linear
0.00.038.835 I print_info: freq_base_train  = 10000.0
0.00.038.835 I print_info: freq_scale_train = 1
0.00.038.835 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.835 I print_info: rope_finetuned   = unknown
0.00.038.841 I print_info: ssm_d_conv       = 0
0.00.038.843 I print_info: ssm_d_inner      = 0
0.00.038.844 I print_info: ssm_d_state      = 0
0.00.038.844 I print_info: ssm_dt_rank      = 0
0.00.038.844 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.844 I print_info: model type       = 1.4B
0.00.038.845 I print_info: model params     = 1.41 B
0.00.038.845 I print_info: general.name     = 1.4B
0.00.038.845 I print_info: vocab type       = BPE
0.00.038.845 I print_info: n_vocab          = 50304
0.00.038.845 I print_info: n_merges         = 50009
0.00.038.846 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.846 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.846 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.846 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.846 I print_info: LF token         = 187 'Ċ'
0.00.038.848 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.849 I print_info: max token length = 1024
0.00.601.489 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.503 I load_tensors: offloading output layer to GPU
0.00.601.503 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.540 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.601.541 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.603.103 I llama_init_from_model: n_seq_max     = 1
0.00.603.106 I llama_init_from_model: n_ctx         = 2048
0.00.603.106 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.603.107 I llama_init_from_model: n_batch       = 2048
0.00.603.107 I llama_init_from_model: n_ubatch      = 512
0.00.603.108 I llama_init_from_model: flash_attn    = 0
0.00.603.109 I llama_init_from_model: freq_base     = 10000.0
0.00.603.109 I llama_init_from_model: freq_scale    = 1
0.00.603.111 I ggml_metal_init: allocating
0.00.603.164 I ggml_metal_init: found device: Apple M4
0.00.603.176 I ggml_metal_init: picking default device: Apple M4
0.00.604.701 I ggml_metal_init: using embedded metal library
0.00.610.725 I ggml_metal_init: GPU name:   Apple M4
0.00.610.729 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.730 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.730 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.731 I ggml_metal_init: simdgroup reduction   = true
0.00.610.731 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.731 I ggml_metal_init: has residency sets    = true
0.00.610.732 I ggml_metal_init: has bfloat            = true
0.00.610.732 I ggml_metal_init: use bfloat            = true
0.00.610.733 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.737 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.221 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.438 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.681.446 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.681.494 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.687.316 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.687.320 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.687.320 I llama_init_from_model: graph nodes  = 967
0.00.687.320 I llama_init_from_model: graph splits = 2
0.00.687.325 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.687.451 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.687.451 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.464 I main: llama threadpool init, n_threads = 4
0.00.751.507 I 
0.00.751.532 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.534 I 
0.00.751.713 I sampler seed: 1234
0.00.751.717 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.728 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.728 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.728 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.604.430 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.604.431 I llama_perf_context_print:        load time =     740.89 ms
0.01.604.431 I llama_perf_context_print: prompt eval time =      51.28 ms /     7 tokens (    7.33 ms per token,   136.51 tokens per second)
0.01.604.432 I llama_perf_context_print:        eval time =     798.60 ms /    63 runs   (   12.68 ms per token,    78.89 tokens per second)
0.01.604.433 I llama_perf_context_print:       total time =     853.90 ms /    70 tokens
0.01.604.728 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.108s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.787 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.820 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.023.831 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.833 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.834 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.836 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.837 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.837 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.838 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.838 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.839 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.839 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.840 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.840 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.840 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.842 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.843 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.843 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.524 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.624 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.436 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.438 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.438 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.439 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.439 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.439 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.032.440 I llama_model_loader: - type  f32:  194 tensors
0.00.032.441 I llama_model_loader: - type q5_K:   61 tensors
0.00.032.441 I llama_model_loader: - type q6_K:   37 tensors
0.00.032.441 I print_info: file format = GGUF V3 (latest)
0.00.032.446 I print_info: file type   = Q5_K - Medium
0.00.032.447 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.040.386 I load: special tokens cache size = 25
0.00.046.403 I load: token to piece cache size = 0.2984 MB
0.00.046.406 I print_info: arch             = gptneox
0.00.046.407 I print_info: vocab_only       = 0
0.00.046.407 I print_info: n_ctx_train      = 2048
0.00.046.407 I print_info: n_embd           = 2048
0.00.046.407 I print_info: n_layer          = 24
0.00.046.411 I print_info: n_head           = 16
0.00.046.411 I print_info: n_head_kv        = 16
0.00.046.412 I print_info: n_rot            = 32
0.00.046.412 I print_info: n_swa            = 0
0.00.046.412 I print_info: n_embd_head_k    = 128
0.00.046.412 I print_info: n_embd_head_v    = 128
0.00.046.414 I print_info: n_gqa            = 1
0.00.046.415 I print_info: n_embd_k_gqa     = 2048
0.00.046.415 I print_info: n_embd_v_gqa     = 2048
0.00.046.416 I print_info: f_norm_eps       = 1.0e-05
0.00.046.416 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.417 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.419 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.419 I print_info: f_logit_scale    = 0.0e+00
0.00.046.419 I print_info: n_ff             = 8192
0.00.046.420 I print_info: n_expert         = 0
0.00.046.420 I print_info: n_expert_used    = 0
0.00.046.420 I print_info: causal attn      = 1
0.00.046.420 I print_info: pooling type     = 0
0.00.046.420 I print_info: rope type        = 2
0.00.046.421 I print_info: rope scaling     = linear
0.00.046.421 I print_info: freq_base_train  = 10000.0
0.00.046.421 I print_info: freq_scale_train = 1
0.00.046.422 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.422 I print_info: rope_finetuned   = unknown
0.00.046.422 I print_info: ssm_d_conv       = 0
0.00.046.424 I print_info: ssm_d_inner      = 0
0.00.046.424 I print_info: ssm_d_state      = 0
0.00.046.424 I print_info: ssm_dt_rank      = 0
0.00.046.424 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.424 I print_info: model type       = 1.4B
0.00.046.425 I print_info: model params     = 1.41 B
0.00.046.425 I print_info: general.name     = 1.4B
0.00.046.426 I print_info: vocab type       = BPE
0.00.046.426 I print_info: n_vocab          = 50304
0.00.046.426 I print_info: n_merges         = 50009
0.00.046.427 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.427 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.427 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.427 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.427 I print_info: LF token         = 187 'Ċ'
0.00.046.428 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.429 I print_info: max token length = 1024
0.00.631.995 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.006 I load_tensors: offloading output layer to GPU
0.00.632.007 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.038 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.632.039 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.633.372 I llama_init_from_model: n_seq_max     = 1
0.00.633.378 I llama_init_from_model: n_ctx         = 128
0.00.633.378 I llama_init_from_model: n_ctx_per_seq = 128
0.00.633.379 I llama_init_from_model: n_batch       = 128
0.00.633.379 I llama_init_from_model: n_ubatch      = 128
0.00.633.380 I llama_init_from_model: flash_attn    = 0
0.00.633.382 I llama_init_from_model: freq_base     = 10000.0
0.00.633.382 I llama_init_from_model: freq_scale    = 1
0.00.633.383 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.633.386 I ggml_metal_init: allocating
0.00.633.453 I ggml_metal_init: found device: Apple M4
0.00.633.467 I ggml_metal_init: picking default device: Apple M4
0.00.635.133 I ggml_metal_init: using embedded metal library
0.00.642.018 I ggml_metal_init: GPU name:   Apple M4
0.00.642.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.025 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.026 I ggml_metal_init: simdgroup reduction   = true
0.00.642.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.027 I ggml_metal_init: has residency sets    = true
0.00.642.027 I ggml_metal_init: has bfloat            = true
0.00.642.027 I ggml_metal_init: use bfloat            = true
0.00.642.028 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.780 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.663.343 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.663.346 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.663.401 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.666.693 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.666.695 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.666.695 I llama_init_from_model: graph nodes  = 967
0.00.666.696 I llama_init_from_model: graph splits = 2
0.00.666.699 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.666.699 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.235 I 
0.00.698.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.316 I perplexity: tokenizing the input ..
0.00.705.640 I perplexity: tokenization took 7.323 ms
0.00.705.644 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.728 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.847.209 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.847.229 I llama_perf_context_print:        load time =     689.44 ms
0.00.847.230 I llama_perf_context_print: prompt eval time =     139.86 ms /   128 tokens (    1.09 ms per token,   915.22 tokens per second)
0.00.847.230 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.230 I llama_perf_context_print:       total time =     149.00 ms /   129 tokens
0.00.847.658 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.078s
sys	0m0.144s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.728 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.296 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.300 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.305 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.306 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.307 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.309 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.309 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.315 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.316 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.317 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.318 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.319 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.320 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.320 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.759 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.760 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.760 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.760 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.761 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.761 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.762 I llama_model_loader: - type  f32:  194 tensors
0.00.023.762 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.762 I print_info: file format = GGUF V3 (latest)
0.00.023.763 I print_info: file type   = Q6_K
0.00.023.764 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.213 I load: special tokens cache size = 25
0.00.038.202 I load: token to piece cache size = 0.2984 MB
0.00.038.205 I print_info: arch             = gptneox
0.00.038.205 I print_info: vocab_only       = 0
0.00.038.205 I print_info: n_ctx_train      = 2048
0.00.038.206 I print_info: n_embd           = 2048
0.00.038.206 I print_info: n_layer          = 24
0.00.038.208 I print_info: n_head           = 16
0.00.038.209 I print_info: n_head_kv        = 16
0.00.038.209 I print_info: n_rot            = 32
0.00.038.209 I print_info: n_swa            = 0
0.00.038.211 I print_info: n_embd_head_k    = 128
0.00.038.211 I print_info: n_embd_head_v    = 128
0.00.038.212 I print_info: n_gqa            = 1
0.00.038.213 I print_info: n_embd_k_gqa     = 2048
0.00.038.218 I print_info: n_embd_v_gqa     = 2048
0.00.038.219 I print_info: f_norm_eps       = 1.0e-05
0.00.038.220 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.220 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.220 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.220 I print_info: f_logit_scale    = 0.0e+00
0.00.038.222 I print_info: n_ff             = 8192
0.00.038.223 I print_info: n_expert         = 0
0.00.038.223 I print_info: n_expert_used    = 0
0.00.038.223 I print_info: causal attn      = 1
0.00.038.223 I print_info: pooling type     = 0
0.00.038.223 I print_info: rope type        = 2
0.00.038.223 I print_info: rope scaling     = linear
0.00.038.229 I print_info: freq_base_train  = 10000.0
0.00.038.231 I print_info: freq_scale_train = 1
0.00.038.231 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.231 I print_info: rope_finetuned   = unknown
0.00.038.231 I print_info: ssm_d_conv       = 0
0.00.038.232 I print_info: ssm_d_inner      = 0
0.00.038.232 I print_info: ssm_d_state      = 0
0.00.038.232 I print_info: ssm_dt_rank      = 0
0.00.038.232 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.232 I print_info: model type       = 1.4B
0.00.038.233 I print_info: model params     = 1.41 B
0.00.038.233 I print_info: general.name     = 1.4B
0.00.038.234 I print_info: vocab type       = BPE
0.00.038.234 I print_info: n_vocab          = 50304
0.00.038.234 I print_info: n_merges         = 50009
0.00.038.234 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.234 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.234 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.235 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.236 I print_info: LF token         = 187 'Ċ'
0.00.038.238 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.238 I print_info: max token length = 1024
0.00.650.276 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.279 I load_tensors: offloading output layer to GPU
0.00.650.280 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.305 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.650.307 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.651.185 I llama_init_from_model: n_seq_max     = 1
0.00.651.187 I llama_init_from_model: n_ctx         = 2048
0.00.651.188 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.651.188 I llama_init_from_model: n_batch       = 2048
0.00.651.189 I llama_init_from_model: n_ubatch      = 512
0.00.651.189 I llama_init_from_model: flash_attn    = 0
0.00.651.190 I llama_init_from_model: freq_base     = 10000.0
0.00.651.191 I llama_init_from_model: freq_scale    = 1
0.00.651.192 I ggml_metal_init: allocating
0.00.651.247 I ggml_metal_init: found device: Apple M4
0.00.651.259 I ggml_metal_init: picking default device: Apple M4
0.00.652.692 I ggml_metal_init: using embedded metal library
0.00.658.694 I ggml_metal_init: GPU name:   Apple M4
0.00.658.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.700 I ggml_metal_init: simdgroup reduction   = true
0.00.658.701 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.701 I ggml_metal_init: has residency sets    = true
0.00.658.701 I ggml_metal_init: has bfloat            = true
0.00.658.701 I ggml_metal_init: use bfloat            = true
0.00.658.702 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.704 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.418 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.727.556 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.727.565 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.727.604 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.731.650 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.731.652 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.731.653 I llama_init_from_model: graph nodes  = 967
0.00.731.653 I llama_init_from_model: graph splits = 2
0.00.731.660 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.731.788 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.731.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.781 I main: llama threadpool init, n_threads = 4
0.00.798.827 I 
0.00.798.853 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.798.854 I 
0.00.799.012 I sampler seed: 1234
0.00.799.016 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.071 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.074 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.679.893 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53183.52 tokens per second)
0.01.679.894 I llama_perf_context_print:        load time =     789.11 ms
0.01.679.895 I llama_perf_context_print: prompt eval time =      54.35 ms /     7 tokens (    7.76 ms per token,   128.80 tokens per second)
0.01.679.895 I llama_perf_context_print:        eval time =     823.48 ms /    63 runs   (   13.07 ms per token,    76.51 tokens per second)
0.01.679.896 I llama_perf_context_print:       total time =     882.05 ms /    70 tokens
0.01.680.138 I ggml_metal_free: deallocating

real	0m1.697s
user	0m0.108s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4618 (90f9b88a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.095 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.109 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.110 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.110 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.110 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.111 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.112 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.112 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.113 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.113 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.113 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.114 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.115 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.117 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.117 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.117 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.777 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.773 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.441 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.443 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.444 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.444 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.445 I llama_model_loader: - type  f32:  194 tensors
0.00.025.445 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.446 I print_info: file format = GGUF V3 (latest)
0.00.025.447 I print_info: file type   = Q6_K
0.00.025.450 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.186 I load: special tokens cache size = 25
0.00.039.221 I load: token to piece cache size = 0.2984 MB
0.00.039.224 I print_info: arch             = gptneox
0.00.039.224 I print_info: vocab_only       = 0
0.00.039.224 I print_info: n_ctx_train      = 2048
0.00.039.224 I print_info: n_embd           = 2048
0.00.039.225 I print_info: n_layer          = 24
0.00.039.228 I print_info: n_head           = 16
0.00.039.228 I print_info: n_head_kv        = 16
0.00.039.229 I print_info: n_rot            = 32
0.00.039.230 I print_info: n_swa            = 0
0.00.039.231 I print_info: n_embd_head_k    = 128
0.00.039.231 I print_info: n_embd_head_v    = 128
0.00.039.231 I print_info: n_gqa            = 1
0.00.039.232 I print_info: n_embd_k_gqa     = 2048
0.00.039.233 I print_info: n_embd_v_gqa     = 2048
0.00.039.233 I print_info: f_norm_eps       = 1.0e-05
0.00.039.234 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.234 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.234 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.234 I print_info: f_logit_scale    = 0.0e+00
0.00.039.235 I print_info: n_ff             = 8192
0.00.039.235 I print_info: n_expert         = 0
0.00.039.235 I print_info: n_expert_used    = 0
0.00.039.235 I print_info: causal attn      = 1
0.00.039.236 I print_info: pooling type     = 0
0.00.039.236 I print_info: rope type        = 2
0.00.039.236 I print_info: rope scaling     = linear
0.00.039.236 I print_info: freq_base_train  = 10000.0
0.00.039.237 I print_info: freq_scale_train = 1
0.00.039.237 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.242 I print_info: rope_finetuned   = unknown
0.00.039.242 I print_info: ssm_d_conv       = 0
0.00.039.242 I print_info: ssm_d_inner      = 0
0.00.039.242 I print_info: ssm_d_state      = 0
0.00.039.242 I print_info: ssm_dt_rank      = 0
0.00.039.243 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.243 I print_info: model type       = 1.4B
0.00.039.243 I print_info: model params     = 1.41 B
0.00.039.243 I print_info: general.name     = 1.4B
0.00.039.244 I print_info: vocab type       = BPE
0.00.039.244 I print_info: n_vocab          = 50304
0.00.039.244 I print_info: n_merges         = 50009
0.00.039.244 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.245 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.245 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.245 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.245 I print_info: LF token         = 187 'Ċ'
0.00.039.246 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.246 I print_info: max token length = 1024
0.00.683.536 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.540 I load_tensors: offloading output layer to GPU
0.00.683.541 I load_tensors: offloaded 25/25 layers to GPU
0.00.683.566 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.683.568 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.684.940 I llama_init_from_model: n_seq_max     = 1
0.00.684.943 I llama_init_from_model: n_ctx         = 128
0.00.684.943 I llama_init_from_model: n_ctx_per_seq = 128
0.00.684.943 I llama_init_from_model: n_batch       = 128
0.00.684.948 I llama_init_from_model: n_ubatch      = 128
0.00.684.948 I llama_init_from_model: flash_attn    = 0
0.00.684.949 I llama_init_from_model: freq_base     = 10000.0
0.00.684.950 I llama_init_from_model: freq_scale    = 1
0.00.684.951 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.684.952 I ggml_metal_init: allocating
0.00.684.982 I ggml_metal_init: found device: Apple M4
0.00.684.994 I ggml_metal_init: picking default device: Apple M4
0.00.686.344 I ggml_metal_init: using embedded metal library
0.00.692.209 I ggml_metal_init: GPU name:   Apple M4
0.00.692.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.692.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.692.214 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.692.215 I ggml_metal_init: simdgroup reduction   = true
0.00.692.215 I ggml_metal_init: simdgroup matrix mul. = true
0.00.692.215 I ggml_metal_init: has residency sets    = true
0.00.692.216 I ggml_metal_init: has bfloat            = true
0.00.692.216 I ggml_metal_init: use bfloat            = true
0.00.692.217 I ggml_metal_init: hasUnifiedMemory      = true
0.00.692.221 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.708.850 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.712.312 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.712.315 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.712.360 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.715.455 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.715.457 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.715.457 I llama_init_from_model: graph nodes  = 967
0.00.715.458 I llama_init_from_model: graph splits = 2
0.00.715.460 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.715.460 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.062 I 
0.00.750.153 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.172 I perplexity: tokenizing the input ..
0.00.756.813 I perplexity: tokenization took 6.639 ms
0.00.756.816 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.895.954 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.897.295 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.897.323 I llama_perf_context_print:        load time =     741.11 ms
0.00.897.324 I llama_perf_context_print: prompt eval time =     138.91 ms /   128 tokens (    1.09 ms per token,   921.47 tokens per second)
0.00.897.324 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.897.325 I llama_perf_context_print:       total time =     147.27 ms /   129 tokens
0.00.897.728 I ggml_metal_free: deallocating

real	0m0.914s
user	0m0.075s
sys	0m0.150s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4618 (90f9b88a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14f105850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14f105ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14f106330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14f108fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14f109420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14f109890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14f109e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14f10a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14f10a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14f10aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14f10b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14f10b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14f10c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14f10cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14f10d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14f10daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14f10e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14f10e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14f10f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14f10f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14f10fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14f110610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14f110d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14f1115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14f111cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14f111fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14f1125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14f113230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14f113770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14f113a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14f113ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14f114190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14f114a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14f114f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14f115220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14f1156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14f115b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14f116000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14f1164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14f116940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14f116de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14f117280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14f117720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14f117bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14f117e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14f118490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14f118aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14f1193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14f1199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14f119fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14f11a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14f11ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14f11b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14f11b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14f11c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14f11c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14f11c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14f11cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14f11d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14f11da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14f11dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14f11e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14f11e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14f11eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14f11ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14f11f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14f11f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14f11fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14f1201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14f120670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14f120b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14f120fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14f121450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14f1219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14f121ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14f122440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14f122990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14f122ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14f123430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14f123980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14f123ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14f124420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14f124970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14f124ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14f125410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14f125960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14f125eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14f126400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14f126950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14f126ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14f1273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14f127940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14f127e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14f1283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14f128930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14f128e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14f1293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14f1190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14f129840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14f129ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14f12a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14f12aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14f12afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14f12b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14f12ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14f12bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14f12c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14f12ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14f12cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14f12d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14f12da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14f12dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14f12e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14f12e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14f12ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14f12f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14f12f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14f12fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14f1300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14f130560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14f130a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14f130ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14f131340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14f1317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14f131c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14f132120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14f1325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14f132a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14f132f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14f1333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14f133840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14f133ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14f134180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14f134620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14f134ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14f134f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14f135400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14f1358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14f135d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14f1361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14f136680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14f136b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14f136fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14f137460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14f137900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14f137da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14f138240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14f1386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14f138b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14f139020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14f1394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14f139960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14f139e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14f13a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14f13a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14f13abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14f13b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14f13b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14f13b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14f13be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14f13c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14f13c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14f13cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14f13d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14f13d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14f13da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14f13dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14f13e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14f13e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14f13eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14f13f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14f13f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14f13fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14f13ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14f1403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14f140860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14f140d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14f1411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14f141640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14f141ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14f141f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14f142420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14f1428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14f142d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14f143200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14f1436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14f143b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14f143fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14f144480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14f144920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14f144dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14f145260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14f145700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14f145c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14f1461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14f1466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14f146c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14f146f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14f147510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14f147b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14f148130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14f148920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14f148dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14f149080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14f149690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14f149ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14f14a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14f14a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14f14add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14f14b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14f14ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14f14bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14f14c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14f14ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14f14cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14f14d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14f14da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14f14df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14f14e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14f14e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14f14ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14f14f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14f14f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14f14ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14f150480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14f1509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14f150f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14f151470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14f1519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14f151f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14f152460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14f1529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14f152f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14f153450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14f1539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14f153ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14f154440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14f154990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14f154ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14f155430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14f155980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14f155ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14f156420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14f156970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14f156ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14f157410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14f157960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14f157eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14f158400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14f158950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14f158ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14f1593f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14f159940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14f159e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14f15a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14f15a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14f15ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14f15b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14f15b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14f15be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14f15c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14f15c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14f15ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14f15d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14f15d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14f15de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14f15e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14f15e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14f15ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14f15f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14f15f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14f15fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14f15ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14f160400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14f1608a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14f160d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14f1611e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14f161680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14f161b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14f161fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14f162460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14f162900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14f162e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14f163570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14f163c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14f1643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14f164ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14f164d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14f165580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14f165840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14f165e50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.858.687 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.858.691 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136f365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136f36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136f37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136f384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136f3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136f3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136f3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136f3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136f3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136f3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136f3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136f3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136f3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136f3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136f3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136f3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136f40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136f41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136f41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136f42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136f42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136f433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136f43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136f44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136f44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136f45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136f45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136f461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136f46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136f47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136f478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136f47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136f48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136f48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136f49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136f49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136f4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136f4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136f4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136f4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136f4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136f4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136f4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136f4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136f4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136f4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136f4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136f4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136f4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136f4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136f502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136f50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136f50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136f519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136f52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136f52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136f530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136f53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136f54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136f547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136f54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136f55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136f55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136f56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136f56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136f58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136f58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136f59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136f59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136f5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136f5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136f5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136f5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136f5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136f5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136f5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136f5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136f5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136f5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136f5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136f5e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136c046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136c04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136c04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136c05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136c058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136c05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136c06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136c065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136c06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136c06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136c07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136c07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136c08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136c08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136c09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136c09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136c0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136c0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136c0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136c0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136c0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136c0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136c0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136c0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136c0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136c0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136c0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136c0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136c0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136c0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136c0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136c0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136c0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136c10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136c104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136c10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136c10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136c111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136c11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136c11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136c11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136c123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136c12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136c12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136c13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136c13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136c139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136c13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136c142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136c14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136c14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136c15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136c15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136c158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136c15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136c161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136c16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136c16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136c170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136c17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136c17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136c17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136c18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136c186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136c18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136c18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136c19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136c198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136c19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136c1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136c1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136c1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136c1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136c1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136c1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136c1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136c1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136c1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136c1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136c1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136c1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136c1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136c1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136c1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136c1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136c1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136c1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136c1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136c1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136c1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136c1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136c20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136c20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136c20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136c21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136c214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136c21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136c21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136c22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136c226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136c22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136c22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136c233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136c23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136c23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136c243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136c24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136c24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136c25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136c25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136c259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136c25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136c262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136c26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136c26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136c27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136c27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136c278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136c27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136c281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136c28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136c28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136c28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136c29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136c29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136c29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136c2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136c2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136c2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136c2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136c2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136c2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136c2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136c2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136c2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136c2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136c2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136c2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136c2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136c2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136c2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136c2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136c2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136c2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136c2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136c2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136c2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136c2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136c30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136c306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136c30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136c30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136c31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136c318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136c31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136c32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136c32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136c32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136c32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136c33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136c337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136c33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136c340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136c34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136c34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136c34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136c35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136c356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136c35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136c35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136c36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136c36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136c36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136c37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136c375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136c37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136c37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136c38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136c387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136c38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136c39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136c394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136c39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136c39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136c3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136c3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136c3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136c3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136c3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136c3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136c3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136c3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136c3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136c3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136c3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136c3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136c3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136c3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136c3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136c3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136c3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136c3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136c3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136c3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136c3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136c3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136c403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136c40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136c40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136c41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136c41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136c41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136c42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136c426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136c42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136c42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136c433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136c43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136c43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136c44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136c445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136c44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136c44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136c45300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136c45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136c45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136c46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136c464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136c46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136c46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136c47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136c47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136c47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136c47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136c483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136c48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136c48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136c49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136c49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136c49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136c49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136c4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136c4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136c4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136c4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136c4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136c4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136c4bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136c4c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136c4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136c4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136c4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136c4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136c4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136c4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136c4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136c4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136c4e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136c4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136c4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136c4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136c4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136c50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136c50480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136c508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136c50d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136c511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136c51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136c51ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136c51f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136c52390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136c52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136c52c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136c530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136c53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136c539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136c53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136c542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136c54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136c54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136c54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136c55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136c558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136c56340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136c56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136c57180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136c578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136c57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136c57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136c585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136c58be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.954s
user	0m0.282s
sys	0m0.312s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4618 (90f9b88a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ff0d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ff0d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ff0df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ff0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ff0ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ff0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ff0f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ff0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ff10120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ff10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ff10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ff11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ff11b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ff122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ff12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ff13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ff13940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ff14060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ff14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ff14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ff15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ff15d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ff164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ff16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ff17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ff17730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ff17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ff189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ff18ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ff191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ff19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ff19910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ff1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ff1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ff1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ff1ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ff1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ff1b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ff1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ff1c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ff1c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ff1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ff1cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ff1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ff1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ff1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ff1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ff1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ff1f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ff1f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ff1fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ff20380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ff20990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ff20fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ff21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ff21c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ff220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ff22390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ff229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ff23190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ff23450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ff238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ff23d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ff24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ff246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ff24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ff25010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ff254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ff25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ff25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ff26290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ff26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ff26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ff27120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ff27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ff27bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ff28110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ff28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ff28bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ff29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ff29650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ff29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ff2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ff2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ff2ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ff2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ff2b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ff2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ff2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ff2c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ff2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ff2d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ff2d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ff2db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ff2e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ff2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ff2eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ff1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ff2efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ff2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ff2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ff30210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ff30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ff30cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ff31200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ff31750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ff31ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ff321f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ff32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ff32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ff331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ff33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ff33c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ff34120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ff345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ff34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ff34f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ff353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ff35840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ff35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ff36180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ff36620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ff36ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ff36f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ff37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ff378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ff37d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ff381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ff38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ff38b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ff38fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ff39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ff39900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ff39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ff3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ff3a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ff3ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ff3b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ff3b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ff3b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ff3be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ff3c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ff3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ff3cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ff3d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ff3d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ff3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ff3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ff3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ff3e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ff3ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ff3f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ff3f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ff3fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ff3fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ff40360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ff40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ff40ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ff41140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ff415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ff41a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ff41f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ff423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ff42860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ff42d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ff431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ff43640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ff43ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ff43f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ff44420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ff448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ff44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ff45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ff456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ff45b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ff45fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ff46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ff46920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ff46dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ff47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ff47700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ff47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ff48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ff484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ff48980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ff48e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ff492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ff49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ff49c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ff4a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ff4a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ff4a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ff4ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ff4b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ff4b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ff4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ff4c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ff4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ff4cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ff4d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ff4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ff4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ff4e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ff4e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ff4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ff4f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ff4fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ff500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ff50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ff509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ff511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ff516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ff51c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ff52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ff526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ff52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ff53180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ff536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ff53c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ff54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ff546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ff54c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ff55160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ff556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ff55c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ff56150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ff566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ff56bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ff57140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ff57690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ff57be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ff58130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ff58680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ff58bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ff59120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ff59670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ff59bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ff5a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ff5a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ff5abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ff5b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ff5b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ff5bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ff5c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ff5c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ff5cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ff5d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ff5d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ff5db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ff5e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ff5e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ff5eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ff5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ff5f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ff5fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ff600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ff60600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ff60b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ff610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ff615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ff61b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ff62090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ff625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ff62b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ff63080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ff635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ff63b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ff63fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ff64460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ff64900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ff64da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ff65240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ff656e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ff65b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ff66020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ff664c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ff66960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ff66e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ff672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ff67740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ff67be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ff68080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ff685d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ff68cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ff69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ff69b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ff6a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ff6a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ff6ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ff6afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ff6b5d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.098.002 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.007 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ff6b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ff4cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ff4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ff4d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ff20640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ff20030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ff22650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ff4f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ff179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ff1e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ff1ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ff1f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ff1d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ff1fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ff169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ff22c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ff2f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ff6a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ff19bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ff19e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ff4f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ff4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ff18000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ff182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ff18580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ff6ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ff6bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ff6bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ff6c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ff6c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ff6c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ff6cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ff6cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ff6d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ff6d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ff6d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ff6d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ff6db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ff6ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ff6e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ff6e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ff6e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ff6e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ff6ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ff6ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ff6f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ff6f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ff6f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ff6f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ff6fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ff6fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ff701b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ff70470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ff70730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ff709f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ff70cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ff70f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ff71230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ff714f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ff717b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ff71a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ff71d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ff71ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ff722b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ff72570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ff72830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ff72af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ff72db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ff73070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ff73330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ff735f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ff738b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ff73b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ff73e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ff740f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ff743b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ff74670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ff74930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ff74bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ff74eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ff75170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ff75430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ff756f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ff759b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ff75c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ff75f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ff761f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ff764b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ff76770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ff76a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ff76cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ff76fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ff77270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ff77530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ff777f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ff77ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ff77d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ff78030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ff782f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ff785b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ff78870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ff78b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ff78df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ff790b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ff79370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ff79630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ff798f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ff79bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ff79e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ff7a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ff7a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ff7a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ff7a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ff7ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ff7aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ff7b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ff7b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ff7b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ff7b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ff7bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ff7bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ff7c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ff7c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ff7c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ff7ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ff7cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ff7cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ff7d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ff7d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ff7d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ff7daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ff7ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ff7e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ff7e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ff7e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ff7e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ff7eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ff7ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ff7f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ff7f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ff7f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ff7f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ff7fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ff7feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ff80170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ff80430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ff806f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ff809b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ff80c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ff80f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ff811f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ff814b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ff81770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ff81a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ff81cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ff81fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ff82270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ff82530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ff827f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ff82ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ff82d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ff83030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ff832f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ff835b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ff83870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ff83b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ff83df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ff840b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ff84370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ff84630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ff848f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ff84bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ff84e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ff85130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ff853f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ff856b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ff85970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ff85c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ff85ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ff861b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ff86470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ff86730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ff869f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ff86cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ff86f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ff87230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ff874f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ff877b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ff87a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ff87d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ff87ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ff882b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ff88570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ff88830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ff88af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ff88db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ff89070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ff89330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ff895f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ff898b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ff89b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ff89e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ff8a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ff8a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ff8a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ff8abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ff8ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ff8b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ff8b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ff8bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ff8c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ff8c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ff8c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ff8cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ff8d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ff8d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ff8db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ff8dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ff8e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ff8e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ff8ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ff8f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ff8f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ff8fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ff8fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ff90330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ff907a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ff90c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ff91080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ff914f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ff91960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ff91dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ff92240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ff926b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ff92b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ff92f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ff93400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ff93870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ff93ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ff94150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ff945c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ff94a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ff94ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ff95310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ff95780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ff95bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ff96060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ff964d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ff96940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ff96db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ff97220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ff97690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ff97b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ff97f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ff983e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ff98850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ff98cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ff99130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ff995a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ff99a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ff99e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ff9a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ff9a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ff9abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ff9b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ff9b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ff9b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ff9bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ff9c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ff9c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ff9cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ff9cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ff9d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ff9d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ff9dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ff9e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ff9e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ff9e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ff9ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ff9f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ff9f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ff9fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ffa0020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ffa0a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ffa11b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ffa18d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ffa1ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ffa22b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ffa2aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ffa2d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ffa3370 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11fe087c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11fe08c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11fe090a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11fe09910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11fe09e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11fe0a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11fe0a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11fe0ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11fe0b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11fe0b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11fe0bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11fe0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11fe0c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11fe0d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11fe0d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11fe0dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11fe0e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11fe0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11fe0f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11fe0fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11fe105f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11fe10d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11fe11430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11fe11b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11fe12270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11fe12530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11fe12b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11fe13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11fe13760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11fe13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11fe143f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11fe146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11fe14f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11fe15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11fe15740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11fe15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11fe16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11fe16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11fe169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11fe16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11fe17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11fe177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11fe17c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11fe180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11fe183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11fe189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11fe18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11fe195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11fe19be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11fe1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11fe1a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11fe1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11fe1b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11fe1ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11fe1c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11fe1c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11fe1cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11fe1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11fe1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11fe1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11fe1e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11fe1e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11fe1ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11fe1eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11fe1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11fe1f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11fe1fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11fe20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11fe205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11fe20a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11fe20f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11fe213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11fe21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11fe21d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11fe222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11fe22830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11fe22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11fe232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11fe23820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11fe23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11fe242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11fe24810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11fe24d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11fe252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11fe25800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11fe25d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11fe262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11fe267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11fe26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11fe27290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11fe277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11fe27d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11fe28280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11fe287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11fe28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11fe29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11fe297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11fe29d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11fe2a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11fe2a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11fe2ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11fe2b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11fe2b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11fe2bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11fe2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11fe2c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11fe2cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11fe2d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11fe2d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11fe2dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11fe2e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11fe2e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11fe2ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11fe2f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11fe2f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11fe2faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11fe2ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11fe303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11fe30880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11fe30d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11fe311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11fe31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11fe31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11fe31fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11fe32440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11fe328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11fe32d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11fe33220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11fe336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11fe33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11fe34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11fe344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11fe34940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11fe34de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11fe35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11fe35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11fe35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11fe36060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11fe36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11fe369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11fe36e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11fe372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11fe37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11fe37c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11fe380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11fe38560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11fe38a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11fe38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11fe39340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11fe397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11fe39c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11fe3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11fe3a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11fe3aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11fe3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11fe3b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11fe3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11fe3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11fe3c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11fe3c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11fe3cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11fe3cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11fe3d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11fe3d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11fe3dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11fe3e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11fe3e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11fe3eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11fe3efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11fe3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11fe3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11fe3fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11fe40240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11fe406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11fe40b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11fe41020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11fe414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11fe41960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11fe41e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11fe422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11fe42740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11fe42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11fe43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11fe43520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11fe439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11fe43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11fe44300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11fe447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11fe44c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11fe450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11fe45580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11fe45a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11fe45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11fe46410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11fe46960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11fe46eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11fe47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11fe476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11fe47cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11fe482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11fe488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11fe490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11fe49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11fe49840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11fe49e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11fe4a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11fe4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11fe4b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11fe4b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11fe4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11fe4c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11fe4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11fe4cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11fe4d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11fe4d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11fe4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11fe4e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11fe4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11fe4ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11fe4f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11fe4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11fe4fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11fe501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11fe506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11fe50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11fe51190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11fe516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11fe51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11fe52180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11fe526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11fe52c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11fe53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11fe536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11fe53c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11fe54160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11fe546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11fe54c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11fe55150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11fe556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11fe55bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11fe56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11fe56690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11fe56be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11fe57130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11fe57680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11fe57bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11fe58120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11fe58670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11fe58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11fe59110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11fe59660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11fe59bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11fe5a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11fe5a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11fe5aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11fe5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11fe5b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11fe5bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11fe5c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11fe5c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11fe5cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11fe5d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11fe5d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11fe5db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11fe5e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11fe5e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11fe5eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11fe5f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11fe5f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11fe5f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11fe5fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11fe60280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11fe60720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11fe60bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11fe61060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11fe61500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11fe619a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11fe61e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11fe622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11fe62780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11fe62c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11fe630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11fe63610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11fe63d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11fe64450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11fe64b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11fe65290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11fe65550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11fe65d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11fe66000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11fe66610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.965s
user	0m0.235s
sys	0m0.190s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.50 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.17 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.67 sec*proc (2 tests)

Total Test time (real) =   1.68 sec
        1.70 real         0.54 user         0.20 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.28 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.33 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.61 sec*proc (2 tests)

Total Test time (real) =   0.62 sec
        0.62 real         0.13 user         0.09 sys
```
