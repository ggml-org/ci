Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:44 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.643s
user	0m0.687s
sys	0m0.970s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  7%] Built target build_info
[  7%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  7%] Built target sha1
[  7%] Built target sha256
[  7%] Built target xxhash
[  8%] Linking CXX shared library libggml-base.dylib
[  8%] Built target ggml-base
[  9%] Generate assembly for embedded Metal library
Embedding Metal library
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/amx/amx.cpp.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 13%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/amx/mmq.cpp.o
[ 13%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Linking C executable ../bin/test-c
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 29%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Linking CXX executable ../../bin/llama-run
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Built target llava
[ 32%] Linking CXX static library libcommon.a
[ 32%] Built target llama-run
[ 32%] Built target test-c
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llama-simple
[ 32%] Built target llama-simple-chat
[ 33%] Linking CXX shared library libllava_shared.dylib
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Built target common
[ 34%] Built target llava_static
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 40%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-sampling
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-chat-template
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-sampling
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-chat-template
[ 48%] Built target test-arg-parser
[ 48%] Built target test-log
[ 48%] Built target test-grammar-parser
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Built target test-grammar-integration
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Built target test-llama-grammar
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-rope
[ 61%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 62%] Built target test-quantize-fns
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-barrier
[ 62%] Built target test-autorelease
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Built target test-rope
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Built target test-quantize-perf
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 65%] Built target test-json-schema-to-grammar
[ 65%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 65%] Built target llama-batched-bench
[ 66%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Built target llama-batched
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gguf-split
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-gbnf-validator
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-imatrix
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-infill
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Built target llama-bench
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-parallel
[ 84%] Built target llama-lookup-stats
[ 85%] Built target llama-passkey
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-cli
[ 85%] Generating completion.js.hpp
[ 85%] Built target llama-perplexity
[ 85%] Generating deps_daisyui.min.css.hpp
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 88%] Generating deps_markdown-it.js.hpp
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Generating deps_tailwindcss.js.hpp
[ 90%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Built target llama-retrieval
[ 91%] Generating deps_vue.esm-browser.js.hpp
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Generating index.html.hpp
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-speculative
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.462s
user	0m6.003s
sys	0m9.279s

main: quantize time =  4457.85 ms
main:    total time =  4457.85 ms

main: quantize time =  1871.65 ms
main:    total time =  1871.65 ms

main: quantize time =  2134.93 ms
main:    total time =  2134.93 ms

main: quantize time =  2366.73 ms
main:    total time =  2366.73 ms

main: quantize time =  2658.32 ms
main:    total time =  2658.32 ms

main: quantize time =  5548.20 ms
main:    total time =  5548.20 ms

main: quantize time =  5915.58 ms
main:    total time =  5915.58 ms

main: quantize time =  7065.81 ms
main:    total time =  7065.81 ms

main: quantize time =  6087.74 ms
main:    total time =  6087.74 ms

main: quantize time =  4756.21 ms
main:    total time =  4756.21 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.129 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.219 I main: llama backend init
0.00.000.223 I main: load the model and apply lora adapter, if any
0.00.061.860 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.072.890 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.072.901 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.072.918 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.072.919 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.072.919 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.072.920 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.072.921 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.072.923 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.072.923 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.072.924 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.072.925 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.072.927 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.072.928 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.072.928 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.072.934 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.072.934 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.072.935 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.079.800 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.082.046 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.089.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.089.026 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.089.027 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.089.028 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.089.028 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.089.030 I llama_model_loader: - type  f32:  194 tensors
0.00.089.030 I llama_model_loader: - type  f16:   98 tensors
0.00.127.606 I llm_load_vocab: special tokens cache size = 25
0.00.135.328 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.135.332 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.135.333 I llm_load_print_meta: arch             = gptneox
0.00.135.333 I llm_load_print_meta: vocab type       = BPE
0.00.135.333 I llm_load_print_meta: n_vocab          = 50304
0.00.135.334 I llm_load_print_meta: n_merges         = 50009
0.00.135.334 I llm_load_print_meta: vocab_only       = 0
0.00.135.334 I llm_load_print_meta: n_ctx_train      = 2048
0.00.135.334 I llm_load_print_meta: n_embd           = 2048
0.00.135.334 I llm_load_print_meta: n_layer          = 24
0.00.135.339 I llm_load_print_meta: n_head           = 16
0.00.135.340 I llm_load_print_meta: n_head_kv        = 16
0.00.135.340 I llm_load_print_meta: n_rot            = 32
0.00.135.341 I llm_load_print_meta: n_swa            = 0
0.00.135.341 I llm_load_print_meta: n_embd_head_k    = 128
0.00.135.341 I llm_load_print_meta: n_embd_head_v    = 128
0.00.135.342 I llm_load_print_meta: n_gqa            = 1
0.00.135.343 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.135.345 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.135.346 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.135.346 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.135.347 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.135.347 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.135.347 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.135.347 I llm_load_print_meta: n_ff             = 8192
0.00.135.348 I llm_load_print_meta: n_expert         = 0
0.00.135.349 I llm_load_print_meta: n_expert_used    = 0
0.00.135.349 I llm_load_print_meta: causal attn      = 1
0.00.135.349 I llm_load_print_meta: pooling type     = 0
0.00.135.349 I llm_load_print_meta: rope type        = 2
0.00.135.350 I llm_load_print_meta: rope scaling     = linear
0.00.135.350 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.135.350 I llm_load_print_meta: freq_scale_train = 1
0.00.135.351 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.135.351 I llm_load_print_meta: rope_finetuned   = unknown
0.00.135.351 I llm_load_print_meta: ssm_d_conv       = 0
0.00.135.351 I llm_load_print_meta: ssm_d_inner      = 0
0.00.135.351 I llm_load_print_meta: ssm_d_state      = 0
0.00.135.351 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.135.351 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.135.363 I llm_load_print_meta: model type       = 1.4B
0.00.135.364 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.135.364 I llm_load_print_meta: model params     = 1.41 B
0.00.135.365 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.135.365 I llm_load_print_meta: general.name     = 1.4B
0.00.135.365 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.135.366 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.135.366 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.135.366 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.135.366 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.135.367 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.135.367 I llm_load_print_meta: max token length = 1024
0.00.138.031 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.138.031 I llm_load_tensors: offloading output layer to GPU
0.00.138.031 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.138.049 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.138.051 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.139.089 I llama_new_context_with_model: n_seq_max     = 1
0.00.139.090 I llama_new_context_with_model: n_ctx         = 2048
0.00.139.090 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.139.090 I llama_new_context_with_model: n_batch       = 2048
0.00.139.091 I llama_new_context_with_model: n_ubatch      = 512
0.00.139.091 I llama_new_context_with_model: flash_attn    = 0
0.00.139.091 I llama_new_context_with_model: freq_base     = 10000.0
0.00.139.092 I llama_new_context_with_model: freq_scale    = 1
0.00.139.092 I ggml_metal_init: allocating
0.00.139.102 I ggml_metal_init: found device: Apple M4
0.00.139.105 I ggml_metal_init: picking default device: Apple M4
0.00.139.786 I ggml_metal_init: using embedded metal library
0.00.150.680 I ggml_metal_init: GPU name:   Apple M4
0.00.150.683 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.150.683 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.150.684 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.150.684 I ggml_metal_init: simdgroup reduction   = true
0.00.150.684 I ggml_metal_init: simdgroup matrix mul. = true
0.00.150.684 I ggml_metal_init: has bfloat            = true
0.00.150.684 I ggml_metal_init: use bfloat            = true
0.00.150.685 I ggml_metal_init: hasUnifiedMemory      = true
0.00.150.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.189.384 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.189.391 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.189.409 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.190.358 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.190.360 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.190.360 I llama_new_context_with_model: graph nodes  = 967
0.00.190.360 I llama_new_context_with_model: graph splits = 2
0.00.190.380 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.267.000 I main: llama threadpool init, n_threads = 4
0.00.267.032 I 
0.00.267.068 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.267.070 I 
0.00.267.138 I sampler seed: 1234
0.00.267.142 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.267.176 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.267.177 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.267.178 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.128.461 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52437.22 tokens per second)
0.02.128.461 I llama_perf_context_print:        load time =     205.13 ms
0.02.128.462 I llama_perf_context_print: prompt eval time =      38.49 ms /     7 tokens (    5.50 ms per token,   181.85 tokens per second)
0.02.128.463 I llama_perf_context_print:        eval time =    1819.70 ms /    63 runs   (   28.88 ms per token,    34.62 tokens per second)
0.02.128.463 I llama_perf_context_print:       total time =    1861.46 ms /    70 tokens
0.02.128.645 I ggml_metal_free: deallocating

real	0m2.433s
user	0m0.151s
sys	0m0.099s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.666 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.289 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.294 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.296 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.296 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.297 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.297 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.297 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.298 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.298 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.299 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.299 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.299 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.301 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.305 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.341 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.504 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.615 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.620 I llama_model_loader: - type  f32:  194 tensors
0.00.040.620 I llama_model_loader: - type q8_0:   98 tensors
0.00.065.869 I llm_load_vocab: special tokens cache size = 25
0.00.072.801 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.805 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.805 I llm_load_print_meta: arch             = gptneox
0.00.072.806 I llm_load_print_meta: vocab type       = BPE
0.00.072.806 I llm_load_print_meta: n_vocab          = 50304
0.00.072.806 I llm_load_print_meta: n_merges         = 50009
0.00.072.808 I llm_load_print_meta: vocab_only       = 0
0.00.072.808 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.808 I llm_load_print_meta: n_embd           = 2048
0.00.072.810 I llm_load_print_meta: n_layer          = 24
0.00.072.815 I llm_load_print_meta: n_head           = 16
0.00.072.816 I llm_load_print_meta: n_head_kv        = 16
0.00.072.816 I llm_load_print_meta: n_rot            = 32
0.00.072.816 I llm_load_print_meta: n_swa            = 0
0.00.072.816 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.816 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.817 I llm_load_print_meta: n_gqa            = 1
0.00.072.818 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.818 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.819 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.819 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.820 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.821 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.821 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.821 I llm_load_print_meta: n_ff             = 8192
0.00.072.821 I llm_load_print_meta: n_expert         = 0
0.00.072.822 I llm_load_print_meta: n_expert_used    = 0
0.00.072.823 I llm_load_print_meta: causal attn      = 1
0.00.072.823 I llm_load_print_meta: pooling type     = 0
0.00.072.823 I llm_load_print_meta: rope type        = 2
0.00.072.823 I llm_load_print_meta: rope scaling     = linear
0.00.072.823 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.825 I llm_load_print_meta: freq_scale_train = 1
0.00.072.825 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.825 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.825 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.825 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.825 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.826 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.826 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.839 I llm_load_print_meta: model type       = 1.4B
0.00.072.840 I llm_load_print_meta: model ftype      = Q8_0
0.00.072.840 I llm_load_print_meta: model params     = 1.41 B
0.00.072.841 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.072.841 I llm_load_print_meta: general.name     = 1.4B
0.00.072.841 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.842 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.842 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.842 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.842 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.072.843 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.843 I llm_load_print_meta: max token length = 1024
0.00.075.426 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.426 I llm_load_tensors: offloading output layer to GPU
0.00.075.426 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.437 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.075.438 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.076.485 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.486 I llama_new_context_with_model: n_ctx         = 2048
0.00.076.486 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.076.487 I llama_new_context_with_model: n_batch       = 2048
0.00.076.487 I llama_new_context_with_model: n_ubatch      = 512
0.00.076.487 I llama_new_context_with_model: flash_attn    = 0
0.00.076.488 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.488 I llama_new_context_with_model: freq_scale    = 1
0.00.076.488 I ggml_metal_init: allocating
0.00.076.496 I ggml_metal_init: found device: Apple M4
0.00.076.498 I ggml_metal_init: picking default device: Apple M4
0.00.077.258 I ggml_metal_init: using embedded metal library
0.00.079.735 I ggml_metal_init: GPU name:   Apple M4
0.00.079.738 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.738 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.739 I ggml_metal_init: simdgroup reduction   = true
0.00.079.739 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.740 I ggml_metal_init: has bfloat            = true
0.00.079.740 I ggml_metal_init: use bfloat            = true
0.00.079.740 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.741 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.142 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.150 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.175 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.303 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.306 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.307 I llama_new_context_with_model: graph nodes  = 967
0.00.118.307 I llama_new_context_with_model: graph splits = 2
0.00.118.324 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.502.491 I main: llama threadpool init, n_threads = 4
0.01.502.538 I 
0.01.502.570 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.502.571 I 
0.01.502.792 I sampler seed: 1234
0.01.502.797 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.502.840 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.502.842 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.502.842 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.613.132 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48830.81 tokens per second)
0.02.613.133 I llama_perf_context_print:        load time =    1492.82 ms
0.02.613.134 I llama_perf_context_print: prompt eval time =      43.00 ms /     7 tokens (    6.14 ms per token,   162.80 tokens per second)
0.02.613.135 I llama_perf_context_print:        eval time =    1064.27 ms /    63 runs   (   16.89 ms per token,    59.20 tokens per second)
0.02.613.135 I llama_perf_context_print:       total time =    1110.65 ms /    70 tokens
0.02.613.330 I ggml_metal_free: deallocating

real	0m2.634s
user	0m0.121s
sys	0m0.227s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.012.069 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.482 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.493 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.493 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.494 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.495 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.495 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.496 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.496 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.497 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.497 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.499 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.500 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.500 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.765 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.939 I llama_model_loader: - type  f32:  194 tensors
0.00.030.939 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.939 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.529 I llm_load_vocab: special tokens cache size = 25
0.00.057.209 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.211 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.212 I llm_load_print_meta: arch             = gptneox
0.00.057.212 I llm_load_print_meta: vocab type       = BPE
0.00.057.212 I llm_load_print_meta: n_vocab          = 50304
0.00.057.212 I llm_load_print_meta: n_merges         = 50009
0.00.057.213 I llm_load_print_meta: vocab_only       = 0
0.00.057.213 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.213 I llm_load_print_meta: n_embd           = 2048
0.00.057.213 I llm_load_print_meta: n_layer          = 24
0.00.057.217 I llm_load_print_meta: n_head           = 16
0.00.057.218 I llm_load_print_meta: n_head_kv        = 16
0.00.057.218 I llm_load_print_meta: n_rot            = 32
0.00.057.218 I llm_load_print_meta: n_swa            = 0
0.00.057.218 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.218 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.219 I llm_load_print_meta: n_gqa            = 1
0.00.057.220 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.220 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.221 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.221 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.222 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.222 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.225 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.226 I llm_load_print_meta: n_ff             = 8192
0.00.057.226 I llm_load_print_meta: n_expert         = 0
0.00.057.227 I llm_load_print_meta: n_expert_used    = 0
0.00.057.227 I llm_load_print_meta: causal attn      = 1
0.00.057.227 I llm_load_print_meta: pooling type     = 0
0.00.057.227 I llm_load_print_meta: rope type        = 2
0.00.057.227 I llm_load_print_meta: rope scaling     = linear
0.00.057.228 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.229 I llm_load_print_meta: freq_scale_train = 1
0.00.057.230 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.230 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.230 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.230 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.230 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.230 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.230 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.244 I llm_load_print_meta: model type       = 1.4B
0.00.057.244 I llm_load_print_meta: model ftype      = Q4_0
0.00.057.244 I llm_load_print_meta: model params     = 1.41 B
0.00.057.245 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.057.245 I llm_load_print_meta: general.name     = 1.4B
0.00.057.245 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.245 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.246 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.246 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.246 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.057.246 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.246 I llm_load_print_meta: max token length = 1024
0.00.059.427 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.427 I llm_load_tensors: offloading output layer to GPU
0.00.059.428 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.438 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.059.439 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.060.366 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.367 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.367 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.368 I llama_new_context_with_model: n_batch       = 2048
0.00.060.368 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.368 I llama_new_context_with_model: flash_attn    = 0
0.00.060.368 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.369 I llama_new_context_with_model: freq_scale    = 1
0.00.060.369 I ggml_metal_init: allocating
0.00.060.375 I ggml_metal_init: found device: Apple M4
0.00.060.378 I ggml_metal_init: picking default device: Apple M4
0.00.061.058 I ggml_metal_init: using embedded metal library
0.00.063.195 I ggml_metal_init: GPU name:   Apple M4
0.00.063.197 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.197 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.198 I ggml_metal_init: simdgroup reduction   = true
0.00.063.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.198 I ggml_metal_init: has bfloat            = true
0.00.063.198 I ggml_metal_init: use bfloat            = true
0.00.063.199 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.391 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.400 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.423 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.615 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.617 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.618 I llama_new_context_with_model: graph nodes  = 967
0.00.105.618 I llama_new_context_with_model: graph splits = 2
0.00.105.634 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.893.985 I main: llama threadpool init, n_threads = 4
0.00.894.021 I 
0.00.894.049 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.894.050 I 
0.00.894.287 I sampler seed: 1234
0.00.894.293 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.894.336 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.894.340 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.894.340 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.574.913 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56259.90 tokens per second)
0.01.574.914 I llama_perf_context_print:        load time =     881.91 ms
0.01.574.915 I llama_perf_context_print: prompt eval time =      36.49 ms /     7 tokens (    5.21 ms per token,   191.85 tokens per second)
0.01.574.915 I llama_perf_context_print:        eval time =     640.98 ms /    63 runs   (   10.17 ms per token,    98.29 tokens per second)
0.01.574.916 I llama_perf_context_print:       total time =     680.93 ms /    70 tokens
0.01.575.094 I ggml_metal_free: deallocating

real	0m1.599s
user	0m0.112s
sys	0m0.168s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.013.768 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.973 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.978 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.980 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.984 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.988 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.989 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.990 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.993 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.435 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.807 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.565 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.566 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.567 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.567 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.568 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.040.569 I llama_model_loader: - type  f32:  194 tensors
0.00.040.569 I llama_model_loader: - type q4_1:   97 tensors
0.00.040.569 I llama_model_loader: - type q6_K:    1 tensors
0.00.075.011 I llm_load_vocab: special tokens cache size = 25
0.00.084.497 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.500 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.501 I llm_load_print_meta: arch             = gptneox
0.00.084.501 I llm_load_print_meta: vocab type       = BPE
0.00.084.501 I llm_load_print_meta: n_vocab          = 50304
0.00.084.502 I llm_load_print_meta: n_merges         = 50009
0.00.084.502 I llm_load_print_meta: vocab_only       = 0
0.00.084.502 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.502 I llm_load_print_meta: n_embd           = 2048
0.00.084.503 I llm_load_print_meta: n_layer          = 24
0.00.084.506 I llm_load_print_meta: n_head           = 16
0.00.084.507 I llm_load_print_meta: n_head_kv        = 16
0.00.084.507 I llm_load_print_meta: n_rot            = 32
0.00.084.508 I llm_load_print_meta: n_swa            = 0
0.00.084.508 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.508 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.509 I llm_load_print_meta: n_gqa            = 1
0.00.084.510 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.511 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.511 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.514 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.516 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.516 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.516 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.517 I llm_load_print_meta: n_ff             = 8192
0.00.084.517 I llm_load_print_meta: n_expert         = 0
0.00.084.518 I llm_load_print_meta: n_expert_used    = 0
0.00.084.518 I llm_load_print_meta: causal attn      = 1
0.00.084.518 I llm_load_print_meta: pooling type     = 0
0.00.084.518 I llm_load_print_meta: rope type        = 2
0.00.084.518 I llm_load_print_meta: rope scaling     = linear
0.00.084.519 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.519 I llm_load_print_meta: freq_scale_train = 1
0.00.084.520 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.520 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.520 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.520 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.520 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.520 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.521 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.533 I llm_load_print_meta: model type       = 1.4B
0.00.084.534 I llm_load_print_meta: model ftype      = Q4_1
0.00.084.534 I llm_load_print_meta: model params     = 1.41 B
0.00.084.535 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.084.535 I llm_load_print_meta: general.name     = 1.4B
0.00.084.536 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.536 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.536 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.536 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.537 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.084.539 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.539 I llm_load_print_meta: max token length = 1024
0.00.087.214 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.214 I llm_load_tensors: offloading output layer to GPU
0.00.087.215 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.225 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.087.227 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.088.478 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.479 I llama_new_context_with_model: n_ctx         = 2048
0.00.088.479 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.088.480 I llama_new_context_with_model: n_batch       = 2048
0.00.088.480 I llama_new_context_with_model: n_ubatch      = 512
0.00.088.480 I llama_new_context_with_model: flash_attn    = 0
0.00.088.481 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.481 I llama_new_context_with_model: freq_scale    = 1
0.00.088.481 I ggml_metal_init: allocating
0.00.088.485 I ggml_metal_init: found device: Apple M4
0.00.088.487 I ggml_metal_init: picking default device: Apple M4
0.00.089.241 I ggml_metal_init: using embedded metal library
0.00.092.005 I ggml_metal_init: GPU name:   Apple M4
0.00.092.007 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.007 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.008 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.008 I ggml_metal_init: simdgroup reduction   = true
0.00.092.008 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.008 I ggml_metal_init: has bfloat            = true
0.00.092.008 I ggml_metal_init: use bfloat            = true
0.00.092.009 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.010 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.120.899 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.120.904 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.120.922 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.121.771 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.121.772 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.121.772 I llama_new_context_with_model: graph nodes  = 967
0.00.121.772 I llama_new_context_with_model: graph splits = 2
0.00.121.781 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.939.258 I main: llama threadpool init, n_threads = 4
0.00.939.347 I 
0.00.939.411 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.939.413 I 
0.00.939.927 I sampler seed: 1234
0.00.939.934 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.939.977 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.939.979 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.939.979 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.681.463 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.01.681.463 I llama_perf_context_print:        load time =     925.48 ms
0.01.681.465 I llama_perf_context_print: prompt eval time =      41.55 ms /     7 tokens (    5.94 ms per token,   168.45 tokens per second)
0.01.681.465 I llama_perf_context_print:        eval time =     697.07 ms /    63 runs   (   11.06 ms per token,    90.38 tokens per second)
0.01.681.466 I llama_perf_context_print:       total time =     742.21 ms /    70 tokens
0.01.681.670 I ggml_metal_free: deallocating

real	0m1.719s
user	0m0.142s
sys	0m0.186s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.177 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.374 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.384 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.386 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.386 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.387 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.387 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.388 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.388 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.388 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.389 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.389 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.391 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.391 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.391 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.356 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.411 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.269 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.269 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.269 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.270 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.270 I llama_model_loader: - type  f32:  194 tensors
0.00.026.271 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.271 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.381 I llm_load_vocab: special tokens cache size = 25
0.00.052.253 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.255 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.256 I llm_load_print_meta: arch             = gptneox
0.00.052.256 I llm_load_print_meta: vocab type       = BPE
0.00.052.257 I llm_load_print_meta: n_vocab          = 50304
0.00.052.257 I llm_load_print_meta: n_merges         = 50009
0.00.052.257 I llm_load_print_meta: vocab_only       = 0
0.00.052.257 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.257 I llm_load_print_meta: n_embd           = 2048
0.00.052.257 I llm_load_print_meta: n_layer          = 24
0.00.052.260 I llm_load_print_meta: n_head           = 16
0.00.052.261 I llm_load_print_meta: n_head_kv        = 16
0.00.052.261 I llm_load_print_meta: n_rot            = 32
0.00.052.262 I llm_load_print_meta: n_swa            = 0
0.00.052.262 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.262 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.263 I llm_load_print_meta: n_gqa            = 1
0.00.052.266 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.266 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.267 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.267 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.268 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.268 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.268 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.269 I llm_load_print_meta: n_ff             = 8192
0.00.052.269 I llm_load_print_meta: n_expert         = 0
0.00.052.269 I llm_load_print_meta: n_expert_used    = 0
0.00.052.269 I llm_load_print_meta: causal attn      = 1
0.00.052.269 I llm_load_print_meta: pooling type     = 0
0.00.052.269 I llm_load_print_meta: rope type        = 2
0.00.052.271 I llm_load_print_meta: rope scaling     = linear
0.00.052.272 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.272 I llm_load_print_meta: freq_scale_train = 1
0.00.052.272 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.272 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.272 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.273 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.273 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.273 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.273 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.285 I llm_load_print_meta: model type       = 1.4B
0.00.052.285 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.285 I llm_load_print_meta: model params     = 1.41 B
0.00.052.286 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.286 I llm_load_print_meta: general.name     = 1.4B
0.00.052.286 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.290 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.290 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.290 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.290 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.291 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.291 I llm_load_print_meta: max token length = 1024
0.00.053.918 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.919 I llm_load_tensors: offloading output layer to GPU
0.00.053.919 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.928 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.929 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.771 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.772 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.772 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.772 I llama_new_context_with_model: n_batch       = 2048
0.00.054.772 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.773 I llama_new_context_with_model: flash_attn    = 0
0.00.054.773 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.773 I llama_new_context_with_model: freq_scale    = 1
0.00.054.774 I ggml_metal_init: allocating
0.00.054.777 I ggml_metal_init: found device: Apple M4
0.00.054.780 I ggml_metal_init: picking default device: Apple M4
0.00.055.341 I ggml_metal_init: using embedded metal library
0.00.057.274 I ggml_metal_init: GPU name:   Apple M4
0.00.057.276 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.276 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.276 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.276 I ggml_metal_init: simdgroup reduction   = true
0.00.057.277 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.277 I ggml_metal_init: has bfloat            = true
0.00.057.277 I ggml_metal_init: use bfloat            = true
0.00.057.277 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.279 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.780 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.788 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.808 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.897 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.898 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.899 I llama_new_context_with_model: graph nodes  = 967
0.00.086.899 I llama_new_context_with_model: graph splits = 2
0.00.086.912 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.376 I main: llama threadpool init, n_threads = 4
0.00.759.412 I 
0.00.759.452 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.759.452 I 
0.00.759.620 I sampler seed: 1234
0.00.759.624 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.639 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.640 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.640 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.550.583 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.550.585 I llama_perf_context_print:        load time =     749.20 ms
0.01.550.585 I llama_perf_context_print: prompt eval time =      36.69 ms /     7 tokens (    5.24 ms per token,   190.78 tokens per second)
0.01.550.586 I llama_perf_context_print:        eval time =     751.15 ms /    63 runs   (   11.92 ms per token,    83.87 tokens per second)
0.01.550.587 I llama_perf_context_print:       total time =     791.21 ms /    70 tokens
0.01.550.751 I ggml_metal_free: deallocating

real	0m1.569s
user	0m0.109s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.979 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.913 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.918 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.924 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.924 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.925 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.925 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.925 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.926 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.926 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.927 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.927 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.927 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.928 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.928 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.930 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.930 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.930 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.092 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.122 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.123 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.123 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.124 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.124 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.124 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.125 I llama_model_loader: - type  f32:  194 tensors
0.00.026.125 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.126 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.983 I llm_load_vocab: special tokens cache size = 25
0.00.052.934 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.936 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.937 I llm_load_print_meta: arch             = gptneox
0.00.052.937 I llm_load_print_meta: vocab type       = BPE
0.00.052.937 I llm_load_print_meta: n_vocab          = 50304
0.00.052.938 I llm_load_print_meta: n_merges         = 50009
0.00.052.938 I llm_load_print_meta: vocab_only       = 0
0.00.052.938 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.938 I llm_load_print_meta: n_embd           = 2048
0.00.052.938 I llm_load_print_meta: n_layer          = 24
0.00.052.941 I llm_load_print_meta: n_head           = 16
0.00.052.942 I llm_load_print_meta: n_head_kv        = 16
0.00.052.942 I llm_load_print_meta: n_rot            = 32
0.00.052.942 I llm_load_print_meta: n_swa            = 0
0.00.052.943 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.944 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.944 I llm_load_print_meta: n_gqa            = 1
0.00.052.945 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.946 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.946 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.947 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.947 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.947 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.947 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.948 I llm_load_print_meta: n_ff             = 8192
0.00.052.948 I llm_load_print_meta: n_expert         = 0
0.00.052.948 I llm_load_print_meta: n_expert_used    = 0
0.00.052.948 I llm_load_print_meta: causal attn      = 1
0.00.052.948 I llm_load_print_meta: pooling type     = 0
0.00.052.949 I llm_load_print_meta: rope type        = 2
0.00.052.949 I llm_load_print_meta: rope scaling     = linear
0.00.052.951 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.952 I llm_load_print_meta: freq_scale_train = 1
0.00.052.952 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.952 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.952 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.952 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.952 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.953 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.953 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.960 I llm_load_print_meta: model type       = 1.4B
0.00.052.960 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.960 I llm_load_print_meta: model params     = 1.41 B
0.00.052.961 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.961 I llm_load_print_meta: general.name     = 1.4B
0.00.052.961 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.961 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.962 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.962 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.962 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.962 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.964 I llm_load_print_meta: max token length = 1024
0.00.054.739 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.739 I llm_load_tensors: offloading output layer to GPU
0.00.054.739 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.744 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.745 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.634 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.635 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.635 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.636 I llama_new_context_with_model: n_batch       = 2048
0.00.055.636 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.636 I llama_new_context_with_model: flash_attn    = 0
0.00.055.636 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.637 I llama_new_context_with_model: freq_scale    = 1
0.00.055.637 I ggml_metal_init: allocating
0.00.055.643 I ggml_metal_init: found device: Apple M4
0.00.055.645 I ggml_metal_init: picking default device: Apple M4
0.00.056.196 I ggml_metal_init: using embedded metal library
0.00.058.126 I ggml_metal_init: GPU name:   Apple M4
0.00.058.128 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.128 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.129 I ggml_metal_init: simdgroup reduction   = true
0.00.058.129 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.129 I ggml_metal_init: has bfloat            = true
0.00.058.129 I ggml_metal_init: use bfloat            = true
0.00.058.129 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.130 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.347 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.352 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.371 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.458 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.459 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.460 I llama_new_context_with_model: graph nodes  = 967
0.00.086.460 I llama_new_context_with_model: graph splits = 2
0.00.086.472 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.813.986 I main: llama threadpool init, n_threads = 4
0.00.814.025 I 
0.00.814.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.814.056 I 
0.00.814.276 I sampler seed: 1234
0.00.814.280 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.814.295 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.814.295 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.814.295 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.656.230 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.656.230 I llama_perf_context_print:        load time =     804.00 ms
0.01.656.231 I llama_perf_context_print: prompt eval time =      36.59 ms /     7 tokens (    5.23 ms per token,   191.33 tokens per second)
0.01.656.232 I llama_perf_context_print:        eval time =     802.29 ms /    63 runs   (   12.73 ms per token,    78.53 tokens per second)
0.01.656.232 I llama_perf_context_print:       total time =     842.25 ms /    70 tokens
0.01.656.407 I ggml_metal_free: deallocating

real	0m1.677s
user	0m0.110s
sys	0m0.177s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.879 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.485 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.489 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.491 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.494 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.494 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.495 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.495 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.496 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.496 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.497 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.497 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.499 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.499 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.499 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.604 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.504 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.506 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.507 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.507 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.508 I llama_model_loader: - type  f32:  194 tensors
0.00.024.508 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.508 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.508 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.021 I llm_load_vocab: special tokens cache size = 25
0.00.050.918 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.920 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.921 I llm_load_print_meta: arch             = gptneox
0.00.050.921 I llm_load_print_meta: vocab type       = BPE
0.00.050.921 I llm_load_print_meta: n_vocab          = 50304
0.00.050.921 I llm_load_print_meta: n_merges         = 50009
0.00.050.922 I llm_load_print_meta: vocab_only       = 0
0.00.050.922 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.922 I llm_load_print_meta: n_embd           = 2048
0.00.050.922 I llm_load_print_meta: n_layer          = 24
0.00.050.925 I llm_load_print_meta: n_head           = 16
0.00.050.926 I llm_load_print_meta: n_head_kv        = 16
0.00.050.926 I llm_load_print_meta: n_rot            = 32
0.00.050.926 I llm_load_print_meta: n_swa            = 0
0.00.050.926 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.929 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.930 I llm_load_print_meta: n_gqa            = 1
0.00.050.930 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.931 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.932 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.932 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.933 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.933 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.933 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.934 I llm_load_print_meta: n_ff             = 8192
0.00.050.934 I llm_load_print_meta: n_expert         = 0
0.00.050.934 I llm_load_print_meta: n_expert_used    = 0
0.00.050.934 I llm_load_print_meta: causal attn      = 1
0.00.050.934 I llm_load_print_meta: pooling type     = 0
0.00.050.934 I llm_load_print_meta: rope type        = 2
0.00.050.935 I llm_load_print_meta: rope scaling     = linear
0.00.050.936 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.937 I llm_load_print_meta: freq_scale_train = 1
0.00.050.937 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.937 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.937 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.937 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.938 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.938 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.938 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.949 I llm_load_print_meta: model type       = 1.4B
0.00.050.950 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.950 I llm_load_print_meta: model params     = 1.41 B
0.00.050.951 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.951 I llm_load_print_meta: general.name     = 1.4B
0.00.050.951 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.951 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.951 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.951 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.952 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.953 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.953 I llm_load_print_meta: max token length = 1024
0.00.052.546 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.546 I llm_load_tensors: offloading output layer to GPU
0.00.052.547 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.556 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.557 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.443 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.443 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.444 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.444 I llama_new_context_with_model: n_batch       = 2048
0.00.053.444 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.444 I llama_new_context_with_model: flash_attn    = 0
0.00.053.445 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.445 I llama_new_context_with_model: freq_scale    = 1
0.00.053.445 I ggml_metal_init: allocating
0.00.053.451 I ggml_metal_init: found device: Apple M4
0.00.053.454 I ggml_metal_init: picking default device: Apple M4
0.00.054.009 I ggml_metal_init: using embedded metal library
0.00.055.945 I ggml_metal_init: GPU name:   Apple M4
0.00.055.947 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.947 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.947 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.947 I ggml_metal_init: simdgroup reduction   = true
0.00.055.948 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.948 I ggml_metal_init: has bfloat            = true
0.00.055.948 I ggml_metal_init: use bfloat            = true
0.00.055.948 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.949 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.426 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.432 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.448 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.450 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.452 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.452 I llama_new_context_with_model: graph nodes  = 967
0.00.084.452 I llama_new_context_with_model: graph splits = 2
0.00.084.465 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.518.754 I main: llama threadpool init, n_threads = 4
0.00.518.796 I 
0.00.518.824 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.518.824 I 
0.00.519.058 I sampler seed: 1234
0.00.519.064 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.519.101 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.519.102 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.519.102 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.194.505 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.01.194.505 I llama_perf_context_print:        load time =     508.87 ms
0.01.194.506 I llama_perf_context_print: prompt eval time =      39.42 ms /     7 tokens (    5.63 ms per token,   177.58 tokens per second)
0.01.194.507 I llama_perf_context_print:        eval time =     633.61 ms /    63 runs   (   10.06 ms per token,    99.43 tokens per second)
0.01.194.508 I llama_perf_context_print:       total time =     675.76 ms /    70 tokens
0.01.194.761 I ggml_metal_free: deallocating

real	0m1.215s
user	0m0.108s
sys	0m0.118s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.483 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.821 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.832 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.832 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.833 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.833 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.833 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.834 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.835 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.835 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.837 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.837 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.837 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.838 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.839 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.840 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.960 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.034 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.135 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.137 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.137 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.137 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.138 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.138 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.138 I llama_model_loader: - type  f32:  194 tensors
0.00.025.139 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.139 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.139 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.139 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.255 I llm_load_vocab: special tokens cache size = 25
0.00.052.141 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.144 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.144 I llm_load_print_meta: arch             = gptneox
0.00.052.144 I llm_load_print_meta: vocab type       = BPE
0.00.052.145 I llm_load_print_meta: n_vocab          = 50304
0.00.052.145 I llm_load_print_meta: n_merges         = 50009
0.00.052.145 I llm_load_print_meta: vocab_only       = 0
0.00.052.145 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.145 I llm_load_print_meta: n_embd           = 2048
0.00.052.146 I llm_load_print_meta: n_layer          = 24
0.00.052.149 I llm_load_print_meta: n_head           = 16
0.00.052.150 I llm_load_print_meta: n_head_kv        = 16
0.00.052.152 I llm_load_print_meta: n_rot            = 32
0.00.052.152 I llm_load_print_meta: n_swa            = 0
0.00.052.152 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.152 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.153 I llm_load_print_meta: n_gqa            = 1
0.00.052.154 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.154 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.155 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.155 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.155 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.155 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.156 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.156 I llm_load_print_meta: n_ff             = 8192
0.00.052.156 I llm_load_print_meta: n_expert         = 0
0.00.052.157 I llm_load_print_meta: n_expert_used    = 0
0.00.052.157 I llm_load_print_meta: causal attn      = 1
0.00.052.157 I llm_load_print_meta: pooling type     = 0
0.00.052.158 I llm_load_print_meta: rope type        = 2
0.00.052.162 I llm_load_print_meta: rope scaling     = linear
0.00.052.162 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.163 I llm_load_print_meta: freq_scale_train = 1
0.00.052.168 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.170 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.171 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.171 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.171 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.171 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.171 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.184 I llm_load_print_meta: model type       = 1.4B
0.00.052.184 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.185 I llm_load_print_meta: model params     = 1.41 B
0.00.052.185 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.185 I llm_load_print_meta: general.name     = 1.4B
0.00.052.186 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.186 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.187 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.187 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.188 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.188 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.188 I llm_load_print_meta: max token length = 1024
0.00.054.136 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.136 I llm_load_tensors: offloading output layer to GPU
0.00.054.136 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.146 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.147 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.049 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.049 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.050 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.050 I llama_new_context_with_model: n_batch       = 2048
0.00.055.050 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.050 I llama_new_context_with_model: flash_attn    = 0
0.00.055.051 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.051 I llama_new_context_with_model: freq_scale    = 1
0.00.055.051 I ggml_metal_init: allocating
0.00.055.055 I ggml_metal_init: found device: Apple M4
0.00.055.057 I ggml_metal_init: picking default device: Apple M4
0.00.055.630 I ggml_metal_init: using embedded metal library
0.00.057.593 I ggml_metal_init: GPU name:   Apple M4
0.00.057.595 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.595 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.596 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.596 I ggml_metal_init: simdgroup reduction   = true
0.00.057.596 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.596 I ggml_metal_init: has bfloat            = true
0.00.057.596 I ggml_metal_init: use bfloat            = true
0.00.057.597 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.597 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.947 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.959 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.976 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.997 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.998 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.999 I llama_new_context_with_model: graph nodes  = 967
0.00.085.999 I llama_new_context_with_model: graph splits = 2
0.00.086.012 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.350 I main: llama threadpool init, n_threads = 4
0.00.576.390 I 
0.00.576.418 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.576.420 I 
0.00.576.644 I sampler seed: 1234
0.00.576.649 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.576.694 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.576.696 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.576.696 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.320.857 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57443.37 tokens per second)
0.01.320.857 I llama_perf_context_print:        load time =     566.86 ms
0.01.320.858 I llama_perf_context_print: prompt eval time =      35.64 ms /     7 tokens (    5.09 ms per token,   196.42 tokens per second)
0.01.320.859 I llama_perf_context_print:        eval time =     705.47 ms /    63 runs   (   11.20 ms per token,    89.30 tokens per second)
0.01.320.859 I llama_perf_context_print:       total time =     744.51 ms /    70 tokens
0.01.321.034 I ggml_metal_free: deallocating

real	0m1.341s
user	0m0.110s
sys	0m0.133s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.818 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.159 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.164 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.170 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.170 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.171 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.171 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.172 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.172 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.173 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.173 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.173 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.416 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.493 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.572 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.574 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.575 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.575 I llama_model_loader: - type  f32:  194 tensors
0.00.024.576 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.576 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.576 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.759 I llm_load_vocab: special tokens cache size = 25
0.00.050.621 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.623 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.624 I llm_load_print_meta: arch             = gptneox
0.00.050.624 I llm_load_print_meta: vocab type       = BPE
0.00.050.624 I llm_load_print_meta: n_vocab          = 50304
0.00.050.625 I llm_load_print_meta: n_merges         = 50009
0.00.050.625 I llm_load_print_meta: vocab_only       = 0
0.00.050.625 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.625 I llm_load_print_meta: n_embd           = 2048
0.00.050.625 I llm_load_print_meta: n_layer          = 24
0.00.050.628 I llm_load_print_meta: n_head           = 16
0.00.050.629 I llm_load_print_meta: n_head_kv        = 16
0.00.050.629 I llm_load_print_meta: n_rot            = 32
0.00.050.629 I llm_load_print_meta: n_swa            = 0
0.00.050.630 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.632 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.633 I llm_load_print_meta: n_gqa            = 1
0.00.050.634 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.634 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.635 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.637 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.638 I llm_load_print_meta: n_ff             = 8192
0.00.050.638 I llm_load_print_meta: n_expert         = 0
0.00.050.638 I llm_load_print_meta: n_expert_used    = 0
0.00.050.638 I llm_load_print_meta: causal attn      = 1
0.00.050.638 I llm_load_print_meta: pooling type     = 0
0.00.050.639 I llm_load_print_meta: rope type        = 2
0.00.050.639 I llm_load_print_meta: rope scaling     = linear
0.00.050.639 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.640 I llm_load_print_meta: freq_scale_train = 1
0.00.050.640 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.640 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.640 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.640 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.640 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.641 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.641 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.648 I llm_load_print_meta: model type       = 1.4B
0.00.050.648 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.649 I llm_load_print_meta: model params     = 1.41 B
0.00.050.649 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.649 I llm_load_print_meta: general.name     = 1.4B
0.00.050.650 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.650 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.650 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.650 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.650 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.651 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.651 I llm_load_print_meta: max token length = 1024
0.00.052.447 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.447 I llm_load_tensors: offloading output layer to GPU
0.00.052.447 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.452 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.452 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.484 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.485 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.485 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.485 I llama_new_context_with_model: n_batch       = 2048
0.00.053.485 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.485 I llama_new_context_with_model: flash_attn    = 0
0.00.053.486 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.486 I llama_new_context_with_model: freq_scale    = 1
0.00.053.486 I ggml_metal_init: allocating
0.00.053.490 I ggml_metal_init: found device: Apple M4
0.00.053.492 I ggml_metal_init: picking default device: Apple M4
0.00.054.046 I ggml_metal_init: using embedded metal library
0.00.055.945 I ggml_metal_init: GPU name:   Apple M4
0.00.055.946 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.947 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.947 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.947 I ggml_metal_init: simdgroup reduction   = true
0.00.055.947 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.947 I ggml_metal_init: has bfloat            = true
0.00.055.948 I ggml_metal_init: use bfloat            = true
0.00.055.948 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.949 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.630 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.637 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.657 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.680 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.681 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.681 I llama_new_context_with_model: graph nodes  = 967
0.00.083.682 I llama_new_context_with_model: graph splits = 2
0.00.083.695 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.437 I main: llama threadpool init, n_threads = 4
0.00.614.473 I 
0.00.614.501 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.614.502 I 
0.00.614.653 I sampler seed: 1234
0.00.614.658 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.702 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.713 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.713 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.371.697 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.01.371.698 I llama_perf_context_print:        load time =     605.62 ms
0.01.371.699 I llama_perf_context_print: prompt eval time =      36.36 ms /     7 tokens (    5.19 ms per token,   192.54 tokens per second)
0.01.371.699 I llama_perf_context_print:        eval time =     717.59 ms /    63 runs   (   11.39 ms per token,    87.79 tokens per second)
0.01.371.700 I llama_perf_context_print:       total time =     757.26 ms /    70 tokens
0.01.371.872 I ggml_metal_free: deallocating

real	0m1.389s
user	0m0.109s
sys	0m0.133s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.255 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.532 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.538 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.539 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.540 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.540 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.540 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.542 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.542 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.542 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.543 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.543 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.544 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.544 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.546 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.547 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.547 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.740 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.018 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.019 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.020 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.021 I llama_model_loader: - type  f32:  194 tensors
0.00.026.021 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.022 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.955 I llm_load_vocab: special tokens cache size = 25
0.00.052.888 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.891 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.891 I llm_load_print_meta: arch             = gptneox
0.00.052.892 I llm_load_print_meta: vocab type       = BPE
0.00.052.892 I llm_load_print_meta: n_vocab          = 50304
0.00.052.892 I llm_load_print_meta: n_merges         = 50009
0.00.052.892 I llm_load_print_meta: vocab_only       = 0
0.00.052.893 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.893 I llm_load_print_meta: n_embd           = 2048
0.00.052.893 I llm_load_print_meta: n_layer          = 24
0.00.052.896 I llm_load_print_meta: n_head           = 16
0.00.052.897 I llm_load_print_meta: n_head_kv        = 16
0.00.052.897 I llm_load_print_meta: n_rot            = 32
0.00.052.900 I llm_load_print_meta: n_swa            = 0
0.00.052.900 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.900 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.901 I llm_load_print_meta: n_gqa            = 1
0.00.052.902 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.902 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.903 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.903 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.903 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.903 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.911 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.915 I llm_load_print_meta: n_ff             = 8192
0.00.052.915 I llm_load_print_meta: n_expert         = 0
0.00.052.915 I llm_load_print_meta: n_expert_used    = 0
0.00.052.917 I llm_load_print_meta: causal attn      = 1
0.00.052.918 I llm_load_print_meta: pooling type     = 0
0.00.052.918 I llm_load_print_meta: rope type        = 2
0.00.052.918 I llm_load_print_meta: rope scaling     = linear
0.00.052.919 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.919 I llm_load_print_meta: freq_scale_train = 1
0.00.052.919 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.919 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.919 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.919 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.920 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.920 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.920 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.932 I llm_load_print_meta: model type       = 1.4B
0.00.052.933 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.933 I llm_load_print_meta: model params     = 1.41 B
0.00.052.934 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.934 I llm_load_print_meta: general.name     = 1.4B
0.00.052.934 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.934 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.934 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.935 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.935 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.935 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.935 I llm_load_print_meta: max token length = 1024
0.00.054.927 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.928 I llm_load_tensors: offloading output layer to GPU
0.00.054.928 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.938 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.939 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.766 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.767 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.767 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.767 I llama_new_context_with_model: n_batch       = 2048
0.00.055.767 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.767 I llama_new_context_with_model: flash_attn    = 0
0.00.055.768 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.768 I llama_new_context_with_model: freq_scale    = 1
0.00.055.768 I ggml_metal_init: allocating
0.00.055.773 I ggml_metal_init: found device: Apple M4
0.00.055.775 I ggml_metal_init: picking default device: Apple M4
0.00.056.334 I ggml_metal_init: using embedded metal library
0.00.058.253 I ggml_metal_init: GPU name:   Apple M4
0.00.058.255 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.257 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.257 I ggml_metal_init: simdgroup reduction   = true
0.00.058.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.257 I ggml_metal_init: has bfloat            = true
0.00.058.257 I ggml_metal_init: use bfloat            = true
0.00.058.258 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.649 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.657 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.675 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.665 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.667 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.667 I llama_new_context_with_model: graph nodes  = 967
0.00.086.667 I llama_new_context_with_model: graph splits = 2
0.00.086.680 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.709 I main: llama threadpool init, n_threads = 4
0.00.704.751 I 
0.00.704.780 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.704.781 I 
0.00.705.024 I sampler seed: 1234
0.00.705.029 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.705.044 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.705.046 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.705.046 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.550.818 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.550.819 I llama_perf_context_print:        load time =     694.45 ms
0.01.550.819 I llama_perf_context_print: prompt eval time =      42.24 ms /     7 tokens (    6.03 ms per token,   165.71 tokens per second)
0.01.550.820 I llama_perf_context_print:        eval time =     800.61 ms /    63 runs   (   12.71 ms per token,    78.69 tokens per second)
0.01.550.821 I llama_perf_context_print:       total time =     846.11 ms /    70 tokens
0.01.550.998 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.109s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.937 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.290 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.295 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.296 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.297 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.297 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.299 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.299 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.300 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.300 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.301 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.301 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.302 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.302 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.304 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.304 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.400 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.570 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.572 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.572 I llama_model_loader: - type  f32:  194 tensors
0.00.025.572 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.733 I llm_load_vocab: special tokens cache size = 25
0.00.051.619 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.622 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.623 I llm_load_print_meta: arch             = gptneox
0.00.051.623 I llm_load_print_meta: vocab type       = BPE
0.00.051.623 I llm_load_print_meta: n_vocab          = 50304
0.00.051.623 I llm_load_print_meta: n_merges         = 50009
0.00.051.624 I llm_load_print_meta: vocab_only       = 0
0.00.051.624 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.624 I llm_load_print_meta: n_embd           = 2048
0.00.051.624 I llm_load_print_meta: n_layer          = 24
0.00.051.627 I llm_load_print_meta: n_head           = 16
0.00.051.628 I llm_load_print_meta: n_head_kv        = 16
0.00.051.628 I llm_load_print_meta: n_rot            = 32
0.00.051.628 I llm_load_print_meta: n_swa            = 0
0.00.051.631 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.631 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.632 I llm_load_print_meta: n_gqa            = 1
0.00.051.632 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.633 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.634 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.634 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.634 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.635 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.635 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.635 I llm_load_print_meta: n_ff             = 8192
0.00.051.635 I llm_load_print_meta: n_expert         = 0
0.00.051.636 I llm_load_print_meta: n_expert_used    = 0
0.00.051.636 I llm_load_print_meta: causal attn      = 1
0.00.051.637 I llm_load_print_meta: pooling type     = 0
0.00.051.638 I llm_load_print_meta: rope type        = 2
0.00.051.638 I llm_load_print_meta: rope scaling     = linear
0.00.051.638 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.639 I llm_load_print_meta: freq_scale_train = 1
0.00.051.639 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.639 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.639 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.639 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.639 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.640 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.640 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.651 I llm_load_print_meta: model type       = 1.4B
0.00.051.651 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.652 I llm_load_print_meta: model params     = 1.41 B
0.00.051.653 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.654 I llm_load_print_meta: general.name     = 1.4B
0.00.051.654 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.654 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.654 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.654 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.654 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.655 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.655 I llm_load_print_meta: max token length = 1024
0.00.053.253 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.254 I llm_load_tensors: offloading output layer to GPU
0.00.053.254 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.263 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.265 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.101 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.102 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.102 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.102 I llama_new_context_with_model: n_batch       = 2048
0.00.054.103 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.103 I llama_new_context_with_model: flash_attn    = 0
0.00.054.103 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.103 I llama_new_context_with_model: freq_scale    = 1
0.00.054.104 I ggml_metal_init: allocating
0.00.054.107 I ggml_metal_init: found device: Apple M4
0.00.054.109 I ggml_metal_init: picking default device: Apple M4
0.00.054.647 I ggml_metal_init: using embedded metal library
0.00.056.590 I ggml_metal_init: GPU name:   Apple M4
0.00.056.592 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.592 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.593 I ggml_metal_init: simdgroup reduction   = true
0.00.056.594 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.594 I ggml_metal_init: has bfloat            = true
0.00.056.594 I ggml_metal_init: use bfloat            = true
0.00.056.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.474 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.481 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.501 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.566 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.567 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.568 I llama_new_context_with_model: graph nodes  = 967
0.00.086.568 I llama_new_context_with_model: graph splits = 2
0.00.086.582 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.829 I main: llama threadpool init, n_threads = 4
0.00.769.890 I 
0.00.769.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.769.915 I 
0.00.770.150 I sampler seed: 1234
0.00.770.155 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.170 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.171 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.171 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.643.761 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.01.643.761 I llama_perf_context_print:        load time =     759.89 ms
0.01.643.762 I llama_perf_context_print: prompt eval time =      38.49 ms /     7 tokens (    5.50 ms per token,   181.88 tokens per second)
0.01.643.763 I llama_perf_context_print:        eval time =     832.10 ms /    63 runs   (   13.21 ms per token,    75.71 tokens per second)
0.01.643.764 I llama_perf_context_print:       total time =     873.93 ms /    70 tokens
0.01.643.940 I ggml_metal_free: deallocating

real	0m1.664s
user	0m0.109s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.684 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.993 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.490 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.499 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.508 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.510 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.510 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.511 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.512 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.513 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.514 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.514 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.515 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.515 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.516 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.517 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.519 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.520 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.520 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.225 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.343 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.345 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.345 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.346 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.346 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.347 I llama_model_loader: - type  f32:  194 tensors
0.00.056.347 I llama_model_loader: - type  f16:   98 tensors
0.00.084.011 I llm_load_vocab: special tokens cache size = 25
0.00.090.633 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.636 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.636 I llm_load_print_meta: arch             = gptneox
0.00.090.637 I llm_load_print_meta: vocab type       = BPE
0.00.090.637 I llm_load_print_meta: n_vocab          = 50304
0.00.090.637 I llm_load_print_meta: n_merges         = 50009
0.00.090.637 I llm_load_print_meta: vocab_only       = 0
0.00.090.637 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.637 I llm_load_print_meta: n_embd           = 2048
0.00.090.638 I llm_load_print_meta: n_layer          = 24
0.00.090.640 I llm_load_print_meta: n_head           = 16
0.00.090.641 I llm_load_print_meta: n_head_kv        = 16
0.00.090.641 I llm_load_print_meta: n_rot            = 32
0.00.090.641 I llm_load_print_meta: n_swa            = 0
0.00.090.642 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.643 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.643 I llm_load_print_meta: n_gqa            = 1
0.00.090.644 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.645 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.645 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.646 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.646 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.646 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.646 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.647 I llm_load_print_meta: n_ff             = 8192
0.00.090.647 I llm_load_print_meta: n_expert         = 0
0.00.090.647 I llm_load_print_meta: n_expert_used    = 0
0.00.090.647 I llm_load_print_meta: causal attn      = 1
0.00.090.647 I llm_load_print_meta: pooling type     = 0
0.00.090.647 I llm_load_print_meta: rope type        = 2
0.00.090.648 I llm_load_print_meta: rope scaling     = linear
0.00.090.648 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.648 I llm_load_print_meta: freq_scale_train = 1
0.00.090.648 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.649 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.649 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.649 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.649 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.650 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.651 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.662 I llm_load_print_meta: model type       = 1.4B
0.00.090.663 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.663 I llm_load_print_meta: model params     = 1.41 B
0.00.090.664 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.664 I llm_load_print_meta: general.name     = 1.4B
0.00.090.665 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.666 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.666 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.666 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.666 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.666 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.666 I llm_load_print_meta: max token length = 1024
0.00.093.157 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.157 I llm_load_tensors: offloading output layer to GPU
0.00.093.157 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.167 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.168 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.160 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.160 I llama_new_context_with_model: n_ctx         = 128
0.00.094.161 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.161 I llama_new_context_with_model: n_batch       = 128
0.00.094.161 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.161 I llama_new_context_with_model: flash_attn    = 0
0.00.094.162 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.162 I llama_new_context_with_model: freq_scale    = 1
0.00.094.162 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.162 I ggml_metal_init: allocating
0.00.094.165 I ggml_metal_init: found device: Apple M4
0.00.094.167 I ggml_metal_init: picking default device: Apple M4
0.00.094.737 I ggml_metal_init: using embedded metal library
0.00.096.802 I ggml_metal_init: GPU name:   Apple M4
0.00.096.803 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.803 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.804 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.804 I ggml_metal_init: simdgroup reduction   = true
0.00.096.804 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.804 I ggml_metal_init: has bfloat            = true
0.00.096.805 I ggml_metal_init: use bfloat            = true
0.00.096.805 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.807 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.644 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.648 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.662 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.516 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.517 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.517 I llama_new_context_with_model: graph nodes  = 967
0.00.107.517 I llama_new_context_with_model: graph splits = 2
0.00.107.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.767 I 
0.00.793.855 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.793.866 I perplexity: tokenizing the input ..
0.00.806.759 I perplexity: tokenization took 12.884 ms
0.00.806.766 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.927.718 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.929.293 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.929.340 I llama_perf_context_print:        load time =     767.74 ms
0.00.929.341 I llama_perf_context_print: prompt eval time =     120.01 ms /   128 tokens (    0.94 ms per token,  1066.55 tokens per second)
0.00.929.342 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.929.343 I llama_perf_context_print:       total time =     135.59 ms /   129 tokens
0.00.930.076 I ggml_metal_free: deallocating

real	0m1.124s
user	0m0.122s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.142 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.703 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.709 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.715 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.716 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.716 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.716 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.717 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.723 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.723 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.724 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.724 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.724 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.726 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.728 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.729 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.729 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.989 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.369 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.616 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.617 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.618 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.619 I llama_model_loader: - type  f32:  194 tensors
0.00.029.620 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.369 I llm_load_vocab: special tokens cache size = 25
0.00.059.392 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.395 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.395 I llm_load_print_meta: arch             = gptneox
0.00.059.395 I llm_load_print_meta: vocab type       = BPE
0.00.059.395 I llm_load_print_meta: n_vocab          = 50304
0.00.059.396 I llm_load_print_meta: n_merges         = 50009
0.00.059.396 I llm_load_print_meta: vocab_only       = 0
0.00.059.396 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.396 I llm_load_print_meta: n_embd           = 2048
0.00.059.396 I llm_load_print_meta: n_layer          = 24
0.00.059.400 I llm_load_print_meta: n_head           = 16
0.00.059.400 I llm_load_print_meta: n_head_kv        = 16
0.00.059.401 I llm_load_print_meta: n_rot            = 32
0.00.059.401 I llm_load_print_meta: n_swa            = 0
0.00.059.401 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.401 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.402 I llm_load_print_meta: n_gqa            = 1
0.00.059.403 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.403 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.404 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.404 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.404 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.404 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.405 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.405 I llm_load_print_meta: n_ff             = 8192
0.00.059.405 I llm_load_print_meta: n_expert         = 0
0.00.059.406 I llm_load_print_meta: n_expert_used    = 0
0.00.059.406 I llm_load_print_meta: causal attn      = 1
0.00.059.406 I llm_load_print_meta: pooling type     = 0
0.00.059.406 I llm_load_print_meta: rope type        = 2
0.00.059.408 I llm_load_print_meta: rope scaling     = linear
0.00.059.409 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.409 I llm_load_print_meta: freq_scale_train = 1
0.00.059.409 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.409 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.410 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.410 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.410 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.410 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.410 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.422 I llm_load_print_meta: model type       = 1.4B
0.00.059.422 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.422 I llm_load_print_meta: model params     = 1.41 B
0.00.059.423 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.423 I llm_load_print_meta: general.name     = 1.4B
0.00.059.423 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.423 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.424 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.424 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.424 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.425 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.425 I llm_load_print_meta: max token length = 1024
0.00.061.524 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.524 I llm_load_tensors: offloading output layer to GPU
0.00.061.525 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.535 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.535 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.475 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.475 I llama_new_context_with_model: n_ctx         = 128
0.00.062.476 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.476 I llama_new_context_with_model: n_batch       = 128
0.00.062.476 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.476 I llama_new_context_with_model: flash_attn    = 0
0.00.062.476 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.477 I llama_new_context_with_model: freq_scale    = 1
0.00.062.477 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.477 I ggml_metal_init: allocating
0.00.062.481 I ggml_metal_init: found device: Apple M4
0.00.062.483 I ggml_metal_init: picking default device: Apple M4
0.00.063.034 I ggml_metal_init: using embedded metal library
0.00.064.931 I ggml_metal_init: GPU name:   Apple M4
0.00.064.933 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.933 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.934 I ggml_metal_init: simdgroup reduction   = true
0.00.064.934 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.934 I ggml_metal_init: has bfloat            = true
0.00.064.934 I ggml_metal_init: use bfloat            = true
0.00.064.935 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.935 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.857 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.073.859 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.073.873 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.074.787 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.074.788 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.074.788 I llama_new_context_with_model: graph nodes  = 967
0.00.074.788 I llama_new_context_with_model: graph splits = 2
0.00.074.801 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.893 I 
0.00.788.923 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.788.930 I perplexity: tokenizing the input ..
0.00.796.886 I perplexity: tokenization took 7.954 ms
0.00.796.889 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.918.976 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.920.213 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.920.239 I llama_perf_context_print:        load time =     778.75 ms
0.00.920.240 I llama_perf_context_print: prompt eval time =     121.86 ms /   128 tokens (    0.95 ms per token,  1050.36 tokens per second)
0.00.920.241 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.920.242 I llama_perf_context_print:       total time =     131.35 ms /   129 tokens
0.00.920.678 I ggml_metal_free: deallocating

real	0m0.938s
user	0m0.087s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.587 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.480 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.485 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.491 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.492 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.492 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.494 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.495 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.495 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.496 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.500 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.501 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.502 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.502 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.447 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.510 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.406 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.407 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.407 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.408 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.408 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.408 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.409 I llama_model_loader: - type  f32:  194 tensors
0.00.024.409 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.409 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.456 I llm_load_vocab: special tokens cache size = 25
0.00.050.288 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.291 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.291 I llm_load_print_meta: arch             = gptneox
0.00.050.292 I llm_load_print_meta: vocab type       = BPE
0.00.050.292 I llm_load_print_meta: n_vocab          = 50304
0.00.050.292 I llm_load_print_meta: n_merges         = 50009
0.00.050.292 I llm_load_print_meta: vocab_only       = 0
0.00.050.292 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.292 I llm_load_print_meta: n_embd           = 2048
0.00.050.293 I llm_load_print_meta: n_layer          = 24
0.00.050.296 I llm_load_print_meta: n_head           = 16
0.00.050.297 I llm_load_print_meta: n_head_kv        = 16
0.00.050.297 I llm_load_print_meta: n_rot            = 32
0.00.050.297 I llm_load_print_meta: n_swa            = 0
0.00.050.297 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.297 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.298 I llm_load_print_meta: n_gqa            = 1
0.00.050.299 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.299 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.300 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.300 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.300 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.301 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.301 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.301 I llm_load_print_meta: n_ff             = 8192
0.00.050.302 I llm_load_print_meta: n_expert         = 0
0.00.050.302 I llm_load_print_meta: n_expert_used    = 0
0.00.050.302 I llm_load_print_meta: causal attn      = 1
0.00.050.302 I llm_load_print_meta: pooling type     = 0
0.00.050.302 I llm_load_print_meta: rope type        = 2
0.00.050.302 I llm_load_print_meta: rope scaling     = linear
0.00.050.303 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.305 I llm_load_print_meta: freq_scale_train = 1
0.00.050.306 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.306 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.306 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.306 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.306 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.306 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.306 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.318 I llm_load_print_meta: model type       = 1.4B
0.00.050.319 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.319 I llm_load_print_meta: model params     = 1.41 B
0.00.050.320 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.320 I llm_load_print_meta: general.name     = 1.4B
0.00.050.320 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.320 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.320 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.321 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.321 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.321 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.321 I llm_load_print_meta: max token length = 1024
0.00.052.235 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.235 I llm_load_tensors: offloading output layer to GPU
0.00.052.235 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.246 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.247 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.274 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.275 I llama_new_context_with_model: n_ctx         = 128
0.00.053.275 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.275 I llama_new_context_with_model: n_batch       = 128
0.00.053.275 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.276 I llama_new_context_with_model: flash_attn    = 0
0.00.053.276 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.276 I llama_new_context_with_model: freq_scale    = 1
0.00.053.277 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.277 I ggml_metal_init: allocating
0.00.053.280 I ggml_metal_init: found device: Apple M4
0.00.053.281 I ggml_metal_init: picking default device: Apple M4
0.00.053.827 I ggml_metal_init: using embedded metal library
0.00.055.794 I ggml_metal_init: GPU name:   Apple M4
0.00.055.795 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.795 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.796 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.796 I ggml_metal_init: simdgroup reduction   = true
0.00.055.796 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.796 I ggml_metal_init: has bfloat            = true
0.00.055.796 I ggml_metal_init: use bfloat            = true
0.00.055.797 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.797 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.531 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.533 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.546 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.547 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.548 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.548 I llama_new_context_with_model: graph nodes  = 967
0.00.066.548 I llama_new_context_with_model: graph splits = 2
0.00.066.561 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.861 I 
0.00.611.887 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.611.890 I perplexity: tokenizing the input ..
0.00.619.306 I perplexity: tokenization took 7.414 ms
0.00.619.309 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.742.072 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.743.368 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.743.407 I llama_perf_context_print:        load time =     602.27 ms
0.00.743.408 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.55 tokens per second)
0.00.743.409 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.410 I llama_perf_context_print:       total time =     131.55 ms /   129 tokens
0.00.743.907 I ggml_metal_free: deallocating

real	0m0.760s
user	0m0.076s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.937 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.737 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.740 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.741 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.741 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.743 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.746 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.831 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.881 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.029 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.030 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.031 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.031 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.032 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.032 I llama_model_loader: - type  f32:  194 tensors
0.00.025.033 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.033 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.130 I llm_load_vocab: special tokens cache size = 25
0.00.050.897 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.900 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.900 I llm_load_print_meta: arch             = gptneox
0.00.050.901 I llm_load_print_meta: vocab type       = BPE
0.00.050.901 I llm_load_print_meta: n_vocab          = 50304
0.00.050.901 I llm_load_print_meta: n_merges         = 50009
0.00.050.901 I llm_load_print_meta: vocab_only       = 0
0.00.050.902 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.902 I llm_load_print_meta: n_embd           = 2048
0.00.050.902 I llm_load_print_meta: n_layer          = 24
0.00.050.905 I llm_load_print_meta: n_head           = 16
0.00.050.908 I llm_load_print_meta: n_head_kv        = 16
0.00.050.908 I llm_load_print_meta: n_rot            = 32
0.00.050.908 I llm_load_print_meta: n_swa            = 0
0.00.050.908 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.908 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.909 I llm_load_print_meta: n_gqa            = 1
0.00.050.910 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.911 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.911 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.912 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.912 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.912 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.912 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.917 I llm_load_print_meta: n_ff             = 8192
0.00.050.917 I llm_load_print_meta: n_expert         = 0
0.00.050.917 I llm_load_print_meta: n_expert_used    = 0
0.00.050.917 I llm_load_print_meta: causal attn      = 1
0.00.050.917 I llm_load_print_meta: pooling type     = 0
0.00.050.918 I llm_load_print_meta: rope type        = 2
0.00.050.918 I llm_load_print_meta: rope scaling     = linear
0.00.050.918 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.919 I llm_load_print_meta: freq_scale_train = 1
0.00.050.919 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.921 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.921 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.921 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.921 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.921 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.921 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.933 I llm_load_print_meta: model type       = 1.4B
0.00.050.934 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.934 I llm_load_print_meta: model params     = 1.41 B
0.00.050.934 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.936 I llm_load_print_meta: general.name     = 1.4B
0.00.050.936 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.936 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.936 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.936 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.937 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.937 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.938 I llm_load_print_meta: max token length = 1024
0.00.052.904 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.904 I llm_load_tensors: offloading output layer to GPU
0.00.052.905 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.915 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.916 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.850 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.851 I llama_new_context_with_model: n_ctx         = 128
0.00.053.851 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.851 I llama_new_context_with_model: n_batch       = 128
0.00.053.851 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.851 I llama_new_context_with_model: flash_attn    = 0
0.00.053.852 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.852 I llama_new_context_with_model: freq_scale    = 1
0.00.053.852 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.853 I ggml_metal_init: allocating
0.00.053.856 I ggml_metal_init: found device: Apple M4
0.00.053.858 I ggml_metal_init: picking default device: Apple M4
0.00.054.398 I ggml_metal_init: using embedded metal library
0.00.056.329 I ggml_metal_init: GPU name:   Apple M4
0.00.056.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.331 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.331 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.332 I ggml_metal_init: simdgroup reduction   = true
0.00.056.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.332 I ggml_metal_init: has bfloat            = true
0.00.056.332 I ggml_metal_init: use bfloat            = true
0.00.056.332 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.333 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.622 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.625 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.639 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.549 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.550 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.550 I llama_new_context_with_model: graph nodes  = 967
0.00.066.551 I llama_new_context_with_model: graph splits = 2
0.00.066.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.431 I 
0.00.686.486 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.686.495 I perplexity: tokenizing the input ..
0.00.694.540 I perplexity: tokenization took 8.044 ms
0.00.694.543 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.547 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.818.786 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.818.822 I llama_perf_context_print:        load time =     676.49 ms
0.00.818.823 I llama_perf_context_print: prompt eval time =     122.76 ms /   128 tokens (    0.96 ms per token,  1042.68 tokens per second)
0.00.818.824 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.824 I llama_perf_context_print:       total time =     132.39 ms /   129 tokens
0.00.819.374 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.077s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.278 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.357 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.362 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.364 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.364 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.365 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.365 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.365 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.368 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.368 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.369 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.369 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.369 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.373 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.374 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.375 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.376 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.376 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.462 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.672 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.674 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.674 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.674 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.675 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.675 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.676 I llama_model_loader: - type  f32:  194 tensors
0.00.024.676 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.676 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.598 I llm_load_vocab: special tokens cache size = 25
0.00.051.335 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.338 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.339 I llm_load_print_meta: arch             = gptneox
0.00.051.339 I llm_load_print_meta: vocab type       = BPE
0.00.051.339 I llm_load_print_meta: n_vocab          = 50304
0.00.051.339 I llm_load_print_meta: n_merges         = 50009
0.00.051.340 I llm_load_print_meta: vocab_only       = 0
0.00.051.340 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.340 I llm_load_print_meta: n_embd           = 2048
0.00.051.340 I llm_load_print_meta: n_layer          = 24
0.00.051.343 I llm_load_print_meta: n_head           = 16
0.00.051.344 I llm_load_print_meta: n_head_kv        = 16
0.00.051.344 I llm_load_print_meta: n_rot            = 32
0.00.051.344 I llm_load_print_meta: n_swa            = 0
0.00.051.344 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.344 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.345 I llm_load_print_meta: n_gqa            = 1
0.00.051.346 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.347 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.347 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.348 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.348 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.348 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.348 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.349 I llm_load_print_meta: n_ff             = 8192
0.00.051.349 I llm_load_print_meta: n_expert         = 0
0.00.051.349 I llm_load_print_meta: n_expert_used    = 0
0.00.051.349 I llm_load_print_meta: causal attn      = 1
0.00.051.350 I llm_load_print_meta: pooling type     = 0
0.00.051.350 I llm_load_print_meta: rope type        = 2
0.00.051.350 I llm_load_print_meta: rope scaling     = linear
0.00.051.350 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.351 I llm_load_print_meta: freq_scale_train = 1
0.00.051.351 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.351 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.351 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.352 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.352 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.352 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.352 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.364 I llm_load_print_meta: model type       = 1.4B
0.00.051.364 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.364 I llm_load_print_meta: model params     = 1.41 B
0.00.051.365 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.365 I llm_load_print_meta: general.name     = 1.4B
0.00.051.365 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.367 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.367 I llm_load_print_meta: max token length = 1024
0.00.053.331 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.331 I llm_load_tensors: offloading output layer to GPU
0.00.053.332 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.342 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.343 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.223 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.224 I llama_new_context_with_model: n_ctx         = 128
0.00.054.224 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.225 I llama_new_context_with_model: n_batch       = 128
0.00.054.225 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.225 I llama_new_context_with_model: flash_attn    = 0
0.00.054.225 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.225 I llama_new_context_with_model: freq_scale    = 1
0.00.054.226 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.226 I ggml_metal_init: allocating
0.00.054.229 I ggml_metal_init: found device: Apple M4
0.00.054.231 I ggml_metal_init: picking default device: Apple M4
0.00.054.754 I ggml_metal_init: using embedded metal library
0.00.056.670 I ggml_metal_init: GPU name:   Apple M4
0.00.056.672 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.672 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.673 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.673 I ggml_metal_init: simdgroup reduction   = true
0.00.056.673 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.673 I ggml_metal_init: has bfloat            = true
0.00.056.673 I ggml_metal_init: use bfloat            = true
0.00.056.674 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.535 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.539 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.553 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.452 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.453 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.454 I llama_new_context_with_model: graph nodes  = 967
0.00.066.454 I llama_new_context_with_model: graph splits = 2
0.00.066.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.303 I 
0.00.667.339 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.667.342 I perplexity: tokenizing the input ..
0.00.675.084 I perplexity: tokenization took 7.74 ms
0.00.675.087 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.496 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.811.734 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.811.769 I llama_perf_context_print:        load time =     658.02 ms
0.00.811.770 I llama_perf_context_print: prompt eval time =     135.18 ms /   128 tokens (    1.06 ms per token,   946.90 tokens per second)
0.00.811.770 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.771 I llama_perf_context_print:       total time =     144.47 ms /   129 tokens
0.00.812.231 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.077s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.375 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.325 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.329 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.331 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.331 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.332 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.332 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.332 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.333 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.334 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.334 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.335 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.335 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.335 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.337 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.337 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.338 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.357 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.475 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.515 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.516 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.516 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.516 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.517 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.517 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.518 I llama_model_loader: - type  f32:  194 tensors
0.00.027.518 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.518 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.326 I llm_load_vocab: special tokens cache size = 25
0.00.054.185 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.188 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.188 I llm_load_print_meta: arch             = gptneox
0.00.054.188 I llm_load_print_meta: vocab type       = BPE
0.00.054.189 I llm_load_print_meta: n_vocab          = 50304
0.00.054.189 I llm_load_print_meta: n_merges         = 50009
0.00.054.189 I llm_load_print_meta: vocab_only       = 0
0.00.054.189 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.189 I llm_load_print_meta: n_embd           = 2048
0.00.054.189 I llm_load_print_meta: n_layer          = 24
0.00.054.192 I llm_load_print_meta: n_head           = 16
0.00.054.193 I llm_load_print_meta: n_head_kv        = 16
0.00.054.193 I llm_load_print_meta: n_rot            = 32
0.00.054.193 I llm_load_print_meta: n_swa            = 0
0.00.054.193 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.193 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.194 I llm_load_print_meta: n_gqa            = 1
0.00.054.195 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.197 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.198 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.198 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.198 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.206 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.208 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.212 I llm_load_print_meta: n_ff             = 8192
0.00.054.212 I llm_load_print_meta: n_expert         = 0
0.00.054.212 I llm_load_print_meta: n_expert_used    = 0
0.00.054.212 I llm_load_print_meta: causal attn      = 1
0.00.054.212 I llm_load_print_meta: pooling type     = 0
0.00.054.212 I llm_load_print_meta: rope type        = 2
0.00.054.213 I llm_load_print_meta: rope scaling     = linear
0.00.054.213 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.213 I llm_load_print_meta: freq_scale_train = 1
0.00.054.213 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.214 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.214 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.214 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.214 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.214 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.214 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.226 I llm_load_print_meta: model type       = 1.4B
0.00.054.227 I llm_load_print_meta: model ftype      = Q5_1
0.00.054.227 I llm_load_print_meta: model params     = 1.41 B
0.00.054.228 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.054.228 I llm_load_print_meta: general.name     = 1.4B
0.00.054.228 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.228 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.229 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.229 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.229 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.229 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.229 I llm_load_print_meta: max token length = 1024
0.00.056.302 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.302 I llm_load_tensors: offloading output layer to GPU
0.00.056.302 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.312 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.056.313 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.057.205 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.205 I llama_new_context_with_model: n_ctx         = 128
0.00.057.206 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.057.206 I llama_new_context_with_model: n_batch       = 128
0.00.057.206 I llama_new_context_with_model: n_ubatch      = 128
0.00.057.206 I llama_new_context_with_model: flash_attn    = 0
0.00.057.206 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.207 I llama_new_context_with_model: freq_scale    = 1
0.00.057.207 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.208 I ggml_metal_init: allocating
0.00.057.213 I ggml_metal_init: found device: Apple M4
0.00.057.215 I ggml_metal_init: picking default device: Apple M4
0.00.057.974 I ggml_metal_init: using embedded metal library
0.00.059.906 I ggml_metal_init: GPU name:   Apple M4
0.00.059.908 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.908 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.908 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.909 I ggml_metal_init: simdgroup reduction   = true
0.00.059.909 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.909 I ggml_metal_init: has bfloat            = true
0.00.059.909 I ggml_metal_init: use bfloat            = true
0.00.059.909 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.910 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.960 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.969 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.986 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.846 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.847 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.847 I llama_new_context_with_model: graph nodes  = 967
0.00.069.847 I llama_new_context_with_model: graph splits = 2
0.00.069.860 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.935 I 
0.00.762.974 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.762.983 I perplexity: tokenizing the input ..
0.00.771.051 I perplexity: tokenization took 8.064 ms
0.00.771.054 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.905.170 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.906.606 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.906.629 I llama_perf_context_print:        load time =     750.55 ms
0.00.906.632 I llama_perf_context_print: prompt eval time =     133.88 ms /   128 tokens (    1.05 ms per token,   956.12 tokens per second)
0.00.906.634 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.906.634 I llama_perf_context_print:       total time =     143.70 ms /   129 tokens
0.00.907.000 I ggml_metal_free: deallocating

real	0m0.923s
user	0m0.077s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.906 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.614 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.619 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.621 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.621 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.622 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.622 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.622 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.624 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.624 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.624 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.625 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.625 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.627 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.627 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.627 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.766 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.941 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.941 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.942 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.942 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.943 I llama_model_loader: - type  f32:  194 tensors
0.00.024.943 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.943 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.944 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.474 I llm_load_vocab: special tokens cache size = 25
0.00.051.513 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.517 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.518 I llm_load_print_meta: arch             = gptneox
0.00.051.518 I llm_load_print_meta: vocab type       = BPE
0.00.051.518 I llm_load_print_meta: n_vocab          = 50304
0.00.051.520 I llm_load_print_meta: n_merges         = 50009
0.00.051.520 I llm_load_print_meta: vocab_only       = 0
0.00.051.522 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.522 I llm_load_print_meta: n_embd           = 2048
0.00.051.522 I llm_load_print_meta: n_layer          = 24
0.00.051.526 I llm_load_print_meta: n_head           = 16
0.00.051.526 I llm_load_print_meta: n_head_kv        = 16
0.00.051.526 I llm_load_print_meta: n_rot            = 32
0.00.051.527 I llm_load_print_meta: n_swa            = 0
0.00.051.528 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.528 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.529 I llm_load_print_meta: n_gqa            = 1
0.00.051.529 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.530 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.530 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.535 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.536 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.536 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.536 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.538 I llm_load_print_meta: n_ff             = 8192
0.00.051.538 I llm_load_print_meta: n_expert         = 0
0.00.051.539 I llm_load_print_meta: n_expert_used    = 0
0.00.051.539 I llm_load_print_meta: causal attn      = 1
0.00.051.539 I llm_load_print_meta: pooling type     = 0
0.00.051.539 I llm_load_print_meta: rope type        = 2
0.00.051.539 I llm_load_print_meta: rope scaling     = linear
0.00.051.540 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.540 I llm_load_print_meta: freq_scale_train = 1
0.00.051.540 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.540 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.540 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.540 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.540 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.541 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.542 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.554 I llm_load_print_meta: model type       = 1.4B
0.00.051.554 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.555 I llm_load_print_meta: model params     = 1.41 B
0.00.051.555 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.555 I llm_load_print_meta: general.name     = 1.4B
0.00.051.556 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.556 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.556 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.556 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.556 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.557 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.557 I llm_load_print_meta: max token length = 1024
0.00.053.420 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.420 I llm_load_tensors: offloading output layer to GPU
0.00.053.421 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.431 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.432 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.438 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.439 I llama_new_context_with_model: n_ctx         = 128
0.00.054.439 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.439 I llama_new_context_with_model: n_batch       = 128
0.00.054.439 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.439 I llama_new_context_with_model: flash_attn    = 0
0.00.054.440 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.440 I llama_new_context_with_model: freq_scale    = 1
0.00.054.440 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.441 I ggml_metal_init: allocating
0.00.054.449 I ggml_metal_init: found device: Apple M4
0.00.054.452 I ggml_metal_init: picking default device: Apple M4
0.00.055.059 I ggml_metal_init: using embedded metal library
0.00.057.107 I ggml_metal_init: GPU name:   Apple M4
0.00.057.109 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.109 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.110 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.111 I ggml_metal_init: simdgroup reduction   = true
0.00.057.111 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.111 I ggml_metal_init: has bfloat            = true
0.00.057.112 I ggml_metal_init: use bfloat            = true
0.00.057.112 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.432 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.439 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.457 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.353 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.354 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.355 I llama_new_context_with_model: graph nodes  = 967
0.00.067.355 I llama_new_context_with_model: graph splits = 2
0.00.067.368 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.463.326 I 
0.00.463.356 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.463.359 I perplexity: tokenizing the input ..
0.00.470.518 I perplexity: tokenization took 7.157 ms
0.00.470.521 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.602.346 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.603.598 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.603.630 I llama_perf_context_print:        load time =     453.42 ms
0.00.603.631 I llama_perf_context_print: prompt eval time =     131.59 ms /   128 tokens (    1.03 ms per token,   972.74 tokens per second)
0.00.603.632 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.603.632 I llama_perf_context_print:       total time =     140.30 ms /   129 tokens
0.00.604.153 I ggml_metal_free: deallocating

real	0m0.621s
user	0m0.077s
sys	0m0.068s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.592 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.429 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.434 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.436 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.437 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.437 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.437 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.438 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.438 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.440 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.441 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.441 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.441 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.442 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.443 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.453 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.454 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.312 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.361 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.399 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.401 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.401 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.401 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.402 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.402 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.403 I llama_model_loader: - type  f32:  194 tensors
0.00.023.403 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.403 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.404 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.404 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.164 I llm_load_vocab: special tokens cache size = 25
0.00.050.164 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.166 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.167 I llm_load_print_meta: arch             = gptneox
0.00.050.167 I llm_load_print_meta: vocab type       = BPE
0.00.050.167 I llm_load_print_meta: n_vocab          = 50304
0.00.050.168 I llm_load_print_meta: n_merges         = 50009
0.00.050.168 I llm_load_print_meta: vocab_only       = 0
0.00.050.168 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.168 I llm_load_print_meta: n_embd           = 2048
0.00.050.168 I llm_load_print_meta: n_layer          = 24
0.00.050.171 I llm_load_print_meta: n_head           = 16
0.00.050.172 I llm_load_print_meta: n_head_kv        = 16
0.00.050.172 I llm_load_print_meta: n_rot            = 32
0.00.050.172 I llm_load_print_meta: n_swa            = 0
0.00.050.172 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.173 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.175 I llm_load_print_meta: n_gqa            = 1
0.00.050.176 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.177 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.177 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.178 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.178 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.178 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.178 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.179 I llm_load_print_meta: n_ff             = 8192
0.00.050.179 I llm_load_print_meta: n_expert         = 0
0.00.050.179 I llm_load_print_meta: n_expert_used    = 0
0.00.050.180 I llm_load_print_meta: causal attn      = 1
0.00.050.180 I llm_load_print_meta: pooling type     = 0
0.00.050.180 I llm_load_print_meta: rope type        = 2
0.00.050.180 I llm_load_print_meta: rope scaling     = linear
0.00.050.180 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.181 I llm_load_print_meta: freq_scale_train = 1
0.00.050.181 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.181 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.181 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.181 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.182 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.182 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.182 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.189 I llm_load_print_meta: model type       = 1.4B
0.00.050.189 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.189 I llm_load_print_meta: model params     = 1.41 B
0.00.050.190 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.190 I llm_load_print_meta: general.name     = 1.4B
0.00.050.190 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.190 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.191 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.191 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.191 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.191 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.191 I llm_load_print_meta: max token length = 1024
0.00.051.904 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.904 I llm_load_tensors: offloading output layer to GPU
0.00.051.904 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.909 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.909 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.767 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.767 I llama_new_context_with_model: n_ctx         = 128
0.00.052.767 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.768 I llama_new_context_with_model: n_batch       = 128
0.00.052.768 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.768 I llama_new_context_with_model: flash_attn    = 0
0.00.052.768 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.769 I llama_new_context_with_model: freq_scale    = 1
0.00.052.769 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.769 I ggml_metal_init: allocating
0.00.052.772 I ggml_metal_init: found device: Apple M4
0.00.052.775 I ggml_metal_init: picking default device: Apple M4
0.00.053.300 I ggml_metal_init: using embedded metal library
0.00.055.213 I ggml_metal_init: GPU name:   Apple M4
0.00.055.215 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.215 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.216 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.216 I ggml_metal_init: simdgroup reduction   = true
0.00.055.216 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.216 I ggml_metal_init: has bfloat            = true
0.00.055.216 I ggml_metal_init: use bfloat            = true
0.00.055.217 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.789 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.793 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.810 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.644 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.645 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.645 I llama_new_context_with_model: graph nodes  = 967
0.00.065.646 I llama_new_context_with_model: graph splits = 2
0.00.065.654 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.548.126 I 
0.00.548.158 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.548.162 I perplexity: tokenizing the input ..
0.00.555.830 I perplexity: tokenization took 7.667 ms
0.00.555.833 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.687.236 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.688.375 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.688.403 I llama_perf_context_print:        load time =     539.53 ms
0.00.688.404 I llama_perf_context_print: prompt eval time =     131.17 ms /   128 tokens (    1.02 ms per token,   975.86 tokens per second)
0.00.688.405 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.688.406 I llama_perf_context_print:       total time =     140.28 ms /   129 tokens
0.00.688.880 I ggml_metal_free: deallocating

real	0m0.703s
user	0m0.078s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.568 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.872 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.877 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.878 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.879 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.879 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.879 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.880 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.881 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.881 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.881 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.883 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.883 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.883 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.884 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.885 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.885 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.886 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.840 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.943 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.006 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.008 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.008 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.008 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.008 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.009 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.009 I llama_model_loader: - type  f32:  194 tensors
0.00.025.009 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.010 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.010 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.945 I llm_load_vocab: special tokens cache size = 25
0.00.050.889 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.892 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.892 I llm_load_print_meta: arch             = gptneox
0.00.050.892 I llm_load_print_meta: vocab type       = BPE
0.00.050.893 I llm_load_print_meta: n_vocab          = 50304
0.00.050.893 I llm_load_print_meta: n_merges         = 50009
0.00.050.893 I llm_load_print_meta: vocab_only       = 0
0.00.050.893 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.893 I llm_load_print_meta: n_embd           = 2048
0.00.050.894 I llm_load_print_meta: n_layer          = 24
0.00.050.896 I llm_load_print_meta: n_head           = 16
0.00.050.897 I llm_load_print_meta: n_head_kv        = 16
0.00.050.897 I llm_load_print_meta: n_rot            = 32
0.00.050.898 I llm_load_print_meta: n_swa            = 0
0.00.050.898 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.898 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.899 I llm_load_print_meta: n_gqa            = 1
0.00.050.900 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.900 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.901 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.901 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.901 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.902 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.902 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.902 I llm_load_print_meta: n_ff             = 8192
0.00.050.903 I llm_load_print_meta: n_expert         = 0
0.00.050.903 I llm_load_print_meta: n_expert_used    = 0
0.00.050.903 I llm_load_print_meta: causal attn      = 1
0.00.050.903 I llm_load_print_meta: pooling type     = 0
0.00.050.903 I llm_load_print_meta: rope type        = 2
0.00.050.903 I llm_load_print_meta: rope scaling     = linear
0.00.050.904 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.904 I llm_load_print_meta: freq_scale_train = 1
0.00.050.904 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.904 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.905 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.905 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.905 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.907 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.908 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.919 I llm_load_print_meta: model type       = 1.4B
0.00.050.919 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.919 I llm_load_print_meta: model params     = 1.41 B
0.00.050.920 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.921 I llm_load_print_meta: general.name     = 1.4B
0.00.050.922 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.922 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.922 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.922 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.922 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.922 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.923 I llm_load_print_meta: max token length = 1024
0.00.052.512 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.512 I llm_load_tensors: offloading output layer to GPU
0.00.052.513 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.522 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.523 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.353 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.354 I llama_new_context_with_model: n_ctx         = 128
0.00.053.354 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.355 I llama_new_context_with_model: n_batch       = 128
0.00.053.355 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.355 I llama_new_context_with_model: flash_attn    = 0
0.00.053.355 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.356 I llama_new_context_with_model: freq_scale    = 1
0.00.053.356 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.356 I ggml_metal_init: allocating
0.00.053.360 I ggml_metal_init: found device: Apple M4
0.00.053.362 I ggml_metal_init: picking default device: Apple M4
0.00.053.896 I ggml_metal_init: using embedded metal library
0.00.055.820 I ggml_metal_init: GPU name:   Apple M4
0.00.055.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.822 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.823 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.823 I ggml_metal_init: simdgroup reduction   = true
0.00.055.823 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.823 I ggml_metal_init: has bfloat            = true
0.00.055.823 I ggml_metal_init: use bfloat            = true
0.00.055.824 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.976 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.980 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.995 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.860 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.861 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.862 I llama_new_context_with_model: graph nodes  = 967
0.00.065.862 I llama_new_context_with_model: graph splits = 2
0.00.065.874 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.572.976 I 
0.00.573.006 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.573.010 I perplexity: tokenizing the input ..
0.00.581.357 I perplexity: tokenization took 8.345 ms
0.00.581.364 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.715.278 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.716.443 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.716.471 I llama_perf_context_print:        load time =     563.40 ms
0.00.716.472 I llama_perf_context_print: prompt eval time =     133.67 ms /   128 tokens (    1.04 ms per token,   957.55 tokens per second)
0.00.716.473 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.716.473 I llama_perf_context_print:       total time =     143.50 ms /   129 tokens
0.00.716.835 I ggml_metal_free: deallocating

real	0m0.734s
user	0m0.077s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.690 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.573 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.578 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.580 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.581 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.581 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.582 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.583 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.583 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.583 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.584 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.584 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.585 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.586 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.586 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.587 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.637 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.741 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.962 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.963 I llama_model_loader: - type  f32:  194 tensors
0.00.023.964 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.964 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.746 I llm_load_vocab: special tokens cache size = 25
0.00.050.595 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.598 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.599 I llm_load_print_meta: arch             = gptneox
0.00.050.599 I llm_load_print_meta: vocab type       = BPE
0.00.050.599 I llm_load_print_meta: n_vocab          = 50304
0.00.050.599 I llm_load_print_meta: n_merges         = 50009
0.00.050.600 I llm_load_print_meta: vocab_only       = 0
0.00.050.600 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.600 I llm_load_print_meta: n_embd           = 2048
0.00.050.600 I llm_load_print_meta: n_layer          = 24
0.00.050.603 I llm_load_print_meta: n_head           = 16
0.00.050.606 I llm_load_print_meta: n_head_kv        = 16
0.00.050.606 I llm_load_print_meta: n_rot            = 32
0.00.050.606 I llm_load_print_meta: n_swa            = 0
0.00.050.606 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.607 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.607 I llm_load_print_meta: n_gqa            = 1
0.00.050.608 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.609 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.609 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.610 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.610 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.610 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.610 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.611 I llm_load_print_meta: n_ff             = 8192
0.00.050.611 I llm_load_print_meta: n_expert         = 0
0.00.050.612 I llm_load_print_meta: n_expert_used    = 0
0.00.050.612 I llm_load_print_meta: causal attn      = 1
0.00.050.612 I llm_load_print_meta: pooling type     = 0
0.00.050.613 I llm_load_print_meta: rope type        = 2
0.00.050.614 I llm_load_print_meta: rope scaling     = linear
0.00.050.614 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.614 I llm_load_print_meta: freq_scale_train = 1
0.00.050.615 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.615 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.615 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.615 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.615 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.617 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.617 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.623 I llm_load_print_meta: model type       = 1.4B
0.00.050.624 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.624 I llm_load_print_meta: model params     = 1.41 B
0.00.050.626 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.626 I llm_load_print_meta: general.name     = 1.4B
0.00.050.627 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.627 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.627 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.627 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.628 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.629 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.629 I llm_load_print_meta: max token length = 1024
0.00.052.411 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.411 I llm_load_tensors: offloading output layer to GPU
0.00.052.411 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.416 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.418 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.339 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.339 I llama_new_context_with_model: n_ctx         = 128
0.00.053.340 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.340 I llama_new_context_with_model: n_batch       = 128
0.00.053.340 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.340 I llama_new_context_with_model: flash_attn    = 0
0.00.053.341 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.341 I llama_new_context_with_model: freq_scale    = 1
0.00.053.341 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.342 I ggml_metal_init: allocating
0.00.053.348 I ggml_metal_init: found device: Apple M4
0.00.053.350 I ggml_metal_init: picking default device: Apple M4
0.00.053.902 I ggml_metal_init: using embedded metal library
0.00.056.246 I ggml_metal_init: GPU name:   Apple M4
0.00.056.248 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.248 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.249 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.249 I ggml_metal_init: simdgroup reduction   = true
0.00.056.249 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.249 I ggml_metal_init: has bfloat            = true
0.00.056.249 I ggml_metal_init: use bfloat            = true
0.00.056.250 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.250 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.309 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.315 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.330 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.205 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.206 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.206 I llama_new_context_with_model: graph nodes  = 967
0.00.066.207 I llama_new_context_with_model: graph splits = 2
0.00.066.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.375 I 
0.00.649.397 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.649.400 I perplexity: tokenizing the input ..
0.00.656.892 I perplexity: tokenization took 7.49 ms
0.00.656.896 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.891 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.799.141 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.799.171 I llama_perf_context_print:        load time =     640.68 ms
0.00.799.172 I llama_perf_context_print: prompt eval time =     140.77 ms /   128 tokens (    1.10 ms per token,   909.26 tokens per second)
0.00.799.173 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.173 I llama_perf_context_print:       total time =     149.80 ms /   129 tokens
0.00.799.586 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.077s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.785 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.392 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.396 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.399 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.399 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.400 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.400 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.400 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.401 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.402 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.402 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.402 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.403 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.403 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.403 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.406 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.407 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.407 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.450 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.505 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.583 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.584 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.584 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.585 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.585 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.585 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.586 I llama_model_loader: - type  f32:  194 tensors
0.00.025.586 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.573 I llm_load_vocab: special tokens cache size = 25
0.00.051.332 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.334 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.335 I llm_load_print_meta: arch             = gptneox
0.00.051.335 I llm_load_print_meta: vocab type       = BPE
0.00.051.335 I llm_load_print_meta: n_vocab          = 50304
0.00.051.335 I llm_load_print_meta: n_merges         = 50009
0.00.051.335 I llm_load_print_meta: vocab_only       = 0
0.00.051.336 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.336 I llm_load_print_meta: n_embd           = 2048
0.00.051.336 I llm_load_print_meta: n_layer          = 24
0.00.051.339 I llm_load_print_meta: n_head           = 16
0.00.051.340 I llm_load_print_meta: n_head_kv        = 16
0.00.051.340 I llm_load_print_meta: n_rot            = 32
0.00.051.341 I llm_load_print_meta: n_swa            = 0
0.00.051.341 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.341 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.342 I llm_load_print_meta: n_gqa            = 1
0.00.051.342 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.343 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.344 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.344 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.344 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.344 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.344 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.345 I llm_load_print_meta: n_ff             = 8192
0.00.051.345 I llm_load_print_meta: n_expert         = 0
0.00.051.345 I llm_load_print_meta: n_expert_used    = 0
0.00.051.345 I llm_load_print_meta: causal attn      = 1
0.00.051.346 I llm_load_print_meta: pooling type     = 0
0.00.051.347 I llm_load_print_meta: rope type        = 2
0.00.051.347 I llm_load_print_meta: rope scaling     = linear
0.00.051.348 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.348 I llm_load_print_meta: freq_scale_train = 1
0.00.051.349 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.349 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.349 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.349 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.349 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.349 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.349 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.361 I llm_load_print_meta: model type       = 1.4B
0.00.051.361 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.363 I llm_load_print_meta: model params     = 1.41 B
0.00.051.364 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.364 I llm_load_print_meta: general.name     = 1.4B
0.00.051.364 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.364 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.366 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: max token length = 1024
0.00.053.397 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.397 I llm_load_tensors: offloading output layer to GPU
0.00.053.397 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.407 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.408 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.292 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.293 I llama_new_context_with_model: n_ctx         = 128
0.00.054.293 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.293 I llama_new_context_with_model: n_batch       = 128
0.00.054.294 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.294 I llama_new_context_with_model: flash_attn    = 0
0.00.054.294 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.294 I llama_new_context_with_model: freq_scale    = 1
0.00.054.295 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.295 I ggml_metal_init: allocating
0.00.054.298 I ggml_metal_init: found device: Apple M4
0.00.054.300 I ggml_metal_init: picking default device: Apple M4
0.00.054.846 I ggml_metal_init: using embedded metal library
0.00.056.804 I ggml_metal_init: GPU name:   Apple M4
0.00.056.806 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.806 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.806 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.807 I ggml_metal_init: simdgroup reduction   = true
0.00.056.807 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.807 I ggml_metal_init: has bfloat            = true
0.00.056.807 I ggml_metal_init: use bfloat            = true
0.00.056.807 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.809 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.993 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.995 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.010 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.912 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.913 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.913 I llama_new_context_with_model: graph nodes  = 967
0.00.066.914 I llama_new_context_with_model: graph splits = 2
0.00.066.926 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.161.227 I 
0.00.161.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.161.271 I perplexity: tokenizing the input ..
0.00.168.600 I perplexity: tokenization took 7.326 ms
0.00.168.606 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.308.627 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.309.868 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.309.895 I llama_perf_context_print:        load time =     150.44 ms
0.00.309.898 I llama_perf_context_print: prompt eval time =     139.79 ms /   128 tokens (    1.09 ms per token,   915.64 tokens per second)
0.00.309.899 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.309.899 I llama_perf_context_print:       total time =     148.67 ms /   129 tokens
0.00.310.348 I ggml_metal_free: deallocating

real	0m0.325s
user	0m0.076s
sys	0m0.044s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.277 I build: 4216 (90415f31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.618 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.965 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.970 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.972 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.973 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.973 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.973 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.974 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.975 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.975 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.976 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.976 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.976 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.977 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.978 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.979 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.979 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.804 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.714 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.043 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.045 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.046 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.046 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.046 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.047 I llama_model_loader: - type  f32:  194 tensors
0.00.050.047 I llama_model_loader: - type  f16:   98 tensors
0.00.077.437 I llm_load_vocab: special tokens cache size = 25
0.00.083.648 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.651 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.651 I llm_load_print_meta: arch             = gptneox
0.00.083.651 I llm_load_print_meta: vocab type       = BPE
0.00.083.652 I llm_load_print_meta: n_vocab          = 50304
0.00.083.652 I llm_load_print_meta: n_merges         = 50009
0.00.083.652 I llm_load_print_meta: vocab_only       = 0
0.00.083.652 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.652 I llm_load_print_meta: n_embd           = 2048
0.00.083.653 I llm_load_print_meta: n_layer          = 24
0.00.083.655 I llm_load_print_meta: n_head           = 16
0.00.083.656 I llm_load_print_meta: n_head_kv        = 16
0.00.083.656 I llm_load_print_meta: n_rot            = 32
0.00.083.656 I llm_load_print_meta: n_swa            = 0
0.00.083.656 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.656 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.657 I llm_load_print_meta: n_gqa            = 1
0.00.083.658 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.659 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.659 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.660 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.660 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.660 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.660 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.661 I llm_load_print_meta: n_ff             = 8192
0.00.083.661 I llm_load_print_meta: n_expert         = 0
0.00.083.664 I llm_load_print_meta: n_expert_used    = 0
0.00.083.664 I llm_load_print_meta: causal attn      = 1
0.00.083.664 I llm_load_print_meta: pooling type     = 0
0.00.083.664 I llm_load_print_meta: rope type        = 2
0.00.083.664 I llm_load_print_meta: rope scaling     = linear
0.00.083.665 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.665 I llm_load_print_meta: freq_scale_train = 1
0.00.083.665 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.666 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.666 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.666 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.666 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.666 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.666 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.674 I llm_load_print_meta: model type       = 1.4B
0.00.083.674 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.083.674 I llm_load_print_meta: model params     = 1.41 B
0.00.083.675 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.083.675 I llm_load_print_meta: general.name     = 1.4B
0.00.083.675 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.676 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.676 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.676 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.676 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.083.677 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.677 I llm_load_print_meta: max token length = 1024
0.00.085.620 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.620 I llm_load_tensors: offloading output layer to GPU
0.00.085.620 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.625 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.085.626 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.086.558 I llama_new_context_with_model: n_seq_max     = 1
0.00.086.559 I llama_new_context_with_model: n_ctx         = 128
0.00.086.559 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.086.559 I llama_new_context_with_model: n_batch       = 128
0.00.086.559 I llama_new_context_with_model: n_ubatch      = 128
0.00.086.560 I llama_new_context_with_model: flash_attn    = 0
0.00.086.560 I llama_new_context_with_model: freq_base     = 10000.0
0.00.086.560 I llama_new_context_with_model: freq_scale    = 1
0.00.086.561 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.086.561 I ggml_metal_init: allocating
0.00.086.569 I ggml_metal_init: found device: Apple M4
0.00.086.572 I ggml_metal_init: picking default device: Apple M4
0.00.087.138 I ggml_metal_init: using embedded metal library
0.00.089.197 I ggml_metal_init: GPU name:   Apple M4
0.00.089.199 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.200 I ggml_metal_init: simdgroup reduction   = true
0.00.089.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.200 I ggml_metal_init: has bfloat            = true
0.00.089.200 I ggml_metal_init: use bfloat            = true
0.00.089.201 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.341 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.098.345 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.098.359 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.214 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.099.215 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.099.216 I llama_new_context_with_model: graph nodes  = 967
0.00.099.216 I llama_new_context_with_model: graph splits = 2
0.00.099.223 I 
0.00.099.243 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.099.244 I compute_imatrix: tokenizing the input ..
0.00.105.850 I compute_imatrix: tokenization took 6.605 ms
0.00.105.851 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.449.442 I compute_imatrix: 1.34 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.451.810 I llama_perf_context_print:        load time =    1427.82 ms
0.01.451.811 I llama_perf_context_print: prompt eval time =    1342.95 ms /   128 tokens (   10.49 ms per token,    95.31 tokens per second)
0.01.451.812 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.451.812 I llama_perf_context_print:       total time =    1430.19 ms /   129 tokens
0.01.452.286 I ggml_metal_free: deallocating

real	0m1.638s
user	0m0.159s
sys	0m0.247s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4216 (90415f31)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14340a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14340a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14340ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14340b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14340b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14340be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14340c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14340c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14340cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14340d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14340d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14340de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14340e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14340f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14340f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143410090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1434107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143410ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1434115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143411dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1434124e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143412c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143413320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143413bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1434142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1434145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143414bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143415820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143415d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143416020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1434164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143416780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143417010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143417550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143417810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143417cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143418150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1434185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143418a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143418f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1434193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143419870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143419d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14341a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14341a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14341aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14341b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14341b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14341bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14341c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14341cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14341d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14341d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14341de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14341e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14341eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14341ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14341f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14341f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143420000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1434202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143420760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143420c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1434210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143421540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1434219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143421e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143422320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1434227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143422c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143423100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1434235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143423a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143423ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143424380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143424820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143424cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143425160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143425600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143425aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143425f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1434263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143426880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143426d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1434271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143427660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143427b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143427fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143428440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1434288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143428d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143429220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1434296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143429b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14342a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14342a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14342a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14341b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14342af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14342b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14342b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14342bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14342c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14342c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14342cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14342cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14342d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14342d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14342ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14342e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14342e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14342ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14342f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14342f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14342f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14342fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1434302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143430770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143430c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1434310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143431550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1434319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143431e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143432330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1434327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143432c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143433110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1434335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143433a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143433ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143434390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143434830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143434cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143435170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143435610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143435ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143435f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1434363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143436890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143436d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1434371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143437670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143437b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143437fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143438450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1434388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143438d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143439230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1434396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143439b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14343a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14343a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14343a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14343aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14343b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14343b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14343be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14343c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14343c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14343cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14343d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14343d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14343dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14343e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14343ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14343f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14343f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14343fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143440270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1434407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143440d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143441260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1434417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143441d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143442250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1434427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143442cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143443240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143443790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143443ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143444230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143444780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143444cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143445220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143445770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143445cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143446210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143446760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143446cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143447200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143447750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143447ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1434481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143448740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143448c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1434491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143449730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143449c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14344a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14344a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14344ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14344b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14344b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14344bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14344c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14344c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14344cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14344d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14344d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14344dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14344e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14344e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14344ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14344f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14344f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14344fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143450170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1434506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143450c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143451160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1434516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143451c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143452150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1434526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143452b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143452fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143453480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143453920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143453dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143454260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143454700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143454ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143455040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1434554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143455980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143455e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1434562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143456810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143456f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143457650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143457d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143458490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143458750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143458d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143459370 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.134.871 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143206980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143207060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1432074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143207940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143207db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143208220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143208690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143208b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143205510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143205980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143208f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1432095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14320a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14320a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14320b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14320b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14320bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14320c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14320cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14320d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14320dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14320e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14320ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14320f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14320f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14320fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14320fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1432102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143210720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143210b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143211090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1432115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143211a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143211cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143212140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1432125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143212b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143213010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143213510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143213a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143213f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143214410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143214910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143214e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143215310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143215780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143215bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143216060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1432164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143216940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143216db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143217220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143217690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143217b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143217f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143218740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143218be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143218ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1432194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143219ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14321a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14321a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14321aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14321af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14321b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14321b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14321bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14321c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14321c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14321cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14321cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14321d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14321d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14321dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14321e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14321e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14321eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14321efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14321f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14321f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14321fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143220260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143220700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143220ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143221040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1432214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143221980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143221e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1432222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143222760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143222c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1432230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143223540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1432239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143223e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143224320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1432247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143224c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143225100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1432255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143225a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143225ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143226380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143226820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143226cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143227160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143227600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143227aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143227f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1432283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143228880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143228d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1432291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143229660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143229b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143229fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14322a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14322a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14322ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14322b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14322b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14322bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14322c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14322c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14322c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14322cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14322d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14322d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14322dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14322e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14322e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14322e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14322ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14322f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14322f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14322fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1432300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143230560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143230a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143230ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143231340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1432317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143231c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143232120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1432325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143232a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143232f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1432333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143233840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143233ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143234180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143234620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143234ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143235010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143235560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143235ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143236000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1432362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1432368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143236ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1432374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143237b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143238110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143238900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143238da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143239240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1432396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143239e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14323a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14323a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14323ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14323b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14323b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14323be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14323c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14323c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14323ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14323d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14323d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14323de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14323e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14323e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14323ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14323f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14323f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14323fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143240380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1432408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1433052f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143305760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143305bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143306040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1433064b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143306920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143306d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143307200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143307670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143307ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143307f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1433083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143308830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143308ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143309110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143309580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1433099f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143309e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14330a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14330a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14330abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14330b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14330b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14330b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14330bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14330c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14330c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14330cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14330cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14330d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14330d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14330dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14330e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14330e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14330e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14330ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14330f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14330f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14330fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143310000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143310470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1433108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143310d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1433111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143311630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143311aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143311f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143312380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1433127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143312c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143313730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143313e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143314570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143314c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143314f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143315210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143315680 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1433055a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143305a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143305e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1433062f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143306760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143306bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143307040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1433074b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143307920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143307d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143308200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1433087e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1433090d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143309850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14330a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14330a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14330ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14330b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14330bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14330c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14330cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14330d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14330da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14330e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14330e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14330ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14330f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14330f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14330f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14330fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1433102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143310730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143310ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143310e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1433112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143311740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143311bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143312020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143312490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143312900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143312d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1433131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143313650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143313ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143313f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1433143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143314810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143314c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1433150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143315560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1433159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143315e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1433162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143316720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143316d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143317230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1433176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143317990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143317fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143318790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143318c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1433190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143319570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143319a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143319eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14331a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14331a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14331ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14331b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14331b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14331ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14331bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14331c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14331c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14331ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14331d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14331d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14331dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14331df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14331e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14331e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14331ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14331f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14331f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14331fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14331ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143320470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143320910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143320db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143321250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1433216f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143321b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143322030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1433224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143322970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143322e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1433232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143323750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143323bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143324090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143324530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1433249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143324e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143325310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1433257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143325c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1433260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143326590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143326a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143326ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143327370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143327810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143327cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143328150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1433285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143328a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143328f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1433293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143329870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143329d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14332a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14332a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14332aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14332af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14332b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14332b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14332bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14332c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14332c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14332cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14332cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14332d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14332d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14332ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14332e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14332e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14332ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14332f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14332f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14332f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14332fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1433302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143330770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143330c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1433310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143331550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1433319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143331e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143332330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1433327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143332c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143333110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1433335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143333b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143334050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1433345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143334af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143334db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1433353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1433359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143335fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1433365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143336c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1433373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143337890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143337d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1433381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143338980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143338ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143339420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143339970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143339ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14333a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14333a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14333aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14333b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14333b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14333bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14333c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14333c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14333ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14333d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14333d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14333de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14333e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14333e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14333ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14333f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14333f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14333fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1433403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143340900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143340e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1433413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1433418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143341e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143342390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1433428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143342e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143343380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1433438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143343e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143344370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1433448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143344e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143345360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1433458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143345e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143346350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1433468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143346df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143347340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143347890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143347de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143348330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143348880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143348dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143349320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143349870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143349dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14334a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14334a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14334adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14334b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14334b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14334bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14334c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14334c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14334ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14334cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14334d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14334d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14334dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14334e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14334e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14334ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14334ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14334f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14334fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1433502b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1433509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1433510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1433513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1433519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143351fd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.790s
user	0m0.290s
sys	0m0.314s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4216 (90415f31)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a60b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a60ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a60c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a60c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a60cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a60d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a60d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a60dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a60e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a60e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a60ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a60f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a60fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a610400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a610c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a611330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a611a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a612170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a612890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a613060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a613780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a613ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a6145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a614e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a615580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a615840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a615e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a616ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a617000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a6172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a617760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a617a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a6182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a6187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a618ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a618f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a6193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a619890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a619d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a61a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a61a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a61ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a61afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a61b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a61b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a61bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a61c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a61cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a61d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a61d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a61de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a61e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a61eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a61f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a61f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a61fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a6201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a6204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a620ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a6212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a621560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a621a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a621ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a622340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a6227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a622c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a623120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a6235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a623a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a623f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a6243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a624840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a624ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a625180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a625620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a625ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a625f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a626400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a6268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a626d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a6271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a627680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a627b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a627fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a628460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a628900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a628da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a629240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a6296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a629b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a62a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a62a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a62a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a62ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a62b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a62b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a62bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a61c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a62c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a62c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a62cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a62d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a62d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a62d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a62ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a62e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a62e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a62ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a62f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a62f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a62f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a62fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a6302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a630790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a630c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a6310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a631570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a631a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a631eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a632350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a6327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a632c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a633130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a6335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a633a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a633f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a6343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a634850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a634cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a635190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a635630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a635ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a635f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a636410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a6368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a636d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a6371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a637690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a637b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a637fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a638470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a638910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a638db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a639250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a6396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a639b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a63a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a63a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a63a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a63ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a63b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a63b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a63bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a63c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a63c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a63cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a63d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a63d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a63da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a63e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a63e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a63ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a63f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a63fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a63fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a640370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a640810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a640fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a641510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a641a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a641fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a642500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a642a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a642fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a6434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a643a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a643f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a6444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a644a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a644f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a6454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a645a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a645f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a6464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a646a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a646f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a6474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a647a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a647f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a6484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a6489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a648f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a649490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a6499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a649f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a64a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a64a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a64af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a64b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a64b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a64bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a64c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a64c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a64cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a64d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a64d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a64def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a64e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a64e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a64eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a64f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a64f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a64fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a650420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a650970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a650ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a651410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a651960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a651eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a652400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a652950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a652ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a6533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a653940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a653de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a654280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a654720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a654bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a655060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a655500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a6559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a655e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a6562e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a656780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a656c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a6570c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a657560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a657ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a6581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a6588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a659010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a659730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a6599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a65a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a65a610 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.094.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b804ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b805150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b8055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b805a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b805ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b806310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b806780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b806bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b807060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b8074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b807940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b808020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b808b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b8092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b809b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b80a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b80a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b80b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b80b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b80bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b80c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b80cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b80d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b80dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b80e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b80e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b80e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b80ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b80f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b80f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b80fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b80ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b8103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b810690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b810b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b810f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b8113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b811850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b811cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b812130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b8125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b812a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b812e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b8132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b813760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b813bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b814040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b8144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b814920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b814d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b815200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b815670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b815ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b815f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b8163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b816830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b816da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b8172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b817710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b817b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b817ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b818460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b8188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b818d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b8191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b819620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b819a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b819f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b81a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b81a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b81ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b81b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b81b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b81b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b81be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b81c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b81c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b81cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b81cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b81d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b81d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b81dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b81e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b81e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b81ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b81eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b81f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b81f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b81fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b8200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b820510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b820980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b820df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b821260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b8216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b821b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b821fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b822420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b822890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b822d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b823170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b8235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b823a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b823ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b824330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b8247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b824c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b825080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b8254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b825960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b825dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b826240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b8266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b826b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b826f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b827400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b827870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b827ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b828150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b8285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b828a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b828ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b829310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b829780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b829bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b82a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b82a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b82a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b82adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b82b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b82b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b82bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b82bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b82c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b82c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b82ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b82d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b82d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b82da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b82de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b82e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b82e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b82ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b82f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b82f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b82f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b82fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b830200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b830670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b830ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b830f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b8313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b831830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b831ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b832110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b832580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b8329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b832e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b8332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b833740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b833bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b834020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b834490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b834900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b834d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b8351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b835650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b8361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b8364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b836760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b836bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b837040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b8374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b837920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b837d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b838200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b838670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b838ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b838f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b8393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b839830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b839ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b83a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b83a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b83a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b83ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b83b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b83b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b83bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b83c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b83c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b83c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b83cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b83d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b83d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b83dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b83df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b83e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b83e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b83ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b83f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b83f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b83f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b83fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b8402b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b840720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b840b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b841000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b841470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b8418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b841d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b8421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b842630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b842aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b842f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b843380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b8437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b843c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b8440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b844540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b8449b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b844e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b845290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b845700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b845b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b845fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b846450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b8468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b846d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b8471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b847610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b847a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b847ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b848360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b8487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b848c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b8490b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b849520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b84a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b84a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b84aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b84b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b84b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b84bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b84bfb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a60c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a60d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a60c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a60cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a60b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a64a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a64a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a64ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a64af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a64b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a64b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a64bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a64c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a64cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a64d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a64dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a64e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a64ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a64f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a64fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a650160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a650850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a650f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a651630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a651d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a652190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a652600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a652a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a652ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a653350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a6537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a653c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a6540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a654360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a6547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a654c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a6550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a655520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a655990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a655e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a656270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a6566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a656b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a656fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a657430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a6578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a657d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a658180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a6585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a658a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a658ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a659340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a6597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a659c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a65a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a65a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a6187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a618c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a6190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a6199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a619e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a61a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a61a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a61ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a61afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a61b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a61b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a61bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a61c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a61c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a61ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a61cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a61d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a61d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a61dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a61e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a61e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a61e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a61edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a61f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a61f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a61fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a61ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a620420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a620890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a620d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a621170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a6215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a621a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a621ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a622330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a6227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a622c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a623080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a6234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a623960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a623dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a624240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a6246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a624b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a624f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a625400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a625870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a625ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a626150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a6265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a626a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a626ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a627310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a627780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a627bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a628060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a6284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a628940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a628db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a629220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a629690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a629b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a629f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a62a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a62a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a62acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a62b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a62b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a62ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a62be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a62c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a62c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a62cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a62d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a62d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a62d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a62dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a62e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a62e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a62eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a62ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a62f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a62f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a62fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a630110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a630580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a6309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a630e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a6312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a631740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a631bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a632020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a632900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a632d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a6331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a633650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a633ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a6343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a634810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a634c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a6350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a635560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a6359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a635e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a6362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a636720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a636b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a637000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a637780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a637bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a638060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a6384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a638940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a638db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a639220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a639690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a639b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a639f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a63a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a63a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a63acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a63b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a63b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a63ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a63be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a63c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a63c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a63cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a63d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a63d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a63d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a63dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a63e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a63e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a63eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a63ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a63f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a63f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a63fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a640110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a640580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a6409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a640e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a6412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a641740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a641bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a642020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a642490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a642900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a642d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a6431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a643650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a643ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a643f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a6443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a644810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a644c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a6450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a645560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a6459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a645e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a6462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a646720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a646b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a647000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a647470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a6478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a647d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a6481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a648630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a648aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a648f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a649380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a6497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a649c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a616ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a617460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a6178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a617d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a60e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a60ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a60f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a60fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a610030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a6104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a610910 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.935s
user	0m0.240s
sys	0m0.139s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
