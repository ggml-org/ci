Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu
Requirement already satisfied: numpy~=1.26.4 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)
Requirement already satisfied: sentencepiece~=0.2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)
Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.46.3)
Requirement already satisfied: gguf>=0.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.15.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.5)
Requirement already satisfied: torch~=2.2.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2)
Requirement already satisfied: filelock in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.16.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.26.2)
Requirement already satisfied: packaging>=20.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)
Requirement already satisfied: requests in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)
Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.20.3)
Requirement already satisfied: safetensors>=0.4.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.5)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.0)
Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)
Requirement already satisfied: sympy in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.3)
Requirement already satisfied: networkx in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)
Requirement already satisfied: jinja2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)
Requirement already satisfied: fsspec in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.10.0)
Requirement already satisfied: MarkupSafe>=2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from jinja2->torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.8.30)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from sympy->torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)
Obtaining file:///Users/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (1.26.4)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (6.0.2)
Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (0.2.0)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (4.67.0)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.15.0-py3-none-any.whl size=3464 sha256=2a70d5769527de1a88776f0d57cef2d72e4fe38c378b24e9c59d0872482e9918
  Stored in directory: /private/var/folders/3z/y7wjb5257l1dj_dxrng2zq0h0000gn/T/pip-ephem-wheel-cache-8h5xxskq/wheels/57/e8/ad/c90c2fcd445a1780e6969131e27698e4f239a16d34ccd95852
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.15.0
    Uninstalling gguf-0.15.0:
      Successfully uninstalled gguf-0.15.0
Successfully installed gguf-0.15.0
+ gg_run_ctest_debug
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (3.1s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-debug

real	0m3.397s
user	0m0.858s
sys	0m1.394s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_debug-make.log
++ nproc
+ make -j10
[  0%] Generating build details from Git
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Built target build_info
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 11%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Linking CXX shared library libggml-blas.dylib
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Built target llama-gguf-hash
[ 21%] Built target llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Built target test-c
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 35%] Built target llama-simple
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Built target llama-simple-chat
[ 36%] Built target llava_shared
[ 36%] Built target llava_static
[ 36%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-log
[ 44%] Built target test-log
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 47%] Built target test-sampling
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-0
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Linking CXX executable ../bin/test-arg-parser
[ 51%] Built target test-llama-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Linking CXX executable ../bin/test-gguf
[ 51%] Linking CXX executable ../bin/test-backend-ops
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Built target test-tokenizer-0
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Built target test-arg-parser
[ 54%] Built target test-gguf
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Built target test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-barrier
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-quantize-fns
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 59%] Built target test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 59%] Built target test-chat-template
[ 59%] Built target test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-rope
[ 60%] Built target test-quantize-fns
[ 60%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 60%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 60%] Built target test-quantize-perf
[ 60%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 60%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 60%] Built target test-rope
[ 61%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 63%] Linking CXX executable ../bin/test-grammar-integration
[ 63%] Built target test-autorelease
[ 63%] Built target test-grammar-integration
[ 64%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 65%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Built target llama-batched-bench
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target llama-batched
[ 70%] Built target llama-eval-callback
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-embedding
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Built target llama-gguf-split
[ 75%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Built target llama-gritlm
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Built target test-json-schema-to-grammar
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Built target llama-infill
[ 77%] Built target llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Built target llama-lookup-create
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-imatrix
[ 79%] Built target llama-lookahead
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Built target llama-lookup
[ 81%] Generating loading.html.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-imatrix
[ 82%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Built target llama-lookup-stats
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Built target llama-parallel
[ 85%] Linking CXX executable ../../bin/llama-cli
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Built target llama-cli
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-passkey
[ 85%] Built target llama-passkey
[ 86%] Linking CXX executable ../../bin/llama-bench
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-quantize
[ 86%] Built target llama-bench
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Built target llama-quantize
[ 86%] Built target llama-save-load-state
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 86%] Built target llama-retrieval
[ 86%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-perplexity
[ 86%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Built target llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 88%] Built target llama-speculative-simple
[ 89%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 89%] Built target llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 91%] Built target llama-tokenize
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-run
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Built target llama-run
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Built target llama-gen-docs
[ 95%] Linking CXX executable ../../bin/llama-vdot
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-q8dot
[ 96%] Built target llama-vdot
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-q8dot
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-tts
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-tts
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-qwen2vl-cli
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m15.683s
user	1m0.744s
sys	0m17.718s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.37 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.69 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.65 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.41 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.30 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.94 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.26 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.23 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.14 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.22 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.86 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.76 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.93 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.15 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 223.20 sec*proc (28 tests)

Total Test time (real) = 223.21 sec

real	3m43.329s
user	7m36.046s
sys	0m6.302s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ tee /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_release.log
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.591s
user	0m0.876s
sys	0m1.237s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_release-make.log
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target build_info
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Built target llama-gguf
[ 21%] Built target llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Built target test-c
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Built target llama-simple
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llama-simple-chat
[ 35%] Built target llava
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-log
[ 44%] Built target test-log
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-grammar-parser
[ 48%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 49%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-gguf
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Linking CXX executable ../bin/test-arg-parser
[ 51%] Linking CXX executable ../bin/test-llama-grammar
[ 51%] Linking CXX executable ../bin/test-backend-ops
[ 51%] Linking CXX executable ../bin/test-tokenizer-0
[ 51%] Built target test-gguf
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Built target test-arg-parser
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Built target test-llama-grammar
[ 53%] Built target test-tokenizer-0
[ 53%] Built target test-backend-ops
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Linking CXX executable ../bin/test-barrier
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 59%] Built target test-barrier
[ 60%] Linking CXX executable ../bin/test-rope
[ 60%] Built target test-model-load-cancel
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 60%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 60%] Built target test-quantize-fns
[ 60%] Built target test-quantize-perf
[ 60%] Built target test-rope
[ 60%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 60%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 60%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 60%] Built target test-chat-template
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 62%] Built target test-autorelease
[ 63%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Built target llama-batched-bench
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 66%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Built target llama-gbnf-validator
[ 69%] Built target llama-batched
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 69%] Built target llama-eval-callback
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Built target llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Linking CXX executable ../bin/test-grammar-integration
[ 74%] Linking CXX executable ../../bin/llama-gritlm
[ 74%] Built target test-grammar-integration
[ 74%] Built target llama-gritlm
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Linking CXX executable ../../bin/llama-lookup-merge
[ 75%] Built target llama-lookup-create
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Built target llama-lookup-merge
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-infill
[ 78%] Built target llama-lookup
[ 78%] Built target llama-lookahead
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Built target test-json-schema-to-grammar
[ 79%] Built target llama-infill
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Generating loading.html.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Generating index.html.gz.hpp
[ 82%] Linking CXX executable ../../bin/llama-imatrix
[ 82%] Built target llama-imatrix
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-lookup-stats
[ 83%] Built target llama-lookup-stats
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-passkey
[ 85%] Linking CXX executable ../../bin/llama-cli
[ 85%] Linking CXX executable ../../bin/llama-parallel
[ 85%] Built target llama-passkey
[ 85%] Built target llama-cli
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Built target llama-parallel
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Linking CXX executable ../../bin/llama-quantize
[ 85%] Built target llama-save-load-state
[ 85%] Built target llama-quantize
[ 85%] Built target llama-retrieval
[ 85%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 87%] Built target llama-speculative-simple
[ 87%] Built target llama-tokenize
[ 87%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 87%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-bench
[ 88%] Built target llama-perplexity
[ 89%] Linking CXX executable ../../bin/llama-gen-docs
[ 89%] Built target llama-bench
[ 90%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 91%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 91%] Built target llama-gen-docs
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-run
[ 94%] Linking CXX executable ../../bin/llama-speculative
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Built target llama-run
[ 94%] Built target llama-speculative
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-vdot
[ 96%] Linking CXX executable ../../bin/llama-q8dot
[ 96%] Built target llama-vdot
[ 96%] Built target llama-q8dot
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-export-lora
[ 97%] Built target llama-llava-cli
[ 97%] Built target llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-tts
[ 99%] Built target llama-tts
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m21.447s
user	1m20.800s
sys	0m17.968s
+ '[' -z ']'
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.17 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.45 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.31 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.43 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.03 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.27 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.52 sec*proc (28 tests)

Total Test time (real) =  51.53 sec

real	0m51.548s
user	1m11.797s
sys	0m5.601s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/embd_bge_small.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:23:45 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:23:45 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json [711396/711396] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:23:46 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:23:46 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:23:47 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:23:48 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:23:48 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:23:49 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:23:49 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.568s
user	0m0.883s
sys	0m1.227s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/embd_bge_small-make.log
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Built target sha1
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple
[ 37%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 42%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-sampling
[ 49%] Built target test-grammar-integration
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Built target test-log
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-autorelease
[ 62%] Built target test-gguf
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-backend-ops
[ 63%] Built target test-barrier
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-batched
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 78%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-parallel
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-passkey
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-cli
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup
[ 84%] Generating loading.html.hpp
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Built target llama-perplexity
[ 86%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-run
[ 91%] Built target llama-speculative
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tts
[ 91%] Built target llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-gen-docs
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.036s
user	0m5.996s
sys	0m10.065s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/bge-small --outfile ../models-mnt/bge-small/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: bge-small
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,               torch.float32 --> F16, shape = {384, 30522}
INFO:hf-to-gguf:position_embd.weight,            torch.float32 --> F32, shape = {384, 512}
INFO:hf-to-gguf:token_types.weight,              torch.float32 --> F32, shape = {384, 2}
INFO:hf-to-gguf:token_embd_norm.weight,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:token_embd_norm.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.0.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.0.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.1.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.1.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.2.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.2.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.3.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.3.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.4.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.4.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.4.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.5.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.5.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.5.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.6.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.6.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.6.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.7.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.7.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.7.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.8.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.8.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.8.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.9.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.9.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.9.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.10.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.10.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.10.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.11.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.11.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.11.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 512
INFO:hf-to-gguf:gguf: embedding length = 384
INFO:hf-to-gguf:gguf: feed forward length = 1536
INFO:hf-to-gguf:gguf: head count = 12
INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Setting special token type unk to 100
INFO:gguf.vocab:Setting special token type sep to 102
INFO:gguf.vocab:Setting special token type pad to 0
WARNING:gguf.vocab:No handler for special token type cls with id 101 - skipping
INFO:gguf.vocab:Setting special token type mask to 103
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/bge-small/ggml-model-f16.gguf: n_tensors = 197, total_size = 66.9M
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/66.9M [00:00<?, ?byte/s]Writing:  35%|      | 23.4M/66.9M [00:00<00:00, 224Mbyte/s]Writing:  69%|   | 45.9M/66.9M [00:00<00:00, 207Mbyte/s]Writing: 100%|| 66.9M/66.9M [00:00<00:00, 208Mbyte/s]Writing: 100%|| 66.9M/66.9M [00:00<00:00, 208Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/bge-small/ggml-model-f16.gguf
+ model_f16=../models-mnt/bge-small/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/bge-small/ggml-model-q8_0.gguf
+ ./bin/llama-quantize ../models-mnt/bge-small/ggml-model-f16.gguf ../models-mnt/bge-small/ggml-model-q8_0.gguf q8_0
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/bge-small/ggml-model-f16.gguf' to '../models-mnt/bge-small/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Bge Small
llama_model_loader: - kv   3:                           general.basename str              = bge
llama_model_loader: - kv   4:                         general.size_label str              = small
llama_model_loader: - kv   5:                           bert.block_count u32              = 12
llama_model_loader: - kv   6:                        bert.context_length u32              = 512
llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  124 tensors
llama_model_loader: - type  f16:   73 tensors
[   1/ 197]                 position_embd.weight - [  384,   512,     1,     1], type =    f32, size =    0.750 MB
[   2/ 197]                    token_embd.weight - [  384, 30522,     1,     1], type =    f16, converting to q8_0 .. size =    22.35 MiB ->    11.88 MiB
[   3/ 197]                 token_embd_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   4/ 197]               token_embd_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   5/ 197]                   token_types.weight - [  384,     2,     1,     1], type =    f32, size =    0.003 MB
[   6/ 197]                    blk.0.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   7/ 197]                  blk.0.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   8/ 197]               blk.0.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   9/ 197]             blk.0.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  10/ 197]          blk.0.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  11/ 197]        blk.0.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  12/ 197]                    blk.0.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  13/ 197]                  blk.0.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  14/ 197]                    blk.0.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  15/ 197]                  blk.0.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  16/ 197]                  blk.0.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  17/ 197]                blk.0.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  18/ 197]                    blk.0.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  19/ 197]                  blk.0.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  20/ 197]         blk.0.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  21/ 197]       blk.0.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  22/ 197]                    blk.1.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  23/ 197]                  blk.1.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  24/ 197]               blk.1.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  25/ 197]             blk.1.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  26/ 197]          blk.1.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  27/ 197]        blk.1.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  28/ 197]                    blk.1.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  29/ 197]                  blk.1.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  30/ 197]                    blk.1.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  31/ 197]                  blk.1.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  32/ 197]                  blk.1.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  33/ 197]                blk.1.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  34/ 197]                    blk.1.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  35/ 197]                  blk.1.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  36/ 197]         blk.1.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  37/ 197]       blk.1.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  38/ 197]                    blk.2.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  39/ 197]                  blk.2.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  40/ 197]               blk.2.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  41/ 197]             blk.2.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  42/ 197]          blk.2.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  43/ 197]        blk.2.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  44/ 197]                    blk.2.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  45/ 197]                  blk.2.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  46/ 197]                    blk.2.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  47/ 197]                  blk.2.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  48/ 197]                  blk.2.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  49/ 197]                blk.2.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  50/ 197]                    blk.2.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  51/ 197]                  blk.2.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  52/ 197]         blk.2.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  53/ 197]       blk.2.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  54/ 197]                    blk.3.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  55/ 197]                  blk.3.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  56/ 197]               blk.3.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  57/ 197]             blk.3.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  58/ 197]          blk.3.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  59/ 197]        blk.3.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  60/ 197]                    blk.3.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  61/ 197]                  blk.3.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  62/ 197]                    blk.3.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  63/ 197]                  blk.3.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  64/ 197]                  blk.3.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  65/ 197]                blk.3.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  66/ 197]                    blk.3.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  67/ 197]                  blk.3.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  68/ 197]         blk.3.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  69/ 197]       blk.3.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  70/ 197]                    blk.4.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  71/ 197]                  blk.4.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  72/ 197]               blk.4.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  73/ 197]             blk.4.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  74/ 197]          blk.4.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  75/ 197]        blk.4.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  76/ 197]                    blk.4.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  77/ 197]                  blk.4.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  78/ 197]                    blk.4.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  79/ 197]                  blk.4.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  80/ 197]                  blk.4.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  81/ 197]                blk.4.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  82/ 197]                    blk.4.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  83/ 197]                  blk.4.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  84/ 197]         blk.4.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  85/ 197]       blk.4.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  86/ 197]                    blk.5.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  87/ 197]                  blk.5.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  88/ 197]               blk.5.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  89/ 197]             blk.5.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  90/ 197]          blk.5.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  91/ 197]        blk.5.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  92/ 197]                    blk.5.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  93/ 197]                  blk.5.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  94/ 197]                    blk.5.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  95/ 197]                  blk.5.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  96/ 197]                  blk.5.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  97/ 197]                blk.5.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  98/ 197]                    blk.5.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  99/ 197]                  blk.5.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 100/ 197]         blk.5.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 101/ 197]       blk.5.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 102/ 197]                    blk.6.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 103/ 197]                  blk.6.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 104/ 197]               blk.6.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 105/ 197]             blk.6.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 106/ 197]          blk.6.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 107/ 197]        blk.6.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 108/ 197]                    blk.6.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 109/ 197]                  blk.6.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 110/ 197]                    blk.6.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 111/ 197]                  blk.6.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 112/ 197]                  blk.6.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 113/ 197]                blk.6.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 114/ 197]                    blk.6.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 115/ 197]                  blk.6.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 116/ 197]         blk.6.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 117/ 197]       blk.6.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 118/ 197]                    blk.7.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 119/ 197]                  blk.7.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 120/ 197]               blk.7.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 121/ 197]             blk.7.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 122/ 197]          blk.7.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 123/ 197]        blk.7.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 124/ 197]                    blk.7.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 125/ 197]                  blk.7.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 126/ 197]                    blk.7.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 127/ 197]                  blk.7.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 128/ 197]                  blk.7.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 129/ 197]                blk.7.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 130/ 197]                    blk.7.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 131/ 197]                  blk.7.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 132/ 197]         blk.7.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 133/ 197]       blk.7.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 134/ 197]                    blk.8.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 135/ 197]                  blk.8.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 136/ 197]               blk.8.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 137/ 197]             blk.8.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 138/ 197]          blk.8.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 139/ 197]        blk.8.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 140/ 197]                    blk.8.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 141/ 197]                  blk.8.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 142/ 197]                    blk.8.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 143/ 197]                  blk.8.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 144/ 197]                  blk.8.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 145/ 197]                blk.8.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 146/ 197]                    blk.8.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 147/ 197]                  blk.8.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 148/ 197]         blk.8.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 149/ 197]       blk.8.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 150/ 197]                    blk.9.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 151/ 197]                  blk.9.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 152/ 197]               blk.9.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 153/ 197]             blk.9.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 154/ 197]          blk.9.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 155/ 197]        blk.9.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 156/ 197]                    blk.9.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 157/ 197]                  blk.9.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 158/ 197]                    blk.9.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 159/ 197]                  blk.9.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 160/ 197]                  blk.9.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 161/ 197]                blk.9.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 162/ 197]                    blk.9.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 163/ 197]                  blk.9.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 164/ 197]         blk.9.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 165/ 197]       blk.9.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 166/ 197]                   blk.10.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 167/ 197]                 blk.10.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 168/ 197]              blk.10.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 169/ 197]            blk.10.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 170/ 197]         blk.10.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 171/ 197]       blk.10.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 172/ 197]                   blk.10.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 173/ 197]                 blk.10.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 174/ 197]                   blk.10.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 175/ 197]                 blk.10.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 176/ 197]                 blk.10.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 177/ 197]               blk.10.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 178/ 197]                   blk.10.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 179/ 197]                 blk.10.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 180/ 197]        blk.10.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 181/ 197]      blk.10.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 182/ 197]                   blk.11.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 183/ 197]                 blk.11.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 184/ 197]              blk.11.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 185/ 197]            blk.11.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 186/ 197]         blk.11.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 187/ 197]       blk.11.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 188/ 197]                   blk.11.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 189/ 197]                 blk.11.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 190/ 197]                   blk.11.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 191/ 197]                 blk.11.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 192/ 197]                 blk.11.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 193/ 197]               blk.11.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 194/ 197]                   blk.11.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 195/ 197]                 blk.11.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 196/ 197]        blk.11.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 197/ 197]      blk.11.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
llama_model_quantize_impl: model size  =    63.84 MB
llama_model_quantize_impl: quant size  =    34.38 MB

main: quantize time =    76.77 ms
main:    total time =    76.77 ms
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/embd_bge_small-tg-f16.log
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.097 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.805 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.713 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.725 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.726 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.727 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.727 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.728 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.729 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.739 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.739 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.740 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.740 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.743 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.744 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.744 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.747 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.747 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.748 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.749 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.445 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.024.091 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.092 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.024.093 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.024.093 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.024.094 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.024.094 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.024.094 I llama_model_loader: - type  f32:  124 tensors
0.00.024.095 I llama_model_loader: - type  f16:   73 tensors
0.00.024.096 I print_info: file format = GGUF V3 (latest)
0.00.024.096 I print_info: file type   = F16
0.00.024.097 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.026.547 I load: special tokens cache size = 5
0.00.027.769 I load: token to piece cache size = 0.2032 MB
0.00.027.773 I print_info: arch             = bert
0.00.027.773 I print_info: vocab_only       = 0
0.00.027.773 I print_info: n_ctx_train      = 512
0.00.027.773 I print_info: n_embd           = 384
0.00.027.774 I print_info: n_layer          = 12
0.00.027.777 I print_info: n_head           = 12
0.00.027.777 I print_info: n_head_kv        = 12
0.00.027.780 I print_info: n_rot            = 32
0.00.027.781 I print_info: n_swa            = 0
0.00.027.781 I print_info: n_embd_head_k    = 32
0.00.027.781 I print_info: n_embd_head_v    = 32
0.00.027.782 I print_info: n_gqa            = 1
0.00.027.782 I print_info: n_embd_k_gqa     = 384
0.00.027.783 I print_info: n_embd_v_gqa     = 384
0.00.027.784 I print_info: f_norm_eps       = 1.0e-12
0.00.027.784 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.027.784 I print_info: f_clamp_kqv      = 0.0e+00
0.00.027.785 I print_info: f_max_alibi_bias = 0.0e+00
0.00.027.785 I print_info: f_logit_scale    = 0.0e+00
0.00.027.786 I print_info: n_ff             = 1536
0.00.027.790 I print_info: n_expert         = 0
0.00.027.792 I print_info: n_expert_used    = 0
0.00.027.792 I print_info: causal attn      = 0
0.00.027.792 I print_info: pooling type     = 2
0.00.027.792 I print_info: rope type        = 2
0.00.027.792 I print_info: rope scaling     = linear
0.00.027.793 I print_info: freq_base_train  = 10000.0
0.00.027.793 I print_info: freq_scale_train = 1
0.00.027.793 I print_info: n_ctx_orig_yarn  = 512
0.00.027.794 I print_info: rope_finetuned   = unknown
0.00.027.794 I print_info: ssm_d_conv       = 0
0.00.027.794 I print_info: ssm_d_inner      = 0
0.00.027.794 I print_info: ssm_d_state      = 0
0.00.027.794 I print_info: ssm_dt_rank      = 0
0.00.027.794 I print_info: ssm_dt_b_c_rms   = 0
0.00.027.795 I print_info: model type       = 33M
0.00.027.796 I print_info: model params     = 33.21 M
0.00.027.796 I print_info: general.name     = Bge Small
0.00.027.797 I print_info: vocab type       = WPM
0.00.027.797 I print_info: n_vocab          = 30522
0.00.027.797 I print_info: n_merges         = 0
0.00.027.798 I print_info: BOS token        = 101 '[CLS]'
0.00.027.798 I print_info: UNK token        = 100 '[UNK]'
0.00.027.798 I print_info: SEP token        = 102 '[SEP]'
0.00.027.798 I print_info: PAD token        = 0 '[PAD]'
0.00.027.798 I print_info: MASK token       = 103 '[MASK]'
0.00.027.798 I print_info: LF token         = 0 '[PAD]'
0.00.027.799 I print_info: max token length = 21
0.00.029.066 I load_tensors: offloading 12 repeating layers to GPU
0.00.029.066 I load_tensors: offloading output layer to GPU
0.00.029.067 I load_tensors: offloaded 13/13 layers to GPU
0.00.029.088 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.029.089 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.029.254 I llama_init_from_model: n_seq_max     = 1
0.00.029.255 I llama_init_from_model: n_ctx         = 512
0.00.029.255 I llama_init_from_model: n_ctx_per_seq = 512
0.00.029.255 I llama_init_from_model: n_batch       = 2048
0.00.029.256 I llama_init_from_model: n_ubatch      = 2048
0.00.029.256 I llama_init_from_model: flash_attn    = 0
0.00.029.256 I llama_init_from_model: freq_base     = 10000.0
0.00.029.258 I llama_init_from_model: freq_scale    = 1
0.00.029.258 I ggml_metal_init: allocating
0.00.029.261 I ggml_metal_init: found device: Apple M4
0.00.029.264 I ggml_metal_init: picking default device: Apple M4
0.00.029.900 I ggml_metal_init: using embedded metal library
0.00.032.502 I ggml_metal_init: GPU name:   Apple M4
0.00.032.504 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.032.504 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.032.504 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.032.505 I ggml_metal_init: simdgroup reduction   = true
0.00.032.505 I ggml_metal_init: simdgroup matrix mul. = true
0.00.032.505 I ggml_metal_init: has bfloat            = true
0.00.032.505 I ggml_metal_init: use bfloat            = true
0.00.032.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.032.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.042.010 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.042.537 I init:      Metal KV buffer size =     9.00 MiB
0.00.042.540 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.042.541 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.043.247 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.043.248 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.043.249 I llama_init_from_model: graph nodes  = 429
0.00.043.249 I llama_init_from_model: graph splits = 2
0.00.043.250 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.043.250 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.048.992 I 
0.00.049.028 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.049.581 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.053.794 I llama_perf_context_print:        load time =      33.18 ms
0.00.053.795 I llama_perf_context_print: prompt eval time =       4.09 ms /     9 tokens (    0.45 ms per token,  2199.41 tokens per second)
0.00.053.795 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.053.796 I llama_perf_context_print:       total time =       4.80 ms /    10 tokens
0.00.054.139 I ggml_metal_free: deallocating

real	0m0.224s
user	0m0.037s
sys	0m0.024s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/embd_bge_small-tg-q8_0.log
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.726 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.152 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.156 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.157 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.158 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.158 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.159 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.159 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.160 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.160 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.161 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.161 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.161 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.164 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.164 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.165 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.165 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.165 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.166 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.330 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.914 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.916 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.916 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.916 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.917 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.917 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.917 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.918 I llama_model_loader: - type  f32:  124 tensors
0.00.014.918 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.918 I print_info: file format = GGUF V3 (latest)
0.00.014.919 I print_info: file type   = Q8_0
0.00.014.920 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.194 I load: special tokens cache size = 5
0.00.018.419 I load: token to piece cache size = 0.2032 MB
0.00.018.422 I print_info: arch             = bert
0.00.018.422 I print_info: vocab_only       = 0
0.00.018.422 I print_info: n_ctx_train      = 512
0.00.018.423 I print_info: n_embd           = 384
0.00.018.423 I print_info: n_layer          = 12
0.00.018.426 I print_info: n_head           = 12
0.00.018.427 I print_info: n_head_kv        = 12
0.00.018.427 I print_info: n_rot            = 32
0.00.018.427 I print_info: n_swa            = 0
0.00.018.427 I print_info: n_embd_head_k    = 32
0.00.018.427 I print_info: n_embd_head_v    = 32
0.00.018.428 I print_info: n_gqa            = 1
0.00.018.429 I print_info: n_embd_k_gqa     = 384
0.00.018.429 I print_info: n_embd_v_gqa     = 384
0.00.018.430 I print_info: f_norm_eps       = 1.0e-12
0.00.018.430 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.432 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.432 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.432 I print_info: f_logit_scale    = 0.0e+00
0.00.018.434 I print_info: n_ff             = 1536
0.00.018.435 I print_info: n_expert         = 0
0.00.018.435 I print_info: n_expert_used    = 0
0.00.018.435 I print_info: causal attn      = 0
0.00.018.435 I print_info: pooling type     = 2
0.00.018.435 I print_info: rope type        = 2
0.00.018.436 I print_info: rope scaling     = linear
0.00.018.437 I print_info: freq_base_train  = 10000.0
0.00.018.437 I print_info: freq_scale_train = 1
0.00.018.437 I print_info: n_ctx_orig_yarn  = 512
0.00.018.437 I print_info: rope_finetuned   = unknown
0.00.018.437 I print_info: ssm_d_conv       = 0
0.00.018.437 I print_info: ssm_d_inner      = 0
0.00.018.438 I print_info: ssm_d_state      = 0
0.00.018.438 I print_info: ssm_dt_rank      = 0
0.00.018.439 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.439 I print_info: model type       = 33M
0.00.018.439 I print_info: model params     = 33.21 M
0.00.018.439 I print_info: general.name     = Bge Small
0.00.018.440 I print_info: vocab type       = WPM
0.00.018.440 I print_info: n_vocab          = 30522
0.00.018.440 I print_info: n_merges         = 0
0.00.018.440 I print_info: BOS token        = 101 '[CLS]'
0.00.018.441 I print_info: UNK token        = 100 '[UNK]'
0.00.018.441 I print_info: SEP token        = 102 '[SEP]'
0.00.018.441 I print_info: PAD token        = 0 '[PAD]'
0.00.018.441 I print_info: MASK token       = 103 '[MASK]'
0.00.018.441 I print_info: LF token         = 0 '[PAD]'
0.00.018.442 I print_info: max token length = 21
0.00.019.688 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.688 I load_tensors: offloading output layer to GPU
0.00.019.689 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.696 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.697 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.019.834 I llama_init_from_model: n_seq_max     = 1
0.00.019.835 I llama_init_from_model: n_ctx         = 512
0.00.019.835 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.835 I llama_init_from_model: n_batch       = 2048
0.00.019.835 I llama_init_from_model: n_ubatch      = 2048
0.00.019.836 I llama_init_from_model: flash_attn    = 0
0.00.019.836 I llama_init_from_model: freq_base     = 10000.0
0.00.019.836 I llama_init_from_model: freq_scale    = 1
0.00.019.837 I ggml_metal_init: allocating
0.00.019.840 I ggml_metal_init: found device: Apple M4
0.00.019.842 I ggml_metal_init: picking default device: Apple M4
0.00.020.434 I ggml_metal_init: using embedded metal library
0.00.022.807 I ggml_metal_init: GPU name:   Apple M4
0.00.022.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.809 I ggml_metal_init: simdgroup reduction   = true
0.00.022.810 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.810 I ggml_metal_init: has bfloat            = true
0.00.022.810 I ggml_metal_init: use bfloat            = true
0.00.022.810 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.812 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.266 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.739 I init:      Metal KV buffer size =     9.00 MiB
0.00.033.741 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.743 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.372 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.373 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.373 I llama_init_from_model: graph nodes  = 429
0.00.034.373 I llama_init_from_model: graph splits = 2
0.00.034.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.014 I 
0.00.039.039 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.600 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.046 I llama_perf_context_print:        load time =      29.28 ms
0.00.044.048 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2077.08 tokens per second)
0.00.044.050 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.051 I llama_perf_context_print:       total time =       5.03 ms /    10 tokens
0.00.044.334 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.029s
sys	0m0.016s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_rerank_tiny
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/rerank_tiny.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:23:58 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json [1206/1206] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:23:59 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json [2030772/2030772] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:24:00 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json [1215/1215] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:24:00 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json [280/280] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json:
2025-01-16 10:24:00 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt:
2025-01-16 10:24:01 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json:
2025-01-16 10:24:01 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:24:01 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json [1206/1206] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/1_Pooling https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
+ local out=models-mnt/rerank-tiny/1_Pooling
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/1_Pooling
+ cd models-mnt/rerank-tiny/1_Pooling
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json:
2025-01-16 10:24:02 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ path_models=../models-mnt/rerank-tiny
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/rerank_tiny-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.522s
user	0m0.856s
sys	0m1.204s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/rerank_tiny-make.log
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target build_info
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-cpu
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Built target llava
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target llava_static
[ 37%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 37%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-sampling
[ 50%] Built target test-log
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Built target test-grammar-integration
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Built target test-arg-parser
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-autorelease
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-model-load-cancel
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Built target test-rope
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Built target llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Built target llama-batched
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gguf-split
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gritlm
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-infill
[ 77%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Built target llama-bench
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup-create
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup
[ 84%] Generating loading.html.hpp
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-passkey
[ 84%] Built target llama-parallel
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Built target llama-quantize
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-retrieval
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-run
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Built target llama-gen-docs
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.042s
user	0m6.005s
sys	0m9.609s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/rerank-tiny --outfile ../models-mnt/rerank-tiny/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: rerank-tiny
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,              torch.bfloat16 --> F16, shape = {384, 61056}
INFO:hf-to-gguf:token_types.weight,             torch.bfloat16 --> F32, shape = {384, 2}
INFO:hf-to-gguf:token_embd_norm.weight,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:token_embd_norm.bias,           torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.0.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.1.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.2.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.3.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:cls.weight,                     torch.bfloat16 --> F16, shape = {384, 1}
INFO:hf-to-gguf:cls.bias,                       torch.bfloat16 --> F32, shape = {1}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 8192
INFO:hf-to-gguf:gguf: embedding length = 384
INFO:hf-to-gguf:gguf: feed forward length = 1536
INFO:hf-to-gguf:gguf: head count = 12
INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 39382 merge(s).
INFO:gguf.vocab:Setting special token type bos to 0
INFO:gguf.vocab:Setting special token type eos to 2
INFO:gguf.vocab:Setting special token type unk to 3
INFO:gguf.vocab:Setting special token type sep to 2
INFO:gguf.vocab:Setting special token type pad to 1
WARNING:gguf.vocab:No handler for special token type cls with id 0 - skipping
INFO:gguf.vocab:Setting special token type mask to 4
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/rerank-tiny/ggml-model-f16.gguf: n_tensors = 70, total_size = 65.8M
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/65.8M [00:00<?, ?byte/s]Writing:  77%|  | 50.4M/65.8M [00:00<00:00, 494Mbyte/s]Writing: 100%|| 65.8M/65.8M [00:00<00:00, 403Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/rerank-tiny/ggml-model-f16.gguf
+ model_f16=../models-mnt/rerank-tiny/ggml-model-f16.gguf
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/rerank_tiny-rk-f16.log
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.201 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.207 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.760 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.766 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.768 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.769 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.770 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.770 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.771 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.772 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.773 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.774 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.774 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.775 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.778 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.778 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.779 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.780 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.780 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.364 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.473 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.934 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.936 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.936 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.937 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.937 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.937 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.938 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.938 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.938 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.939 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.939 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.940 I llama_model_loader: - type  f32:   40 tensors
0.00.050.940 I llama_model_loader: - type  f16:   30 tensors
0.00.050.941 I print_info: file format = GGUF V3 (latest)
0.00.050.941 I print_info: file type   = F16
0.00.050.943 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.067.949 W load: empty token at index 5
0.00.072.434 W load: model vocab missing newline token, using special_pad_id instead
0.00.073.776 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.073.807 I load: special tokens cache size = 5
0.00.339.195 I load: token to piece cache size = 1.5060 MB
0.00.339.201 I print_info: arch             = jina-bert-v2
0.00.339.201 I print_info: vocab_only       = 0
0.00.339.201 I print_info: n_ctx_train      = 8192
0.00.339.202 I print_info: n_embd           = 384
0.00.339.204 I print_info: n_layer          = 4
0.00.339.212 I print_info: n_head           = 12
0.00.339.212 I print_info: n_head_kv        = 12
0.00.339.212 I print_info: n_rot            = 32
0.00.339.212 I print_info: n_swa            = 0
0.00.339.213 I print_info: n_embd_head_k    = 32
0.00.339.213 I print_info: n_embd_head_v    = 32
0.00.339.213 I print_info: n_gqa            = 1
0.00.339.214 I print_info: n_embd_k_gqa     = 384
0.00.339.214 I print_info: n_embd_v_gqa     = 384
0.00.339.215 I print_info: f_norm_eps       = 1.0e-12
0.00.339.216 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.339.216 I print_info: f_clamp_kqv      = 0.0e+00
0.00.339.216 I print_info: f_max_alibi_bias = 8.0e+00
0.00.339.216 I print_info: f_logit_scale    = 0.0e+00
0.00.339.217 I print_info: n_ff             = 1536
0.00.339.217 I print_info: n_expert         = 0
0.00.339.218 I print_info: n_expert_used    = 0
0.00.339.218 I print_info: causal attn      = 0
0.00.339.218 I print_info: pooling type     = -1
0.00.339.220 I print_info: rope type        = -1
0.00.339.221 I print_info: rope scaling     = linear
0.00.339.221 I print_info: freq_base_train  = 10000.0
0.00.339.221 I print_info: freq_scale_train = 1
0.00.339.221 I print_info: n_ctx_orig_yarn  = 8192
0.00.339.222 I print_info: rope_finetuned   = unknown
0.00.339.222 I print_info: ssm_d_conv       = 0
0.00.339.222 I print_info: ssm_d_inner      = 0
0.00.339.222 I print_info: ssm_d_state      = 0
0.00.339.222 I print_info: ssm_dt_rank      = 0
0.00.339.222 I print_info: ssm_dt_b_c_rms   = 0
0.00.339.223 I print_info: model type       = 33M
0.00.339.223 I print_info: model params     = 32.90 M
0.00.339.223 I print_info: general.name     = Jina Bert Implementation
0.00.339.224 I print_info: vocab type       = BPE
0.00.339.224 I print_info: n_vocab          = 61056
0.00.339.225 I print_info: n_merges         = 39382
0.00.339.225 I print_info: BOS token        = 0 '<s>'
0.00.339.225 I print_info: EOS token        = 2 '</s>'
0.00.339.225 I print_info: UNK token        = 3 '<unk>'
0.00.339.225 I print_info: SEP token        = 2 '</s>'
0.00.339.229 I print_info: PAD token        = 1 '<pad>'
0.00.339.230 I print_info: MASK token       = 4 '<mask>'
0.00.339.230 I print_info: EOG token        = 2 '</s>'
0.00.339.230 I print_info: max token length = 45
0.00.340.486 I load_tensors: offloading 4 repeating layers to GPU
0.00.340.486 I load_tensors: offloading output layer to GPU
0.00.340.490 I load_tensors: offloaded 5/5 layers to GPU
0.00.340.517 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.340.518 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.340.793 I llama_init_from_model: n_seq_max     = 1
0.00.340.794 I llama_init_from_model: n_ctx         = 8192
0.00.340.795 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.340.795 I llama_init_from_model: n_batch       = 2048
0.00.340.795 I llama_init_from_model: n_ubatch      = 2048
0.00.340.795 I llama_init_from_model: flash_attn    = 0
0.00.340.795 I llama_init_from_model: freq_base     = 10000.0
0.00.340.796 I llama_init_from_model: freq_scale    = 1
0.00.340.796 I ggml_metal_init: allocating
0.00.340.799 I ggml_metal_init: found device: Apple M4
0.00.340.801 I ggml_metal_init: picking default device: Apple M4
0.00.341.774 I ggml_metal_init: using embedded metal library
0.00.344.923 I ggml_metal_init: GPU name:   Apple M4
0.00.344.924 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.925 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.925 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.925 I ggml_metal_init: simdgroup reduction   = true
0.00.344.925 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.926 I ggml_metal_init: has bfloat            = true
0.00.344.926 I ggml_metal_init: use bfloat            = true
0.00.344.926 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.927 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.354.413 I init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.356.892 I init:      Metal KV buffer size =    48.00 MiB
0.00.356.896 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.356.902 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.357.506 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.357.507 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.357.507 I llama_init_from_model: graph nodes  = 154
0.00.357.507 I llama_init_from_model: graph splits = 2
0.00.357.509 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.357.509 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.373.540 I 
0.00.373.575 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.373.721 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.373.722 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.373.725 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.373.725 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.373.729 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.373.729 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.374.224 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.377.678 I llama_perf_context_print:        load time =     350.32 ms
0.00.377.683 I llama_perf_context_print: prompt eval time =       3.45 ms /    62 tokens (    0.06 ms per token, 17991.87 tokens per second)
0.00.377.685 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.377.685 I llama_perf_context_print:       total time =       4.14 ms /    63 tokens
0.00.378.391 I ggml_metal_free: deallocating

real	0m1.156s
user	0m0.358s
sys	0m0.045s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 0'
+ check_score 'rerank score 0' 'rerank score 0:    0.023' 0.00 0.05
+ qnt='rerank score 0'
++ echo 'rerank score 0:    0.023'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.023
++ echo '0.023 < 0.00'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.023 > 0.05'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 0' 0.023
+ return 0
  - rerank score 0 @ 0.023 OK
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 1'
+ check_score 'rerank score 1' 'rerank score 1:    0.024' 0.00 0.05
+ qnt='rerank score 1'
++ echo 'rerank score 1:    0.024'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.024
++ echo '0.024 < 0.00'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.024 > 0.05'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 1' 0.024
+ return 0
  - rerank score 1 @ 0.024 OK
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 2'
+ check_score 'rerank score 2' 'rerank score 2:    0.199' 0.10 0.30
+ qnt='rerank score 2'
++ echo 'rerank score 2:    0.199'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.199
++ echo '0.199 < 0.10'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.199 > 0.30'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 2' 0.199
+ return 0
  - rerank score 2 @ 0.199 OK
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_pythia_1_4b
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:24:10 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json [570/570] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:24:11 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json [2113710/2113710] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:24:12 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json [396/396] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-01-16 10:24:12 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json [99/99] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ cd /Users/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ head -n 60 models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ path_models=../models-mnt/pythia/1.4B
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.560s
user	0m0.863s
sys	0m1.252s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-make.log
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 38%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-bpe
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-0
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Built target test-json-schema-to-grammar
[ 53%] Built target test-log
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Built target test-arg-parser
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-barrier
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Built target test-chat-template
[ 60%] Built target test-gguf
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Built target test-backend-ops
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-autorelease
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Built target test-quantize-fns
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Built target test-quantize-perf
[ 69%] Built target llama-batched-bench
[ 69%] Built target test-rope
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target llama-batched
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Built target llama-embedding
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-eval-callback
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Built target llama-gguf-split
[ 76%] Linking CXX executable ../../bin/llama-infill
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-gritlm
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Built target llama-bench
[ 78%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Built target llama-infill
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Built target llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Built target llama-lookup-merge
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Built target llama-lookup-create
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup
[ 84%] Generating loading.html.hpp
[ 84%] Linking CXX executable ../../bin/llama-passkey
[ 84%] Linking CXX executable ../../bin/llama-perplexity
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-quantize
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Generating index.html.gz.hpp
[ 86%] Built target llama-cli
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-parallel
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Built target llama-perplexity
[ 87%] Built target llama-passkey
[ 87%] Built target llama-quantize
[ 87%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-run
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Built target llama-save-load-state
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Built target llama-speculative
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-speculative-simple
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Built target llama-tokenize
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-tts
[ 97%] Built target llama-gen-docs
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-export-lora
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.992s
user	0m5.989s
sys	0m9.382s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/pythia/1.4B --outfile ../models-mnt/pythia/1.4B/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: 1.4B
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 50304}
INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.0.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.0.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.0.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.0.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.1.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.1.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.1.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.1.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.2.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.2.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.2.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.2.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.3.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.3.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.3.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.3.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.4.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.4.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.4.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.4.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.5.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.5.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.5.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.5.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.6.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.6.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.6.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.6.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.7.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.7.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.7.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.7.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.8.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.8.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.8.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.8.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.9.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.9.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.9.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.9.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.10.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.10.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.10.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.10.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.11.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.11.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.11.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.11.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.12.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.12.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.12.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.12.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.13.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.13.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.13.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.13.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.14.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.14.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.14.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.14.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.15.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.15.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.15.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.15.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.16.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.16.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.16.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.16.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.17.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.17.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.17.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.17.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.18.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.18.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.18.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.18.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.19.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.19.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.19.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.19.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.20.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.20.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.20.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.20.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.21.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.21.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.21.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.21.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.22.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.22.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.22.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.22.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.23.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.23.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.23.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.23.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output_norm.bias,          torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output.weight,             torch.float16 --> F16, shape = {2048, 50304}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 50009 merge(s).
INFO:gguf.vocab:Setting special token type bos to 0
INFO:gguf.vocab:Setting special token type eos to 0
INFO:gguf.vocab:Setting special token type unk to 0
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/pythia/1.4B/ggml-model-f16.gguf: n_tensors = 292, total_size = 2.8G
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/2.83G [00:00<?, ?byte/s]Writing:   7%|         | 206M/2.83G [00:00<00:06, 376Mbyte/s]Writing:  10%|         | 273M/2.83G [00:00<00:06, 401Mbyte/s]Writing:  12%|        | 332M/2.83G [00:00<00:05, 441Mbyte/s]Writing:  14%|        | 408M/2.83G [00:00<00:05, 434Mbyte/s]Writing:  17%|        | 475M/2.83G [00:01<00:05, 462Mbyte/s]Writing:  19%|        | 542M/2.83G [00:01<00:04, 489Mbyte/s]Writing:  22%|       | 609M/2.83G [00:01<00:04, 480Mbyte/s]Writing:  24%|       | 676M/2.83G [00:01<00:04, 484Mbyte/s]Writing:  26%|       | 744M/2.83G [00:01<00:04, 494Mbyte/s]Writing:  29%|       | 811M/2.83G [00:01<00:04, 484Mbyte/s]Writing:  31%|       | 878M/2.83G [00:01<00:03, 496Mbyte/s]Writing:  33%|      | 945M/2.83G [00:02<00:03, 518Mbyte/s]Writing:  36%|      | 1.01G/2.83G [00:02<00:03, 497Mbyte/s]Writing:  38%|      | 1.08G/2.83G [00:02<00:03, 508Mbyte/s]Writing:  41%|      | 1.15G/2.83G [00:02<00:03, 510Mbyte/s]Writing:  43%|     | 1.21G/2.83G [00:02<00:03, 495Mbyte/s]Writing:  45%|     | 1.28G/2.83G [00:02<00:03, 503Mbyte/s]Writing:  48%|     | 1.35G/2.83G [00:02<00:02, 509Mbyte/s]Writing:  50%|     | 1.42G/2.83G [00:02<00:02, 489Mbyte/s]Writing:  52%|    | 1.48G/2.83G [00:03<00:02, 494Mbyte/s]Writing:  55%|    | 1.55G/2.83G [00:03<00:02, 507Mbyte/s]Writing:  57%|    | 1.62G/2.83G [00:03<00:02, 497Mbyte/s]Writing:  59%|    | 1.68G/2.83G [00:03<00:02, 499Mbyte/s]Writing:  62%|   | 1.75G/2.83G [00:03<00:02, 509Mbyte/s]Writing:  64%|   | 1.82G/2.83G [00:03<00:02, 498Mbyte/s]Writing:  67%|   | 1.89G/2.83G [00:03<00:01, 507Mbyte/s]Writing:  69%|   | 1.95G/2.83G [00:04<00:01, 514Mbyte/s]Writing:  71%|  | 2.02G/2.83G [00:04<00:01, 503Mbyte/s]Writing:  74%|  | 2.09G/2.83G [00:04<00:01, 498Mbyte/s]Writing:  76%|  | 2.15G/2.83G [00:04<00:01, 506Mbyte/s]Writing:  78%|  | 2.22G/2.83G [00:04<00:01, 465Mbyte/s]Writing:  81%|  | 2.29G/2.83G [00:04<00:01, 475Mbyte/s]Writing:  83%| | 2.35G/2.83G [00:04<00:00, 493Mbyte/s]Writing:  86%| | 2.42G/2.83G [00:05<00:00, 460Mbyte/s]Writing:  88%| | 2.49G/2.83G [00:05<00:00, 465Mbyte/s]Writing:  90%| | 2.55G/2.83G [00:05<00:00, 474Mbyte/s]Writing:  93%|| 2.62G/2.83G [00:05<00:00, 446Mbyte/s]Writing: 100%|| 2.83G/2.83G [00:06<00:00, 379Mbyte/s]Writing: 100%|| 2.83G/2.83G [00:06<00:00, 463Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/pythia/1.4B/ggml-model-f16.gguf
+ model_f16=../models-mnt/pythia/1.4B/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf
+ wiki_test_60=../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf q8_0
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q8_0 .. size =   196.50 MiB ->   104.39 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q8_0 .. size =   196.50 MiB ->   104.39 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1435.23 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf q4_0

main: quantize time =  3516.99 ms
main:    total time =  3516.99 ms
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_0 .. size =   196.50 MiB ->    55.27 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   786.31 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf q4_1

main: quantize time =  2032.36 ms
main:    total time =  2032.36 ms
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_1 .. size =   196.50 MiB ->    61.41 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   864.46 MB

main: quantize time =  1849.60 ms
main:    total time =  1849.60 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf q5_0
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_0 .. size =   196.50 MiB ->    67.55 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   942.60 MB

main: quantize time =  2034.66 ms
main:    total time =  2034.66 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf q5_1
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_1 .. size =   196.50 MiB ->    73.69 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1020.74 MB

main: quantize time =  2519.45 ms
main:    total time =  2519.45 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf q2_k
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q2_K .. size =   196.50 MiB ->    32.24 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   542.04 MB

main: quantize time =  5051.72 ms
main:    total time =  5051.72 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf q3_k
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q3_K .. size =   196.50 MiB ->    42.22 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   724.27 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf q4_k

main: quantize time =  5693.54 ms
main:    total time =  5693.54 ms
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_K .. size =   196.50 MiB ->    55.27 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   871.81 MB

main: quantize time =  6675.27 ms
main:    total time =  6675.27 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf q5_k
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf' as Q5_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_K .. size =   196.50 MiB ->    67.55 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1006.35 MB

main: quantize time =  5999.38 ms
main:    total time =  5999.38 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf q6_k
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf' as Q6_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1108.64 MB

main: quantize time =  4509.18 ms
main:    total time =  4509.18 ms
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.248 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.410 I main: llama backend init
0.00.000.426 I main: load the model and apply lora adapter, if any
0.00.039.699 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.053.344 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.053.397 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.053.402 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.053.403 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.053.404 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.053.404 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.053.405 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.053.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.053.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.053.417 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.053.418 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.053.418 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.053.419 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.053.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.053.425 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.053.426 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.053.427 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.062.775 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.064.907 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.072.317 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.072.320 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.072.321 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.072.321 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.072.322 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.072.323 I llama_model_loader: - type  f32:  194 tensors
0.00.072.323 I llama_model_loader: - type  f16:   98 tensors
0.00.072.325 I print_info: file format = GGUF V3 (latest)
0.00.072.326 I print_info: file type   = all F32 (guessed)
0.00.072.327 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.100.905 I load: special tokens cache size = 25
0.00.107.997 I load: token to piece cache size = 0.2984 MB
0.00.108.000 I print_info: arch             = gptneox
0.00.108.000 I print_info: vocab_only       = 0
0.00.108.000 I print_info: n_ctx_train      = 2048
0.00.108.000 I print_info: n_embd           = 2048
0.00.108.001 I print_info: n_layer          = 24
0.00.108.003 I print_info: n_head           = 16
0.00.108.004 I print_info: n_head_kv        = 16
0.00.108.004 I print_info: n_rot            = 32
0.00.108.006 I print_info: n_swa            = 0
0.00.108.006 I print_info: n_embd_head_k    = 128
0.00.108.007 I print_info: n_embd_head_v    = 128
0.00.108.008 I print_info: n_gqa            = 1
0.00.108.009 I print_info: n_embd_k_gqa     = 2048
0.00.108.009 I print_info: n_embd_v_gqa     = 2048
0.00.108.010 I print_info: f_norm_eps       = 1.0e-05
0.00.108.010 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.108.011 I print_info: f_clamp_kqv      = 0.0e+00
0.00.108.011 I print_info: f_max_alibi_bias = 0.0e+00
0.00.108.011 I print_info: f_logit_scale    = 0.0e+00
0.00.108.012 I print_info: n_ff             = 8192
0.00.108.012 I print_info: n_expert         = 0
0.00.108.012 I print_info: n_expert_used    = 0
0.00.108.012 I print_info: causal attn      = 1
0.00.108.012 I print_info: pooling type     = 0
0.00.108.013 I print_info: rope type        = 2
0.00.108.014 I print_info: rope scaling     = linear
0.00.108.014 I print_info: freq_base_train  = 10000.0
0.00.108.014 I print_info: freq_scale_train = 1
0.00.108.014 I print_info: n_ctx_orig_yarn  = 2048
0.00.108.015 I print_info: rope_finetuned   = unknown
0.00.108.015 I print_info: ssm_d_conv       = 0
0.00.108.015 I print_info: ssm_d_inner      = 0
0.00.108.015 I print_info: ssm_d_state      = 0
0.00.108.015 I print_info: ssm_dt_rank      = 0
0.00.108.015 I print_info: ssm_dt_b_c_rms   = 0
0.00.108.016 I print_info: model type       = 1.4B
0.00.108.016 I print_info: model params     = 1.41 B
0.00.108.016 I print_info: general.name     = 1.4B
0.00.108.016 I print_info: vocab type       = BPE
0.00.108.017 I print_info: n_vocab          = 50304
0.00.108.017 I print_info: n_merges         = 50009
0.00.108.017 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.108.017 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.108.017 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.108.021 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.108.021 I print_info: LF token         = 128 ''
0.00.108.026 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.108.028 I print_info: max token length = 1024
0.00.110.741 I load_tensors: offloading 24 repeating layers to GPU
0.00.110.742 I load_tensors: offloading output layer to GPU
0.00.110.742 I load_tensors: offloaded 25/25 layers to GPU
0.00.110.760 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.110.761 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.111.052 I llama_init_from_model: n_seq_max     = 1
0.00.111.053 I llama_init_from_model: n_ctx         = 2048
0.00.111.053 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.111.053 I llama_init_from_model: n_batch       = 2048
0.00.111.053 I llama_init_from_model: n_ubatch      = 512
0.00.111.054 I llama_init_from_model: flash_attn    = 0
0.00.111.054 I llama_init_from_model: freq_base     = 10000.0
0.00.111.054 I llama_init_from_model: freq_scale    = 1
0.00.111.055 I ggml_metal_init: allocating
0.00.111.058 I ggml_metal_init: found device: Apple M4
0.00.111.060 I ggml_metal_init: picking default device: Apple M4
0.00.111.735 I ggml_metal_init: using embedded metal library
0.00.121.194 I ggml_metal_init: GPU name:   Apple M4
0.00.121.196 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.121.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.121.196 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.121.197 I ggml_metal_init: simdgroup reduction   = true
0.00.121.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.121.197 I ggml_metal_init: has bfloat            = true
0.00.121.197 I ggml_metal_init: use bfloat            = true
0.00.121.197 I ggml_metal_init: hasUnifiedMemory      = true
0.00.121.198 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.144.778 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.165.191 I init:      Metal KV buffer size =   384.00 MiB
0.00.165.197 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.165.217 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.166.141 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.166.143 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.166.143 I llama_init_from_model: graph nodes  = 967
0.00.166.144 I llama_init_from_model: graph splits = 2
0.00.166.147 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.166.261 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.166.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.247.035 I main: llama threadpool init, n_threads = 4
0.00.247.073 I 
0.00.247.107 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.247.108 I 
0.00.247.172 I sampler seed: 1234
0.00.247.177 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.247.201 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.247.203 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.247.203 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.073.779 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.02.073.780 I llama_perf_context_print:        load time =     207.32 ms
0.02.073.781 I llama_perf_context_print: prompt eval time =      43.47 ms /     7 tokens (    6.21 ms per token,   161.04 tokens per second)
0.02.073.782 I llama_perf_context_print:        eval time =    1780.18 ms /    63 runs   (   28.26 ms per token,    35.39 tokens per second)
0.02.073.783 I llama_perf_context_print:       total time =    1826.75 ms /    70 tokens
0.02.077.590 I ggml_metal_free: deallocating

real	0m2.399s
user	0m0.144s
sys	0m0.112s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.891 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.219 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.225 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.227 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.228 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.234 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.236 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.236 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.236 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.236 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.237 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.237 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.237 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.239 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.240 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.073 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.107 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.891 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.894 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.895 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.895 I llama_model_loader: - type  f32:  194 tensors
0.00.034.896 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.897 I print_info: file format = GGUF V3 (latest)
0.00.034.897 I print_info: file type   = Q8_0
0.00.034.899 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.751 I load: special tokens cache size = 25
0.00.061.810 I load: token to piece cache size = 0.2984 MB
0.00.061.814 I print_info: arch             = gptneox
0.00.061.814 I print_info: vocab_only       = 0
0.00.061.814 I print_info: n_ctx_train      = 2048
0.00.061.814 I print_info: n_embd           = 2048
0.00.061.815 I print_info: n_layer          = 24
0.00.061.820 I print_info: n_head           = 16
0.00.061.821 I print_info: n_head_kv        = 16
0.00.061.821 I print_info: n_rot            = 32
0.00.061.823 I print_info: n_swa            = 0
0.00.061.823 I print_info: n_embd_head_k    = 128
0.00.061.823 I print_info: n_embd_head_v    = 128
0.00.061.824 I print_info: n_gqa            = 1
0.00.061.825 I print_info: n_embd_k_gqa     = 2048
0.00.061.826 I print_info: n_embd_v_gqa     = 2048
0.00.061.826 I print_info: f_norm_eps       = 1.0e-05
0.00.061.827 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.827 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.827 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.827 I print_info: f_logit_scale    = 0.0e+00
0.00.061.828 I print_info: n_ff             = 8192
0.00.061.828 I print_info: n_expert         = 0
0.00.061.829 I print_info: n_expert_used    = 0
0.00.061.829 I print_info: causal attn      = 1
0.00.061.829 I print_info: pooling type     = 0
0.00.061.829 I print_info: rope type        = 2
0.00.061.829 I print_info: rope scaling     = linear
0.00.061.830 I print_info: freq_base_train  = 10000.0
0.00.061.830 I print_info: freq_scale_train = 1
0.00.061.830 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.830 I print_info: rope_finetuned   = unknown
0.00.061.831 I print_info: ssm_d_conv       = 0
0.00.061.831 I print_info: ssm_d_inner      = 0
0.00.061.831 I print_info: ssm_d_state      = 0
0.00.061.833 I print_info: ssm_dt_rank      = 0
0.00.061.833 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.833 I print_info: model type       = 1.4B
0.00.061.834 I print_info: model params     = 1.41 B
0.00.061.834 I print_info: general.name     = 1.4B
0.00.061.834 I print_info: vocab type       = BPE
0.00.061.835 I print_info: n_vocab          = 50304
0.00.061.835 I print_info: n_merges         = 50009
0.00.061.835 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.835 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.835 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.835 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.836 I print_info: LF token         = 128 ''
0.00.061.836 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.840 I print_info: max token length = 1024
0.00.064.264 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.264 I load_tensors: offloading output layer to GPU
0.00.064.265 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.276 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.277 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.612 I llama_init_from_model: n_seq_max     = 1
0.00.064.613 I llama_init_from_model: n_ctx         = 2048
0.00.064.613 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.613 I llama_init_from_model: n_batch       = 2048
0.00.064.613 I llama_init_from_model: n_ubatch      = 512
0.00.064.614 I llama_init_from_model: flash_attn    = 0
0.00.064.614 I llama_init_from_model: freq_base     = 10000.0
0.00.064.614 I llama_init_from_model: freq_scale    = 1
0.00.064.615 I ggml_metal_init: allocating
0.00.064.618 I ggml_metal_init: found device: Apple M4
0.00.064.620 I ggml_metal_init: picking default device: Apple M4
0.00.065.371 I ggml_metal_init: using embedded metal library
0.00.067.998 I ggml_metal_init: GPU name:   Apple M4
0.00.068.000 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.000 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.000 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.001 I ggml_metal_init: simdgroup reduction   = true
0.00.068.001 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.001 I ggml_metal_init: has bfloat            = true
0.00.068.001 I ggml_metal_init: use bfloat            = true
0.00.068.002 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.002 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.390 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.970 I init:      Metal KV buffer size =   384.00 MiB
0.00.102.977 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.002 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.268 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.104.270 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.104.271 I llama_init_from_model: graph nodes  = 967
0.00.104.271 I llama_init_from_model: graph splits = 2
0.00.104.275 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.404 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.404 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.480.850 I main: llama threadpool init, n_threads = 4
0.01.480.899 I 
0.01.480.935 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.480.937 I 
0.01.481.178 I sampler seed: 1234
0.01.481.182 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.481.200 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.481.200 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.481.201 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.580.349 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49650.35 tokens per second)
0.02.580.350 I llama_perf_context_print:        load time =    1470.95 ms
0.02.580.351 I llama_perf_context_print: prompt eval time =      49.83 ms /     7 tokens (    7.12 ms per token,   140.48 tokens per second)
0.02.580.351 I llama_perf_context_print:        eval time =    1046.26 ms /    63 runs   (   16.61 ms per token,    60.21 tokens per second)
0.02.580.352 I llama_perf_context_print:       total time =    1099.50 ms /    70 tokens
0.02.584.287 I ggml_metal_free: deallocating

real	0m2.602s
user	0m0.113s
sys	0m0.214s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.013.882 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.197 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.203 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.209 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.212 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.212 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.212 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.213 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.213 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.215 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.215 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.498 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.005 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.007 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.007 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.008 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.008 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.008 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.009 I llama_model_loader: - type  f32:  194 tensors
0.00.040.009 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.009 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.010 I print_info: file format = GGUF V3 (latest)
0.00.040.011 I print_info: file type   = Q4_0
0.00.040.012 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.065.599 I load: special tokens cache size = 25
0.00.075.089 I load: token to piece cache size = 0.2984 MB
0.00.075.093 I print_info: arch             = gptneox
0.00.075.094 I print_info: vocab_only       = 0
0.00.075.094 I print_info: n_ctx_train      = 2048
0.00.075.094 I print_info: n_embd           = 2048
0.00.075.094 I print_info: n_layer          = 24
0.00.075.099 I print_info: n_head           = 16
0.00.075.100 I print_info: n_head_kv        = 16
0.00.075.101 I print_info: n_rot            = 32
0.00.075.101 I print_info: n_swa            = 0
0.00.075.101 I print_info: n_embd_head_k    = 128
0.00.075.101 I print_info: n_embd_head_v    = 128
0.00.075.103 I print_info: n_gqa            = 1
0.00.075.104 I print_info: n_embd_k_gqa     = 2048
0.00.075.104 I print_info: n_embd_v_gqa     = 2048
0.00.075.105 I print_info: f_norm_eps       = 1.0e-05
0.00.075.106 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.106 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.106 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.106 I print_info: f_logit_scale    = 0.0e+00
0.00.075.108 I print_info: n_ff             = 8192
0.00.075.108 I print_info: n_expert         = 0
0.00.075.108 I print_info: n_expert_used    = 0
0.00.075.108 I print_info: causal attn      = 1
0.00.075.108 I print_info: pooling type     = 0
0.00.075.109 I print_info: rope type        = 2
0.00.075.109 I print_info: rope scaling     = linear
0.00.075.109 I print_info: freq_base_train  = 10000.0
0.00.075.110 I print_info: freq_scale_train = 1
0.00.075.110 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.110 I print_info: rope_finetuned   = unknown
0.00.075.111 I print_info: ssm_d_conv       = 0
0.00.075.111 I print_info: ssm_d_inner      = 0
0.00.075.111 I print_info: ssm_d_state      = 0
0.00.075.111 I print_info: ssm_dt_rank      = 0
0.00.075.111 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.112 I print_info: model type       = 1.4B
0.00.075.112 I print_info: model params     = 1.41 B
0.00.075.113 I print_info: general.name     = 1.4B
0.00.075.113 I print_info: vocab type       = BPE
0.00.075.113 I print_info: n_vocab          = 50304
0.00.075.115 I print_info: n_merges         = 50009
0.00.075.115 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.116 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.116 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.116 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.117 I print_info: LF token         = 128 ''
0.00.075.117 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.119 I print_info: max token length = 1024
0.00.078.112 I load_tensors: offloading 24 repeating layers to GPU
0.00.078.113 I load_tensors: offloading output layer to GPU
0.00.078.113 I load_tensors: offloaded 25/25 layers to GPU
0.00.078.125 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.078.128 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.078.599 I llama_init_from_model: n_seq_max     = 1
0.00.078.601 I llama_init_from_model: n_ctx         = 2048
0.00.078.601 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.078.601 I llama_init_from_model: n_batch       = 2048
0.00.078.602 I llama_init_from_model: n_ubatch      = 512
0.00.078.602 I llama_init_from_model: flash_attn    = 0
0.00.078.603 I llama_init_from_model: freq_base     = 10000.0
0.00.078.603 I llama_init_from_model: freq_scale    = 1
0.00.078.604 I ggml_metal_init: allocating
0.00.078.608 I ggml_metal_init: found device: Apple M4
0.00.078.611 I ggml_metal_init: picking default device: Apple M4
0.00.079.612 I ggml_metal_init: using embedded metal library
0.00.083.702 I ggml_metal_init: GPU name:   Apple M4
0.00.083.705 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.705 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.706 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.706 I ggml_metal_init: simdgroup reduction   = true
0.00.083.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.706 I ggml_metal_init: has bfloat            = true
0.00.083.707 I ggml_metal_init: use bfloat            = true
0.00.083.707 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.679 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.122.173 I init:      Metal KV buffer size =   384.00 MiB
0.00.122.184 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.213 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.123.235 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.123.237 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.123.238 I llama_init_from_model: graph nodes  = 967
0.00.123.238 I llama_init_from_model: graph splits = 2
0.00.123.242 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.123.370 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.123.371 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.725.092 I main: llama threadpool init, n_threads = 4
0.00.725.128 I 
0.00.725.156 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.725.156 I 
0.00.725.362 I sampler seed: 1234
0.00.725.366 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.725.383 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.725.383 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.725.383 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.402.914 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.01.402.915 I llama_perf_context_print:        load time =     711.20 ms
0.01.402.916 I llama_perf_context_print: prompt eval time =      44.97 ms /     7 tokens (    6.42 ms per token,   155.66 tokens per second)
0.01.402.917 I llama_perf_context_print:        eval time =     629.48 ms /    63 runs   (    9.99 ms per token,   100.08 tokens per second)
0.01.402.920 I llama_perf_context_print:       total time =     677.83 ms /    70 tokens
0.01.405.652 I ggml_metal_free: deallocating

real	0m1.429s
user	0m0.127s
sys	0m0.165s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.704 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.676 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.687 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.687 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.688 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.688 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.689 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.689 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.690 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.690 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.691 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.691 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.692 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.695 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.695 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.695 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.530 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.538 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.368 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.370 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.370 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.370 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.371 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.371 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.371 I llama_model_loader: - type  f32:  194 tensors
0.00.032.372 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.372 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.372 I print_info: file format = GGUF V3 (latest)
0.00.032.373 I print_info: file type   = Q4_1
0.00.032.374 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.052.057 I load: special tokens cache size = 25
0.00.058.528 I load: token to piece cache size = 0.2984 MB
0.00.058.532 I print_info: arch             = gptneox
0.00.058.532 I print_info: vocab_only       = 0
0.00.058.532 I print_info: n_ctx_train      = 2048
0.00.058.532 I print_info: n_embd           = 2048
0.00.058.532 I print_info: n_layer          = 24
0.00.058.536 I print_info: n_head           = 16
0.00.058.536 I print_info: n_head_kv        = 16
0.00.058.537 I print_info: n_rot            = 32
0.00.058.537 I print_info: n_swa            = 0
0.00.058.537 I print_info: n_embd_head_k    = 128
0.00.058.538 I print_info: n_embd_head_v    = 128
0.00.058.539 I print_info: n_gqa            = 1
0.00.058.539 I print_info: n_embd_k_gqa     = 2048
0.00.058.540 I print_info: n_embd_v_gqa     = 2048
0.00.058.541 I print_info: f_norm_eps       = 1.0e-05
0.00.058.541 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.541 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.541 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.542 I print_info: f_logit_scale    = 0.0e+00
0.00.058.542 I print_info: n_ff             = 8192
0.00.058.543 I print_info: n_expert         = 0
0.00.058.543 I print_info: n_expert_used    = 0
0.00.058.543 I print_info: causal attn      = 1
0.00.058.543 I print_info: pooling type     = 0
0.00.058.543 I print_info: rope type        = 2
0.00.058.543 I print_info: rope scaling     = linear
0.00.058.544 I print_info: freq_base_train  = 10000.0
0.00.058.544 I print_info: freq_scale_train = 1
0.00.058.544 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.544 I print_info: rope_finetuned   = unknown
0.00.058.545 I print_info: ssm_d_conv       = 0
0.00.058.545 I print_info: ssm_d_inner      = 0
0.00.058.546 I print_info: ssm_d_state      = 0
0.00.058.547 I print_info: ssm_dt_rank      = 0
0.00.058.547 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.547 I print_info: model type       = 1.4B
0.00.058.547 I print_info: model params     = 1.41 B
0.00.058.547 I print_info: general.name     = 1.4B
0.00.058.548 I print_info: vocab type       = BPE
0.00.058.548 I print_info: n_vocab          = 50304
0.00.058.548 I print_info: n_merges         = 50009
0.00.058.548 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.549 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.549 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.549 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.549 I print_info: LF token         = 128 ''
0.00.058.550 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.550 I print_info: max token length = 1024
0.00.060.498 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.499 I load_tensors: offloading output layer to GPU
0.00.060.499 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.509 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.060.510 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.060.785 I llama_init_from_model: n_seq_max     = 1
0.00.060.786 I llama_init_from_model: n_ctx         = 2048
0.00.060.786 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.060.786 I llama_init_from_model: n_batch       = 2048
0.00.060.786 I llama_init_from_model: n_ubatch      = 512
0.00.060.786 I llama_init_from_model: flash_attn    = 0
0.00.060.787 I llama_init_from_model: freq_base     = 10000.0
0.00.060.787 I llama_init_from_model: freq_scale    = 1
0.00.060.787 I ggml_metal_init: allocating
0.00.060.791 I ggml_metal_init: found device: Apple M4
0.00.060.793 I ggml_metal_init: picking default device: Apple M4
0.00.061.394 I ggml_metal_init: using embedded metal library
0.00.063.766 I ggml_metal_init: GPU name:   Apple M4
0.00.063.767 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.768 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.768 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.768 I ggml_metal_init: simdgroup reduction   = true
0.00.063.769 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.769 I ggml_metal_init: has bfloat            = true
0.00.063.769 I ggml_metal_init: use bfloat            = true
0.00.063.769 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.770 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.688 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.373 I init:      Metal KV buffer size =   384.00 MiB
0.00.093.380 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.398 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.094.443 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.094.445 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.094.445 I llama_init_from_model: graph nodes  = 967
0.00.094.445 I llama_init_from_model: graph splits = 2
0.00.094.448 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.094.588 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.588 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.909 I main: llama threadpool init, n_threads = 4
0.00.815.950 I 
0.00.815.981 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.981 I 
0.00.816.203 I sampler seed: 1234
0.00.816.207 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.816.227 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.816.227 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.816.228 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.541.901 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63677.13 tokens per second)
0.01.541.901 I llama_perf_context_print:        load time =     807.20 ms
0.01.541.902 I llama_perf_context_print: prompt eval time =      45.68 ms /     7 tokens (    6.53 ms per token,   153.24 tokens per second)
0.01.541.903 I llama_perf_context_print:        eval time =     677.14 ms /    63 runs   (   10.75 ms per token,    93.04 tokens per second)
0.01.541.904 I llama_perf_context_print:       total time =     725.99 ms /    70 tokens
0.01.544.897 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.109s
sys	0m0.159s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.014.919 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.413 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.042.418 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.424 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.425 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.427 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.429 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.431 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.431 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.431 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.478 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.238 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.240 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.240 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.241 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.241 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.241 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.054.242 I llama_model_loader: - type  f32:  194 tensors
0.00.054.242 I llama_model_loader: - type q5_0:   97 tensors
0.00.054.243 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.243 I print_info: file format = GGUF V3 (latest)
0.00.054.244 I print_info: file type   = Q5_0
0.00.054.245 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.087.265 I load: special tokens cache size = 25
0.00.096.848 I load: token to piece cache size = 0.2984 MB
0.00.096.852 I print_info: arch             = gptneox
0.00.096.852 I print_info: vocab_only       = 0
0.00.096.853 I print_info: n_ctx_train      = 2048
0.00.096.853 I print_info: n_embd           = 2048
0.00.096.853 I print_info: n_layer          = 24
0.00.096.856 I print_info: n_head           = 16
0.00.096.857 I print_info: n_head_kv        = 16
0.00.096.857 I print_info: n_rot            = 32
0.00.096.858 I print_info: n_swa            = 0
0.00.096.858 I print_info: n_embd_head_k    = 128
0.00.096.858 I print_info: n_embd_head_v    = 128
0.00.096.859 I print_info: n_gqa            = 1
0.00.096.860 I print_info: n_embd_k_gqa     = 2048
0.00.096.861 I print_info: n_embd_v_gqa     = 2048
0.00.096.862 I print_info: f_norm_eps       = 1.0e-05
0.00.096.862 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.096.862 I print_info: f_clamp_kqv      = 0.0e+00
0.00.096.863 I print_info: f_max_alibi_bias = 0.0e+00
0.00.096.863 I print_info: f_logit_scale    = 0.0e+00
0.00.096.864 I print_info: n_ff             = 8192
0.00.096.864 I print_info: n_expert         = 0
0.00.096.864 I print_info: n_expert_used    = 0
0.00.096.864 I print_info: causal attn      = 1
0.00.096.864 I print_info: pooling type     = 0
0.00.096.865 I print_info: rope type        = 2
0.00.096.865 I print_info: rope scaling     = linear
0.00.096.866 I print_info: freq_base_train  = 10000.0
0.00.096.869 I print_info: freq_scale_train = 1
0.00.096.869 I print_info: n_ctx_orig_yarn  = 2048
0.00.096.869 I print_info: rope_finetuned   = unknown
0.00.096.869 I print_info: ssm_d_conv       = 0
0.00.096.870 I print_info: ssm_d_inner      = 0
0.00.096.870 I print_info: ssm_d_state      = 0
0.00.096.870 I print_info: ssm_dt_rank      = 0
0.00.096.870 I print_info: ssm_dt_b_c_rms   = 0
0.00.096.870 I print_info: model type       = 1.4B
0.00.096.871 I print_info: model params     = 1.41 B
0.00.096.871 I print_info: general.name     = 1.4B
0.00.096.872 I print_info: vocab type       = BPE
0.00.096.872 I print_info: n_vocab          = 50304
0.00.096.872 I print_info: n_merges         = 50009
0.00.096.873 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.096.873 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.096.874 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.096.881 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.096.884 I print_info: LF token         = 128 ''
0.00.096.885 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.096.885 I print_info: max token length = 1024
0.00.099.066 I load_tensors: offloading 24 repeating layers to GPU
0.00.099.066 I load_tensors: offloading output layer to GPU
0.00.099.067 I load_tensors: offloaded 25/25 layers to GPU
0.00.099.077 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.099.078 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.099.441 I llama_init_from_model: n_seq_max     = 1
0.00.099.442 I llama_init_from_model: n_ctx         = 2048
0.00.099.442 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.099.442 I llama_init_from_model: n_batch       = 2048
0.00.099.442 I llama_init_from_model: n_ubatch      = 512
0.00.099.443 I llama_init_from_model: flash_attn    = 0
0.00.099.443 I llama_init_from_model: freq_base     = 10000.0
0.00.099.443 I llama_init_from_model: freq_scale    = 1
0.00.099.444 I ggml_metal_init: allocating
0.00.099.447 I ggml_metal_init: found device: Apple M4
0.00.099.450 I ggml_metal_init: picking default device: Apple M4
0.00.100.215 I ggml_metal_init: using embedded metal library
0.00.103.656 I ggml_metal_init: GPU name:   Apple M4
0.00.103.658 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.103.659 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.103.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.103.660 I ggml_metal_init: simdgroup reduction   = true
0.00.103.660 I ggml_metal_init: simdgroup matrix mul. = true
0.00.103.660 I ggml_metal_init: has bfloat            = true
0.00.103.660 I ggml_metal_init: use bfloat            = true
0.00.103.661 I ggml_metal_init: hasUnifiedMemory      = true
0.00.103.662 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.819 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.136.840 I init:      Metal KV buffer size =   384.00 MiB
0.00.136.848 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.136.871 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.137.882 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.137.883 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.137.883 I llama_init_from_model: graph nodes  = 967
0.00.137.883 I llama_init_from_model: graph splits = 2
0.00.137.886 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.138.010 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.138.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.163 I main: llama threadpool init, n_threads = 4
0.00.803.279 I 
0.00.803.352 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.353 I 
0.00.803.856 I sampler seed: 1234
0.00.803.869 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.934 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.937 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.937 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.602.197 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.01.602.198 I llama_perf_context_print:        load time =     788.23 ms
0.01.602.199 I llama_perf_context_print: prompt eval time =      54.89 ms /     7 tokens (    7.84 ms per token,   127.52 tokens per second)
0.01.602.199 I llama_perf_context_print:        eval time =     740.32 ms /    63 runs   (   11.75 ms per token,    85.10 tokens per second)
0.01.602.200 I llama_perf_context_print:       total time =     799.05 ms /    70 tokens
0.01.605.308 I ggml_metal_free: deallocating

real	0m1.635s
user	0m0.146s
sys	0m0.176s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.474 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.272 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.025.277 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.284 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.284 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.285 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.285 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.287 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.287 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.288 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.289 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.289 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.290 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.291 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.291 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.128 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.052 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.052 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.053 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.053 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.034.054 I llama_model_loader: - type  f32:  194 tensors
0.00.034.054 I llama_model_loader: - type q5_1:   97 tensors
0.00.034.054 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.055 I print_info: file format = GGUF V3 (latest)
0.00.034.055 I print_info: file type   = Q5_1
0.00.034.056 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.053.554 I load: special tokens cache size = 25
0.00.059.643 I load: token to piece cache size = 0.2984 MB
0.00.059.646 I print_info: arch             = gptneox
0.00.059.646 I print_info: vocab_only       = 0
0.00.059.646 I print_info: n_ctx_train      = 2048
0.00.059.646 I print_info: n_embd           = 2048
0.00.059.647 I print_info: n_layer          = 24
0.00.059.650 I print_info: n_head           = 16
0.00.059.651 I print_info: n_head_kv        = 16
0.00.059.651 I print_info: n_rot            = 32
0.00.059.651 I print_info: n_swa            = 0
0.00.059.651 I print_info: n_embd_head_k    = 128
0.00.059.654 I print_info: n_embd_head_v    = 128
0.00.059.655 I print_info: n_gqa            = 1
0.00.059.657 I print_info: n_embd_k_gqa     = 2048
0.00.059.658 I print_info: n_embd_v_gqa     = 2048
0.00.059.658 I print_info: f_norm_eps       = 1.0e-05
0.00.059.659 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.659 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.660 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.661 I print_info: f_logit_scale    = 0.0e+00
0.00.059.661 I print_info: n_ff             = 8192
0.00.059.661 I print_info: n_expert         = 0
0.00.059.661 I print_info: n_expert_used    = 0
0.00.059.662 I print_info: causal attn      = 1
0.00.059.662 I print_info: pooling type     = 0
0.00.059.662 I print_info: rope type        = 2
0.00.059.662 I print_info: rope scaling     = linear
0.00.059.663 I print_info: freq_base_train  = 10000.0
0.00.059.663 I print_info: freq_scale_train = 1
0.00.059.663 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.663 I print_info: rope_finetuned   = unknown
0.00.059.663 I print_info: ssm_d_conv       = 0
0.00.059.663 I print_info: ssm_d_inner      = 0
0.00.059.664 I print_info: ssm_d_state      = 0
0.00.059.664 I print_info: ssm_dt_rank      = 0
0.00.059.664 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.664 I print_info: model type       = 1.4B
0.00.059.668 I print_info: model params     = 1.41 B
0.00.059.669 I print_info: general.name     = 1.4B
0.00.059.669 I print_info: vocab type       = BPE
0.00.059.669 I print_info: n_vocab          = 50304
0.00.059.669 I print_info: n_merges         = 50009
0.00.059.670 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.670 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.670 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.670 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.670 I print_info: LF token         = 128 ''
0.00.059.672 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.672 I print_info: max token length = 1024
0.00.061.657 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.657 I load_tensors: offloading output layer to GPU
0.00.061.657 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.668 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.061.669 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.062.027 I llama_init_from_model: n_seq_max     = 1
0.00.062.028 I llama_init_from_model: n_ctx         = 2048
0.00.062.028 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.028 I llama_init_from_model: n_batch       = 2048
0.00.062.029 I llama_init_from_model: n_ubatch      = 512
0.00.062.029 I llama_init_from_model: flash_attn    = 0
0.00.062.029 I llama_init_from_model: freq_base     = 10000.0
0.00.062.029 I llama_init_from_model: freq_scale    = 1
0.00.062.030 I ggml_metal_init: allocating
0.00.062.033 I ggml_metal_init: found device: Apple M4
0.00.062.034 I ggml_metal_init: picking default device: Apple M4
0.00.062.656 I ggml_metal_init: using embedded metal library
0.00.065.037 I ggml_metal_init: GPU name:   Apple M4
0.00.065.039 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.039 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.039 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.040 I ggml_metal_init: simdgroup reduction   = true
0.00.065.040 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.040 I ggml_metal_init: has bfloat            = true
0.00.065.040 I ggml_metal_init: use bfloat            = true
0.00.065.040 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.041 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.797 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.574 I init:      Metal KV buffer size =   384.00 MiB
0.00.096.584 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.603 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.097.591 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.097.592 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.097.592 I llama_init_from_model: graph nodes  = 967
0.00.097.593 I llama_init_from_model: graph splits = 2
0.00.097.595 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.724 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.882.055 I main: llama threadpool init, n_threads = 4
0.00.882.107 I 
0.00.882.140 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.882.142 I 
0.00.882.360 I sampler seed: 1234
0.00.882.366 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.882.407 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.882.411 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.882.412 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.730.898 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.01.730.898 I llama_perf_context_print:        load time =     872.57 ms
0.01.730.899 I llama_perf_context_print: prompt eval time =      46.22 ms /     7 tokens (    6.60 ms per token,   151.46 tokens per second)
0.01.730.900 I llama_perf_context_print:        eval time =     799.20 ms /    63 runs   (   12.69 ms per token,    78.83 tokens per second)
0.01.730.904 I llama_perf_context_print:       total time =     848.85 ms /    70 tokens
0.01.734.097 I ggml_metal_free: deallocating

real	0m1.750s
user	0m0.109s
sys	0m0.166s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q2_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.819 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.744 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.756 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.756 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.757 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.757 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.757 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.758 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.759 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.759 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.759 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.760 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.760 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.760 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.762 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.762 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.763 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.566 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.496 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.498 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.498 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.499 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.499 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.499 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.500 I llama_model_loader: - type  f32:  194 tensors
0.00.025.500 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.501 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.501 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.502 I print_info: file format = GGUF V3 (latest)
0.00.025.502 I print_info: file type   = Q2_K - Medium
0.00.025.503 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.666 I load: special tokens cache size = 25
0.00.051.907 I load: token to piece cache size = 0.2984 MB
0.00.051.911 I print_info: arch             = gptneox
0.00.051.912 I print_info: vocab_only       = 0
0.00.051.912 I print_info: n_ctx_train      = 2048
0.00.051.912 I print_info: n_embd           = 2048
0.00.051.912 I print_info: n_layer          = 24
0.00.051.917 I print_info: n_head           = 16
0.00.051.917 I print_info: n_head_kv        = 16
0.00.051.918 I print_info: n_rot            = 32
0.00.051.918 I print_info: n_swa            = 0
0.00.051.918 I print_info: n_embd_head_k    = 128
0.00.051.918 I print_info: n_embd_head_v    = 128
0.00.051.919 I print_info: n_gqa            = 1
0.00.051.920 I print_info: n_embd_k_gqa     = 2048
0.00.051.920 I print_info: n_embd_v_gqa     = 2048
0.00.051.921 I print_info: f_norm_eps       = 1.0e-05
0.00.051.921 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.921 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.921 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.921 I print_info: f_logit_scale    = 0.0e+00
0.00.051.923 I print_info: n_ff             = 8192
0.00.051.924 I print_info: n_expert         = 0
0.00.051.924 I print_info: n_expert_used    = 0
0.00.051.924 I print_info: causal attn      = 1
0.00.051.924 I print_info: pooling type     = 0
0.00.051.924 I print_info: rope type        = 2
0.00.051.924 I print_info: rope scaling     = linear
0.00.051.926 I print_info: freq_base_train  = 10000.0
0.00.051.927 I print_info: freq_scale_train = 1
0.00.051.927 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.927 I print_info: rope_finetuned   = unknown
0.00.051.927 I print_info: ssm_d_conv       = 0
0.00.051.927 I print_info: ssm_d_inner      = 0
0.00.051.927 I print_info: ssm_d_state      = 0
0.00.051.928 I print_info: ssm_dt_rank      = 0
0.00.051.928 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.928 I print_info: model type       = 1.4B
0.00.051.928 I print_info: model params     = 1.41 B
0.00.051.928 I print_info: general.name     = 1.4B
0.00.051.929 I print_info: vocab type       = BPE
0.00.051.929 I print_info: n_vocab          = 50304
0.00.051.929 I print_info: n_merges         = 50009
0.00.051.929 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.929 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.930 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.930 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.931 I print_info: LF token         = 128 ''
0.00.051.932 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.932 I print_info: max token length = 1024
0.00.053.880 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.880 I load_tensors: offloading output layer to GPU
0.00.053.881 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.891 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.892 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.054.178 I llama_init_from_model: n_seq_max     = 1
0.00.054.179 I llama_init_from_model: n_ctx         = 2048
0.00.054.179 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.180 I llama_init_from_model: n_batch       = 2048
0.00.054.180 I llama_init_from_model: n_ubatch      = 512
0.00.054.180 I llama_init_from_model: flash_attn    = 0
0.00.054.180 I llama_init_from_model: freq_base     = 10000.0
0.00.054.181 I llama_init_from_model: freq_scale    = 1
0.00.054.181 I ggml_metal_init: allocating
0.00.054.184 I ggml_metal_init: found device: Apple M4
0.00.054.186 I ggml_metal_init: picking default device: Apple M4
0.00.054.840 I ggml_metal_init: using embedded metal library
0.00.057.306 I ggml_metal_init: GPU name:   Apple M4
0.00.057.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.308 I ggml_metal_init: simdgroup reduction   = true
0.00.057.309 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.309 I ggml_metal_init: has bfloat            = true
0.00.057.309 I ggml_metal_init: use bfloat            = true
0.00.057.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.528 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.427 I init:      Metal KV buffer size =   384.00 MiB
0.00.087.433 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.456 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.393 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.394 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.394 I llama_init_from_model: graph nodes  = 967
0.00.088.394 I llama_init_from_model: graph splits = 2
0.00.088.399 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.547 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.419 I main: llama threadpool init, n_threads = 4
0.00.616.463 I 
0.00.616.495 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.497 I 
0.00.616.640 I sampler seed: 1234
0.00.616.645 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.616.685 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.616.701 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.616.701 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.283.959 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.283.959 I llama_perf_context_print:        load time =     606.59 ms
0.01.283.960 I llama_perf_context_print: prompt eval time =      35.70 ms /     7 tokens (    5.10 ms per token,   196.10 tokens per second)
0.01.283.961 I llama_perf_context_print:        eval time =     629.18 ms /    63 runs   (    9.99 ms per token,   100.13 tokens per second)
0.01.283.961 I llama_perf_context_print:       total time =     667.54 ms /    70 tokens
0.01.286.813 I ggml_metal_free: deallocating

real	0m1.308s
user	0m0.109s
sys	0m0.093s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.678 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.668 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.022.674 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.676 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.677 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.682 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.684 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.684 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.685 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.685 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.685 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.432 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.031.434 I llama_model_loader: - type  f32:  194 tensors
0.00.031.434 I llama_model_loader: - type q3_K:   25 tensors
0.00.031.434 I llama_model_loader: - type q4_K:   71 tensors
0.00.031.434 I llama_model_loader: - type q5_K:    1 tensors
0.00.031.435 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.435 I print_info: file format = GGUF V3 (latest)
0.00.031.436 I print_info: file type   = Q3_K - Medium
0.00.031.437 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.052.376 I load: special tokens cache size = 25
0.00.058.564 I load: token to piece cache size = 0.2984 MB
0.00.058.569 I print_info: arch             = gptneox
0.00.058.569 I print_info: vocab_only       = 0
0.00.058.569 I print_info: n_ctx_train      = 2048
0.00.058.570 I print_info: n_embd           = 2048
0.00.058.570 I print_info: n_layer          = 24
0.00.058.575 I print_info: n_head           = 16
0.00.058.575 I print_info: n_head_kv        = 16
0.00.058.577 I print_info: n_rot            = 32
0.00.058.577 I print_info: n_swa            = 0
0.00.058.578 I print_info: n_embd_head_k    = 128
0.00.058.578 I print_info: n_embd_head_v    = 128
0.00.058.578 I print_info: n_gqa            = 1
0.00.058.579 I print_info: n_embd_k_gqa     = 2048
0.00.058.580 I print_info: n_embd_v_gqa     = 2048
0.00.058.581 I print_info: f_norm_eps       = 1.0e-05
0.00.058.583 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.585 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.585 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.585 I print_info: f_logit_scale    = 0.0e+00
0.00.058.585 I print_info: n_ff             = 8192
0.00.058.587 I print_info: n_expert         = 0
0.00.058.587 I print_info: n_expert_used    = 0
0.00.058.587 I print_info: causal attn      = 1
0.00.058.587 I print_info: pooling type     = 0
0.00.058.587 I print_info: rope type        = 2
0.00.058.587 I print_info: rope scaling     = linear
0.00.058.588 I print_info: freq_base_train  = 10000.0
0.00.058.588 I print_info: freq_scale_train = 1
0.00.058.588 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.588 I print_info: rope_finetuned   = unknown
0.00.058.588 I print_info: ssm_d_conv       = 0
0.00.058.588 I print_info: ssm_d_inner      = 0
0.00.058.590 I print_info: ssm_d_state      = 0
0.00.058.590 I print_info: ssm_dt_rank      = 0
0.00.058.590 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.590 I print_info: model type       = 1.4B
0.00.058.590 I print_info: model params     = 1.41 B
0.00.058.591 I print_info: general.name     = 1.4B
0.00.058.591 I print_info: vocab type       = BPE
0.00.058.591 I print_info: n_vocab          = 50304
0.00.058.591 I print_info: n_merges         = 50009
0.00.058.592 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.592 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.592 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.592 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.592 I print_info: LF token         = 128 ''
0.00.058.593 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.593 I print_info: max token length = 1024
0.00.060.553 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.553 I load_tensors: offloading output layer to GPU
0.00.060.553 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.564 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.060.565 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.060.850 I llama_init_from_model: n_seq_max     = 1
0.00.060.851 I llama_init_from_model: n_ctx         = 2048
0.00.060.851 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.060.851 I llama_init_from_model: n_batch       = 2048
0.00.060.851 I llama_init_from_model: n_ubatch      = 512
0.00.060.852 I llama_init_from_model: flash_attn    = 0
0.00.060.852 I llama_init_from_model: freq_base     = 10000.0
0.00.060.852 I llama_init_from_model: freq_scale    = 1
0.00.060.853 I ggml_metal_init: allocating
0.00.060.856 I ggml_metal_init: found device: Apple M4
0.00.060.858 I ggml_metal_init: picking default device: Apple M4
0.00.061.489 I ggml_metal_init: using embedded metal library
0.00.063.869 I ggml_metal_init: GPU name:   Apple M4
0.00.063.870 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.871 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.871 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.871 I ggml_metal_init: simdgroup reduction   = true
0.00.063.872 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.872 I ggml_metal_init: has bfloat            = true
0.00.063.872 I ggml_metal_init: use bfloat            = true
0.00.063.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.873 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.965 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.118 I init:      Metal KV buffer size =   384.00 MiB
0.00.094.125 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.145 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.095.142 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.095.144 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.095.144 I llama_init_from_model: graph nodes  = 967
0.00.095.144 I llama_init_from_model: graph splits = 2
0.00.095.148 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.272 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.555.237 I main: llama threadpool init, n_threads = 4
0.00.555.277 I 
0.00.555.309 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.555.310 I 
0.00.555.534 I sampler seed: 1234
0.00.555.538 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.555.549 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.555.549 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.555.555 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.303.809 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.01.303.809 I llama_perf_context_print:        load time =     546.55 ms
0.01.303.810 I llama_perf_context_print: prompt eval time =      40.52 ms /     7 tokens (    5.79 ms per token,   172.75 tokens per second)
0.01.303.811 I llama_perf_context_print:        eval time =     704.80 ms /    63 runs   (   11.19 ms per token,    89.39 tokens per second)
0.01.303.811 I llama_perf_context_print:       total time =     748.57 ms /    70 tokens
0.01.306.814 I ggml_metal_free: deallocating

real	0m1.323s
user	0m0.111s
sys	0m0.130s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.574 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.102 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.107 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.108 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.110 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.111 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.111 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.112 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.115 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.115 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.116 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.116 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.116 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.121 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.121 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.124 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.125 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.125 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.901 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.923 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.667 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.668 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.668 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.669 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.669 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.669 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.670 I llama_model_loader: - type  f32:  194 tensors
0.00.024.670 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.670 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.670 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.671 I print_info: file format = GGUF V3 (latest)
0.00.024.671 I print_info: file type   = Q4_K - Medium
0.00.024.672 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.807 I load: special tokens cache size = 25
0.00.049.907 I load: token to piece cache size = 0.2984 MB
0.00.049.910 I print_info: arch             = gptneox
0.00.049.910 I print_info: vocab_only       = 0
0.00.049.911 I print_info: n_ctx_train      = 2048
0.00.049.911 I print_info: n_embd           = 2048
0.00.049.911 I print_info: n_layer          = 24
0.00.049.913 I print_info: n_head           = 16
0.00.049.914 I print_info: n_head_kv        = 16
0.00.049.914 I print_info: n_rot            = 32
0.00.049.915 I print_info: n_swa            = 0
0.00.049.915 I print_info: n_embd_head_k    = 128
0.00.049.915 I print_info: n_embd_head_v    = 128
0.00.049.916 I print_info: n_gqa            = 1
0.00.049.917 I print_info: n_embd_k_gqa     = 2048
0.00.049.917 I print_info: n_embd_v_gqa     = 2048
0.00.049.918 I print_info: f_norm_eps       = 1.0e-05
0.00.049.918 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.918 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.919 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.919 I print_info: f_logit_scale    = 0.0e+00
0.00.049.919 I print_info: n_ff             = 8192
0.00.049.922 I print_info: n_expert         = 0
0.00.049.922 I print_info: n_expert_used    = 0
0.00.049.922 I print_info: causal attn      = 1
0.00.049.924 I print_info: pooling type     = 0
0.00.049.925 I print_info: rope type        = 2
0.00.049.925 I print_info: rope scaling     = linear
0.00.049.926 I print_info: freq_base_train  = 10000.0
0.00.049.926 I print_info: freq_scale_train = 1
0.00.049.926 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.928 I print_info: rope_finetuned   = unknown
0.00.049.928 I print_info: ssm_d_conv       = 0
0.00.049.928 I print_info: ssm_d_inner      = 0
0.00.049.928 I print_info: ssm_d_state      = 0
0.00.049.928 I print_info: ssm_dt_rank      = 0
0.00.049.928 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.929 I print_info: model type       = 1.4B
0.00.049.930 I print_info: model params     = 1.41 B
0.00.049.930 I print_info: general.name     = 1.4B
0.00.049.931 I print_info: vocab type       = BPE
0.00.049.931 I print_info: n_vocab          = 50304
0.00.049.931 I print_info: n_merges         = 50009
0.00.049.932 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.932 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.932 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.932 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.932 I print_info: LF token         = 128 ''
0.00.049.933 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.933 I print_info: max token length = 1024
0.00.051.829 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.829 I load_tensors: offloading output layer to GPU
0.00.051.829 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.840 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.841 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.112 I llama_init_from_model: n_seq_max     = 1
0.00.052.113 I llama_init_from_model: n_ctx         = 2048
0.00.052.113 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.113 I llama_init_from_model: n_batch       = 2048
0.00.052.113 I llama_init_from_model: n_ubatch      = 512
0.00.052.113 I llama_init_from_model: flash_attn    = 0
0.00.052.114 I llama_init_from_model: freq_base     = 10000.0
0.00.052.114 I llama_init_from_model: freq_scale    = 1
0.00.052.115 I ggml_metal_init: allocating
0.00.052.117 I ggml_metal_init: found device: Apple M4
0.00.052.119 I ggml_metal_init: picking default device: Apple M4
0.00.052.725 I ggml_metal_init: using embedded metal library
0.00.055.065 I ggml_metal_init: GPU name:   Apple M4
0.00.055.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.068 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.068 I ggml_metal_init: simdgroup reduction   = true
0.00.055.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.069 I ggml_metal_init: has bfloat            = true
0.00.055.069 I ggml_metal_init: use bfloat            = true
0.00.055.069 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.070 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.695 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.301 I init:      Metal KV buffer size =   384.00 MiB
0.00.083.307 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.328 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.337 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.338 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.339 I llama_init_from_model: graph nodes  = 967
0.00.084.339 I llama_init_from_model: graph splits = 2
0.00.084.342 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.462 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.462 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.686 I main: llama threadpool init, n_threads = 4
0.00.601.734 I 
0.00.601.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.601.768 I 
0.00.602.009 I sampler seed: 1234
0.00.602.013 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.602.024 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.602.024 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.602.024 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.364.497 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56754.60 tokens per second)
0.01.364.497 I llama_perf_context_print:        load time =     593.11 ms
0.01.364.498 I llama_perf_context_print: prompt eval time =      51.11 ms /     7 tokens (    7.30 ms per token,   136.97 tokens per second)
0.01.364.499 I llama_perf_context_print:        eval time =     708.29 ms /    63 runs   (   11.24 ms per token,    88.95 tokens per second)
0.01.364.499 I llama_perf_context_print:       total time =     762.82 ms /    70 tokens
0.01.367.439 I ggml_metal_free: deallocating

real	0m1.382s
user	0m0.108s
sys	0m0.133s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.499 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.188 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.193 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.195 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.195 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.195 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.196 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.196 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.197 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.197 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.198 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.198 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.199 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.199 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.199 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.201 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.201 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.202 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.938 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.947 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.658 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.660 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.660 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.660 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.661 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.661 I llama_model_loader: - type  f32:  194 tensors
0.00.024.661 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.662 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.662 I print_info: file format = GGUF V3 (latest)
0.00.024.663 I print_info: file type   = Q5_K - Medium
0.00.024.664 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.488 I load: special tokens cache size = 25
0.00.050.472 I load: token to piece cache size = 0.2984 MB
0.00.050.475 I print_info: arch             = gptneox
0.00.050.475 I print_info: vocab_only       = 0
0.00.050.475 I print_info: n_ctx_train      = 2048
0.00.050.476 I print_info: n_embd           = 2048
0.00.050.476 I print_info: n_layer          = 24
0.00.050.479 I print_info: n_head           = 16
0.00.050.480 I print_info: n_head_kv        = 16
0.00.050.480 I print_info: n_rot            = 32
0.00.050.480 I print_info: n_swa            = 0
0.00.050.480 I print_info: n_embd_head_k    = 128
0.00.050.480 I print_info: n_embd_head_v    = 128
0.00.050.481 I print_info: n_gqa            = 1
0.00.050.482 I print_info: n_embd_k_gqa     = 2048
0.00.050.483 I print_info: n_embd_v_gqa     = 2048
0.00.050.483 I print_info: f_norm_eps       = 1.0e-05
0.00.050.483 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.485 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.485 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.487 I print_info: f_logit_scale    = 0.0e+00
0.00.050.487 I print_info: n_ff             = 8192
0.00.050.488 I print_info: n_expert         = 0
0.00.050.488 I print_info: n_expert_used    = 0
0.00.050.488 I print_info: causal attn      = 1
0.00.050.488 I print_info: pooling type     = 0
0.00.050.488 I print_info: rope type        = 2
0.00.050.488 I print_info: rope scaling     = linear
0.00.050.490 I print_info: freq_base_train  = 10000.0
0.00.050.490 I print_info: freq_scale_train = 1
0.00.050.491 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.491 I print_info: rope_finetuned   = unknown
0.00.050.491 I print_info: ssm_d_conv       = 0
0.00.050.491 I print_info: ssm_d_inner      = 0
0.00.050.491 I print_info: ssm_d_state      = 0
0.00.050.491 I print_info: ssm_dt_rank      = 0
0.00.050.491 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.492 I print_info: model type       = 1.4B
0.00.050.492 I print_info: model params     = 1.41 B
0.00.050.492 I print_info: general.name     = 1.4B
0.00.050.497 I print_info: vocab type       = BPE
0.00.050.497 I print_info: n_vocab          = 50304
0.00.050.497 I print_info: n_merges         = 50009
0.00.050.497 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.497 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.498 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.498 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.498 I print_info: LF token         = 128 ''
0.00.050.498 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.498 I print_info: max token length = 1024
0.00.052.455 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.455 I load_tensors: offloading output layer to GPU
0.00.052.455 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.465 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.466 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.729 I llama_init_from_model: n_seq_max     = 1
0.00.052.729 I llama_init_from_model: n_ctx         = 2048
0.00.052.730 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.730 I llama_init_from_model: n_batch       = 2048
0.00.052.730 I llama_init_from_model: n_ubatch      = 512
0.00.052.730 I llama_init_from_model: flash_attn    = 0
0.00.052.730 I llama_init_from_model: freq_base     = 10000.0
0.00.052.731 I llama_init_from_model: freq_scale    = 1
0.00.052.731 I ggml_metal_init: allocating
0.00.052.734 I ggml_metal_init: found device: Apple M4
0.00.052.736 I ggml_metal_init: picking default device: Apple M4
0.00.053.309 I ggml_metal_init: using embedded metal library
0.00.055.623 I ggml_metal_init: GPU name:   Apple M4
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.626 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.626 I ggml_metal_init: simdgroup reduction   = true
0.00.055.626 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.626 I ggml_metal_init: has bfloat            = true
0.00.055.626 I ggml_metal_init: use bfloat            = true
0.00.055.627 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.627 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.103 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.977 I init:      Metal KV buffer size =   384.00 MiB
0.00.084.986 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.015 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.014 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.016 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.016 I llama_init_from_model: graph nodes  = 967
0.00.086.016 I llama_init_from_model: graph splits = 2
0.00.086.019 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.755 I main: llama threadpool init, n_threads = 4
0.00.706.810 I 
0.00.706.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.846 I 
0.00.707.087 I sampler seed: 1234
0.00.707.095 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.133 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.137 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.137 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.547.000 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.01.547.001 I llama_perf_context_print:        load time =     697.25 ms
0.01.547.001 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.36 ms per token,   135.80 tokens per second)
0.01.547.002 I llama_perf_context_print:        eval time =     785.38 ms /    63 runs   (   12.47 ms per token,    80.22 tokens per second)
0.01.547.002 I llama_perf_context_print:       total time =     840.25 ms /    70 tokens
0.01.549.327 I ggml_metal_free: deallocating

real	0m1.565s
user	0m0.108s
sys	0m0.167s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.851 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.487 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.492 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.493 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.494 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.494 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.495 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.496 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.496 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.500 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.502 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.300 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.023 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.023 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.024 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.024 I llama_model_loader: - type  f32:  194 tensors
0.00.024.024 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.025 I print_info: file format = GGUF V3 (latest)
0.00.024.025 I print_info: file type   = Q6_K
0.00.024.026 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.106 I load: special tokens cache size = 25
0.00.049.132 I load: token to piece cache size = 0.2984 MB
0.00.049.135 I print_info: arch             = gptneox
0.00.049.136 I print_info: vocab_only       = 0
0.00.049.136 I print_info: n_ctx_train      = 2048
0.00.049.136 I print_info: n_embd           = 2048
0.00.049.136 I print_info: n_layer          = 24
0.00.049.139 I print_info: n_head           = 16
0.00.049.139 I print_info: n_head_kv        = 16
0.00.049.141 I print_info: n_rot            = 32
0.00.049.142 I print_info: n_swa            = 0
0.00.049.142 I print_info: n_embd_head_k    = 128
0.00.049.142 I print_info: n_embd_head_v    = 128
0.00.049.144 I print_info: n_gqa            = 1
0.00.049.145 I print_info: n_embd_k_gqa     = 2048
0.00.049.146 I print_info: n_embd_v_gqa     = 2048
0.00.049.146 I print_info: f_norm_eps       = 1.0e-05
0.00.049.148 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.148 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.148 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.148 I print_info: f_logit_scale    = 0.0e+00
0.00.049.149 I print_info: n_ff             = 8192
0.00.049.149 I print_info: n_expert         = 0
0.00.049.149 I print_info: n_expert_used    = 0
0.00.049.150 I print_info: causal attn      = 1
0.00.049.150 I print_info: pooling type     = 0
0.00.049.150 I print_info: rope type        = 2
0.00.049.150 I print_info: rope scaling     = linear
0.00.049.150 I print_info: freq_base_train  = 10000.0
0.00.049.151 I print_info: freq_scale_train = 1
0.00.049.151 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.151 I print_info: rope_finetuned   = unknown
0.00.049.151 I print_info: ssm_d_conv       = 0
0.00.049.151 I print_info: ssm_d_inner      = 0
0.00.049.151 I print_info: ssm_d_state      = 0
0.00.049.151 I print_info: ssm_dt_rank      = 0
0.00.049.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.152 I print_info: model type       = 1.4B
0.00.049.152 I print_info: model params     = 1.41 B
0.00.049.152 I print_info: general.name     = 1.4B
0.00.049.153 I print_info: vocab type       = BPE
0.00.049.153 I print_info: n_vocab          = 50304
0.00.049.153 I print_info: n_merges         = 50009
0.00.049.157 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.157 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.157 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.157 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.158 I print_info: LF token         = 128 ''
0.00.049.158 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.158 I print_info: max token length = 1024
0.00.051.225 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.225 I load_tensors: offloading output layer to GPU
0.00.051.225 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.235 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.236 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.511 I llama_init_from_model: n_seq_max     = 1
0.00.051.511 I llama_init_from_model: n_ctx         = 2048
0.00.051.511 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.512 I llama_init_from_model: n_batch       = 2048
0.00.051.512 I llama_init_from_model: n_ubatch      = 512
0.00.051.512 I llama_init_from_model: flash_attn    = 0
0.00.051.512 I llama_init_from_model: freq_base     = 10000.0
0.00.051.513 I llama_init_from_model: freq_scale    = 1
0.00.051.513 I ggml_metal_init: allocating
0.00.051.516 I ggml_metal_init: found device: Apple M4
0.00.051.518 I ggml_metal_init: picking default device: Apple M4
0.00.052.118 I ggml_metal_init: using embedded metal library
0.00.054.488 I ggml_metal_init: GPU name:   Apple M4
0.00.054.490 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.490 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.490 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.491 I ggml_metal_init: simdgroup reduction   = true
0.00.054.491 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.491 I ggml_metal_init: has bfloat            = true
0.00.054.491 I ggml_metal_init: use bfloat            = true
0.00.054.491 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.492 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.223 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.609 I init:      Metal KV buffer size =   384.00 MiB
0.00.084.618 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.642 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.745 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.747 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.747 I llama_init_from_model: graph nodes  = 967
0.00.085.747 I llama_init_from_model: graph splits = 2
0.00.085.750 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.880 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.117 I main: llama threadpool init, n_threads = 4
0.00.761.160 I 
0.00.761.210 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.212 I 
0.00.761.451 I sampler seed: 1234
0.00.761.457 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.512 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.523 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.523 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.642.568 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.642.569 I llama_perf_context_print:        load time =     752.26 ms
0.01.642.570 I llama_perf_context_print: prompt eval time =      54.49 ms /     7 tokens (    7.78 ms per token,   128.47 tokens per second)
0.01.642.570 I llama_perf_context_print:        eval time =     823.58 ms /    63 runs   (   13.07 ms per token,    76.49 tokens per second)
0.01.642.571 I llama_perf_context_print:       total time =     881.46 ms /    70 tokens
0.01.645.429 I ggml_metal_free: deallocating

real	0m1.661s
user	0m0.108s
sys	0m0.178s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.624 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.965 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.999 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.009 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.010 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.010 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.010 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.011 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.012 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.013 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.013 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.013 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.014 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.014 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.015 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.017 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.017 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.017 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.679 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.667 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.669 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.669 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.670 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.670 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.671 I llama_model_loader: - type  f32:  194 tensors
0.00.053.671 I llama_model_loader: - type  f16:   98 tensors
0.00.053.672 I print_info: file format = GGUF V3 (latest)
0.00.053.673 I print_info: file type   = all F32 (guessed)
0.00.053.674 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.090 I load: special tokens cache size = 25
0.00.087.926 I load: token to piece cache size = 0.2984 MB
0.00.087.930 I print_info: arch             = gptneox
0.00.087.930 I print_info: vocab_only       = 0
0.00.087.930 I print_info: n_ctx_train      = 2048
0.00.087.930 I print_info: n_embd           = 2048
0.00.087.930 I print_info: n_layer          = 24
0.00.087.933 I print_info: n_head           = 16
0.00.087.934 I print_info: n_head_kv        = 16
0.00.087.934 I print_info: n_rot            = 32
0.00.087.934 I print_info: n_swa            = 0
0.00.087.934 I print_info: n_embd_head_k    = 128
0.00.087.934 I print_info: n_embd_head_v    = 128
0.00.087.935 I print_info: n_gqa            = 1
0.00.087.936 I print_info: n_embd_k_gqa     = 2048
0.00.087.936 I print_info: n_embd_v_gqa     = 2048
0.00.087.937 I print_info: f_norm_eps       = 1.0e-05
0.00.087.937 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.937 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.938 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.938 I print_info: f_logit_scale    = 0.0e+00
0.00.087.938 I print_info: n_ff             = 8192
0.00.087.939 I print_info: n_expert         = 0
0.00.087.939 I print_info: n_expert_used    = 0
0.00.087.939 I print_info: causal attn      = 1
0.00.087.939 I print_info: pooling type     = 0
0.00.087.939 I print_info: rope type        = 2
0.00.087.939 I print_info: rope scaling     = linear
0.00.087.940 I print_info: freq_base_train  = 10000.0
0.00.087.940 I print_info: freq_scale_train = 1
0.00.087.940 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.940 I print_info: rope_finetuned   = unknown
0.00.087.940 I print_info: ssm_d_conv       = 0
0.00.087.941 I print_info: ssm_d_inner      = 0
0.00.087.941 I print_info: ssm_d_state      = 0
0.00.087.941 I print_info: ssm_dt_rank      = 0
0.00.087.941 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.941 I print_info: model type       = 1.4B
0.00.087.941 I print_info: model params     = 1.41 B
0.00.087.941 I print_info: general.name     = 1.4B
0.00.087.942 I print_info: vocab type       = BPE
0.00.087.942 I print_info: n_vocab          = 50304
0.00.087.942 I print_info: n_merges         = 50009
0.00.087.943 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.943 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.943 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.943 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.943 I print_info: LF token         = 128 ''
0.00.087.944 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.944 I print_info: max token length = 1024
0.00.090.470 I load_tensors: offloading 24 repeating layers to GPU
0.00.090.471 I load_tensors: offloading output layer to GPU
0.00.090.471 I load_tensors: offloaded 25/25 layers to GPU
0.00.090.481 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.482 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.090.758 I llama_init_from_model: n_seq_max     = 1
0.00.090.759 I llama_init_from_model: n_ctx         = 128
0.00.090.759 I llama_init_from_model: n_ctx_per_seq = 128
0.00.090.759 I llama_init_from_model: n_batch       = 128
0.00.090.759 I llama_init_from_model: n_ubatch      = 128
0.00.090.759 I llama_init_from_model: flash_attn    = 0
0.00.090.760 I llama_init_from_model: freq_base     = 10000.0
0.00.090.760 I llama_init_from_model: freq_scale    = 1
0.00.090.760 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.761 I ggml_metal_init: allocating
0.00.090.764 I ggml_metal_init: found device: Apple M4
0.00.090.766 I ggml_metal_init: picking default device: Apple M4
0.00.091.372 I ggml_metal_init: using embedded metal library
0.00.093.959 I ggml_metal_init: GPU name:   Apple M4
0.00.093.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.962 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.962 I ggml_metal_init: simdgroup reduction   = true
0.00.093.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.962 I ggml_metal_init: has bfloat            = true
0.00.093.963 I ggml_metal_init: use bfloat            = true
0.00.093.963 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.964 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.481 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.750 I init:      Metal KV buffer size =    24.00 MiB
0.00.104.752 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.774 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.105.596 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.105.597 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.105.597 I llama_init_from_model: graph nodes  = 967
0.00.105.598 I llama_init_from_model: graph splits = 2
0.00.105.599 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.599 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.199.249 I 
0.01.199.316 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.199.322 I perplexity: tokenizing the input ..
0.01.213.020 I perplexity: tokenization took 13.695 ms
0.01.213.032 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.336.071 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.337.939 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.337.996 I llama_perf_context_print:        load time =    1176.27 ms
0.01.337.998 I llama_perf_context_print: prompt eval time =     122.12 ms /   128 tokens (    0.95 ms per token,  1048.18 tokens per second)
0.01.337.999 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.338.001 I llama_perf_context_print:       total time =     138.75 ms /   129 tokens
0.01.339.233 I ggml_metal_free: deallocating

real	0m1.528s
user	0m0.123s
sys	0m0.212s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.124 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.341 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.540 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.545 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.548 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.548 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.549 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.549 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.550 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.551 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.551 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.553 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.553 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.554 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.554 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.557 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.558 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.091 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.497 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.891 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.894 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.895 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.896 I llama_model_loader: - type  f32:  194 tensors
0.00.032.896 I llama_model_loader: - type q8_0:   98 tensors
0.00.032.897 I print_info: file format = GGUF V3 (latest)
0.00.032.898 I print_info: file type   = Q8_0
0.00.032.899 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.054.860 I load: special tokens cache size = 25
0.00.061.324 I load: token to piece cache size = 0.2984 MB
0.00.061.327 I print_info: arch             = gptneox
0.00.061.327 I print_info: vocab_only       = 0
0.00.061.327 I print_info: n_ctx_train      = 2048
0.00.061.328 I print_info: n_embd           = 2048
0.00.061.328 I print_info: n_layer          = 24
0.00.061.332 I print_info: n_head           = 16
0.00.061.332 I print_info: n_head_kv        = 16
0.00.061.332 I print_info: n_rot            = 32
0.00.061.333 I print_info: n_swa            = 0
0.00.061.333 I print_info: n_embd_head_k    = 128
0.00.061.333 I print_info: n_embd_head_v    = 128
0.00.061.333 I print_info: n_gqa            = 1
0.00.061.335 I print_info: n_embd_k_gqa     = 2048
0.00.061.336 I print_info: n_embd_v_gqa     = 2048
0.00.061.336 I print_info: f_norm_eps       = 1.0e-05
0.00.061.337 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.337 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.337 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.339 I print_info: f_logit_scale    = 0.0e+00
0.00.061.339 I print_info: n_ff             = 8192
0.00.061.339 I print_info: n_expert         = 0
0.00.061.339 I print_info: n_expert_used    = 0
0.00.061.340 I print_info: causal attn      = 1
0.00.061.340 I print_info: pooling type     = 0
0.00.061.340 I print_info: rope type        = 2
0.00.061.340 I print_info: rope scaling     = linear
0.00.061.342 I print_info: freq_base_train  = 10000.0
0.00.061.342 I print_info: freq_scale_train = 1
0.00.061.342 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.342 I print_info: rope_finetuned   = unknown
0.00.061.342 I print_info: ssm_d_conv       = 0
0.00.061.342 I print_info: ssm_d_inner      = 0
0.00.061.342 I print_info: ssm_d_state      = 0
0.00.061.343 I print_info: ssm_dt_rank      = 0
0.00.061.343 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.343 I print_info: model type       = 1.4B
0.00.061.343 I print_info: model params     = 1.41 B
0.00.061.343 I print_info: general.name     = 1.4B
0.00.061.344 I print_info: vocab type       = BPE
0.00.061.344 I print_info: n_vocab          = 50304
0.00.061.344 I print_info: n_merges         = 50009
0.00.061.344 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.345 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.345 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.345 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.345 I print_info: LF token         = 128 ''
0.00.061.345 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.346 I print_info: max token length = 1024
0.00.063.614 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.615 I load_tensors: offloading output layer to GPU
0.00.063.615 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.626 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.627 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.010 I llama_init_from_model: n_seq_max     = 1
0.00.064.010 I llama_init_from_model: n_ctx         = 128
0.00.064.010 I llama_init_from_model: n_ctx_per_seq = 128
0.00.064.011 I llama_init_from_model: n_batch       = 128
0.00.064.011 I llama_init_from_model: n_ubatch      = 128
0.00.064.011 I llama_init_from_model: flash_attn    = 0
0.00.064.011 I llama_init_from_model: freq_base     = 10000.0
0.00.064.011 I llama_init_from_model: freq_scale    = 1
0.00.064.012 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.012 I ggml_metal_init: allocating
0.00.064.014 I ggml_metal_init: found device: Apple M4
0.00.064.016 I ggml_metal_init: picking default device: Apple M4
0.00.064.656 I ggml_metal_init: using embedded metal library
0.00.067.114 I ggml_metal_init: GPU name:   Apple M4
0.00.067.115 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.116 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.116 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.117 I ggml_metal_init: simdgroup reduction   = true
0.00.067.117 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.117 I ggml_metal_init: has bfloat            = true
0.00.067.117 I ggml_metal_init: use bfloat            = true
0.00.067.117 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.118 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.346 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.078.665 I init:      Metal KV buffer size =    24.00 MiB
0.00.078.669 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.685 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.079.649 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.079.651 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.079.651 I llama_init_from_model: graph nodes  = 967
0.00.079.651 I llama_init_from_model: graph splits = 2
0.00.079.653 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.079.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.820.536 I 
0.00.820.593 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.820.600 I perplexity: tokenizing the input ..
0.00.828.649 I perplexity: tokenization took 8.047 ms
0.00.828.652 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.952.817 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.953.974 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.954.001 I llama_perf_context_print:        load time =     809.19 ms
0.00.954.002 I llama_perf_context_print: prompt eval time =     123.94 ms /   128 tokens (    0.97 ms per token,  1032.72 tokens per second)
0.00.954.003 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.954.003 I llama_perf_context_print:       total time =     133.47 ms /   129 tokens
0.00.954.675 I ggml_metal_free: deallocating

real	0m0.971s
user	0m0.090s
sys	0m0.138s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.494 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.553 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.554 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.555 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.556 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.557 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.557 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.558 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.558 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.563 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.563 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.182 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.184 I llama_model_loader: - type  f32:  194 tensors
0.00.028.184 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.184 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.185 I print_info: file format = GGUF V3 (latest)
0.00.028.185 I print_info: file type   = Q4_0
0.00.028.186 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.167 I load: special tokens cache size = 25
0.00.053.246 I load: token to piece cache size = 0.2984 MB
0.00.053.249 I print_info: arch             = gptneox
0.00.053.249 I print_info: vocab_only       = 0
0.00.053.249 I print_info: n_ctx_train      = 2048
0.00.053.249 I print_info: n_embd           = 2048
0.00.053.250 I print_info: n_layer          = 24
0.00.053.252 I print_info: n_head           = 16
0.00.053.253 I print_info: n_head_kv        = 16
0.00.053.255 I print_info: n_rot            = 32
0.00.053.256 I print_info: n_swa            = 0
0.00.053.256 I print_info: n_embd_head_k    = 128
0.00.053.256 I print_info: n_embd_head_v    = 128
0.00.053.257 I print_info: n_gqa            = 1
0.00.053.266 I print_info: n_embd_k_gqa     = 2048
0.00.053.271 I print_info: n_embd_v_gqa     = 2048
0.00.053.273 I print_info: f_norm_eps       = 1.0e-05
0.00.053.273 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.274 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.274 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.274 I print_info: f_logit_scale    = 0.0e+00
0.00.053.274 I print_info: n_ff             = 8192
0.00.053.275 I print_info: n_expert         = 0
0.00.053.275 I print_info: n_expert_used    = 0
0.00.053.275 I print_info: causal attn      = 1
0.00.053.275 I print_info: pooling type     = 0
0.00.053.275 I print_info: rope type        = 2
0.00.053.275 I print_info: rope scaling     = linear
0.00.053.276 I print_info: freq_base_train  = 10000.0
0.00.053.276 I print_info: freq_scale_train = 1
0.00.053.276 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.276 I print_info: rope_finetuned   = unknown
0.00.053.276 I print_info: ssm_d_conv       = 0
0.00.053.277 I print_info: ssm_d_inner      = 0
0.00.053.277 I print_info: ssm_d_state      = 0
0.00.053.277 I print_info: ssm_dt_rank      = 0
0.00.053.278 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.278 I print_info: model type       = 1.4B
0.00.053.278 I print_info: model params     = 1.41 B
0.00.053.279 I print_info: general.name     = 1.4B
0.00.053.279 I print_info: vocab type       = BPE
0.00.053.279 I print_info: n_vocab          = 50304
0.00.053.279 I print_info: n_merges         = 50009
0.00.053.281 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.281 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.281 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.282 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.282 I print_info: LF token         = 128 ''
0.00.053.282 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.282 I print_info: max token length = 1024
0.00.055.174 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.174 I load_tensors: offloading output layer to GPU
0.00.055.175 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.185 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.187 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.055.583 I llama_init_from_model: n_seq_max     = 1
0.00.055.584 I llama_init_from_model: n_ctx         = 128
0.00.055.584 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.584 I llama_init_from_model: n_batch       = 128
0.00.055.584 I llama_init_from_model: n_ubatch      = 128
0.00.055.584 I llama_init_from_model: flash_attn    = 0
0.00.055.585 I llama_init_from_model: freq_base     = 10000.0
0.00.055.585 I llama_init_from_model: freq_scale    = 1
0.00.055.585 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.586 I ggml_metal_init: allocating
0.00.055.589 I ggml_metal_init: found device: Apple M4
0.00.055.590 I ggml_metal_init: picking default device: Apple M4
0.00.056.176 I ggml_metal_init: using embedded metal library
0.00.059.276 I ggml_metal_init: GPU name:   Apple M4
0.00.059.277 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.278 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.278 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.278 I ggml_metal_init: simdgroup reduction   = true
0.00.059.278 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.278 I ggml_metal_init: has bfloat            = true
0.00.059.279 I ggml_metal_init: use bfloat            = true
0.00.059.279 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.281 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.162 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.620 I init:      Metal KV buffer size =    24.00 MiB
0.00.071.625 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.641 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.072.479 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.072.480 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.072.480 I llama_init_from_model: graph nodes  = 967
0.00.072.481 I llama_init_from_model: graph splits = 2
0.00.072.481 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.527 I 
0.00.627.569 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.575 I perplexity: tokenizing the input ..
0.00.634.801 I perplexity: tokenization took 7.223 ms
0.00.634.805 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.756.908 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.758.316 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.758.338 I llama_perf_context_print:        load time =     615.03 ms
0.00.758.339 I llama_perf_context_print: prompt eval time =     121.85 ms /   128 tokens (    0.95 ms per token,  1050.51 tokens per second)
0.00.758.340 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.758.341 I llama_perf_context_print:       total time =     130.81 ms /   129 tokens
0.00.758.859 I ggml_metal_free: deallocating

real	0m0.775s
user	0m0.077s
sys	0m0.092s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.954 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.323 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.329 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.335 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.336 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.336 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.337 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.337 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.338 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.338 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.339 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.339 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.341 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.341 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.341 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.343 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.343 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.344 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.269 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.289 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.168 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.171 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.172 I llama_model_loader: - type  f32:  194 tensors
0.00.025.172 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.173 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.173 I print_info: file format = GGUF V3 (latest)
0.00.025.174 I print_info: file type   = Q4_1
0.00.025.175 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.506 I load: special tokens cache size = 25
0.00.051.638 I load: token to piece cache size = 0.2984 MB
0.00.051.642 I print_info: arch             = gptneox
0.00.051.643 I print_info: vocab_only       = 0
0.00.051.643 I print_info: n_ctx_train      = 2048
0.00.051.643 I print_info: n_embd           = 2048
0.00.051.643 I print_info: n_layer          = 24
0.00.051.648 I print_info: n_head           = 16
0.00.051.648 I print_info: n_head_kv        = 16
0.00.051.649 I print_info: n_rot            = 32
0.00.051.649 I print_info: n_swa            = 0
0.00.051.649 I print_info: n_embd_head_k    = 128
0.00.051.649 I print_info: n_embd_head_v    = 128
0.00.051.650 I print_info: n_gqa            = 1
0.00.051.651 I print_info: n_embd_k_gqa     = 2048
0.00.051.651 I print_info: n_embd_v_gqa     = 2048
0.00.051.652 I print_info: f_norm_eps       = 1.0e-05
0.00.051.652 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.652 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.653 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.653 I print_info: f_logit_scale    = 0.0e+00
0.00.051.653 I print_info: n_ff             = 8192
0.00.051.653 I print_info: n_expert         = 0
0.00.051.654 I print_info: n_expert_used    = 0
0.00.051.654 I print_info: causal attn      = 1
0.00.051.654 I print_info: pooling type     = 0
0.00.051.654 I print_info: rope type        = 2
0.00.051.654 I print_info: rope scaling     = linear
0.00.051.654 I print_info: freq_base_train  = 10000.0
0.00.051.655 I print_info: freq_scale_train = 1
0.00.051.655 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.655 I print_info: rope_finetuned   = unknown
0.00.051.655 I print_info: ssm_d_conv       = 0
0.00.051.655 I print_info: ssm_d_inner      = 0
0.00.051.655 I print_info: ssm_d_state      = 0
0.00.051.656 I print_info: ssm_dt_rank      = 0
0.00.051.657 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.657 I print_info: model type       = 1.4B
0.00.051.658 I print_info: model params     = 1.41 B
0.00.051.658 I print_info: general.name     = 1.4B
0.00.051.658 I print_info: vocab type       = BPE
0.00.051.658 I print_info: n_vocab          = 50304
0.00.051.658 I print_info: n_merges         = 50009
0.00.051.659 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.659 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.659 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.659 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.660 I print_info: LF token         = 128 ''
0.00.051.660 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.660 I print_info: max token length = 1024
0.00.053.643 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.643 I load_tensors: offloading output layer to GPU
0.00.053.643 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.654 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.655 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.975 I llama_init_from_model: n_seq_max     = 1
0.00.053.976 I llama_init_from_model: n_ctx         = 128
0.00.053.976 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.976 I llama_init_from_model: n_batch       = 128
0.00.053.977 I llama_init_from_model: n_ubatch      = 128
0.00.053.977 I llama_init_from_model: flash_attn    = 0
0.00.053.977 I llama_init_from_model: freq_base     = 10000.0
0.00.053.978 I llama_init_from_model: freq_scale    = 1
0.00.053.978 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.978 I ggml_metal_init: allocating
0.00.053.982 I ggml_metal_init: found device: Apple M4
0.00.053.984 I ggml_metal_init: picking default device: Apple M4
0.00.054.580 I ggml_metal_init: using embedded metal library
0.00.056.982 I ggml_metal_init: GPU name:   Apple M4
0.00.056.984 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.984 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.985 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.985 I ggml_metal_init: simdgroup reduction   = true
0.00.056.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.985 I ggml_metal_init: has bfloat            = true
0.00.056.985 I ggml_metal_init: use bfloat            = true
0.00.056.986 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.987 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.163 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.435 I init:      Metal KV buffer size =    24.00 MiB
0.00.068.441 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.461 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.321 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.323 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.323 I llama_init_from_model: graph nodes  = 967
0.00.069.323 I llama_init_from_model: graph splits = 2
0.00.069.325 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.325 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.533 I 
0.00.618.562 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.565 I perplexity: tokenizing the input ..
0.00.626.491 I perplexity: tokenization took 7.924 ms
0.00.626.495 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.749.254 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.750.430 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.750.463 I llama_perf_context_print:        load time =     609.57 ms
0.00.750.464 I llama_perf_context_print: prompt eval time =     122.53 ms /   128 tokens (    0.96 ms per token,  1044.64 tokens per second)
0.00.750.465 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.465 I llama_perf_context_print:       total time =     131.93 ms /   129 tokens
0.00.751.131 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.080s
sys	0m0.101s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.882 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.104 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.108 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.110 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.117 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.118 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.118 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.119 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.120 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.120 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.120 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.121 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.121 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.121 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.124 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.125 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.965 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.007 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.816 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.817 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.817 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.817 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.818 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.818 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.819 I llama_model_loader: - type  f32:  194 tensors
0.00.026.819 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.819 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.820 I print_info: file format = GGUF V3 (latest)
0.00.026.820 I print_info: file type   = Q5_0
0.00.026.821 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.471 I load: special tokens cache size = 25
0.00.052.599 I load: token to piece cache size = 0.2984 MB
0.00.052.602 I print_info: arch             = gptneox
0.00.052.603 I print_info: vocab_only       = 0
0.00.052.603 I print_info: n_ctx_train      = 2048
0.00.052.603 I print_info: n_embd           = 2048
0.00.052.603 I print_info: n_layer          = 24
0.00.052.606 I print_info: n_head           = 16
0.00.052.607 I print_info: n_head_kv        = 16
0.00.052.607 I print_info: n_rot            = 32
0.00.052.608 I print_info: n_swa            = 0
0.00.052.608 I print_info: n_embd_head_k    = 128
0.00.052.608 I print_info: n_embd_head_v    = 128
0.00.052.609 I print_info: n_gqa            = 1
0.00.052.609 I print_info: n_embd_k_gqa     = 2048
0.00.052.610 I print_info: n_embd_v_gqa     = 2048
0.00.052.611 I print_info: f_norm_eps       = 1.0e-05
0.00.052.611 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.611 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.612 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.612 I print_info: f_logit_scale    = 0.0e+00
0.00.052.613 I print_info: n_ff             = 8192
0.00.052.613 I print_info: n_expert         = 0
0.00.052.614 I print_info: n_expert_used    = 0
0.00.052.614 I print_info: causal attn      = 1
0.00.052.614 I print_info: pooling type     = 0
0.00.052.614 I print_info: rope type        = 2
0.00.052.616 I print_info: rope scaling     = linear
0.00.052.617 I print_info: freq_base_train  = 10000.0
0.00.052.617 I print_info: freq_scale_train = 1
0.00.052.617 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.617 I print_info: rope_finetuned   = unknown
0.00.052.617 I print_info: ssm_d_conv       = 0
0.00.052.617 I print_info: ssm_d_inner      = 0
0.00.052.618 I print_info: ssm_d_state      = 0
0.00.052.618 I print_info: ssm_dt_rank      = 0
0.00.052.618 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.618 I print_info: model type       = 1.4B
0.00.052.618 I print_info: model params     = 1.41 B
0.00.052.619 I print_info: general.name     = 1.4B
0.00.052.619 I print_info: vocab type       = BPE
0.00.052.619 I print_info: n_vocab          = 50304
0.00.052.619 I print_info: n_merges         = 50009
0.00.052.620 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.620 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.624 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.624 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.624 I print_info: LF token         = 128 ''
0.00.052.625 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.625 I print_info: max token length = 1024
0.00.054.617 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.618 I load_tensors: offloading output layer to GPU
0.00.054.618 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.628 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.629 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.954 I llama_init_from_model: n_seq_max     = 1
0.00.054.955 I llama_init_from_model: n_ctx         = 128
0.00.054.955 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.955 I llama_init_from_model: n_batch       = 128
0.00.054.955 I llama_init_from_model: n_ubatch      = 128
0.00.054.955 I llama_init_from_model: flash_attn    = 0
0.00.054.956 I llama_init_from_model: freq_base     = 10000.0
0.00.054.956 I llama_init_from_model: freq_scale    = 1
0.00.054.956 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.957 I ggml_metal_init: allocating
0.00.054.960 I ggml_metal_init: found device: Apple M4
0.00.054.962 I ggml_metal_init: picking default device: Apple M4
0.00.055.525 I ggml_metal_init: using embedded metal library
0.00.057.928 I ggml_metal_init: GPU name:   Apple M4
0.00.057.930 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.930 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.930 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.931 I ggml_metal_init: simdgroup reduction   = true
0.00.057.931 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.931 I ggml_metal_init: has bfloat            = true
0.00.057.931 I ggml_metal_init: use bfloat            = true
0.00.057.931 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.932 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.764 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.027 I init:      Metal KV buffer size =    24.00 MiB
0.00.069.029 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.042 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.882 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.883 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.883 I llama_init_from_model: graph nodes  = 967
0.00.069.883 I llama_init_from_model: graph splits = 2
0.00.069.884 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.884 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.656 I 
0.00.701.694 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.700 I perplexity: tokenizing the input ..
0.00.709.585 I perplexity: tokenization took 7.884 ms
0.00.709.589 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.519 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.845.684 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.845.711 I llama_perf_context_print:        load time =     690.77 ms
0.00.845.715 I llama_perf_context_print: prompt eval time =     134.71 ms /   128 tokens (    1.05 ms per token,   950.22 tokens per second)
0.00.845.715 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.716 I llama_perf_context_print:       total time =     144.06 ms /   129 tokens
0.00.846.358 I ggml_metal_free: deallocating

real	0m0.862s
user	0m0.079s
sys	0m0.118s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.881 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.090 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.095 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.097 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.098 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.098 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.098 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.099 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.100 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.100 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.103 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.103 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.104 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.104 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.104 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.107 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.108 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.108 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.843 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.887 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.597 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.599 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.599 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.599 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.600 I llama_model_loader: - type  f32:  194 tensors
0.00.024.600 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.600 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.601 I print_info: file format = GGUF V3 (latest)
0.00.024.601 I print_info: file type   = Q5_1
0.00.024.602 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.445 I load: special tokens cache size = 25
0.00.049.505 I load: token to piece cache size = 0.2984 MB
0.00.049.508 I print_info: arch             = gptneox
0.00.049.508 I print_info: vocab_only       = 0
0.00.049.509 I print_info: n_ctx_train      = 2048
0.00.049.509 I print_info: n_embd           = 2048
0.00.049.509 I print_info: n_layer          = 24
0.00.049.512 I print_info: n_head           = 16
0.00.049.513 I print_info: n_head_kv        = 16
0.00.049.513 I print_info: n_rot            = 32
0.00.049.513 I print_info: n_swa            = 0
0.00.049.514 I print_info: n_embd_head_k    = 128
0.00.049.514 I print_info: n_embd_head_v    = 128
0.00.049.514 I print_info: n_gqa            = 1
0.00.049.515 I print_info: n_embd_k_gqa     = 2048
0.00.049.516 I print_info: n_embd_v_gqa     = 2048
0.00.049.519 I print_info: f_norm_eps       = 1.0e-05
0.00.049.520 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.520 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.520 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.520 I print_info: f_logit_scale    = 0.0e+00
0.00.049.522 I print_info: n_ff             = 8192
0.00.049.523 I print_info: n_expert         = 0
0.00.049.523 I print_info: n_expert_used    = 0
0.00.049.523 I print_info: causal attn      = 1
0.00.049.523 I print_info: pooling type     = 0
0.00.049.524 I print_info: rope type        = 2
0.00.049.525 I print_info: rope scaling     = linear
0.00.049.526 I print_info: freq_base_train  = 10000.0
0.00.049.528 I print_info: freq_scale_train = 1
0.00.049.528 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.528 I print_info: rope_finetuned   = unknown
0.00.049.528 I print_info: ssm_d_conv       = 0
0.00.049.528 I print_info: ssm_d_inner      = 0
0.00.049.529 I print_info: ssm_d_state      = 0
0.00.049.529 I print_info: ssm_dt_rank      = 0
0.00.049.529 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.529 I print_info: model type       = 1.4B
0.00.049.530 I print_info: model params     = 1.41 B
0.00.049.530 I print_info: general.name     = 1.4B
0.00.049.530 I print_info: vocab type       = BPE
0.00.049.530 I print_info: n_vocab          = 50304
0.00.049.531 I print_info: n_merges         = 50009
0.00.049.531 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.531 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.531 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.531 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.532 I print_info: LF token         = 128 ''
0.00.049.532 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.533 I print_info: max token length = 1024
0.00.051.532 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.532 I load_tensors: offloading output layer to GPU
0.00.051.532 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.543 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.544 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.814 I llama_init_from_model: n_seq_max     = 1
0.00.051.814 I llama_init_from_model: n_ctx         = 128
0.00.051.815 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.815 I llama_init_from_model: n_batch       = 128
0.00.051.815 I llama_init_from_model: n_ubatch      = 128
0.00.051.815 I llama_init_from_model: flash_attn    = 0
0.00.051.816 I llama_init_from_model: freq_base     = 10000.0
0.00.051.816 I llama_init_from_model: freq_scale    = 1
0.00.051.816 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.817 I ggml_metal_init: allocating
0.00.051.819 I ggml_metal_init: found device: Apple M4
0.00.051.821 I ggml_metal_init: picking default device: Apple M4
0.00.052.407 I ggml_metal_init: using embedded metal library
0.00.054.762 I ggml_metal_init: GPU name:   Apple M4
0.00.054.764 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.764 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.765 I ggml_metal_init: simdgroup reduction   = true
0.00.054.765 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.765 I ggml_metal_init: has bfloat            = true
0.00.054.765 I ggml_metal_init: use bfloat            = true
0.00.054.765 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.766 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.260 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.486 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.488 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.512 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.410 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.411 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.411 I llama_init_from_model: graph nodes  = 967
0.00.066.412 I llama_init_from_model: graph splits = 2
0.00.066.413 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.413 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.691 I 
0.00.729.740 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.746 I perplexity: tokenizing the input ..
0.00.737.884 I perplexity: tokenization took 8.137 ms
0.00.737.887 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.872.912 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.874.088 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.874.108 I llama_perf_context_print:        load time =     720.81 ms
0.00.874.109 I llama_perf_context_print: prompt eval time =     134.80 ms /   128 tokens (    1.05 ms per token,   949.58 tokens per second)
0.00.874.110 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.874.111 I llama_perf_context_print:       total time =     144.42 ms /   129 tokens
0.00.874.627 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.077s
sys	0m0.111s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q2_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.866 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.872 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.874 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.874 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.874 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.875 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.875 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.876 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.879 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.879 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.880 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.880 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.881 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.885 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.887 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.888 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.888 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.714 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.759 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.555 I llama_model_loader: - type  f32:  194 tensors
0.00.026.555 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.555 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.555 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.556 I print_info: file format = GGUF V3 (latest)
0.00.026.556 I print_info: file type   = Q2_K - Medium
0.00.026.557 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.046.292 I load: special tokens cache size = 25
0.00.052.378 I load: token to piece cache size = 0.2984 MB
0.00.052.382 I print_info: arch             = gptneox
0.00.052.382 I print_info: vocab_only       = 0
0.00.052.382 I print_info: n_ctx_train      = 2048
0.00.052.383 I print_info: n_embd           = 2048
0.00.052.383 I print_info: n_layer          = 24
0.00.052.386 I print_info: n_head           = 16
0.00.052.387 I print_info: n_head_kv        = 16
0.00.052.387 I print_info: n_rot            = 32
0.00.052.387 I print_info: n_swa            = 0
0.00.052.390 I print_info: n_embd_head_k    = 128
0.00.052.390 I print_info: n_embd_head_v    = 128
0.00.052.391 I print_info: n_gqa            = 1
0.00.052.392 I print_info: n_embd_k_gqa     = 2048
0.00.052.392 I print_info: n_embd_v_gqa     = 2048
0.00.052.393 I print_info: f_norm_eps       = 1.0e-05
0.00.052.393 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.393 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.394 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.395 I print_info: f_logit_scale    = 0.0e+00
0.00.052.396 I print_info: n_ff             = 8192
0.00.052.396 I print_info: n_expert         = 0
0.00.052.396 I print_info: n_expert_used    = 0
0.00.052.396 I print_info: causal attn      = 1
0.00.052.396 I print_info: pooling type     = 0
0.00.052.397 I print_info: rope type        = 2
0.00.052.397 I print_info: rope scaling     = linear
0.00.052.397 I print_info: freq_base_train  = 10000.0
0.00.052.397 I print_info: freq_scale_train = 1
0.00.052.398 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.398 I print_info: rope_finetuned   = unknown
0.00.052.398 I print_info: ssm_d_conv       = 0
0.00.052.399 I print_info: ssm_d_inner      = 0
0.00.052.399 I print_info: ssm_d_state      = 0
0.00.052.399 I print_info: ssm_dt_rank      = 0
0.00.052.403 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.403 I print_info: model type       = 1.4B
0.00.052.404 I print_info: model params     = 1.41 B
0.00.052.404 I print_info: general.name     = 1.4B
0.00.052.404 I print_info: vocab type       = BPE
0.00.052.405 I print_info: n_vocab          = 50304
0.00.052.406 I print_info: n_merges         = 50009
0.00.052.406 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.406 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.406 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.407 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.407 I print_info: LF token         = 128 ''
0.00.052.407 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.407 I print_info: max token length = 1024
0.00.053.991 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.991 I load_tensors: offloading output layer to GPU
0.00.053.991 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.001 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.002 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.054.282 I llama_init_from_model: n_seq_max     = 1
0.00.054.283 I llama_init_from_model: n_ctx         = 128
0.00.054.283 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.283 I llama_init_from_model: n_batch       = 128
0.00.054.283 I llama_init_from_model: n_ubatch      = 128
0.00.054.283 I llama_init_from_model: flash_attn    = 0
0.00.054.284 I llama_init_from_model: freq_base     = 10000.0
0.00.054.284 I llama_init_from_model: freq_scale    = 1
0.00.054.285 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.285 I ggml_metal_init: allocating
0.00.054.290 I ggml_metal_init: found device: Apple M4
0.00.054.292 I ggml_metal_init: picking default device: Apple M4
0.00.054.894 I ggml_metal_init: using embedded metal library
0.00.057.259 I ggml_metal_init: GPU name:   Apple M4
0.00.057.261 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.261 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.262 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.262 I ggml_metal_init: simdgroup reduction   = true
0.00.057.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.262 I ggml_metal_init: has bfloat            = true
0.00.057.262 I ggml_metal_init: use bfloat            = true
0.00.057.263 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.263 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.229 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.625 I init:      Metal KV buffer size =    24.00 MiB
0.00.068.629 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.645 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.488 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.489 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.489 I llama_init_from_model: graph nodes  = 967
0.00.069.490 I llama_init_from_model: graph splits = 2
0.00.069.491 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.491 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.404.730 I 
0.00.404.768 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.404.771 I perplexity: tokenizing the input ..
0.00.412.872 I perplexity: tokenization took 8.1 ms
0.00.412.877 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.544.982 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.546.217 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.546.247 I llama_perf_context_print:        load time =     393.88 ms
0.00.546.248 I llama_perf_context_print: prompt eval time =     131.87 ms /   128 tokens (    1.03 ms per token,   970.66 tokens per second)
0.00.546.249 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.546.249 I llama_perf_context_print:       total time =     141.52 ms /   129 tokens
0.00.547.010 I ggml_metal_free: deallocating

real	0m0.563s
user	0m0.078s
sys	0m0.073s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.159 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.209 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.215 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.217 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.218 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.218 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.218 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.219 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.220 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.220 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.221 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.221 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.222 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.224 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.225 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.943 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.681 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.682 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.683 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.683 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.683 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.684 I llama_model_loader: - type  f32:  194 tensors
0.00.024.684 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.684 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.685 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.685 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.685 I print_info: file format = GGUF V3 (latest)
0.00.024.686 I print_info: file type   = Q3_K - Medium
0.00.024.687 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.602 I load: special tokens cache size = 25
0.00.049.528 I load: token to piece cache size = 0.2984 MB
0.00.049.531 I print_info: arch             = gptneox
0.00.049.531 I print_info: vocab_only       = 0
0.00.049.531 I print_info: n_ctx_train      = 2048
0.00.049.532 I print_info: n_embd           = 2048
0.00.049.532 I print_info: n_layer          = 24
0.00.049.535 I print_info: n_head           = 16
0.00.049.535 I print_info: n_head_kv        = 16
0.00.049.536 I print_info: n_rot            = 32
0.00.049.536 I print_info: n_swa            = 0
0.00.049.537 I print_info: n_embd_head_k    = 128
0.00.049.537 I print_info: n_embd_head_v    = 128
0.00.049.539 I print_info: n_gqa            = 1
0.00.049.540 I print_info: n_embd_k_gqa     = 2048
0.00.049.541 I print_info: n_embd_v_gqa     = 2048
0.00.049.541 I print_info: f_norm_eps       = 1.0e-05
0.00.049.542 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.542 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.542 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.542 I print_info: f_logit_scale    = 0.0e+00
0.00.049.543 I print_info: n_ff             = 8192
0.00.049.543 I print_info: n_expert         = 0
0.00.049.543 I print_info: n_expert_used    = 0
0.00.049.543 I print_info: causal attn      = 1
0.00.049.543 I print_info: pooling type     = 0
0.00.049.543 I print_info: rope type        = 2
0.00.049.544 I print_info: rope scaling     = linear
0.00.049.544 I print_info: freq_base_train  = 10000.0
0.00.049.550 I print_info: freq_scale_train = 1
0.00.049.552 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.552 I print_info: rope_finetuned   = unknown
0.00.049.553 I print_info: ssm_d_conv       = 0
0.00.049.553 I print_info: ssm_d_inner      = 0
0.00.049.553 I print_info: ssm_d_state      = 0
0.00.049.553 I print_info: ssm_dt_rank      = 0
0.00.049.553 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.553 I print_info: model type       = 1.4B
0.00.049.554 I print_info: model params     = 1.41 B
0.00.049.554 I print_info: general.name     = 1.4B
0.00.049.555 I print_info: vocab type       = BPE
0.00.049.556 I print_info: n_vocab          = 50304
0.00.049.556 I print_info: n_merges         = 50009
0.00.049.556 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.556 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.556 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.557 I print_info: LF token         = 128 ''
0.00.049.557 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.557 I print_info: max token length = 1024
0.00.051.485 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.486 I load_tensors: offloading output layer to GPU
0.00.051.486 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.496 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.497 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.778 I llama_init_from_model: n_seq_max     = 1
0.00.051.778 I llama_init_from_model: n_ctx         = 128
0.00.051.779 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.779 I llama_init_from_model: n_batch       = 128
0.00.051.779 I llama_init_from_model: n_ubatch      = 128
0.00.051.779 I llama_init_from_model: flash_attn    = 0
0.00.051.779 I llama_init_from_model: freq_base     = 10000.0
0.00.051.780 I llama_init_from_model: freq_scale    = 1
0.00.051.780 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.781 I ggml_metal_init: allocating
0.00.051.784 I ggml_metal_init: found device: Apple M4
0.00.051.785 I ggml_metal_init: picking default device: Apple M4
0.00.052.369 I ggml_metal_init: using embedded metal library
0.00.054.737 I ggml_metal_init: GPU name:   Apple M4
0.00.054.738 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.739 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.739 I ggml_metal_init: simdgroup reduction   = true
0.00.054.739 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.740 I ggml_metal_init: has bfloat            = true
0.00.054.740 I ggml_metal_init: use bfloat            = true
0.00.054.740 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.741 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.722 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.969 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.971 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.989 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.804 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.805 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.805 I llama_init_from_model: graph nodes  = 967
0.00.066.806 I llama_init_from_model: graph splits = 2
0.00.066.807 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.472.109 I 
0.00.472.145 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.472.149 I perplexity: tokenizing the input ..
0.00.480.518 I perplexity: tokenization took 8.367 ms
0.00.480.524 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.612.886 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.614.065 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.614.093 I llama_perf_context_print:        load time =     462.95 ms
0.00.614.094 I llama_perf_context_print: prompt eval time =     132.13 ms /   128 tokens (    1.03 ms per token,   968.74 tokens per second)
0.00.614.095 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.614.095 I llama_perf_context_print:       total time =     141.99 ms /   129 tokens
0.00.614.741 I ggml_metal_free: deallocating

real	0m0.628s
user	0m0.078s
sys	0m0.082s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.040 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.165 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.170 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.172 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.172 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.173 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.173 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.175 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.176 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.177 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.178 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.179 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.946 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.684 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.685 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.686 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.686 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.687 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.687 I llama_model_loader: - type  f32:  194 tensors
0.00.024.687 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.688 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.688 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.688 I print_info: file format = GGUF V3 (latest)
0.00.024.689 I print_info: file type   = Q4_K - Medium
0.00.024.690 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.254 I load: special tokens cache size = 25
0.00.050.630 I load: token to piece cache size = 0.2984 MB
0.00.050.633 I print_info: arch             = gptneox
0.00.050.633 I print_info: vocab_only       = 0
0.00.050.633 I print_info: n_ctx_train      = 2048
0.00.050.633 I print_info: n_embd           = 2048
0.00.050.634 I print_info: n_layer          = 24
0.00.050.636 I print_info: n_head           = 16
0.00.050.637 I print_info: n_head_kv        = 16
0.00.050.637 I print_info: n_rot            = 32
0.00.050.638 I print_info: n_swa            = 0
0.00.050.638 I print_info: n_embd_head_k    = 128
0.00.050.638 I print_info: n_embd_head_v    = 128
0.00.050.639 I print_info: n_gqa            = 1
0.00.050.640 I print_info: n_embd_k_gqa     = 2048
0.00.050.640 I print_info: n_embd_v_gqa     = 2048
0.00.050.641 I print_info: f_norm_eps       = 1.0e-05
0.00.050.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.641 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.642 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.642 I print_info: f_logit_scale    = 0.0e+00
0.00.050.643 I print_info: n_ff             = 8192
0.00.050.643 I print_info: n_expert         = 0
0.00.050.643 I print_info: n_expert_used    = 0
0.00.050.643 I print_info: causal attn      = 1
0.00.050.643 I print_info: pooling type     = 0
0.00.050.643 I print_info: rope type        = 2
0.00.050.643 I print_info: rope scaling     = linear
0.00.050.645 I print_info: freq_base_train  = 10000.0
0.00.050.645 I print_info: freq_scale_train = 1
0.00.050.645 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.646 I print_info: rope_finetuned   = unknown
0.00.050.646 I print_info: ssm_d_conv       = 0
0.00.050.646 I print_info: ssm_d_inner      = 0
0.00.050.646 I print_info: ssm_d_state      = 0
0.00.050.648 I print_info: ssm_dt_rank      = 0
0.00.050.648 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.648 I print_info: model type       = 1.4B
0.00.050.649 I print_info: model params     = 1.41 B
0.00.050.649 I print_info: general.name     = 1.4B
0.00.050.650 I print_info: vocab type       = BPE
0.00.050.650 I print_info: n_vocab          = 50304
0.00.050.650 I print_info: n_merges         = 50009
0.00.050.650 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.650 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.651 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.651 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.655 I print_info: LF token         = 128 ''
0.00.050.655 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.656 I print_info: max token length = 1024
0.00.052.652 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.652 I load_tensors: offloading output layer to GPU
0.00.052.652 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.663 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.664 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.943 I llama_init_from_model: n_seq_max     = 1
0.00.052.943 I llama_init_from_model: n_ctx         = 128
0.00.052.943 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.944 I llama_init_from_model: n_batch       = 128
0.00.052.944 I llama_init_from_model: n_ubatch      = 128
0.00.052.944 I llama_init_from_model: flash_attn    = 0
0.00.052.944 I llama_init_from_model: freq_base     = 10000.0
0.00.052.945 I llama_init_from_model: freq_scale    = 1
0.00.052.945 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.945 I ggml_metal_init: allocating
0.00.052.949 I ggml_metal_init: found device: Apple M4
0.00.052.951 I ggml_metal_init: picking default device: Apple M4
0.00.053.552 I ggml_metal_init: using embedded metal library
0.00.055.927 I ggml_metal_init: GPU name:   Apple M4
0.00.055.929 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.929 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.930 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.930 I ggml_metal_init: simdgroup reduction   = true
0.00.055.930 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.930 I ggml_metal_init: has bfloat            = true
0.00.055.930 I ggml_metal_init: use bfloat            = true
0.00.055.931 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.931 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.768 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.067 I init:      Metal KV buffer size =    24.00 MiB
0.00.067.070 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.084 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.030 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.031 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.031 I llama_init_from_model: graph nodes  = 967
0.00.068.032 I llama_init_from_model: graph splits = 2
0.00.068.033 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.033 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.564.232 I 
0.00.564.285 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.564.289 I perplexity: tokenizing the input ..
0.00.572.657 I perplexity: tokenization took 8.366 ms
0.00.572.666 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.706.705 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.707.903 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.707.936 I llama_perf_context_print:        load time =     555.19 ms
0.00.707.937 I llama_perf_context_print: prompt eval time =     133.81 ms /   128 tokens (    1.05 ms per token,   956.57 tokens per second)
0.00.707.937 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.707.938 I llama_perf_context_print:       total time =     143.70 ms /   129 tokens
0.00.708.613 I ggml_metal_free: deallocating

real	0m0.722s
user	0m0.079s
sys	0m0.106s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.843 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.567 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.572 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.575 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.577 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.578 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.580 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.580 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.582 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.582 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.337 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.337 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.101 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.102 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.103 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.103 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.103 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.103 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.104 I llama_model_loader: - type  f32:  194 tensors
0.00.025.104 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.104 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.105 I print_info: file format = GGUF V3 (latest)
0.00.025.105 I print_info: file type   = Q5_K - Medium
0.00.025.106 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.927 I load: special tokens cache size = 25
0.00.050.190 I load: token to piece cache size = 0.2984 MB
0.00.050.193 I print_info: arch             = gptneox
0.00.050.193 I print_info: vocab_only       = 0
0.00.050.194 I print_info: n_ctx_train      = 2048
0.00.050.194 I print_info: n_embd           = 2048
0.00.050.194 I print_info: n_layer          = 24
0.00.050.197 I print_info: n_head           = 16
0.00.050.198 I print_info: n_head_kv        = 16
0.00.050.198 I print_info: n_rot            = 32
0.00.050.200 I print_info: n_swa            = 0
0.00.050.200 I print_info: n_embd_head_k    = 128
0.00.050.200 I print_info: n_embd_head_v    = 128
0.00.050.201 I print_info: n_gqa            = 1
0.00.050.202 I print_info: n_embd_k_gqa     = 2048
0.00.050.202 I print_info: n_embd_v_gqa     = 2048
0.00.050.203 I print_info: f_norm_eps       = 1.0e-05
0.00.050.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.204 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.204 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.204 I print_info: f_logit_scale    = 0.0e+00
0.00.050.205 I print_info: n_ff             = 8192
0.00.050.205 I print_info: n_expert         = 0
0.00.050.205 I print_info: n_expert_used    = 0
0.00.050.205 I print_info: causal attn      = 1
0.00.050.205 I print_info: pooling type     = 0
0.00.050.205 I print_info: rope type        = 2
0.00.050.206 I print_info: rope scaling     = linear
0.00.050.207 I print_info: freq_base_train  = 10000.0
0.00.050.207 I print_info: freq_scale_train = 1
0.00.050.207 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.207 I print_info: rope_finetuned   = unknown
0.00.050.207 I print_info: ssm_d_conv       = 0
0.00.050.208 I print_info: ssm_d_inner      = 0
0.00.050.208 I print_info: ssm_d_state      = 0
0.00.050.208 I print_info: ssm_dt_rank      = 0
0.00.050.208 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.208 I print_info: model type       = 1.4B
0.00.050.209 I print_info: model params     = 1.41 B
0.00.050.209 I print_info: general.name     = 1.4B
0.00.050.209 I print_info: vocab type       = BPE
0.00.050.209 I print_info: n_vocab          = 50304
0.00.050.209 I print_info: n_merges         = 50009
0.00.050.210 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.210 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.210 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.210 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.211 I print_info: LF token         = 128 ''
0.00.050.211 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.215 I print_info: max token length = 1024
0.00.052.163 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.163 I load_tensors: offloading output layer to GPU
0.00.052.163 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.174 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.175 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.507 I llama_init_from_model: n_seq_max     = 1
0.00.052.507 I llama_init_from_model: n_ctx         = 128
0.00.052.507 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.508 I llama_init_from_model: n_batch       = 128
0.00.052.508 I llama_init_from_model: n_ubatch      = 128
0.00.052.508 I llama_init_from_model: flash_attn    = 0
0.00.052.508 I llama_init_from_model: freq_base     = 10000.0
0.00.052.509 I llama_init_from_model: freq_scale    = 1
0.00.052.509 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.509 I ggml_metal_init: allocating
0.00.052.512 I ggml_metal_init: found device: Apple M4
0.00.052.514 I ggml_metal_init: picking default device: Apple M4
0.00.053.087 I ggml_metal_init: using embedded metal library
0.00.055.389 I ggml_metal_init: GPU name:   Apple M4
0.00.055.391 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.392 I ggml_metal_init: simdgroup reduction   = true
0.00.055.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.392 I ggml_metal_init: has bfloat            = true
0.00.055.392 I ggml_metal_init: use bfloat            = true
0.00.055.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.921 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.166 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.168 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.182 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.059 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.060 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.061 I llama_init_from_model: graph nodes  = 967
0.00.067.061 I llama_init_from_model: graph splits = 2
0.00.067.062 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.472 I 
0.00.625.504 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.506 I perplexity: tokenizing the input ..
0.00.633.353 I perplexity: tokenization took 7.845 ms
0.00.633.357 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.129 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.775.305 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.775.335 I llama_perf_context_print:        load time =     615.63 ms
0.00.775.335 I llama_perf_context_print: prompt eval time =     140.55 ms /   128 tokens (    1.10 ms per token,   910.73 tokens per second)
0.00.775.336 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.775.337 I llama_perf_context_print:       total time =     149.86 ms /   129 tokens
0.00.776.034 I ggml_metal_free: deallocating

real	0m0.792s
user	0m0.077s
sys	0m0.111s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.855 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.513 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.520 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.520 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.521 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.521 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.522 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.524 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.525 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.525 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.526 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.526 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.526 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.530 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.530 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.530 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.391 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.404 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.246 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.247 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.247 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.248 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.248 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.248 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.249 I llama_model_loader: - type  f32:  194 tensors
0.00.024.249 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.250 I print_info: file format = GGUF V3 (latest)
0.00.024.250 I print_info: file type   = Q6_K
0.00.024.251 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.832 I load: special tokens cache size = 25
0.00.049.871 I load: token to piece cache size = 0.2984 MB
0.00.049.874 I print_info: arch             = gptneox
0.00.049.874 I print_info: vocab_only       = 0
0.00.049.874 I print_info: n_ctx_train      = 2048
0.00.049.874 I print_info: n_embd           = 2048
0.00.049.875 I print_info: n_layer          = 24
0.00.049.877 I print_info: n_head           = 16
0.00.049.878 I print_info: n_head_kv        = 16
0.00.049.878 I print_info: n_rot            = 32
0.00.049.879 I print_info: n_swa            = 0
0.00.049.879 I print_info: n_embd_head_k    = 128
0.00.049.879 I print_info: n_embd_head_v    = 128
0.00.049.880 I print_info: n_gqa            = 1
0.00.049.880 I print_info: n_embd_k_gqa     = 2048
0.00.049.881 I print_info: n_embd_v_gqa     = 2048
0.00.049.882 I print_info: f_norm_eps       = 1.0e-05
0.00.049.882 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.882 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.883 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.883 I print_info: f_logit_scale    = 0.0e+00
0.00.049.883 I print_info: n_ff             = 8192
0.00.049.884 I print_info: n_expert         = 0
0.00.049.884 I print_info: n_expert_used    = 0
0.00.049.884 I print_info: causal attn      = 1
0.00.049.884 I print_info: pooling type     = 0
0.00.049.884 I print_info: rope type        = 2
0.00.049.885 I print_info: rope scaling     = linear
0.00.049.885 I print_info: freq_base_train  = 10000.0
0.00.049.885 I print_info: freq_scale_train = 1
0.00.049.885 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.886 I print_info: rope_finetuned   = unknown
0.00.049.886 I print_info: ssm_d_conv       = 0
0.00.049.886 I print_info: ssm_d_inner      = 0
0.00.049.886 I print_info: ssm_d_state      = 0
0.00.049.888 I print_info: ssm_dt_rank      = 0
0.00.049.888 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.888 I print_info: model type       = 1.4B
0.00.049.889 I print_info: model params     = 1.41 B
0.00.049.889 I print_info: general.name     = 1.4B
0.00.049.889 I print_info: vocab type       = BPE
0.00.049.889 I print_info: n_vocab          = 50304
0.00.049.889 I print_info: n_merges         = 50009
0.00.049.890 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.890 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.890 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.890 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.890 I print_info: LF token         = 128 ''
0.00.049.891 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.891 I print_info: max token length = 1024
0.00.051.478 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.479 I load_tensors: offloading output layer to GPU
0.00.051.479 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.489 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.490 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.752 I llama_init_from_model: n_seq_max     = 1
0.00.051.752 I llama_init_from_model: n_ctx         = 128
0.00.051.753 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.753 I llama_init_from_model: n_batch       = 128
0.00.051.753 I llama_init_from_model: n_ubatch      = 128
0.00.051.753 I llama_init_from_model: flash_attn    = 0
0.00.051.753 I llama_init_from_model: freq_base     = 10000.0
0.00.051.754 I llama_init_from_model: freq_scale    = 1
0.00.051.754 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.754 I ggml_metal_init: allocating
0.00.051.757 I ggml_metal_init: found device: Apple M4
0.00.051.759 I ggml_metal_init: picking default device: Apple M4
0.00.052.310 I ggml_metal_init: using embedded metal library
0.00.054.606 I ggml_metal_init: GPU name:   Apple M4
0.00.054.608 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.608 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.608 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.609 I ggml_metal_init: simdgroup reduction   = true
0.00.054.609 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.609 I ggml_metal_init: has bfloat            = true
0.00.054.609 I ggml_metal_init: use bfloat            = true
0.00.054.610 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.610 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.322 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.665 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.668 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.682 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.544 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.545 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.546 I llama_init_from_model: graph nodes  = 967
0.00.066.546 I llama_init_from_model: graph splits = 2
0.00.066.547 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.547 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.471.810 I 
0.00.471.860 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.471.866 I perplexity: tokenizing the input ..
0.00.480.314 I perplexity: tokenization took 8.446 ms
0.00.480.317 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.620.272 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.621.441 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.621.469 I llama_perf_context_print:        load time =     462.95 ms
0.00.621.470 I llama_perf_context_print: prompt eval time =     139.73 ms /   128 tokens (    1.09 ms per token,   916.07 tokens per second)
0.00.621.471 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.621.471 I llama_perf_context_print:       total time =     149.66 ms /   129 tokens
0.00.622.238 I ggml_metal_free: deallocating

real	0m0.635s
user	0m0.079s
sys	0m0.100s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-imatrix.log
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.255 I build: 4505 (9027f329) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.037 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.740 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.749 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.749 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.750 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.751 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.751 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.756 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.756 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.757 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.757 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.758 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.765 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.769 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.769 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.814 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.821 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.823 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.824 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.824 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.824 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.825 I llama_model_loader: - type  f32:  194 tensors
0.00.051.825 I llama_model_loader: - type  f16:   98 tensors
0.00.051.826 I print_info: file format = GGUF V3 (latest)
0.00.051.827 I print_info: file type   = all F32 (guessed)
0.00.051.828 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.077.582 I load: special tokens cache size = 25
0.00.084.259 I load: token to piece cache size = 0.2984 MB
0.00.084.262 I print_info: arch             = gptneox
0.00.084.262 I print_info: vocab_only       = 0
0.00.084.263 I print_info: n_ctx_train      = 2048
0.00.084.263 I print_info: n_embd           = 2048
0.00.084.263 I print_info: n_layer          = 24
0.00.084.268 I print_info: n_head           = 16
0.00.084.268 I print_info: n_head_kv        = 16
0.00.084.270 I print_info: n_rot            = 32
0.00.084.270 I print_info: n_swa            = 0
0.00.084.270 I print_info: n_embd_head_k    = 128
0.00.084.270 I print_info: n_embd_head_v    = 128
0.00.084.271 I print_info: n_gqa            = 1
0.00.084.271 I print_info: n_embd_k_gqa     = 2048
0.00.084.272 I print_info: n_embd_v_gqa     = 2048
0.00.084.272 I print_info: f_norm_eps       = 1.0e-05
0.00.084.273 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.084.273 I print_info: f_clamp_kqv      = 0.0e+00
0.00.084.280 I print_info: f_max_alibi_bias = 0.0e+00
0.00.084.281 I print_info: f_logit_scale    = 0.0e+00
0.00.084.288 I print_info: n_ff             = 8192
0.00.084.289 I print_info: n_expert         = 0
0.00.084.289 I print_info: n_expert_used    = 0
0.00.084.289 I print_info: causal attn      = 1
0.00.084.289 I print_info: pooling type     = 0
0.00.084.289 I print_info: rope type        = 2
0.00.084.289 I print_info: rope scaling     = linear
0.00.084.290 I print_info: freq_base_train  = 10000.0
0.00.084.290 I print_info: freq_scale_train = 1
0.00.084.290 I print_info: n_ctx_orig_yarn  = 2048
0.00.084.290 I print_info: rope_finetuned   = unknown
0.00.084.290 I print_info: ssm_d_conv       = 0
0.00.084.291 I print_info: ssm_d_inner      = 0
0.00.084.292 I print_info: ssm_d_state      = 0
0.00.084.292 I print_info: ssm_dt_rank      = 0
0.00.084.292 I print_info: ssm_dt_b_c_rms   = 0
0.00.084.292 I print_info: model type       = 1.4B
0.00.084.292 I print_info: model params     = 1.41 B
0.00.084.293 I print_info: general.name     = 1.4B
0.00.084.294 I print_info: vocab type       = BPE
0.00.084.294 I print_info: n_vocab          = 50304
0.00.084.294 I print_info: n_merges         = 50009
0.00.084.295 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.084.295 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.084.295 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.084.295 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.084.295 I print_info: LF token         = 128 ''
0.00.084.296 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.084.296 I print_info: max token length = 1024
0.00.086.989 I load_tensors: offloading 24 repeating layers to GPU
0.00.086.989 I load_tensors: offloading output layer to GPU
0.00.086.989 I load_tensors: offloaded 25/25 layers to GPU
0.00.087.000 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.001 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.087.294 I llama_init_from_model: n_seq_max     = 1
0.00.087.295 I llama_init_from_model: n_ctx         = 128
0.00.087.295 I llama_init_from_model: n_ctx_per_seq = 128
0.00.087.295 I llama_init_from_model: n_batch       = 128
0.00.087.296 I llama_init_from_model: n_ubatch      = 128
0.00.087.296 I llama_init_from_model: flash_attn    = 0
0.00.087.296 I llama_init_from_model: freq_base     = 10000.0
0.00.087.296 I llama_init_from_model: freq_scale    = 1
0.00.087.297 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.297 I ggml_metal_init: allocating
0.00.087.301 I ggml_metal_init: found device: Apple M4
0.00.087.303 I ggml_metal_init: picking default device: Apple M4
0.00.087.905 I ggml_metal_init: using embedded metal library
0.00.090.428 I ggml_metal_init: GPU name:   Apple M4
0.00.090.430 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.430 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.431 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.431 I ggml_metal_init: simdgroup reduction   = true
0.00.090.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.431 I ggml_metal_init: has bfloat            = true
0.00.090.431 I ggml_metal_init: use bfloat            = true
0.00.090.432 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.432 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.265 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.511 I init:      Metal KV buffer size =    24.00 MiB
0.00.100.513 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.527 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.101.440 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.101.441 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.101.442 I llama_init_from_model: graph nodes  = 967
0.00.101.442 I llama_init_from_model: graph splits = 2
0.00.101.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.101.444 I 
0.00.101.477 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.101.478 I compute_imatrix: tokenizing the input ..
0.00.108.294 I compute_imatrix: tokenization took 6.815 ms
0.00.108.295 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.631.956 I compute_imatrix: 1.52 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.634.380 I llama_perf_context_print:        load time =    1610.92 ms
0.01.634.381 I llama_perf_context_print: prompt eval time =    1523.05 ms /   128 tokens (   11.90 ms per token,    84.04 tokens per second)
0.01.634.382 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.634.382 I llama_perf_context_print:       total time =    1613.34 ms /   129 tokens
0.01.635.234 I ggml_metal_free: deallocating

real	0m1.820s
user	0m0.164s
sys	0m0.240s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-save-load-state.log
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12530a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12530a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12530af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12530b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12530baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12530c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12530c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12530cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12530d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12530d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12530db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12530e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12530eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12530f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12530fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125310260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125310980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1253110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1253117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125311f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1253126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125312dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1253134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125313d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1253144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125314770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125314d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1253159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125315f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1253161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125316690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125316950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1253171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125317720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1253179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125317e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125318320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1253187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125318c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125319100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1253195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125319a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125319ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12531a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12531a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12531ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12531b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12531bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12531c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12531c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12531cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12531d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12531d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12531dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12531e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12531ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12531f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12531f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12531f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1253201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125320490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125320930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125320dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125321270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125321710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125321bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125322050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1253224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125322990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125322e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1253232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125323770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125323c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125324160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1253246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125324c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125325150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1253256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125325bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125326140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125326690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125326be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125327130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125327680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125327bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125328120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125328670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125328bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125329110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125329660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125329bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12532a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12532a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12532aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12532b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12532b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12532bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12531b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12532c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12532c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12532cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12532d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12532d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12532dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12532e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12532e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12532ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12532f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12532f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12532fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125330220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125330770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125330cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125331160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125331600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125331aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125331f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1253323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125332880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125332d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1253331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125333660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125333b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125333fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125334440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1253348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125334d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125335220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1253356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125335b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125336000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1253364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125336940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125336de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125337280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125337720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125337bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125338060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125338500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1253389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125338e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1253392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125339780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125339c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12533a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12533a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12533aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12533aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12533b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12533b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12533bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12533c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12533c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12533ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12533cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12533d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12533d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12533dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12533e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12533e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12533eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12533ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12533f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12533f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12533fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1253401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125340680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125340b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125340fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125341460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125341900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125341da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125342240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1253426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125342b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125343020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1253434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125343960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125343e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1253442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125344740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125344be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125345080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125345520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1253459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125345e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125346300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1253467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125346c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1253470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125347580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125347a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125347ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125348410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125348960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125348eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125349400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1253496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125349cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12534a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12534a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12534b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12534b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12534b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12534be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12534c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12534cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12534d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12534d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12534da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12534e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12534e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12534ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12534f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12534f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12534fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1253501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125350710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125350c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1253511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125351700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125351c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1253521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1253526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125352c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125353190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1253536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125353c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125354180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1253546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125354c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125355170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1253556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125355c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125356160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1253566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125356c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125357150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1253576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125357bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125358140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125358690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125358be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125359130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125359680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125359bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12535a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12535a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12535abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12535b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12535b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12535bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12535c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12535c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12535cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12535d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12535d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12535db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12535e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12535e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12535eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12535f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12535f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12535fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1253600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125360610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125360b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125361000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1253614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125361940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125361de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125362280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125362720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125362bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125363060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125363500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1253639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125363e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1253642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125364780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125364c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1253650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125365610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125365d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125366450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125366b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125367290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125367550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125367d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125368000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125368610 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.143.647 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.651 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1253682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125349f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125349980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12534a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12531d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12531d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12531f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12534c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125314a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12531b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12531be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12531c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12531a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12531ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125313a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12531fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12532c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125367810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125316c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125316ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12534c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12534abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125315040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125315300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1253155c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125368a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125368d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125368ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1253692b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125369570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125369830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125369af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125369db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12536a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12536a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12536a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12536a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12536ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12536ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12536b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12536b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12536b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12536b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12536bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12536beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12536c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12536c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12536c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12536c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12536cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12536cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12536d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12536d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12536d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12536da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12536dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x113804230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x113804730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x113804ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x113805010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x113805480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1138058f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x113805d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1138061d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x113806640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x113806ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x113806f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x113807390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x113807800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x113807c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1138080e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x113808550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1138089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x113808e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1138092a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x113809710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x113809b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x113809ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11380a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11380a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11380ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11380b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11380b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11380ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11380bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11380c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11380c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11380cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11380d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11380d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11380d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11380de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11380e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11380e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11380eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11380efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11380f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11380f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11380fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x113810190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x113810600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x113810a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x113810ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x113811350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1138117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x113811c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1138120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x113812510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x113812980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x113812df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x113813260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1138136d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x113813b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x113813fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x113814420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x113814890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x113814d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x113815170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1138155e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x113815a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x113815ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x113816330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1138167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x113816c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x113817080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1138174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x113817960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x113817dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x113818240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1138186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x113818b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x113818f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x113819400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x113819870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x113819ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11381a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11381a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11381aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11381aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11381b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11381b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11381bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11381c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11381c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11381c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11381cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11381d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11381d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11381db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11381df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11381e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11381e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11381ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11381f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11381f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11381fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11381fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1138202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x113820760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x113820bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x113821040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1138214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x113821920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x113821d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x113822200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x113822670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1138232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x113823560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x113823820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x113823c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x113824100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x113824570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1138249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x113824e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1138252c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x113825730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x113825ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x113826010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x113826480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1138268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x113826d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1138271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x113827640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x113827ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x113827f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x113828390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x113828800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x113828c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1138290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x113829550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1138299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x113829e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11382a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11382a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11382ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11382aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11382b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11382b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11382bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11382c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11382c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11382ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11382cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11382d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11382d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11382dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11382e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11382e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11382ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11382f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11382fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11382ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1138304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x113830aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x113831060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x113831620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x113831be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1138321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x113832760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x113832d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1138332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1138338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x113833e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x113834420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1138349e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x113834fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x113835560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x113835b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1138360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1138366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x113836c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x113837220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1138377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x113837da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x113838360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x113838920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x113838ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1138394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x113839a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11383a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11383a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11383aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11383b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11383b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11383bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11383c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11383c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11383ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11383d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11383d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11383df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11383e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11383eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11383f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11383f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11383fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1138401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1138407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x113840d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x113841320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1138418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x113841ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x113842460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x113842a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x113842fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1138435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x113843b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x113844120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x113844620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x113844b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x113845020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x113845520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x113845a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x113845f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x113846420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x113846920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x113846e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x113847320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x113847820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x113847d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x113848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x113848720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x113848c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x113849630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x113849d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11384a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11384ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11384ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11384b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11384b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11384bf10 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117b046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117b04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117b04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117b05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117b058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117b05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117b06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117b065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117b06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117b06fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117b07420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117b07aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117b085c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117b08d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117b09580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x117b09ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x117b0a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x117b0aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x117b0b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x117b0b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x117b0c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x117b0c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x117b0cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x117b0d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x117b0dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x117b0e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x117b0e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x117b0e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x117b0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x117b0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x117b0f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x117b0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117b0fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117b10110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x117b10580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117b109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117b10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117b112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117b11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117b11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117b12020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x117b12490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117b12900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117b12d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117b131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117b13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117b13ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117b13f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x117b143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117b14810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117b14c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117b150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117b15560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117b159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117b15e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x117b162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117b16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117b16d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x117b17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117b17600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117b17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117b17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117b18350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117b187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117b18c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x117b190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117b19510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117b19980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x117b19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x117b1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x117b1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x117b1ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x117b1afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x117b1b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x117b1b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x117b1bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x117b1c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x117b1c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x117b1ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x117b1cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x117b1d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x117b1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x117b1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x117b1e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x117b1e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x117b1e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x117b1edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x117b1f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x117b1f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117b1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x117b1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117b20400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117b20870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117b20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117b21150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x117b215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117b21a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x117b21ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117b22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117b22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117b22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117b23060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117b234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117b23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x117b24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117b24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117b24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117b24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117b251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117b25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117b25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117b25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117b263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x117b26810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117b26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117b270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117b27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117b279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117b27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117b282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x117b28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117b28b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x117b29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117b29470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117b298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117b29d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x117b2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x117b2a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x117b2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x117b2af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x117b2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x117b2b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117b2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117b2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117b2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117b2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117b2ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117b2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117b2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117b2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x117b2dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117b2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117b2e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117b2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117b2f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117b2f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117b2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117b2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117b30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117b307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117b30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117b310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117b31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117b31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117b31e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117b32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117b326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117b32b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117b32fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117b33430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117b338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117b33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117b34180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117b345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117b34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117b34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117b35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117b357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117b35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x117b36090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117b36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117b36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117b36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117b37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117b376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117b37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117b37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117b38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117b38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117b38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117b39160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117b395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117b39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x117b39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117b3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117b3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117b3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117b3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117b3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117b3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117b3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117b3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x117b3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117b3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117b3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117b3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117b3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117b3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117b3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117b3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117b3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x117b3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117b3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117b3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117b3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117b40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117b404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117b40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117b40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117b41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117b41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117b42050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117b42310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117b42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117b42bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117b43060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117b434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117b43940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117b43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117b44220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117b44690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117b44b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117b44f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117b453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117b45850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x117b45cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117b46130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117b465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117b46a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117b46e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117b472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117b47760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x117b47bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x117b48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x117b484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117b48920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x117b48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x117b49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117b49670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117b49ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x117b49f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x117b4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117b4a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x117b4aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117b4b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x117b4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x117b4b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x117b4be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x117b4c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x117b4c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117b4cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117b4d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117b4d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117b4d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117b4dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117b4e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117b4e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x117b4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117b4ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117b4f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117b4f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x117b4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117b500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117b50560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117b509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117b50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117b512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117b51720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117b51b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117b52000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117b52470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117b528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117b52d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117b531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117b53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117b53aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117b53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117b54380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x117b547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117b54c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117b550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x117b55540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x117b559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x117b56420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x117b56b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x117b57260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x117b57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x117b57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x117b580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117b586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117b58cc0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.812s
user	0m0.298s
sys	0m0.319s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-save-load-state.log
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4505 (9027f329)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15170be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15170c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15170cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15170d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15170d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15170dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15170e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15170e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15170ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15170f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15170f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15170fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151710720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151710ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1517116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151711e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151712520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151712c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151713360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151713b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151714250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151714970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151715090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151715930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151716050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151716310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151716920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151717590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151717ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151717d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151718230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1517184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151718d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1517192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151719580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151719a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151719ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15171a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15171a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15171aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15171b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15171b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15171ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15171bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15171c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15171c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15171ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15171d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15171dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15171e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15171e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15171ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15171f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15171fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151720370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151720810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151720cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151720f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151721580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151721d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151722030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1517224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151722970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151722e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1517232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151723750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151723bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151724090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151724530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1517249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151724e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151725310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1517257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151725d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151726250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1517267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151726cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151727240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151727790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151727ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151728230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151728780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151728cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151729220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151729770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151729cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15172a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15172a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15172acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15172b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15172b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15172bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15172c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15172c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15172cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15172d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15172d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15171d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15172dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15172e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15172e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15172edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15172f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15172f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15172fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151730330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151730880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151730dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151731870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151731dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151732310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151732860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151732d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1517331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151733640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151733ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151733f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151734420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1517348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151734d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151735200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1517356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151735b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151735fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151736480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151736920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151736dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151737260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151737700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151737ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151738040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1517384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151738980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151738e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1517392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151739760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151739c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15173a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15173a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15173a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15173ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15173b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15173b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15173bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15173c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15173c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15173ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15173cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15173d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15173d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15173dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15173e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15173e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15173eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15173ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15173f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15173f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15173fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1517401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151740660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151740b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151740fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151741440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1517418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151741d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151742220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1517426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151742b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151743000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1517434a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151743940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151743de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151744280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151744720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151744bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151745060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151745500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1517459a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151745e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1517462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151746780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151746c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1517470c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151747a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151747ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151748340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1517487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151748c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151749120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1517495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151749a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151749fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15174a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15174aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15174afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15174b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15174b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15174be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15174c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15174cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15174d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15174d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15174d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15174e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15174e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15174ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15174f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15174f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15174fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1517502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151750820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151750d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1517512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151751810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151751d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1517522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151752800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151752d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1517532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1517537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151753d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151754290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1517547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151754d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151755280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1517557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151755d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151756270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1517567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151756d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151757260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1517577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151757d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151758250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1517587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151758cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151759240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151759790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151759ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15175a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15175a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15175acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15175b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15175b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15175bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15175c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15175c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15175ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15175d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15175d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15175dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15175e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15175e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15175ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15175f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15175f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15175fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1517601d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151760720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151760c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1517611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151761710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151761c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1517621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151762700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151762ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151763040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1517634e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151763980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151763e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1517642c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151764760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151764c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1517650a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151765540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1517659e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151765e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151766320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1517667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151766c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1517671b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1517678d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151767ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151768710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151768e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1517690f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1517698e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151769ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15176a1b0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.091.066 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1528068a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152806d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152807180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1528075f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152807a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152807ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152808340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1528087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152808c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152809150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1528095c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152809c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15280a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15280af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15280b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15280be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15280c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15280cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15280d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15280db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15280e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15280e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15280f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15280f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15280ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1528101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1528111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152811b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152811ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1528122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152812720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152812b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152813000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152813470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1528138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152813d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1528141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152814630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152814aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152814f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152815380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1528157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152815c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1528160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152816540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1528169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152816e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152817290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152817700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152817b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152817fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152818450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1528189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152818ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152819330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1528197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152819c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15281a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15281a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15281a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15281add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15281b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15281b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15281bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15281bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15281c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15281c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15281cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15281d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15281d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15281da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15281dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15281e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15281e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15281ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15281f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15281f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15281f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15281fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152820220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152820690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152820b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152820f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1528213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152821850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152821cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152822130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1528225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152822a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152822e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1528232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152823760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152823bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152824040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1528244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152824920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152824d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152825200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152825670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152825ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152825f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1528263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152826830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152826ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152827110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152827580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1528279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152827e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1528282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152828740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152828bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152829020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152829490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152829900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152829d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15282a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15282a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15282aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15282af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15282b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15282b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15282bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15282c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15282c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15282c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15282ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15282d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15282d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15282db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15282e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15282e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15282e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15282ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15282f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15282f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15282faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15282ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152830380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1528307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152830c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1528310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152831540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1528319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152831e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152832290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152832700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152832b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152832fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152833450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1528338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152833d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1528341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152834610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152834a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152834ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152835360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1528357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152835c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1528360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152836520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152836990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152836e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152837a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152837cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152837fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152838420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152838890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152838d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152839170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1528395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152839a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152839ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15283a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15283a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15283ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15283b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15283b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15283b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15283bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15283c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15283c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15283cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15283cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15283d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15283d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15283dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15283e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15283e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15283ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15283eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15283f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15283f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15283fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152840060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1528404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152840940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152840db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152841220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152841780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152841c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152842100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152842570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1528429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152843370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152843880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1528443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1528446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152844c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152845230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1528457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152845db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152846370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152846930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152846ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1528474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152847a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152848030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1528485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152848bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152849170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152849730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152849cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15284a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15284a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15284ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15284b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15284b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15284bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15284c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15284caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15284d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15284d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15284dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15284e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15284e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15284ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15284f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15284f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15284feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152850470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152850a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152850ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1528515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152851b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152852130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1528526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152852cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152853270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152853830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152853df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1528543b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152854970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152854f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1528554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152855ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152856070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152856630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152856bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1528571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152857770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152857d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1528582f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1528588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152858db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1528592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1528597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152859cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15285a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15285a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15285abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15285b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15285b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15285bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15285bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15285c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15285c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15285ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15285d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15285ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15285e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15285ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15285f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15285f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15285fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152860090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1528606a0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15285d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15284e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15284d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152849fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152847770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152856eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152854670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1528523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152850170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1528482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152845ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15284ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15284bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1528512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15284def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152855d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1528488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1528499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152852f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152851e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15284ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152849430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152857470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152846630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152844f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1528454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1528471b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152857a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15284cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1528551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15284b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15284d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15284b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152851870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152848e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152853530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152847d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152856330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152853af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15284f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1528585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152857ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152846070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1528568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152850730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1528529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1528557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1528540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15284c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152843b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152809880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15285f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152860960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152860d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152861040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152861300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152861840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152861b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152861dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152862080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152862340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152862600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1528628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152862b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152862e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152863100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1528633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152863680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152863940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152863c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152863ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152864180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1528646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152864990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152864c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152864f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1528651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152865490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152865750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152865a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152865cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152865f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152866250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152866510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1528667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152866a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152866d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152867010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1528672d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152867590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152867850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152867b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152867dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152868090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152868350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152868610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1528688d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152868b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152868e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152869110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1528693d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152869690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152869950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152869c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152869ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15286a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15286a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15286a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15286a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15286ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15286af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15286b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15286b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15286b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15286ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15286bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15286bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15286c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15286c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15286c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15286cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15286cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15286d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15286d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15286d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15286d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15286db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15286de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15286e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15286e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15286e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15286e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15286ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15286ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15286f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15286f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15286f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15286f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15286fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15286ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1528701d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152870490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152870750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152870a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152870cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152870f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152871250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152871510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1528717d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152871a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152871d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152872010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1528722d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152872590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152872850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152872b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152872dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152873090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152873350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152873610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1528738d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152873b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152873e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152874110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1528743d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152874690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152874950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152874c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152874ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152875190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152875450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152875710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1528759d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152875c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152875f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152876210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1528764d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152876790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152876a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152876d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152876fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152877290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152877550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152877810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152877ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152877d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152878050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152878310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1528785d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152878890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152878b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152878e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1528790d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152879390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152879650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152879910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152879bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152879e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15287a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15287a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15287a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15287a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15287ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15287af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15287b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15287b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15287b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15287ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15287bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15287c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15287c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15287c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15287cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15287cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15287d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15287d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15287d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15287d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15287db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15287de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15287e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15287e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15287ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15287f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15287f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15287fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152880350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1528808a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152880df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152881340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152881890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152881de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152882330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152882880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152882dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152883320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152883870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152883dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152884310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152884860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152884db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152885300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152885850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152885da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1528862f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152886840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152886d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1528872e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152887830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152887d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1528882d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152888820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152888d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1528892c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152889810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152889d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15288a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15288a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15288ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15288b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15288b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15288bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15288c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15288c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15288cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15288d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15288d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15288d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15288dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15288df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15288e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15288e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15288ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15288f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15288f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15288f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15288fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1528902b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152890720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152890b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152891000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152891470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1528918e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1528925d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152892cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152893410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1528936d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152893b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152894140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152894750 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.919s
user	0m0.246s
sys	0m0.135s
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
++ grep '^\[1\]'
+ check_ppl f16 '[1]10.1498,'
+ qnt=f16
++ echo '[1]10.1498,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1498
++ echo '10.1498 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' f16 10.1498
+ return 0
  - f16 @ 10.1498 OK
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
++ grep '^\[1\]'
+ check_ppl q8_0 '[1]10.1362,'
+ qnt=q8_0
++ echo '[1]10.1362,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1362
++ echo '10.1362 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q8_0 10.1362
+ return 0
  - q8_0 @ 10.1362 OK
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
++ grep '^\[1\]'
+ check_ppl q4_0 '[1]11.1740,'
+ qnt=q4_0
++ echo '[1]11.1740,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=11.1740
++ echo '11.1740 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_0 11.1740
+ return 0
  - q4_0 @ 11.1740 OK
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
++ grep '^\[1\]'
+ check_ppl q4_1 '[1]10.5507,'
+ qnt=q4_1
++ echo '[1]10.5507,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.5507
++ echo '10.5507 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_1 10.5507
+ return 0
  - q4_1 @ 10.5507 OK
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
++ grep '^\[1\]'
+ check_ppl q5_0 '[1]10.0972,'
+ qnt=q5_0
++ echo '[1]10.0972,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.0972
++ echo '10.0972 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_0 10.0972
+ return 0
  - q5_0 @ 10.0972 OK
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
++ grep '^\[1\]'
+ check_ppl q5_1 '[1]10.1971,'
+ qnt=q5_1
++ echo '[1]10.1971,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1971
++ echo '10.1971 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_1 10.1971
+ return 0
  - q5_1 @ 10.1971 OK
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
++ grep '^\[1\]'
+ check_ppl q3_k '[1]12.0517,'
+ qnt=q3_k
++ echo '[1]12.0517,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=12.0517
++ echo '12.0517 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q3_k 12.0517
+ return 0
  - q3_k @ 12.0517 OK
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
++ grep '^\[1\]'
+ check_ppl q4_k '[1]10.1031,'
+ qnt=q4_k
++ echo '[1]10.1031,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1031
++ echo '10.1031 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_k 10.1031
+ return 0
  - q4_k @ 10.1031 OK
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
++ grep '^\[1\]'
+ check_ppl q5_k '[1]10.2433,'
+ qnt=q5_k
++ echo '[1]10.2433,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.2433
++ echo '10.2433 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_k 10.2433
+ return 0
  - q5_k @ 10.2433 OK
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
++ grep '^\[1\]'
+ check_ppl q6_k '[1]10.3179,'
+ qnt=q6_k
++ echo '[1]10.3179,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.3179
++ echo '10.3179 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q6_k 10.3179
+ return 0
  - q6_k @ 10.3179 OK
+ cat /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/pythia_1_4b-imatrix.log
+ grep Final
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_debug
+ cd /Users/ggml/work/llama.cpp
+ local model
+ tee /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_with_model_debug.log
++ gg_get_model
++ local gguf_0=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
++ local gguf_1=/Users/ggml/mnt/llama.cpp/models/pythia/2.8B/ggml-model-f16.gguf
++ local gguf_2=/Users/ggml/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf ]]
++ echo -n /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ model=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ cd build-ci-debug
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_with_model_debug-ctest.log
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.53 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.61 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.70 user         0.06 sys
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_release
+ cd /Users/ggml/work/llama.cpp
+ local model
+ tee /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_with_model_release.log
++ gg_get_model
++ local gguf_0=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
++ local gguf_1=/Users/ggml/mnt/llama.cpp/models/pythia/2.8B/ggml-model-f16.gguf
++ local gguf_2=/Users/ggml/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf ]]
++ echo -n /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ model=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/90/27f329803485c96ad0229f4a23174fc1c1ddad/ggml-100-mac-m4/ctest_with_model_release-ctest.log
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
