+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
warning: no usable GPU found, --gpu-layers option will be ignored
warning: one possible reason is that llama.cpp was compiled without GPU support
warning: consult docs/build.md for compilation instructions
0.00.000.257 I build: 4834 (905164fb6) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
0.00.005.534 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.005.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.005.561 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.005.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.005.563 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.005.564 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.005.565 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.005.568 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.005.569 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.005.570 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.005.573 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.005.574 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.005.588 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.005.588 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.005.590 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.005.590 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.005.591 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.005.592 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.005.593 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.010.092 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.011.326 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.011.335 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.011.336 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.011.337 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.011.338 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.011.339 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.011.340 I llama_model_loader: - type  f32:  124 tensors
0.00.011.341 I llama_model_loader: - type  f16:   73 tensors
0.00.011.343 I print_info: file format = GGUF V3 (latest)
0.00.011.344 I print_info: file type   = F16
0.00.011.349 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.029.153 I load: special tokens cache size = 5
0.00.033.819 I load: token to piece cache size = 0.2032 MB
0.00.033.853 I print_info: arch             = bert
0.00.033.860 I print_info: vocab_only       = 0
0.00.033.860 I print_info: n_ctx_train      = 512
0.00.033.861 I print_info: n_embd           = 384
0.00.033.861 I print_info: n_layer          = 12
0.00.033.880 I print_info: n_head           = 12
0.00.033.883 I print_info: n_head_kv        = 12
0.00.033.883 I print_info: n_rot            = 32
0.00.033.884 I print_info: n_swa            = 0
0.00.033.885 I print_info: n_embd_head_k    = 32
0.00.033.885 I print_info: n_embd_head_v    = 32
0.00.033.887 I print_info: n_gqa            = 1
0.00.033.889 I print_info: n_embd_k_gqa     = 384
0.00.033.891 I print_info: n_embd_v_gqa     = 384
0.00.033.892 I print_info: f_norm_eps       = 1.0e-12
0.00.033.893 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.033.894 I print_info: f_clamp_kqv      = 0.0e+00
0.00.033.894 I print_info: f_max_alibi_bias = 0.0e+00
0.00.033.895 I print_info: f_logit_scale    = 0.0e+00
0.00.033.897 I print_info: n_ff             = 1536
0.00.033.898 I print_info: n_expert         = 0
0.00.033.898 I print_info: n_expert_used    = 0
0.00.033.899 I print_info: causal attn      = 0
0.00.033.899 I print_info: pooling type     = 2
0.00.033.900 I print_info: rope type        = 2
0.00.033.901 I print_info: rope scaling     = linear
0.00.033.902 I print_info: freq_base_train  = 10000.0
0.00.033.903 I print_info: freq_scale_train = 1
0.00.033.903 I print_info: n_ctx_orig_yarn  = 512
0.00.033.904 I print_info: rope_finetuned   = unknown
0.00.033.905 I print_info: ssm_d_conv       = 0
0.00.033.906 I print_info: ssm_d_inner      = 0
0.00.033.906 I print_info: ssm_d_state      = 0
0.00.033.906 I print_info: ssm_dt_rank      = 0
0.00.033.907 I print_info: ssm_dt_b_c_rms   = 0
0.00.033.908 I print_info: model type       = 33M
0.00.033.909 I print_info: model params     = 33.21 M
0.00.033.910 I print_info: general.name     = Bge Small
0.00.033.913 I print_info: vocab type       = WPM
0.00.033.914 I print_info: n_vocab          = 30522
0.00.033.915 I print_info: n_merges         = 0
0.00.033.915 I print_info: BOS token        = 101 '[CLS]'
0.00.033.916 I print_info: UNK token        = 100 '[UNK]'
0.00.033.917 I print_info: SEP token        = 102 '[SEP]'
0.00.033.917 I print_info: PAD token        = 0 '[PAD]'
0.00.033.918 I print_info: MASK token       = 103 '[MASK]'
0.00.033.918 I print_info: LF token         = 0 '[PAD]'
0.00.033.919 I print_info: max token length = 21
0.00.033.921 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.039.804 I load_tensors:   CPU_Mapped model buffer size =    63.84 MiB
...............................................
0.00.040.786 I llama_context_base: constructing llama_context_base, gtype = 0
0.00.040.795 I llama_context_base: n_seq_max     = 1
0.00.040.796 I llama_context_base: n_ctx         = 512
0.00.040.796 I llama_context_base: n_ctx_per_seq = 512
0.00.040.796 I llama_context_base: n_batch       = 2048
0.00.040.797 I llama_context_base: n_ubatch      = 2048
0.00.040.797 I llama_context_base: causal_attn   = 0
0.00.040.797 I llama_context_base: flash_attn    = 0
0.00.040.800 I llama_context_base: freq_base     = 10000.0
0.00.040.801 I llama_context_base: freq_scale    = 1
0.00.040.829 I llama_context_base:        CPU  output buffer size =     0.00 MiB
0.00.042.867 I reserve:        CPU compute buffer size =    16.76 MiB
0.00.042.877 I reserve: graph nodes  = 417
0.00.042.878 I reserve: graph splits = 1
0.00.042.880 W get_kv_self: llama_context_base does not have a KV cache
0.00.042.881 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.042.884 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.042.884 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.044.679 W get_kv_self: llama_context_base does not have a KV cache
0.00.044.698 I 
0.00.044.792 I system_info: n_threads = 8 (n_threads_batch = 8) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 
0.00.045.919 W get_kv_self: llama_context_base does not have a KV cache
0.00.045.932 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044018 -0.019981  0.007645 -0.000818  0.001330 -0.037030  0.109342  0.042482  0.092004 -0.015950  0.006887 -0.035733 -0.018014  0.015141  0.018178  0.015835 -0.011318  0.010467 -0.085206 -0.008594  0.091303 -0.017004 -0.060421 -0.024467  0.027441  0.076057  0.027983 -0.014518  0.017642 -0.033133 -0.037863 -0.019135  0.068498 -0.009831 -0.024985  0.072309 -0.046691  0.010928 -0.050285  0.047805  0.032177 -0.011616  0.021910  0.049693  0.010438  0.005854 -0.028842  0.008982 -0.018630 -0.051388 -0.046017  0.030485 -0.035336  0.054301 -0.069661  0.044252  0.029800  0.046485  0.073320 -0.042617  0.076012  0.038856 -0.181330  0.082451  0.042211 -0.064403 -0.060111 -0.017754  0.006361  0.005833  0.017167 -0.026527  0.064628  0.112603  0.035326 -0.067468  0.027077 -0.067295 -0.033522 -0.033125  0.033212  0.013453 -0.003223 -0.037317 -0.052015  0.055138 -0.001984 -0.038245  0.064267  0.028759 -0.043273 -0.029194 -0.039385  0.036151  0.008467 -0.015431 -0.036521  0.018066  0.028684  0.342928 -0.044447  0.056090  0.017698 -0.020786 -0.066902  0.000199 -0.037856 -0.030026 -0.008534 -0.021681  0.000478 -0.003202  0.003905  0.018990 -0.008537  0.025775  0.049303  0.000039  0.050833 -0.042451 -0.031816  0.023585  0.030715 -0.023169 -0.046259 -0.079166  0.115341  0.046902  0.027868 -0.040773  0.067821 -0.022892  0.010298 -0.033054 -0.018334  0.043992  0.024187  0.052327  0.007437  0.008963  0.011144 -0.074644 -0.065661 -0.026731 -0.041188 -0.023838  0.026579  0.006875  0.027617  0.053006 -0.036664  0.057611 -0.000136  0.031780 -0.019696 -0.022009  0.041196 -0.058924  0.019748  0.043040  0.043742  0.041516 -0.022531  0.026940 -0.021837  0.005440 -0.041293 -0.001165  0.024430  0.001808  0.044272 -0.022815  0.043716  0.064772  0.055415  0.037129 -0.000918  0.046234  0.045777 -0.008466  0.063060 -0.073252 -0.011848  0.032111  0.024145  0.014755 -0.033588  0.001127 -0.015913 -0.018981  0.047982  0.111072  0.028420  0.031225 -0.013180 -0.057315  0.006720  0.005003 -0.012187 -0.051493 -0.000857 -0.017657 -0.019255 -0.040852  0.009181 -0.057923  0.050882  0.052272 -0.009796 -0.040289 -0.014060 -0.024894 -0.017215  0.006405  0.006549 -0.026938  0.015473  0.030588  0.002558  0.023165 -0.022283 -0.098706 -0.051033 -0.278012 -0.014965 -0.061332 -0.027172  0.017614 -0.010888 -0.017032  0.035227  0.046996 -0.015366  0.015201 -0.025597  0.047857 -0.005919 -0.000703 -0.060926 -0.069024 -0.060494 -0.035895  0.043477 -0.054960  0.014965  0.000671 -0.058073 -0.010406  0.012584  0.151453  0.127135 -0.013661  0.042020 -0.025534  0.013997 -0.001069 -0.150421  0.044779  0.005486 -0.036236 -0.029770 -0.020345 -0.035085  0.010223  0.033630 -0.048154 -0.051897 -0.017433 -0.023509  0.047249  0.052063 -0.016800 -0.055341  0.025836 -0.005815  0.010578  0.038821  0.008297 -0.009683 -0.105861 -0.027449 -0.096140  0.025113 -0.011146  0.092264  0.056097  0.003675  0.027731  0.002098 -0.050980 -0.039913 -0.013453 -0.044979 -0.015407  0.002925 -0.043402 -0.077949  0.065241 -0.006972 -0.001695 -0.014715  0.071617  0.023574 -0.037263  0.009123  0.001587 -0.032303  0.015560  0.037870  0.000481 -0.053157  0.021348 -0.039917  0.000020  0.013401  0.019918 -0.057718  0.006422 -0.049670 -0.267892  0.039057 -0.067923  0.038353 -0.012333  0.041356 -0.016160  0.052455 -0.071345  0.011362  0.024737 -0.007312  0.082079  0.028646 -0.021427  0.040460 -0.004489 -0.074599 -0.014693  0.020079  0.002267  0.023237  0.197217 -0.043218 -0.025967 -0.004969 -0.019199  0.074254  0.001834 -0.031955 -0.036645 -0.045178  0.000558 -0.011621  0.018092 -0.029698 -0.008489  0.006445  0.050862 -0.014957  0.006036  0.026195 -0.030800  0.048080  0.113985 -0.040909 -0.011594  0.005303 -0.003726  0.025124 -0.059128  0.013593 -0.010531  0.038690  0.051379  0.035471  0.034946 -0.016901  0.026467 -0.014515 -0.050095  0.003261  0.054108  0.039788 -0.039048 

0.00.049.158 I llama_perf_context_print:        load time =      44.38 ms
0.00.049.161 I llama_perf_context_print: prompt eval time =       2.88 ms /     9 tokens (    0.32 ms per token,  3130.43 tokens per second)
0.00.049.163 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.049.164 I llama_perf_context_print:       total time =       4.46 ms /    10 tokens

real	0m0.064s
user	0m0.079s
sys	0m0.016s
