Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.696s
user	0m0.907s
sys	0m1.301s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Built target build_info
[  4%] Built target sha256
[  4%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-quantize-stats
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Built target llama-simple-chat
[ 35%] Built target test-c
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-chat
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Linking CXX executable ../bin/test-barrier
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-gguf
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-arg-parser
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-chat-template
[ 62%] Built target test-barrier
[ 62%] Built target test-gguf
[ 62%] Built target test-backend-ops
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-autorelease
[ 63%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target test-quantize-fns
[ 66%] Built target test-quantize-perf
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Built target llama-imatrix
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Built target llama-infill
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookup-create
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup
[ 80%] Built target llama-cli
[ 80%] Generating loading.html.hpp
[ 81%] Generating index.html.gz.hpp
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-parallel
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-passkey
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-run
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Built target llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.193s
user	0m6.617s
sys	0m10.187s

main: quantize time =  3835.53 ms
main:    total time =  3835.53 ms

main: quantize time =  2720.29 ms
main:    total time =  2720.29 ms

main: quantize time =  2071.85 ms
main:    total time =  2071.85 ms

main: quantize time =  1998.38 ms
main:    total time =  1998.38 ms

main: quantize time =  1431.93 ms
main:    total time =  1431.93 ms

main: quantize time =  5200.71 ms
main:    total time =  5200.71 ms

main: quantize time =  6069.24 ms
main:    total time =  6069.24 ms

main: quantize time =  7147.12 ms
main:    total time =  7147.12 ms

main: quantize time =  6164.89 ms
main:    total time =  6164.89 ms

main: quantize time =  4555.58 ms
main:    total time =  4555.58 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.223 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.396 I main: llama backend init
0.00.000.403 I main: load the model and apply lora adapter, if any
0.00.075.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.088.237 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.088.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.088.254 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.088.255 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.088.271 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.088.272 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.088.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.088.275 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.088.276 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.088.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.088.278 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.088.278 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.088.279 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.088.280 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.088.286 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.088.286 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.088.287 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.095.090 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.097.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.103.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.103.945 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.103.945 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.103.946 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.103.947 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.103.948 I llama_model_loader: - type  f32:  194 tensors
0.00.103.949 I llama_model_loader: - type  f16:   98 tensors
0.00.103.950 I print_info: file format = GGUF V3 (latest)
0.00.103.955 I print_info: file type   = all F32 (guessed)
0.00.103.958 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.122.229 I load: special tokens cache size = 25
0.00.132.766 I load: token to piece cache size = 0.2984 MB
0.00.132.772 I print_info: arch             = gptneox
0.00.132.772 I print_info: vocab_only       = 0
0.00.132.773 I print_info: n_ctx_train      = 2048
0.00.132.773 I print_info: n_embd           = 2048
0.00.132.773 I print_info: n_layer          = 24
0.00.132.779 I print_info: n_head           = 16
0.00.132.780 I print_info: n_head_kv        = 16
0.00.132.780 I print_info: n_rot            = 32
0.00.132.781 I print_info: n_swa            = 0
0.00.132.781 I print_info: n_embd_head_k    = 128
0.00.132.781 I print_info: n_embd_head_v    = 128
0.00.132.782 I print_info: n_gqa            = 1
0.00.132.783 I print_info: n_embd_k_gqa     = 2048
0.00.132.784 I print_info: n_embd_v_gqa     = 2048
0.00.132.784 I print_info: f_norm_eps       = 1.0e-05
0.00.132.785 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.132.785 I print_info: f_clamp_kqv      = 0.0e+00
0.00.132.785 I print_info: f_max_alibi_bias = 0.0e+00
0.00.132.786 I print_info: f_logit_scale    = 0.0e+00
0.00.132.787 I print_info: n_ff             = 8192
0.00.132.787 I print_info: n_expert         = 0
0.00.132.787 I print_info: n_expert_used    = 0
0.00.132.787 I print_info: causal attn      = 1
0.00.132.787 I print_info: pooling type     = 0
0.00.132.788 I print_info: rope type        = 2
0.00.132.791 I print_info: rope scaling     = linear
0.00.132.792 I print_info: freq_base_train  = 10000.0
0.00.132.792 I print_info: freq_scale_train = 1
0.00.132.792 I print_info: n_ctx_orig_yarn  = 2048
0.00.132.792 I print_info: rope_finetuned   = unknown
0.00.132.792 I print_info: ssm_d_conv       = 0
0.00.132.793 I print_info: ssm_d_inner      = 0
0.00.132.793 I print_info: ssm_d_state      = 0
0.00.132.793 I print_info: ssm_dt_rank      = 0
0.00.132.793 I print_info: ssm_dt_b_c_rms   = 0
0.00.132.793 I print_info: model type       = 1.4B
0.00.132.794 I print_info: model params     = 1.41 B
0.00.132.794 I print_info: general.name     = 1.4B
0.00.132.795 I print_info: vocab type       = BPE
0.00.132.795 I print_info: n_vocab          = 50304
0.00.132.795 I print_info: n_merges         = 50009
0.00.132.795 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.132.796 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.132.796 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.132.796 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.132.796 I print_info: LF token         = 187 'Ċ'
0.00.132.797 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.132.797 I print_info: max token length = 1024
0.00.132.798 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.191.130 I load_tensors: offloading 24 repeating layers to GPU
0.00.191.133 I load_tensors: offloading output layer to GPU
0.00.191.134 I load_tensors: offloaded 25/25 layers to GPU
0.00.191.160 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.191.161 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.191.748 I llama_init_from_model: n_seq_max     = 1
0.00.191.749 I llama_init_from_model: n_ctx         = 2048
0.00.191.749 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.191.749 I llama_init_from_model: n_batch       = 2048
0.00.191.749 I llama_init_from_model: n_ubatch      = 512
0.00.191.749 I llama_init_from_model: flash_attn    = 0
0.00.191.750 I llama_init_from_model: freq_base     = 10000.0
0.00.191.750 I llama_init_from_model: freq_scale    = 1
0.00.191.751 I ggml_metal_init: allocating
0.00.191.782 I ggml_metal_init: found device: Apple M4
0.00.191.787 I ggml_metal_init: picking default device: Apple M4
0.00.192.386 I ggml_metal_init: using embedded metal library
0.00.393.219 I ggml_metal_init: GPU name:   Apple M4
0.00.393.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.393.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.393.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.393.240 I ggml_metal_init: simdgroup reduction   = true
0.00.393.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.393.241 I ggml_metal_init: has residency sets    = true
0.00.393.241 I ggml_metal_init: has bfloat            = true
0.00.393.241 I ggml_metal_init: use bfloat            = true
0.00.393.243 I ggml_metal_init: hasUnifiedMemory      = true
0.00.393.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.436.228 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.476.773 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.476.782 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.476.812 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.481.205 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.481.208 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.481.208 I llama_init_from_model: graph nodes  = 967
0.00.481.208 I llama_init_from_model: graph splits = 2
0.00.481.214 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.481.342 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.481.343 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.537.547 I main: llama threadpool init, n_threads = 4
0.00.537.586 I 
0.00.537.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.537.621 I 
0.00.537.749 I sampler seed: 1234
0.00.537.755 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.537.786 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.537.788 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.537.788 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.380.893 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.02.380.894 I llama_perf_context_print:        load time =     460.26 ms
0.02.380.895 I llama_perf_context_print: prompt eval time =      43.98 ms /     7 tokens (    6.28 ms per token,   159.18 tokens per second)
0.02.380.896 I llama_perf_context_print:        eval time =    1796.24 ms /    63 runs   (   28.51 ms per token,    35.07 tokens per second)
0.02.380.896 I llama_perf_context_print:       total time =    1844.72 ms /    70 tokens
0.02.381.075 I ggml_metal_free: deallocating

real	0m2.699s
user	0m0.154s
sys	0m0.157s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.098 I main: llama backend init
0.00.000.101 I main: load the model and apply lora adapter, if any
0.00.010.117 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.172 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.181 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.189 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.189 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.190 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.190 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.193 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.193 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.194 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.194 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.194 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.195 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.200 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.200 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.047 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.100 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.100 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.102 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.102 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.103 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.103 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.103 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.104 I llama_model_loader: - type  f32:  194 tensors
0.00.038.104 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.105 I print_info: file format = GGUF V3 (latest)
0.00.038.106 I print_info: file type   = Q8_0
0.00.038.107 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.230 I load: special tokens cache size = 25
0.00.054.475 I load: token to piece cache size = 0.2984 MB
0.00.054.479 I print_info: arch             = gptneox
0.00.054.479 I print_info: vocab_only       = 0
0.00.054.480 I print_info: n_ctx_train      = 2048
0.00.054.480 I print_info: n_embd           = 2048
0.00.054.480 I print_info: n_layer          = 24
0.00.054.485 I print_info: n_head           = 16
0.00.054.486 I print_info: n_head_kv        = 16
0.00.054.486 I print_info: n_rot            = 32
0.00.054.486 I print_info: n_swa            = 0
0.00.054.486 I print_info: n_embd_head_k    = 128
0.00.054.487 I print_info: n_embd_head_v    = 128
0.00.054.487 I print_info: n_gqa            = 1
0.00.054.488 I print_info: n_embd_k_gqa     = 2048
0.00.054.489 I print_info: n_embd_v_gqa     = 2048
0.00.054.489 I print_info: f_norm_eps       = 1.0e-05
0.00.054.490 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.490 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.490 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.490 I print_info: f_logit_scale    = 0.0e+00
0.00.054.491 I print_info: n_ff             = 8192
0.00.054.491 I print_info: n_expert         = 0
0.00.054.491 I print_info: n_expert_used    = 0
0.00.054.491 I print_info: causal attn      = 1
0.00.054.492 I print_info: pooling type     = 0
0.00.054.492 I print_info: rope type        = 2
0.00.054.492 I print_info: rope scaling     = linear
0.00.054.492 I print_info: freq_base_train  = 10000.0
0.00.054.493 I print_info: freq_scale_train = 1
0.00.054.493 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.493 I print_info: rope_finetuned   = unknown
0.00.054.493 I print_info: ssm_d_conv       = 0
0.00.054.494 I print_info: ssm_d_inner      = 0
0.00.054.494 I print_info: ssm_d_state      = 0
0.00.054.495 I print_info: ssm_dt_rank      = 0
0.00.054.495 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.495 I print_info: model type       = 1.4B
0.00.054.495 I print_info: model params     = 1.41 B
0.00.054.496 I print_info: general.name     = 1.4B
0.00.054.496 I print_info: vocab type       = BPE
0.00.054.496 I print_info: n_vocab          = 50304
0.00.054.497 I print_info: n_merges         = 50009
0.00.054.497 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.499 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.499 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.499 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.500 I print_info: LF token         = 187 'Ċ'
0.00.054.500 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.500 I print_info: max token length = 1024
0.00.054.501 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.192.857 I load_tensors: offloading 24 repeating layers to GPU
0.01.192.862 I load_tensors: offloading output layer to GPU
0.01.192.864 I load_tensors: offloaded 25/25 layers to GPU
0.01.192.889 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.192.891 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.194.209 I llama_init_from_model: n_seq_max     = 1
0.01.194.211 I llama_init_from_model: n_ctx         = 2048
0.01.194.212 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.194.212 I llama_init_from_model: n_batch       = 2048
0.01.194.213 I llama_init_from_model: n_ubatch      = 512
0.01.194.213 I llama_init_from_model: flash_attn    = 0
0.01.194.214 I llama_init_from_model: freq_base     = 10000.0
0.01.194.215 I llama_init_from_model: freq_scale    = 1
0.01.194.216 I ggml_metal_init: allocating
0.01.194.228 I ggml_metal_init: found device: Apple M4
0.01.194.248 I ggml_metal_init: picking default device: Apple M4
0.01.195.709 I ggml_metal_init: using embedded metal library
0.01.201.921 I ggml_metal_init: GPU name:   Apple M4
0.01.201.924 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.201.925 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.201.926 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.201.927 I ggml_metal_init: simdgroup reduction   = true
0.01.201.927 I ggml_metal_init: simdgroup matrix mul. = true
0.01.201.927 I ggml_metal_init: has residency sets    = true
0.01.201.928 I ggml_metal_init: has bfloat            = true
0.01.201.928 I ggml_metal_init: use bfloat            = true
0.01.201.929 I ggml_metal_init: hasUnifiedMemory      = true
0.01.201.930 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.219.581 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.276.934 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.276.941 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.276.964 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.281.298 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.281.299 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.281.300 I llama_init_from_model: graph nodes  = 967
0.01.281.300 I llama_init_from_model: graph splits = 2
0.01.281.305 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.281.435 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.281.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.336.247 I main: llama threadpool init, n_threads = 4
0.01.336.300 I 
0.01.336.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.336.324 I 
0.01.336.501 I sampler seed: 1234
0.01.336.506 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.336.517 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.336.517 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.336.517 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.432.995 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.02.432.996 I llama_perf_context_print:        load time =    1325.43 ms
0.02.432.997 I llama_perf_context_print: prompt eval time =      48.86 ms /     7 tokens (    6.98 ms per token,   143.27 tokens per second)
0.02.432.997 I llama_perf_context_print:        eval time =    1044.67 ms /    63 runs   (   16.58 ms per token,    60.31 tokens per second)
0.02.432.998 I llama_perf_context_print:       total time =    1097.45 ms /    70 tokens
0.02.433.251 I ggml_metal_free: deallocating

real	0m2.452s
user	0m0.113s
sys	0m0.282s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.016.948 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.105 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.039.117 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.119 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.119 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.120 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.120 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.120 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.121 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.122 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.122 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.122 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.123 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.123 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.123 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.125 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.955 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.981 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.984 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.984 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.985 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.049.986 I llama_model_loader: - type  f32:  194 tensors
0.00.049.986 I llama_model_loader: - type q4_0:   97 tensors
0.00.049.986 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.987 I print_info: file format = GGUF V3 (latest)
0.00.049.988 I print_info: file type   = Q4_0
0.00.049.989 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.060.668 I load: special tokens cache size = 25
0.00.069.688 I load: token to piece cache size = 0.2984 MB
0.00.069.692 I print_info: arch             = gptneox
0.00.069.693 I print_info: vocab_only       = 0
0.00.069.693 I print_info: n_ctx_train      = 2048
0.00.069.693 I print_info: n_embd           = 2048
0.00.069.693 I print_info: n_layer          = 24
0.00.069.699 I print_info: n_head           = 16
0.00.069.700 I print_info: n_head_kv        = 16
0.00.069.701 I print_info: n_rot            = 32
0.00.069.701 I print_info: n_swa            = 0
0.00.069.701 I print_info: n_embd_head_k    = 128
0.00.069.701 I print_info: n_embd_head_v    = 128
0.00.069.702 I print_info: n_gqa            = 1
0.00.069.703 I print_info: n_embd_k_gqa     = 2048
0.00.069.704 I print_info: n_embd_v_gqa     = 2048
0.00.069.705 I print_info: f_norm_eps       = 1.0e-05
0.00.069.705 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.706 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.706 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.706 I print_info: f_logit_scale    = 0.0e+00
0.00.069.707 I print_info: n_ff             = 8192
0.00.069.707 I print_info: n_expert         = 0
0.00.069.707 I print_info: n_expert_used    = 0
0.00.069.707 I print_info: causal attn      = 1
0.00.069.708 I print_info: pooling type     = 0
0.00.069.708 I print_info: rope type        = 2
0.00.069.708 I print_info: rope scaling     = linear
0.00.069.712 I print_info: freq_base_train  = 10000.0
0.00.069.712 I print_info: freq_scale_train = 1
0.00.069.712 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.713 I print_info: rope_finetuned   = unknown
0.00.069.713 I print_info: ssm_d_conv       = 0
0.00.069.713 I print_info: ssm_d_inner      = 0
0.00.069.713 I print_info: ssm_d_state      = 0
0.00.069.713 I print_info: ssm_dt_rank      = 0
0.00.069.714 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.714 I print_info: model type       = 1.4B
0.00.069.715 I print_info: model params     = 1.41 B
0.00.069.715 I print_info: general.name     = 1.4B
0.00.069.715 I print_info: vocab type       = BPE
0.00.069.717 I print_info: n_vocab          = 50304
0.00.069.717 I print_info: n_merges         = 50009
0.00.069.717 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.718 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.718 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.718 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.718 I print_info: LF token         = 187 'Ċ'
0.00.069.719 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.719 I print_info: max token length = 1024
0.00.069.719 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.684.833 I load_tensors: offloading 24 repeating layers to GPU
0.00.684.842 I load_tensors: offloading output layer to GPU
0.00.684.843 I load_tensors: offloaded 25/25 layers to GPU
0.00.684.874 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.684.875 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.686.464 I llama_init_from_model: n_seq_max     = 1
0.00.686.466 I llama_init_from_model: n_ctx         = 2048
0.00.686.467 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.686.468 I llama_init_from_model: n_batch       = 2048
0.00.686.468 I llama_init_from_model: n_ubatch      = 512
0.00.686.469 I llama_init_from_model: flash_attn    = 0
0.00.686.471 I llama_init_from_model: freq_base     = 10000.0
0.00.686.471 I llama_init_from_model: freq_scale    = 1
0.00.686.474 I ggml_metal_init: allocating
0.00.686.548 I ggml_metal_init: found device: Apple M4
0.00.686.563 I ggml_metal_init: picking default device: Apple M4
0.00.688.390 I ggml_metal_init: using embedded metal library
0.00.693.679 I ggml_metal_init: GPU name:   Apple M4
0.00.693.683 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.684 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.685 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.686 I ggml_metal_init: simdgroup reduction   = true
0.00.693.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.686 I ggml_metal_init: has residency sets    = true
0.00.693.687 I ggml_metal_init: has bfloat            = true
0.00.693.687 I ggml_metal_init: use bfloat            = true
0.00.693.688 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.689 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.712.687 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.768.742 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.768.748 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.768.774 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.773.650 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.773.653 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.773.653 I llama_init_from_model: graph nodes  = 967
0.00.773.653 I llama_init_from_model: graph splits = 2
0.00.773.660 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.773.773 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.773.774 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.827.271 I main: llama threadpool init, n_threads = 4
0.00.827.315 I 
0.00.827.337 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.827.337 I 
0.00.827.497 I sampler seed: 1234
0.00.827.502 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.827.526 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.827.527 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.827.527 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.510.033 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.510.034 I llama_perf_context_print:        load time =     809.61 ms
0.01.510.035 I llama_perf_context_print: prompt eval time =      39.75 ms /     7 tokens (    5.68 ms per token,   176.08 tokens per second)
0.01.510.035 I llama_perf_context_print:        eval time =     639.88 ms /    63 runs   (   10.16 ms per token,    98.46 tokens per second)
0.01.510.036 I llama_perf_context_print:       total time =     683.47 ms /    70 tokens
0.01.510.238 I ggml_metal_free: deallocating

real	0m1.538s
user	0m0.120s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.020.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.053 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.054 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.054 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.056 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.056 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.059 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.059 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.060 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.838 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.812 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.587 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.588 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.588 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.589 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.589 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.589 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.590 I llama_model_loader: - type  f32:  194 tensors
0.00.028.590 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.590 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.591 I print_info: file format = GGUF V3 (latest)
0.00.028.592 I print_info: file type   = Q4_1
0.00.028.592 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.036.772 I load: special tokens cache size = 25
0.00.042.957 I load: token to piece cache size = 0.2984 MB
0.00.042.960 I print_info: arch             = gptneox
0.00.042.960 I print_info: vocab_only       = 0
0.00.042.960 I print_info: n_ctx_train      = 2048
0.00.042.961 I print_info: n_embd           = 2048
0.00.042.961 I print_info: n_layer          = 24
0.00.042.964 I print_info: n_head           = 16
0.00.042.964 I print_info: n_head_kv        = 16
0.00.042.965 I print_info: n_rot            = 32
0.00.042.965 I print_info: n_swa            = 0
0.00.042.965 I print_info: n_embd_head_k    = 128
0.00.042.967 I print_info: n_embd_head_v    = 128
0.00.042.968 I print_info: n_gqa            = 1
0.00.042.969 I print_info: n_embd_k_gqa     = 2048
0.00.042.969 I print_info: n_embd_v_gqa     = 2048
0.00.042.970 I print_info: f_norm_eps       = 1.0e-05
0.00.042.970 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.971 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.971 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.971 I print_info: f_logit_scale    = 0.0e+00
0.00.042.972 I print_info: n_ff             = 8192
0.00.042.972 I print_info: n_expert         = 0
0.00.042.972 I print_info: n_expert_used    = 0
0.00.042.972 I print_info: causal attn      = 1
0.00.042.972 I print_info: pooling type     = 0
0.00.042.974 I print_info: rope type        = 2
0.00.042.975 I print_info: rope scaling     = linear
0.00.042.976 I print_info: freq_base_train  = 10000.0
0.00.042.976 I print_info: freq_scale_train = 1
0.00.042.976 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.976 I print_info: rope_finetuned   = unknown
0.00.042.976 I print_info: ssm_d_conv       = 0
0.00.042.977 I print_info: ssm_d_inner      = 0
0.00.042.977 I print_info: ssm_d_state      = 0
0.00.042.977 I print_info: ssm_dt_rank      = 0
0.00.042.977 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.977 I print_info: model type       = 1.4B
0.00.042.978 I print_info: model params     = 1.41 B
0.00.042.978 I print_info: general.name     = 1.4B
0.00.042.978 I print_info: vocab type       = BPE
0.00.042.978 I print_info: n_vocab          = 50304
0.00.042.978 I print_info: n_merges         = 50009
0.00.042.979 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.979 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.979 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.979 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.979 I print_info: LF token         = 187 'Ċ'
0.00.042.980 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.980 I print_info: max token length = 1024
0.00.042.980 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.713.536 I load_tensors: offloading 24 repeating layers to GPU
0.00.713.551 I load_tensors: offloading output layer to GPU
0.00.713.552 I load_tensors: offloaded 25/25 layers to GPU
0.00.713.585 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.713.587 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.715.153 I llama_init_from_model: n_seq_max     = 1
0.00.715.155 I llama_init_from_model: n_ctx         = 2048
0.00.715.155 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.715.156 I llama_init_from_model: n_batch       = 2048
0.00.715.157 I llama_init_from_model: n_ubatch      = 512
0.00.715.157 I llama_init_from_model: flash_attn    = 0
0.00.715.159 I llama_init_from_model: freq_base     = 10000.0
0.00.715.159 I llama_init_from_model: freq_scale    = 1
0.00.715.161 I ggml_metal_init: allocating
0.00.715.240 I ggml_metal_init: found device: Apple M4
0.00.715.254 I ggml_metal_init: picking default device: Apple M4
0.00.717.107 I ggml_metal_init: using embedded metal library
0.00.723.687 I ggml_metal_init: GPU name:   Apple M4
0.00.723.691 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.723.692 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.723.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.723.693 I ggml_metal_init: simdgroup reduction   = true
0.00.723.694 I ggml_metal_init: simdgroup matrix mul. = true
0.00.723.694 I ggml_metal_init: has residency sets    = true
0.00.723.694 I ggml_metal_init: has bfloat            = true
0.00.723.694 I ggml_metal_init: use bfloat            = true
0.00.723.695 I ggml_metal_init: hasUnifiedMemory      = true
0.00.723.697 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.741.161 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.796.290 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.796.297 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.796.327 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.800.606 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.800.608 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.800.609 I llama_init_from_model: graph nodes  = 967
0.00.800.609 I llama_init_from_model: graph splits = 2
0.00.800.615 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.800.743 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.800.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.858.201 I main: llama threadpool init, n_threads = 4
0.00.858.242 I 
0.00.858.263 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.858.263 I 
0.00.858.414 I sampler seed: 1234
0.00.858.419 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.858.504 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.858.507 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.858.507 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.592.166 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.01.592.167 I llama_perf_context_print:        load time =     848.68 ms
0.01.592.168 I llama_perf_context_print: prompt eval time =      49.26 ms /     7 tokens (    7.04 ms per token,   142.10 tokens per second)
0.01.592.169 I llama_perf_context_print:        eval time =     681.61 ms /    63 runs   (   10.82 ms per token,    92.43 tokens per second)
0.01.592.169 I llama_perf_context_print:       total time =     734.66 ms /    70 tokens
0.01.592.376 I ggml_metal_free: deallocating

real	0m1.610s
user	0m0.109s
sys	0m0.201s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.012.595 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.511 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.515 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.517 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.517 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.517 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.518 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.518 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.519 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.519 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.520 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.520 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.520 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.521 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.521 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.522 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.523 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.523 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.266 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.046 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.047 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.048 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.048 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.048 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.049 I llama_model_loader: - type  f32:  194 tensors
0.00.029.049 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.049 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.050 I print_info: file format = GGUF V3 (latest)
0.00.029.051 I print_info: file type   = Q5_0
0.00.029.051 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.037.199 I load: special tokens cache size = 25
0.00.043.384 I load: token to piece cache size = 0.2984 MB
0.00.043.388 I print_info: arch             = gptneox
0.00.043.388 I print_info: vocab_only       = 0
0.00.043.389 I print_info: n_ctx_train      = 2048
0.00.043.389 I print_info: n_embd           = 2048
0.00.043.389 I print_info: n_layer          = 24
0.00.043.393 I print_info: n_head           = 16
0.00.043.393 I print_info: n_head_kv        = 16
0.00.043.393 I print_info: n_rot            = 32
0.00.043.394 I print_info: n_swa            = 0
0.00.043.394 I print_info: n_embd_head_k    = 128
0.00.043.394 I print_info: n_embd_head_v    = 128
0.00.043.394 I print_info: n_gqa            = 1
0.00.043.395 I print_info: n_embd_k_gqa     = 2048
0.00.043.395 I print_info: n_embd_v_gqa     = 2048
0.00.043.396 I print_info: f_norm_eps       = 1.0e-05
0.00.043.396 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.396 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.397 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.397 I print_info: f_logit_scale    = 0.0e+00
0.00.043.397 I print_info: n_ff             = 8192
0.00.043.397 I print_info: n_expert         = 0
0.00.043.398 I print_info: n_expert_used    = 0
0.00.043.398 I print_info: causal attn      = 1
0.00.043.398 I print_info: pooling type     = 0
0.00.043.398 I print_info: rope type        = 2
0.00.043.398 I print_info: rope scaling     = linear
0.00.043.399 I print_info: freq_base_train  = 10000.0
0.00.043.399 I print_info: freq_scale_train = 1
0.00.043.399 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.399 I print_info: rope_finetuned   = unknown
0.00.043.402 I print_info: ssm_d_conv       = 0
0.00.043.402 I print_info: ssm_d_inner      = 0
0.00.043.402 I print_info: ssm_d_state      = 0
0.00.043.402 I print_info: ssm_dt_rank      = 0
0.00.043.402 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.402 I print_info: model type       = 1.4B
0.00.043.402 I print_info: model params     = 1.41 B
0.00.043.402 I print_info: general.name     = 1.4B
0.00.043.403 I print_info: vocab type       = BPE
0.00.043.403 I print_info: n_vocab          = 50304
0.00.043.403 I print_info: n_merges         = 50009
0.00.043.404 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.404 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.404 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.404 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.404 I print_info: LF token         = 187 'Ċ'
0.00.043.404 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.405 I print_info: max token length = 1024
0.00.043.405 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.796.507 I load_tensors: offloading 24 repeating layers to GPU
0.00.796.516 I load_tensors: offloading output layer to GPU
0.00.796.516 I load_tensors: offloaded 25/25 layers to GPU
0.00.796.554 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.796.559 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.797.974 I llama_init_from_model: n_seq_max     = 1
0.00.797.978 I llama_init_from_model: n_ctx         = 2048
0.00.797.979 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.797.979 I llama_init_from_model: n_batch       = 2048
0.00.797.979 I llama_init_from_model: n_ubatch      = 512
0.00.797.980 I llama_init_from_model: flash_attn    = 0
0.00.797.982 I llama_init_from_model: freq_base     = 10000.0
0.00.797.982 I llama_init_from_model: freq_scale    = 1
0.00.797.985 I ggml_metal_init: allocating
0.00.798.092 I ggml_metal_init: found device: Apple M4
0.00.798.105 I ggml_metal_init: picking default device: Apple M4
0.00.800.118 I ggml_metal_init: using embedded metal library
0.00.805.927 I ggml_metal_init: GPU name:   Apple M4
0.00.805.932 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.805.933 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.805.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.805.934 I ggml_metal_init: simdgroup reduction   = true
0.00.805.934 I ggml_metal_init: simdgroup matrix mul. = true
0.00.805.935 I ggml_metal_init: has residency sets    = true
0.00.805.935 I ggml_metal_init: has bfloat            = true
0.00.805.935 I ggml_metal_init: use bfloat            = true
0.00.805.936 I ggml_metal_init: hasUnifiedMemory      = true
0.00.805.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.822.397 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.877.032 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.877.038 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.877.062 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.881.793 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.881.795 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.881.795 I llama_init_from_model: graph nodes  = 967
0.00.881.795 I llama_init_from_model: graph splits = 2
0.00.881.800 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.881.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.881.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.938.630 I main: llama threadpool init, n_threads = 4
0.00.938.679 I 
0.00.938.700 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.938.701 I 
0.00.938.865 I sampler seed: 1234
0.00.938.869 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.938.880 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.938.881 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.938.881 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.741.686 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47207.45 tokens per second)
0.01.741.687 I llama_perf_context_print:        load time =     925.33 ms
0.01.741.688 I llama_perf_context_print: prompt eval time =      42.82 ms /     7 tokens (    6.12 ms per token,   163.48 tokens per second)
0.01.741.688 I llama_perf_context_print:        eval time =     757.44 ms /    63 runs   (   12.02 ms per token,    83.17 tokens per second)
0.01.741.689 I llama_perf_context_print:       total time =     803.76 ms /    70 tokens
0.01.741.977 I ggml_metal_free: deallocating

real	0m1.759s
user	0m0.110s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.007.148 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.283 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.288 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.289 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.290 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.290 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.290 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.293 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.293 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.294 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.294 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.295 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.297 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.297 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.298 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.136 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.202 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.135 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.136 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.137 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.137 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.137 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.138 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.138 I llama_model_loader: - type  f32:  194 tensors
0.00.024.139 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.139 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.139 I print_info: file format = GGUF V3 (latest)
0.00.024.140 I print_info: file type   = Q5_1
0.00.024.141 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.279 I load: special tokens cache size = 25
0.00.038.378 I load: token to piece cache size = 0.2984 MB
0.00.038.381 I print_info: arch             = gptneox
0.00.038.381 I print_info: vocab_only       = 0
0.00.038.381 I print_info: n_ctx_train      = 2048
0.00.038.382 I print_info: n_embd           = 2048
0.00.038.382 I print_info: n_layer          = 24
0.00.038.385 I print_info: n_head           = 16
0.00.038.386 I print_info: n_head_kv        = 16
0.00.038.386 I print_info: n_rot            = 32
0.00.038.386 I print_info: n_swa            = 0
0.00.038.386 I print_info: n_embd_head_k    = 128
0.00.038.386 I print_info: n_embd_head_v    = 128
0.00.038.388 I print_info: n_gqa            = 1
0.00.038.389 I print_info: n_embd_k_gqa     = 2048
0.00.038.390 I print_info: n_embd_v_gqa     = 2048
0.00.038.390 I print_info: f_norm_eps       = 1.0e-05
0.00.038.391 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.391 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.391 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.391 I print_info: f_logit_scale    = 0.0e+00
0.00.038.392 I print_info: n_ff             = 8192
0.00.038.392 I print_info: n_expert         = 0
0.00.038.392 I print_info: n_expert_used    = 0
0.00.038.393 I print_info: causal attn      = 1
0.00.038.393 I print_info: pooling type     = 0
0.00.038.394 I print_info: rope type        = 2
0.00.038.396 I print_info: rope scaling     = linear
0.00.038.396 I print_info: freq_base_train  = 10000.0
0.00.038.396 I print_info: freq_scale_train = 1
0.00.038.396 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.397 I print_info: rope_finetuned   = unknown
0.00.038.397 I print_info: ssm_d_conv       = 0
0.00.038.397 I print_info: ssm_d_inner      = 0
0.00.038.397 I print_info: ssm_d_state      = 0
0.00.038.397 I print_info: ssm_dt_rank      = 0
0.00.038.397 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.397 I print_info: model type       = 1.4B
0.00.038.399 I print_info: model params     = 1.41 B
0.00.038.399 I print_info: general.name     = 1.4B
0.00.038.399 I print_info: vocab type       = BPE
0.00.038.400 I print_info: n_vocab          = 50304
0.00.038.400 I print_info: n_merges         = 50009
0.00.038.401 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.401 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.401 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.401 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.401 I print_info: LF token         = 187 'Ċ'
0.00.038.401 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.402 I print_info: max token length = 1024
0.00.038.402 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.605.949 I load_tensors: offloading 24 repeating layers to GPU
0.00.605.965 I load_tensors: offloading output layer to GPU
0.00.605.966 I load_tensors: offloaded 25/25 layers to GPU
0.00.605.997 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.605.998 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.607.355 I llama_init_from_model: n_seq_max     = 1
0.00.607.361 I llama_init_from_model: n_ctx         = 2048
0.00.607.361 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.607.362 I llama_init_from_model: n_batch       = 2048
0.00.607.363 I llama_init_from_model: n_ubatch      = 512
0.00.607.363 I llama_init_from_model: flash_attn    = 0
0.00.607.365 I llama_init_from_model: freq_base     = 10000.0
0.00.607.366 I llama_init_from_model: freq_scale    = 1
0.00.607.368 I ggml_metal_init: allocating
0.00.607.457 I ggml_metal_init: found device: Apple M4
0.00.607.471 I ggml_metal_init: picking default device: Apple M4
0.00.609.277 I ggml_metal_init: using embedded metal library
0.00.613.152 I ggml_metal_init: GPU name:   Apple M4
0.00.613.155 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.156 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.156 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.156 I ggml_metal_init: simdgroup reduction   = true
0.00.613.157 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.157 I ggml_metal_init: has residency sets    = true
0.00.613.157 I ggml_metal_init: has bfloat            = true
0.00.613.157 I ggml_metal_init: use bfloat            = true
0.00.613.158 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.159 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.161 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.660.546 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.660.553 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.660.576 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.664.908 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.664.910 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.664.911 I llama_init_from_model: graph nodes  = 967
0.00.664.911 I llama_init_from_model: graph splits = 2
0.00.664.922 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.665.048 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.665.049 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.560 I main: llama threadpool init, n_threads = 4
0.00.724.610 I 
0.00.724.635 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.636 I 
0.00.724.810 I sampler seed: 1234
0.00.724.815 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.724.826 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.724.827 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.724.827 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.572.806 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52514.79 tokens per second)
0.01.572.807 I llama_perf_context_print:        load time =     716.71 ms
0.01.572.807 I llama_perf_context_print: prompt eval time =      50.82 ms /     7 tokens (    7.26 ms per token,   137.74 tokens per second)
0.01.572.808 I llama_perf_context_print:        eval time =     794.16 ms /    63 runs   (   12.61 ms per token,    79.33 tokens per second)
0.01.572.808 I llama_perf_context_print:       total time =     848.95 ms /    70 tokens
0.01.573.048 I ggml_metal_free: deallocating

real	0m1.590s
user	0m0.103s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.318 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.322 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.324 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.324 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.325 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.325 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.325 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.326 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.327 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.329 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.330 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.330 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.330 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.331 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.332 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.333 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.333 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.091 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.133 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.900 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.901 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.902 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.903 I llama_model_loader: - type  f32:  194 tensors
0.00.024.903 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.903 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.904 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.904 I print_info: file format = GGUF V3 (latest)
0.00.024.904 I print_info: file type   = Q2_K - Medium
0.00.024.905 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.751 I load: special tokens cache size = 25
0.00.038.858 I load: token to piece cache size = 0.2984 MB
0.00.038.861 I print_info: arch             = gptneox
0.00.038.861 I print_info: vocab_only       = 0
0.00.038.861 I print_info: n_ctx_train      = 2048
0.00.038.861 I print_info: n_embd           = 2048
0.00.038.862 I print_info: n_layer          = 24
0.00.038.864 I print_info: n_head           = 16
0.00.038.865 I print_info: n_head_kv        = 16
0.00.038.865 I print_info: n_rot            = 32
0.00.038.866 I print_info: n_swa            = 0
0.00.038.866 I print_info: n_embd_head_k    = 128
0.00.038.868 I print_info: n_embd_head_v    = 128
0.00.038.869 I print_info: n_gqa            = 1
0.00.038.869 I print_info: n_embd_k_gqa     = 2048
0.00.038.870 I print_info: n_embd_v_gqa     = 2048
0.00.038.871 I print_info: f_norm_eps       = 1.0e-05
0.00.038.871 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.871 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.872 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.872 I print_info: f_logit_scale    = 0.0e+00
0.00.038.872 I print_info: n_ff             = 8192
0.00.038.873 I print_info: n_expert         = 0
0.00.038.873 I print_info: n_expert_used    = 0
0.00.038.873 I print_info: causal attn      = 1
0.00.038.873 I print_info: pooling type     = 0
0.00.038.873 I print_info: rope type        = 2
0.00.038.873 I print_info: rope scaling     = linear
0.00.038.874 I print_info: freq_base_train  = 10000.0
0.00.038.874 I print_info: freq_scale_train = 1
0.00.038.874 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.875 I print_info: rope_finetuned   = unknown
0.00.038.875 I print_info: ssm_d_conv       = 0
0.00.038.875 I print_info: ssm_d_inner      = 0
0.00.038.875 I print_info: ssm_d_state      = 0
0.00.038.875 I print_info: ssm_dt_rank      = 0
0.00.038.875 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.876 I print_info: model type       = 1.4B
0.00.038.876 I print_info: model params     = 1.41 B
0.00.038.876 I print_info: general.name     = 1.4B
0.00.038.876 I print_info: vocab type       = BPE
0.00.038.877 I print_info: n_vocab          = 50304
0.00.038.877 I print_info: n_merges         = 50009
0.00.038.877 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.878 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.878 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.878 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.879 I print_info: LF token         = 187 'Ċ'
0.00.038.879 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.879 I print_info: max token length = 1024
0.00.038.879 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.336.051 I load_tensors: offloading 24 repeating layers to GPU
0.00.336.064 I load_tensors: offloading output layer to GPU
0.00.336.065 I load_tensors: offloaded 25/25 layers to GPU
0.00.336.098 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.336.100 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.337.618 I llama_init_from_model: n_seq_max     = 1
0.00.337.624 I llama_init_from_model: n_ctx         = 2048
0.00.337.625 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.337.625 I llama_init_from_model: n_batch       = 2048
0.00.337.625 I llama_init_from_model: n_ubatch      = 512
0.00.337.626 I llama_init_from_model: flash_attn    = 0
0.00.337.627 I llama_init_from_model: freq_base     = 10000.0
0.00.337.628 I llama_init_from_model: freq_scale    = 1
0.00.337.630 I ggml_metal_init: allocating
0.00.337.708 I ggml_metal_init: found device: Apple M4
0.00.337.721 I ggml_metal_init: picking default device: Apple M4
0.00.339.556 I ggml_metal_init: using embedded metal library
0.00.345.031 I ggml_metal_init: GPU name:   Apple M4
0.00.345.049 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.050 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.051 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.051 I ggml_metal_init: simdgroup reduction   = true
0.00.345.052 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.052 I ggml_metal_init: has residency sets    = true
0.00.345.052 I ggml_metal_init: has bfloat            = true
0.00.345.053 I ggml_metal_init: use bfloat            = true
0.00.345.055 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.365.838 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.419.823 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.419.834 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.419.859 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.423.945 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.423.947 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.423.947 I llama_init_from_model: graph nodes  = 967
0.00.423.947 I llama_init_from_model: graph splits = 2
0.00.423.953 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.424.078 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.424.079 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.484.288 I main: llama threadpool init, n_threads = 4
0.00.484.337 I 
0.00.484.361 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.484.362 I 
0.00.484.543 I sampler seed: 1234
0.00.484.547 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.484.559 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.484.559 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.484.559 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.162.478 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55817.61 tokens per second)
0.01.162.479 I llama_perf_context_print:        load time =     473.68 ms
0.01.162.479 I llama_perf_context_print: prompt eval time =      39.89 ms /     7 tokens (    5.70 ms per token,   175.50 tokens per second)
0.01.162.481 I llama_perf_context_print:        eval time =     635.25 ms /    63 runs   (   10.08 ms per token,    99.17 tokens per second)
0.01.162.481 I llama_perf_context_print:       total time =     678.91 ms /    70 tokens
0.01.162.716 I ggml_metal_free: deallocating

real	0m1.180s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.239 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.744 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.746 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.747 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.747 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.747 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.748 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.748 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.749 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.749 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.750 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.750 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.750 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.751 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.753 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.754 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.493 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.461 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.143 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.145 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.145 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.145 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.146 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.146 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.147 I llama_model_loader: - type  f32:  194 tensors
0.00.025.147 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.147 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.147 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.148 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.148 I print_info: file format = GGUF V3 (latest)
0.00.025.149 I print_info: file type   = Q3_K - Medium
0.00.025.149 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.014 I load: special tokens cache size = 25
0.00.039.142 I load: token to piece cache size = 0.2984 MB
0.00.039.144 I print_info: arch             = gptneox
0.00.039.145 I print_info: vocab_only       = 0
0.00.039.145 I print_info: n_ctx_train      = 2048
0.00.039.145 I print_info: n_embd           = 2048
0.00.039.145 I print_info: n_layer          = 24
0.00.039.148 I print_info: n_head           = 16
0.00.039.149 I print_info: n_head_kv        = 16
0.00.039.149 I print_info: n_rot            = 32
0.00.039.149 I print_info: n_swa            = 0
0.00.039.150 I print_info: n_embd_head_k    = 128
0.00.039.151 I print_info: n_embd_head_v    = 128
0.00.039.151 I print_info: n_gqa            = 1
0.00.039.152 I print_info: n_embd_k_gqa     = 2048
0.00.039.153 I print_info: n_embd_v_gqa     = 2048
0.00.039.154 I print_info: f_norm_eps       = 1.0e-05
0.00.039.154 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.154 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.154 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.154 I print_info: f_logit_scale    = 0.0e+00
0.00.039.155 I print_info: n_ff             = 8192
0.00.039.155 I print_info: n_expert         = 0
0.00.039.155 I print_info: n_expert_used    = 0
0.00.039.157 I print_info: causal attn      = 1
0.00.039.159 I print_info: pooling type     = 0
0.00.039.159 I print_info: rope type        = 2
0.00.039.159 I print_info: rope scaling     = linear
0.00.039.160 I print_info: freq_base_train  = 10000.0
0.00.039.160 I print_info: freq_scale_train = 1
0.00.039.160 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.160 I print_info: rope_finetuned   = unknown
0.00.039.161 I print_info: ssm_d_conv       = 0
0.00.039.161 I print_info: ssm_d_inner      = 0
0.00.039.161 I print_info: ssm_d_state      = 0
0.00.039.161 I print_info: ssm_dt_rank      = 0
0.00.039.161 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.161 I print_info: model type       = 1.4B
0.00.039.162 I print_info: model params     = 1.41 B
0.00.039.162 I print_info: general.name     = 1.4B
0.00.039.163 I print_info: vocab type       = BPE
0.00.039.163 I print_info: n_vocab          = 50304
0.00.039.163 I print_info: n_merges         = 50009
0.00.039.163 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.163 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: LF token         = 187 'Ċ'
0.00.039.166 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.166 I print_info: max token length = 1024
0.00.039.166 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.436.940 I load_tensors: offloading 24 repeating layers to GPU
0.00.436.957 I load_tensors: offloading output layer to GPU
0.00.436.958 I load_tensors: offloaded 25/25 layers to GPU
0.00.436.992 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.436.993 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.438.494 I llama_init_from_model: n_seq_max     = 1
0.00.438.497 I llama_init_from_model: n_ctx         = 2048
0.00.438.498 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.438.498 I llama_init_from_model: n_batch       = 2048
0.00.438.498 I llama_init_from_model: n_ubatch      = 512
0.00.438.499 I llama_init_from_model: flash_attn    = 0
0.00.438.501 I llama_init_from_model: freq_base     = 10000.0
0.00.438.501 I llama_init_from_model: freq_scale    = 1
0.00.438.506 I ggml_metal_init: allocating
0.00.438.585 I ggml_metal_init: found device: Apple M4
0.00.438.597 I ggml_metal_init: picking default device: Apple M4
0.00.440.500 I ggml_metal_init: using embedded metal library
0.00.446.803 I ggml_metal_init: GPU name:   Apple M4
0.00.446.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.446.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.446.810 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.446.810 I ggml_metal_init: simdgroup reduction   = true
0.00.446.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.446.811 I ggml_metal_init: has residency sets    = true
0.00.446.811 I ggml_metal_init: has bfloat            = true
0.00.446.812 I ggml_metal_init: use bfloat            = true
0.00.446.813 I ggml_metal_init: hasUnifiedMemory      = true
0.00.446.814 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.465.621 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.524.047 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.524.054 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.524.075 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.528.672 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.528.675 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.528.675 I llama_init_from_model: graph nodes  = 967
0.00.528.675 I llama_init_from_model: graph splits = 2
0.00.528.682 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.528.797 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.528.798 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.586.513 I main: llama threadpool init, n_threads = 4
0.00.586.558 I 
0.00.586.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.586.582 I 
0.00.586.736 I sampler seed: 1234
0.00.586.740 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.586.751 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.586.751 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.586.753 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.332.277 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.332.278 I llama_perf_context_print:        load time =     576.57 ms
0.01.332.278 I llama_perf_context_print: prompt eval time =      50.20 ms /     7 tokens (    7.17 ms per token,   139.43 tokens per second)
0.01.332.279 I llama_perf_context_print:        eval time =     692.33 ms /    63 runs   (   10.99 ms per token,    91.00 tokens per second)
0.01.332.280 I llama_perf_context_print:       total time =     746.46 ms /    70 tokens
0.01.332.497 I ggml_metal_free: deallocating

real	0m1.348s
user	0m0.110s
sys	0m0.180s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.948 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.463 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.468 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.475 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.476 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.476 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.478 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.479 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.480 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.480 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.481 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.400 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.297 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.298 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.298 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.299 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.299 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.299 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.300 I llama_model_loader: - type  f32:  194 tensors
0.00.025.300 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.300 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.301 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.301 I print_info: file format = GGUF V3 (latest)
0.00.025.301 I print_info: file type   = Q4_K - Medium
0.00.025.306 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.438 I load: special tokens cache size = 25
0.00.039.549 I load: token to piece cache size = 0.2984 MB
0.00.039.552 I print_info: arch             = gptneox
0.00.039.552 I print_info: vocab_only       = 0
0.00.039.552 I print_info: n_ctx_train      = 2048
0.00.039.553 I print_info: n_embd           = 2048
0.00.039.553 I print_info: n_layer          = 24
0.00.039.556 I print_info: n_head           = 16
0.00.039.556 I print_info: n_head_kv        = 16
0.00.039.557 I print_info: n_rot            = 32
0.00.039.557 I print_info: n_swa            = 0
0.00.039.557 I print_info: n_embd_head_k    = 128
0.00.039.557 I print_info: n_embd_head_v    = 128
0.00.039.558 I print_info: n_gqa            = 1
0.00.039.559 I print_info: n_embd_k_gqa     = 2048
0.00.039.560 I print_info: n_embd_v_gqa     = 2048
0.00.039.560 I print_info: f_norm_eps       = 1.0e-05
0.00.039.561 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.561 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.562 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.562 I print_info: f_logit_scale    = 0.0e+00
0.00.039.562 I print_info: n_ff             = 8192
0.00.039.563 I print_info: n_expert         = 0
0.00.039.563 I print_info: n_expert_used    = 0
0.00.039.563 I print_info: causal attn      = 1
0.00.039.563 I print_info: pooling type     = 0
0.00.039.563 I print_info: rope type        = 2
0.00.039.564 I print_info: rope scaling     = linear
0.00.039.564 I print_info: freq_base_train  = 10000.0
0.00.039.564 I print_info: freq_scale_train = 1
0.00.039.565 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.565 I print_info: rope_finetuned   = unknown
0.00.039.565 I print_info: ssm_d_conv       = 0
0.00.039.565 I print_info: ssm_d_inner      = 0
0.00.039.567 I print_info: ssm_d_state      = 0
0.00.039.567 I print_info: ssm_dt_rank      = 0
0.00.039.567 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.567 I print_info: model type       = 1.4B
0.00.039.567 I print_info: model params     = 1.41 B
0.00.039.567 I print_info: general.name     = 1.4B
0.00.039.568 I print_info: vocab type       = BPE
0.00.039.568 I print_info: n_vocab          = 50304
0.00.039.568 I print_info: n_merges         = 50009
0.00.039.569 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.569 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.569 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.569 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.570 I print_info: LF token         = 187 'Ċ'
0.00.039.570 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.570 I print_info: max token length = 1024
0.00.039.571 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.547.178 I load_tensors: offloading 24 repeating layers to GPU
0.00.547.193 I load_tensors: offloading output layer to GPU
0.00.547.194 I load_tensors: offloaded 25/25 layers to GPU
0.00.547.229 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.547.230 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.548.727 I llama_init_from_model: n_seq_max     = 1
0.00.548.730 I llama_init_from_model: n_ctx         = 2048
0.00.548.730 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.548.731 I llama_init_from_model: n_batch       = 2048
0.00.548.731 I llama_init_from_model: n_ubatch      = 512
0.00.548.731 I llama_init_from_model: flash_attn    = 0
0.00.548.734 I llama_init_from_model: freq_base     = 10000.0
0.00.548.734 I llama_init_from_model: freq_scale    = 1
0.00.548.737 I ggml_metal_init: allocating
0.00.548.814 I ggml_metal_init: found device: Apple M4
0.00.548.828 I ggml_metal_init: picking default device: Apple M4
0.00.550.746 I ggml_metal_init: using embedded metal library
0.00.557.302 I ggml_metal_init: GPU name:   Apple M4
0.00.557.305 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.557.306 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.557.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.557.308 I ggml_metal_init: simdgroup reduction   = true
0.00.557.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.557.308 I ggml_metal_init: has residency sets    = true
0.00.557.308 I ggml_metal_init: has bfloat            = true
0.00.557.309 I ggml_metal_init: use bfloat            = true
0.00.557.309 I ggml_metal_init: hasUnifiedMemory      = true
0.00.557.311 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.574.981 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.014 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.630.021 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.630.045 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.779 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.634.782 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.634.782 I llama_init_from_model: graph nodes  = 967
0.00.634.783 I llama_init_from_model: graph splits = 2
0.00.634.789 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.634.924 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.634.924 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.853 I main: llama threadpool init, n_threads = 4
0.00.691.898 I 
0.00.691.921 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.921 I 
0.00.692.073 I sampler seed: 1234
0.00.692.078 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.692.098 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.692.098 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.692.098 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.442.695 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51486.58 tokens per second)
0.01.442.696 I llama_perf_context_print:        load time =     682.20 ms
0.01.442.696 I llama_perf_context_print: prompt eval time =      46.86 ms /     7 tokens (    6.69 ms per token,   149.39 tokens per second)
0.01.442.698 I llama_perf_context_print:        eval time =     700.82 ms /    63 runs   (   11.12 ms per token,    89.89 tokens per second)
0.01.442.699 I llama_perf_context_print:       total time =     751.54 ms /    70 tokens
0.01.442.975 I ggml_metal_free: deallocating

real	0m1.459s
user	0m0.109s
sys	0m0.215s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.988 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.746 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.751 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.757 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.757 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.757 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.758 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.758 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.759 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.759 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.760 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.760 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.760 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.761 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.761 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.763 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.763 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.763 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.566 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.556 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.354 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.355 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.355 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.356 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.356 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.356 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.357 I llama_model_loader: - type  f32:  194 tensors
0.00.027.357 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.357 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.358 I print_info: file format = GGUF V3 (latest)
0.00.027.358 I print_info: file type   = Q5_K - Medium
0.00.027.359 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.194 I load: special tokens cache size = 25
0.00.041.342 I load: token to piece cache size = 0.2984 MB
0.00.041.345 I print_info: arch             = gptneox
0.00.041.345 I print_info: vocab_only       = 0
0.00.041.345 I print_info: n_ctx_train      = 2048
0.00.041.345 I print_info: n_embd           = 2048
0.00.041.345 I print_info: n_layer          = 24
0.00.041.348 I print_info: n_head           = 16
0.00.041.349 I print_info: n_head_kv        = 16
0.00.041.349 I print_info: n_rot            = 32
0.00.041.349 I print_info: n_swa            = 0
0.00.041.350 I print_info: n_embd_head_k    = 128
0.00.041.350 I print_info: n_embd_head_v    = 128
0.00.041.350 I print_info: n_gqa            = 1
0.00.041.351 I print_info: n_embd_k_gqa     = 2048
0.00.041.352 I print_info: n_embd_v_gqa     = 2048
0.00.041.352 I print_info: f_norm_eps       = 1.0e-05
0.00.041.354 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.355 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.355 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.356 I print_info: f_logit_scale    = 0.0e+00
0.00.041.356 I print_info: n_ff             = 8192
0.00.041.356 I print_info: n_expert         = 0
0.00.041.357 I print_info: n_expert_used    = 0
0.00.041.357 I print_info: causal attn      = 1
0.00.041.357 I print_info: pooling type     = 0
0.00.041.357 I print_info: rope type        = 2
0.00.041.357 I print_info: rope scaling     = linear
0.00.041.358 I print_info: freq_base_train  = 10000.0
0.00.041.358 I print_info: freq_scale_train = 1
0.00.041.358 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.359 I print_info: rope_finetuned   = unknown
0.00.041.359 I print_info: ssm_d_conv       = 0
0.00.041.359 I print_info: ssm_d_inner      = 0
0.00.041.359 I print_info: ssm_d_state      = 0
0.00.041.359 I print_info: ssm_dt_rank      = 0
0.00.041.359 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.361 I print_info: model type       = 1.4B
0.00.041.361 I print_info: model params     = 1.41 B
0.00.041.361 I print_info: general.name     = 1.4B
0.00.041.362 I print_info: vocab type       = BPE
0.00.041.362 I print_info: n_vocab          = 50304
0.00.041.362 I print_info: n_merges         = 50009
0.00.041.362 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.363 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.363 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.363 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.363 I print_info: LF token         = 187 'Ċ'
0.00.041.364 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.364 I print_info: max token length = 1024
0.00.041.368 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.616.643 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.651 I load_tensors: offloading output layer to GPU
0.00.616.652 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.679 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.616.695 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.618.180 I llama_init_from_model: n_seq_max     = 1
0.00.618.183 I llama_init_from_model: n_ctx         = 2048
0.00.618.184 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.618.184 I llama_init_from_model: n_batch       = 2048
0.00.618.185 I llama_init_from_model: n_ubatch      = 512
0.00.618.185 I llama_init_from_model: flash_attn    = 0
0.00.618.187 I llama_init_from_model: freq_base     = 10000.0
0.00.618.187 I llama_init_from_model: freq_scale    = 1
0.00.618.192 I ggml_metal_init: allocating
0.00.618.243 I ggml_metal_init: found device: Apple M4
0.00.618.255 I ggml_metal_init: picking default device: Apple M4
0.00.620.322 I ggml_metal_init: using embedded metal library
0.00.627.186 I ggml_metal_init: GPU name:   Apple M4
0.00.627.192 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.192 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.194 I ggml_metal_init: simdgroup reduction   = true
0.00.627.194 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.195 I ggml_metal_init: has residency sets    = true
0.00.627.195 I ggml_metal_init: has bfloat            = true
0.00.627.195 I ggml_metal_init: use bfloat            = true
0.00.627.196 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.690 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.974 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.699.980 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.700.002 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.704.303 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.704.305 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.704.305 I llama_init_from_model: graph nodes  = 967
0.00.704.305 I llama_init_from_model: graph splits = 2
0.00.704.311 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.704.428 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.429 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.132 I main: llama threadpool init, n_threads = 4
0.00.767.177 I 
0.00.767.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.201 I 
0.00.767.352 I sampler seed: 1234
0.00.767.356 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.367 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.367 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.367 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.608.078 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.608.078 I llama_perf_context_print:        load time =     755.44 ms
0.01.608.079 I llama_perf_context_print: prompt eval time =      51.51 ms /     7 tokens (    7.36 ms per token,   135.90 tokens per second)
0.01.608.080 I llama_perf_context_print:        eval time =     786.39 ms /    63 runs   (   12.48 ms per token,    80.11 tokens per second)
0.01.608.080 I llama_perf_context_print:       total time =     841.65 ms /    70 tokens
0.01.608.317 I ggml_metal_free: deallocating

real	0m1.627s
user	0m0.109s
sys	0m0.221s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.780 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.260 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.265 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.271 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.271 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.273 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.274 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.274 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.275 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.278 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.279 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.279 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.280 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.283 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.283 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.283 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.155 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.212 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.018 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.021 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.021 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.022 I llama_model_loader: - type  f32:  194 tensors
0.00.026.022 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.023 I print_info: file format = GGUF V3 (latest)
0.00.026.023 I print_info: file type   = Q6_K
0.00.026.024 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.216 I load: special tokens cache size = 25
0.00.040.093 I load: token to piece cache size = 0.2984 MB
0.00.040.096 I print_info: arch             = gptneox
0.00.040.096 I print_info: vocab_only       = 0
0.00.040.096 I print_info: n_ctx_train      = 2048
0.00.040.096 I print_info: n_embd           = 2048
0.00.040.097 I print_info: n_layer          = 24
0.00.040.099 I print_info: n_head           = 16
0.00.040.100 I print_info: n_head_kv        = 16
0.00.040.100 I print_info: n_rot            = 32
0.00.040.100 I print_info: n_swa            = 0
0.00.040.101 I print_info: n_embd_head_k    = 128
0.00.040.102 I print_info: n_embd_head_v    = 128
0.00.040.103 I print_info: n_gqa            = 1
0.00.040.104 I print_info: n_embd_k_gqa     = 2048
0.00.040.104 I print_info: n_embd_v_gqa     = 2048
0.00.040.110 I print_info: f_norm_eps       = 1.0e-05
0.00.040.110 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.111 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.111 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.111 I print_info: f_logit_scale    = 0.0e+00
0.00.040.112 I print_info: n_ff             = 8192
0.00.040.112 I print_info: n_expert         = 0
0.00.040.112 I print_info: n_expert_used    = 0
0.00.040.112 I print_info: causal attn      = 1
0.00.040.112 I print_info: pooling type     = 0
0.00.040.113 I print_info: rope type        = 2
0.00.040.116 I print_info: rope scaling     = linear
0.00.040.117 I print_info: freq_base_train  = 10000.0
0.00.040.117 I print_info: freq_scale_train = 1
0.00.040.117 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.117 I print_info: rope_finetuned   = unknown
0.00.040.118 I print_info: ssm_d_conv       = 0
0.00.040.118 I print_info: ssm_d_inner      = 0
0.00.040.118 I print_info: ssm_d_state      = 0
0.00.040.118 I print_info: ssm_dt_rank      = 0
0.00.040.118 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.118 I print_info: model type       = 1.4B
0.00.040.119 I print_info: model params     = 1.41 B
0.00.040.119 I print_info: general.name     = 1.4B
0.00.040.119 I print_info: vocab type       = BPE
0.00.040.119 I print_info: n_vocab          = 50304
0.00.040.120 I print_info: n_merges         = 50009
0.00.040.121 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.121 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.121 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.121 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.122 I print_info: LF token         = 187 'Ċ'
0.00.040.122 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.122 I print_info: max token length = 1024
0.00.040.122 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.660.987 I load_tensors: offloading 24 repeating layers to GPU
0.00.661.000 I load_tensors: offloading output layer to GPU
0.00.661.001 I load_tensors: offloaded 25/25 layers to GPU
0.00.661.032 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.661.033 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.662.593 I llama_init_from_model: n_seq_max     = 1
0.00.662.598 I llama_init_from_model: n_ctx         = 2048
0.00.662.598 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.662.599 I llama_init_from_model: n_batch       = 2048
0.00.662.599 I llama_init_from_model: n_ubatch      = 512
0.00.662.599 I llama_init_from_model: flash_attn    = 0
0.00.662.602 I llama_init_from_model: freq_base     = 10000.0
0.00.662.602 I llama_init_from_model: freq_scale    = 1
0.00.662.605 I ggml_metal_init: allocating
0.00.662.656 I ggml_metal_init: found device: Apple M4
0.00.662.667 I ggml_metal_init: picking default device: Apple M4
0.00.664.611 I ggml_metal_init: using embedded metal library
0.00.671.200 I ggml_metal_init: GPU name:   Apple M4
0.00.671.203 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.204 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.205 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.206 I ggml_metal_init: simdgroup reduction   = true
0.00.671.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.206 I ggml_metal_init: has residency sets    = true
0.00.671.206 I ggml_metal_init: has bfloat            = true
0.00.671.207 I ggml_metal_init: use bfloat            = true
0.00.671.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.209 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.320 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.742.260 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.742.267 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.742.291 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.746.559 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.746.560 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.746.560 I llama_init_from_model: graph nodes  = 967
0.00.746.561 I llama_init_from_model: graph splits = 2
0.00.746.566 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.746.683 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.746.684 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.812.081 I main: llama threadpool init, n_threads = 4
0.00.812.127 I 
0.00.812.150 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.812.151 I 
0.00.812.303 I sampler seed: 1234
0.00.812.308 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.812.319 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.812.319 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.812.319 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.682.097 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.682.098 I llama_perf_context_print:        load time =     802.57 ms
0.01.682.099 I llama_perf_context_print: prompt eval time =      54.18 ms /     7 tokens (    7.74 ms per token,   129.19 tokens per second)
0.01.682.100 I llama_perf_context_print:        eval time =     812.77 ms /    63 runs   (   12.90 ms per token,    77.51 tokens per second)
0.01.682.100 I llama_perf_context_print:       total time =     870.75 ms /    70 tokens
0.01.682.355 I ggml_metal_free: deallocating

real	0m1.699s
user	0m0.109s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.546 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.189 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.238 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.248 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.252 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.253 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.253 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.254 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.255 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.257 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.258 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.259 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.259 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.260 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.261 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.262 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.265 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.266 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.267 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.483 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.718 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.319 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.322 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.322 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.323 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.323 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.324 I llama_model_loader: - type  f32:  194 tensors
0.00.056.325 I llama_model_loader: - type  f16:   98 tensors
0.00.056.326 I print_info: file format = GGUF V3 (latest)
0.00.056.327 I print_info: file type   = all F32 (guessed)
0.00.056.329 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.038 I load: special tokens cache size = 25
0.00.078.358 I load: token to piece cache size = 0.2984 MB
0.00.078.362 I print_info: arch             = gptneox
0.00.078.362 I print_info: vocab_only       = 0
0.00.078.362 I print_info: n_ctx_train      = 2048
0.00.078.363 I print_info: n_embd           = 2048
0.00.078.363 I print_info: n_layer          = 24
0.00.078.366 I print_info: n_head           = 16
0.00.078.367 I print_info: n_head_kv        = 16
0.00.078.367 I print_info: n_rot            = 32
0.00.078.368 I print_info: n_swa            = 0
0.00.078.368 I print_info: n_embd_head_k    = 128
0.00.078.368 I print_info: n_embd_head_v    = 128
0.00.078.369 I print_info: n_gqa            = 1
0.00.078.370 I print_info: n_embd_k_gqa     = 2048
0.00.078.371 I print_info: n_embd_v_gqa     = 2048
0.00.078.371 I print_info: f_norm_eps       = 1.0e-05
0.00.078.372 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.372 I print_info: f_logit_scale    = 0.0e+00
0.00.078.373 I print_info: n_ff             = 8192
0.00.078.373 I print_info: n_expert         = 0
0.00.078.373 I print_info: n_expert_used    = 0
0.00.078.374 I print_info: causal attn      = 1
0.00.078.374 I print_info: pooling type     = 0
0.00.078.374 I print_info: rope type        = 2
0.00.078.374 I print_info: rope scaling     = linear
0.00.078.377 I print_info: freq_base_train  = 10000.0
0.00.078.378 I print_info: freq_scale_train = 1
0.00.078.378 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.378 I print_info: rope_finetuned   = unknown
0.00.078.378 I print_info: ssm_d_conv       = 0
0.00.078.378 I print_info: ssm_d_inner      = 0
0.00.078.379 I print_info: ssm_d_state      = 0
0.00.078.379 I print_info: ssm_dt_rank      = 0
0.00.078.379 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.379 I print_info: model type       = 1.4B
0.00.078.379 I print_info: model params     = 1.41 B
0.00.078.380 I print_info: general.name     = 1.4B
0.00.078.380 I print_info: vocab type       = BPE
0.00.078.381 I print_info: n_vocab          = 50304
0.00.078.381 I print_info: n_merges         = 50009
0.00.078.381 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.381 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.382 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.382 I print_info: LF token         = 187 'Ċ'
0.00.078.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.382 I print_info: max token length = 1024
0.00.078.383 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.467.561 I load_tensors: offloading 24 repeating layers to GPU
0.01.467.566 I load_tensors: offloading output layer to GPU
0.01.467.566 I load_tensors: offloaded 25/25 layers to GPU
0.01.467.600 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.467.602 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.468.847 I llama_init_from_model: n_seq_max     = 1
0.01.468.848 I llama_init_from_model: n_ctx         = 128
0.01.468.849 I llama_init_from_model: n_ctx_per_seq = 128
0.01.468.849 I llama_init_from_model: n_batch       = 128
0.01.468.849 I llama_init_from_model: n_ubatch      = 128
0.01.468.849 I llama_init_from_model: flash_attn    = 0
0.01.468.851 I llama_init_from_model: freq_base     = 10000.0
0.01.468.851 I llama_init_from_model: freq_scale    = 1
0.01.468.852 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.468.853 I ggml_metal_init: allocating
0.01.468.956 I ggml_metal_init: found device: Apple M4
0.01.468.965 I ggml_metal_init: picking default device: Apple M4
0.01.470.270 I ggml_metal_init: using embedded metal library
0.01.474.630 I ggml_metal_init: GPU name:   Apple M4
0.01.474.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.474.632 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.474.633 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.474.633 I ggml_metal_init: simdgroup reduction   = true
0.01.474.633 I ggml_metal_init: simdgroup matrix mul. = true
0.01.474.634 I ggml_metal_init: has residency sets    = true
0.01.474.634 I ggml_metal_init: has bfloat            = true
0.01.474.634 I ggml_metal_init: use bfloat            = true
0.01.474.635 I ggml_metal_init: hasUnifiedMemory      = true
0.01.474.637 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.487.802 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.489.678 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.489.681 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.489.697 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.491.513 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.491.514 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.491.515 I llama_init_from_model: graph nodes  = 967
0.01.491.515 I llama_init_from_model: graph splits = 2
0.01.491.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.491.516 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.526.063 I 
0.01.526.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.526.115 I perplexity: tokenizing the input ..
0.01.529.956 I perplexity: tokenization took 3.84 ms
0.01.529.975 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.663.539 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.665.230 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.665.246 I llama_perf_context_print:        load time =    1501.86 ms
0.01.665.247 I llama_perf_context_print: prompt eval time =     133.33 ms /   128 tokens (    1.04 ms per token,   960.02 tokens per second)
0.01.665.248 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.665.248 I llama_perf_context_print:       total time =     139.18 ms /   129 tokens
0.01.665.717 I ggml_metal_free: deallocating

real	0m1.860s
user	0m0.102s
sys	0m0.261s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.001.607 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.387 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.408 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.034.420 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.426 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.427 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.428 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.428 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.429 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.430 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.430 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.431 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.431 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.432 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.433 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.436 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.437 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.675 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.914 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.839 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.843 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.843 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.844 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.844 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.845 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.050.846 I llama_model_loader: - type  f32:  194 tensors
0.00.050.846 I llama_model_loader: - type q8_0:   98 tensors
0.00.050.847 I print_info: file format = GGUF V3 (latest)
0.00.050.851 I print_info: file type   = Q8_0
0.00.050.852 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.062.062 I load: special tokens cache size = 25
0.00.068.275 I load: token to piece cache size = 0.2984 MB
0.00.068.280 I print_info: arch             = gptneox
0.00.068.280 I print_info: vocab_only       = 0
0.00.068.280 I print_info: n_ctx_train      = 2048
0.00.068.282 I print_info: n_embd           = 2048
0.00.068.283 I print_info: n_layer          = 24
0.00.068.288 I print_info: n_head           = 16
0.00.068.289 I print_info: n_head_kv        = 16
0.00.068.289 I print_info: n_rot            = 32
0.00.068.289 I print_info: n_swa            = 0
0.00.068.289 I print_info: n_embd_head_k    = 128
0.00.068.289 I print_info: n_embd_head_v    = 128
0.00.068.290 I print_info: n_gqa            = 1
0.00.068.290 I print_info: n_embd_k_gqa     = 2048
0.00.068.291 I print_info: n_embd_v_gqa     = 2048
0.00.068.292 I print_info: f_norm_eps       = 1.0e-05
0.00.068.292 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.292 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.294 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.294 I print_info: f_logit_scale    = 0.0e+00
0.00.068.295 I print_info: n_ff             = 8192
0.00.068.295 I print_info: n_expert         = 0
0.00.068.295 I print_info: n_expert_used    = 0
0.00.068.295 I print_info: causal attn      = 1
0.00.068.295 I print_info: pooling type     = 0
0.00.068.295 I print_info: rope type        = 2
0.00.068.296 I print_info: rope scaling     = linear
0.00.068.296 I print_info: freq_base_train  = 10000.0
0.00.068.296 I print_info: freq_scale_train = 1
0.00.068.296 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.297 I print_info: rope_finetuned   = unknown
0.00.068.298 I print_info: ssm_d_conv       = 0
0.00.068.298 I print_info: ssm_d_inner      = 0
0.00.068.298 I print_info: ssm_d_state      = 0
0.00.068.298 I print_info: ssm_dt_rank      = 0
0.00.068.298 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.299 I print_info: model type       = 1.4B
0.00.068.299 I print_info: model params     = 1.41 B
0.00.068.299 I print_info: general.name     = 1.4B
0.00.068.300 I print_info: vocab type       = BPE
0.00.068.300 I print_info: n_vocab          = 50304
0.00.068.300 I print_info: n_merges         = 50009
0.00.068.300 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.301 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.302 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.302 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.302 I print_info: LF token         = 187 'Ċ'
0.00.068.302 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.302 I print_info: max token length = 1024
0.00.068.303 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.992.916 I load_tensors: offloading 24 repeating layers to GPU
0.00.992.933 I load_tensors: offloading output layer to GPU
0.00.992.933 I load_tensors: offloaded 25/25 layers to GPU
0.00.992.972 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.992.973 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.994.014 I llama_init_from_model: n_seq_max     = 1
0.00.994.017 I llama_init_from_model: n_ctx         = 128
0.00.994.018 I llama_init_from_model: n_ctx_per_seq = 128
0.00.994.018 I llama_init_from_model: n_batch       = 128
0.00.994.018 I llama_init_from_model: n_ubatch      = 128
0.00.994.019 I llama_init_from_model: flash_attn    = 0
0.00.994.020 I llama_init_from_model: freq_base     = 10000.0
0.00.994.020 I llama_init_from_model: freq_scale    = 1
0.00.994.020 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.994.022 I ggml_metal_init: allocating
0.00.994.053 I ggml_metal_init: found device: Apple M4
0.00.994.064 I ggml_metal_init: picking default device: Apple M4
0.00.995.647 I ggml_metal_init: using embedded metal library
0.00.999.693 I ggml_metal_init: GPU name:   Apple M4
0.00.999.697 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.999.698 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.999.698 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.999.698 I ggml_metal_init: simdgroup reduction   = true
0.00.999.699 I ggml_metal_init: simdgroup matrix mul. = true
0.00.999.699 I ggml_metal_init: has residency sets    = true
0.00.999.699 I ggml_metal_init: has bfloat            = true
0.00.999.699 I ggml_metal_init: use bfloat            = true
0.00.999.700 I ggml_metal_init: hasUnifiedMemory      = true
0.00.999.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.010.325 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.012.014 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.012.017 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.012.039 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.013.677 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.013.679 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.013.679 I llama_init_from_model: graph nodes  = 967
0.01.013.679 I llama_init_from_model: graph splits = 2
0.01.013.681 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.013.681 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.036.563 I 
0.01.036.603 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.036.614 I perplexity: tokenizing the input ..
0.01.040.540 I perplexity: tokenization took 3.925 ms
0.01.040.552 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.164.034 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.165.555 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.165.569 I llama_perf_context_print:        load time =    1016.17 ms
0.01.165.570 I llama_perf_context_print: prompt eval time =     123.24 ms /   128 tokens (    0.96 ms per token,  1038.63 tokens per second)
0.01.165.571 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.165.571 I llama_perf_context_print:       total time =     129.01 ms /   129 tokens
0.01.165.933 I ggml_metal_free: deallocating

real	0m1.206s
user	0m0.083s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.265 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.548 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.668 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.675 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.682 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.683 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.684 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.685 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.686 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.662 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.737 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.600 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.601 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.602 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.602 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.602 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.603 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.604 I llama_model_loader: - type  f32:  194 tensors
0.00.026.604 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.604 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.605 I print_info: file format = GGUF V3 (latest)
0.00.026.606 I print_info: file type   = Q4_0
0.00.026.607 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.948 I load: special tokens cache size = 25
0.00.041.173 I load: token to piece cache size = 0.2984 MB
0.00.041.177 I print_info: arch             = gptneox
0.00.041.177 I print_info: vocab_only       = 0
0.00.041.178 I print_info: n_ctx_train      = 2048
0.00.041.178 I print_info: n_embd           = 2048
0.00.041.178 I print_info: n_layer          = 24
0.00.041.182 I print_info: n_head           = 16
0.00.041.183 I print_info: n_head_kv        = 16
0.00.041.183 I print_info: n_rot            = 32
0.00.041.183 I print_info: n_swa            = 0
0.00.041.183 I print_info: n_embd_head_k    = 128
0.00.041.183 I print_info: n_embd_head_v    = 128
0.00.041.184 I print_info: n_gqa            = 1
0.00.041.185 I print_info: n_embd_k_gqa     = 2048
0.00.041.186 I print_info: n_embd_v_gqa     = 2048
0.00.041.186 I print_info: f_norm_eps       = 1.0e-05
0.00.041.187 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.187 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.187 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.187 I print_info: f_logit_scale    = 0.0e+00
0.00.041.188 I print_info: n_ff             = 8192
0.00.041.188 I print_info: n_expert         = 0
0.00.041.188 I print_info: n_expert_used    = 0
0.00.041.188 I print_info: causal attn      = 1
0.00.041.188 I print_info: pooling type     = 0
0.00.041.188 I print_info: rope type        = 2
0.00.041.189 I print_info: rope scaling     = linear
0.00.041.189 I print_info: freq_base_train  = 10000.0
0.00.041.189 I print_info: freq_scale_train = 1
0.00.041.189 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.190 I print_info: rope_finetuned   = unknown
0.00.041.190 I print_info: ssm_d_conv       = 0
0.00.041.190 I print_info: ssm_d_inner      = 0
0.00.041.190 I print_info: ssm_d_state      = 0
0.00.041.190 I print_info: ssm_dt_rank      = 0
0.00.041.193 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.193 I print_info: model type       = 1.4B
0.00.041.193 I print_info: model params     = 1.41 B
0.00.041.193 I print_info: general.name     = 1.4B
0.00.041.194 I print_info: vocab type       = BPE
0.00.041.194 I print_info: n_vocab          = 50304
0.00.041.194 I print_info: n_merges         = 50009
0.00.041.194 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.195 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.195 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.195 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.195 I print_info: LF token         = 187 'Ċ'
0.00.041.195 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.195 I print_info: max token length = 1024
0.00.041.198 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.692.284 I load_tensors: offloading 24 repeating layers to GPU
0.00.692.290 I load_tensors: offloading output layer to GPU
0.00.692.291 I load_tensors: offloaded 25/25 layers to GPU
0.00.692.312 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.692.315 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.693.239 I llama_init_from_model: n_seq_max     = 1
0.00.693.242 I llama_init_from_model: n_ctx         = 128
0.00.693.243 I llama_init_from_model: n_ctx_per_seq = 128
0.00.693.243 I llama_init_from_model: n_batch       = 128
0.00.693.244 I llama_init_from_model: n_ubatch      = 128
0.00.693.244 I llama_init_from_model: flash_attn    = 0
0.00.693.245 I llama_init_from_model: freq_base     = 10000.0
0.00.693.246 I llama_init_from_model: freq_scale    = 1
0.00.693.247 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.693.248 I ggml_metal_init: allocating
0.00.693.307 I ggml_metal_init: found device: Apple M4
0.00.693.321 I ggml_metal_init: picking default device: Apple M4
0.00.694.394 I ggml_metal_init: using embedded metal library
0.00.698.595 I ggml_metal_init: GPU name:   Apple M4
0.00.698.603 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.698.604 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.698.604 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.698.605 I ggml_metal_init: simdgroup reduction   = true
0.00.698.605 I ggml_metal_init: simdgroup matrix mul. = true
0.00.698.605 I ggml_metal_init: has residency sets    = true
0.00.698.606 I ggml_metal_init: has bfloat            = true
0.00.698.606 I ggml_metal_init: use bfloat            = true
0.00.698.607 I ggml_metal_init: hasUnifiedMemory      = true
0.00.698.610 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.355 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.014 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.716.016 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.716.032 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.717.624 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.717.625 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.717.626 I llama_init_from_model: graph nodes  = 967
0.00.717.626 I llama_init_from_model: graph splits = 2
0.00.717.627 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.717.627 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.752 I 
0.00.743.819 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.845 I perplexity: tokenizing the input ..
0.00.749.703 I perplexity: tokenization took 5.856 ms
0.00.749.716 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.885.309 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.888.375 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.888.400 I llama_perf_context_print:        load time =     733.20 ms
0.00.888.401 I llama_perf_context_print: prompt eval time =     135.35 ms /   128 tokens (    1.06 ms per token,   945.67 tokens per second)
0.00.888.402 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.888.403 I llama_perf_context_print:       total time =     144.65 ms /   129 tokens
0.00.889.060 I ggml_metal_free: deallocating

real	0m0.910s
user	0m0.083s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.119 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.034 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.347 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.355 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.356 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.361 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.361 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.365 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.365 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.365 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.365 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.366 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.371 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.371 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.395 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.453 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.306 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.308 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.309 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.309 I llama_model_loader: - type  f32:  194 tensors
0.00.025.310 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.310 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.311 I print_info: file format = GGUF V3 (latest)
0.00.025.311 I print_info: file type   = Q4_1
0.00.025.313 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.405 I load: special tokens cache size = 25
0.00.039.422 I load: token to piece cache size = 0.2984 MB
0.00.039.427 I print_info: arch             = gptneox
0.00.039.427 I print_info: vocab_only       = 0
0.00.039.427 I print_info: n_ctx_train      = 2048
0.00.039.427 I print_info: n_embd           = 2048
0.00.039.427 I print_info: n_layer          = 24
0.00.039.431 I print_info: n_head           = 16
0.00.039.434 I print_info: n_head_kv        = 16
0.00.039.434 I print_info: n_rot            = 32
0.00.039.434 I print_info: n_swa            = 0
0.00.039.435 I print_info: n_embd_head_k    = 128
0.00.039.435 I print_info: n_embd_head_v    = 128
0.00.039.435 I print_info: n_gqa            = 1
0.00.039.436 I print_info: n_embd_k_gqa     = 2048
0.00.039.436 I print_info: n_embd_v_gqa     = 2048
0.00.039.437 I print_info: f_norm_eps       = 1.0e-05
0.00.039.437 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.437 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.437 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.438 I print_info: f_logit_scale    = 0.0e+00
0.00.039.438 I print_info: n_ff             = 8192
0.00.039.438 I print_info: n_expert         = 0
0.00.039.438 I print_info: n_expert_used    = 0
0.00.039.439 I print_info: causal attn      = 1
0.00.039.439 I print_info: pooling type     = 0
0.00.039.439 I print_info: rope type        = 2
0.00.039.439 I print_info: rope scaling     = linear
0.00.039.439 I print_info: freq_base_train  = 10000.0
0.00.039.441 I print_info: freq_scale_train = 1
0.00.039.442 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.442 I print_info: rope_finetuned   = unknown
0.00.039.442 I print_info: ssm_d_conv       = 0
0.00.039.442 I print_info: ssm_d_inner      = 0
0.00.039.442 I print_info: ssm_d_state      = 0
0.00.039.442 I print_info: ssm_dt_rank      = 0
0.00.039.442 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.442 I print_info: model type       = 1.4B
0.00.039.443 I print_info: model params     = 1.41 B
0.00.039.443 I print_info: general.name     = 1.4B
0.00.039.443 I print_info: vocab type       = BPE
0.00.039.443 I print_info: n_vocab          = 50304
0.00.039.444 I print_info: n_merges         = 50009
0.00.039.444 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.444 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.445 I print_info: LF token         = 187 'Ċ'
0.00.039.445 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.445 I print_info: max token length = 1024
0.00.039.445 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.216 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.234 I load_tensors: offloading output layer to GPU
0.00.664.235 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.274 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.664.275 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.665.783 I llama_init_from_model: n_seq_max     = 1
0.00.665.786 I llama_init_from_model: n_ctx         = 128
0.00.665.787 I llama_init_from_model: n_ctx_per_seq = 128
0.00.665.787 I llama_init_from_model: n_batch       = 128
0.00.665.788 I llama_init_from_model: n_ubatch      = 128
0.00.665.788 I llama_init_from_model: flash_attn    = 0
0.00.665.790 I llama_init_from_model: freq_base     = 10000.0
0.00.665.791 I llama_init_from_model: freq_scale    = 1
0.00.665.791 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.665.794 I ggml_metal_init: allocating
0.00.665.909 I ggml_metal_init: found device: Apple M4
0.00.665.923 I ggml_metal_init: picking default device: Apple M4
0.00.667.881 I ggml_metal_init: using embedded metal library
0.00.674.595 I ggml_metal_init: GPU name:   Apple M4
0.00.674.602 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.603 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.604 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.604 I ggml_metal_init: simdgroup reduction   = true
0.00.674.605 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.605 I ggml_metal_init: has residency sets    = true
0.00.674.605 I ggml_metal_init: has bfloat            = true
0.00.674.605 I ggml_metal_init: use bfloat            = true
0.00.674.606 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.620 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.402 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.853 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.695.856 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.918 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.699.125 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.699.127 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.699.128 I llama_init_from_model: graph nodes  = 967
0.00.699.128 I llama_init_from_model: graph splits = 2
0.00.699.131 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.699.132 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.379 I 
0.00.729.456 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.478 I perplexity: tokenizing the input ..
0.00.736.834 I perplexity: tokenization took 7.354 ms
0.00.736.856 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.871.160 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.872.576 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.872.592 I llama_perf_context_print:        load time =     720.34 ms
0.00.872.593 I llama_perf_context_print: prompt eval time =     133.41 ms /   128 tokens (    1.04 ms per token,   959.42 tokens per second)
0.00.872.593 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.872.594 I llama_perf_context_print:       total time =     143.22 ms /   129 tokens
0.00.873.027 I ggml_metal_free: deallocating

real	0m0.894s
user	0m0.083s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.158 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.162 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.163 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.164 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.165 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.165 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.166 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.166 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.166 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.167 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.170 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.170 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.039 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.030 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.787 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.789 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.789 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.789 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.790 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.790 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.790 I llama_model_loader: - type  f32:  194 tensors
0.00.025.791 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.791 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.792 I print_info: file format = GGUF V3 (latest)
0.00.025.792 I print_info: file type   = Q5_0
0.00.025.799 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.887 I load: special tokens cache size = 25
0.00.039.945 I load: token to piece cache size = 0.2984 MB
0.00.039.949 I print_info: arch             = gptneox
0.00.039.949 I print_info: vocab_only       = 0
0.00.039.949 I print_info: n_ctx_train      = 2048
0.00.039.949 I print_info: n_embd           = 2048
0.00.039.950 I print_info: n_layer          = 24
0.00.039.954 I print_info: n_head           = 16
0.00.039.955 I print_info: n_head_kv        = 16
0.00.039.955 I print_info: n_rot            = 32
0.00.039.955 I print_info: n_swa            = 0
0.00.039.955 I print_info: n_embd_head_k    = 128
0.00.039.955 I print_info: n_embd_head_v    = 128
0.00.039.956 I print_info: n_gqa            = 1
0.00.039.957 I print_info: n_embd_k_gqa     = 2048
0.00.039.957 I print_info: n_embd_v_gqa     = 2048
0.00.039.958 I print_info: f_norm_eps       = 1.0e-05
0.00.039.958 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.959 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.959 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.960 I print_info: f_logit_scale    = 0.0e+00
0.00.039.961 I print_info: n_ff             = 8192
0.00.039.961 I print_info: n_expert         = 0
0.00.039.961 I print_info: n_expert_used    = 0
0.00.039.961 I print_info: causal attn      = 1
0.00.039.961 I print_info: pooling type     = 0
0.00.039.961 I print_info: rope type        = 2
0.00.039.963 I print_info: rope scaling     = linear
0.00.039.963 I print_info: freq_base_train  = 10000.0
0.00.039.963 I print_info: freq_scale_train = 1
0.00.039.963 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.964 I print_info: rope_finetuned   = unknown
0.00.039.965 I print_info: ssm_d_conv       = 0
0.00.039.965 I print_info: ssm_d_inner      = 0
0.00.039.965 I print_info: ssm_d_state      = 0
0.00.039.965 I print_info: ssm_dt_rank      = 0
0.00.039.965 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.965 I print_info: model type       = 1.4B
0.00.039.966 I print_info: model params     = 1.41 B
0.00.039.966 I print_info: general.name     = 1.4B
0.00.039.966 I print_info: vocab type       = BPE
0.00.039.966 I print_info: n_vocab          = 50304
0.00.039.967 I print_info: n_merges         = 50009
0.00.039.967 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.967 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.967 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.967 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.968 I print_info: LF token         = 187 'Ċ'
0.00.039.968 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.968 I print_info: max token length = 1024
0.00.039.968 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.741.900 I load_tensors: offloading 24 repeating layers to GPU
0.00.741.917 I load_tensors: offloading output layer to GPU
0.00.741.917 I load_tensors: offloaded 25/25 layers to GPU
0.00.741.949 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.741.950 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.743.572 I llama_init_from_model: n_seq_max     = 1
0.00.743.574 I llama_init_from_model: n_ctx         = 128
0.00.743.575 I llama_init_from_model: n_ctx_per_seq = 128
0.00.743.575 I llama_init_from_model: n_batch       = 128
0.00.743.575 I llama_init_from_model: n_ubatch      = 128
0.00.743.576 I llama_init_from_model: flash_attn    = 0
0.00.743.578 I llama_init_from_model: freq_base     = 10000.0
0.00.743.579 I llama_init_from_model: freq_scale    = 1
0.00.743.579 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.743.596 I ggml_metal_init: allocating
0.00.743.686 I ggml_metal_init: found device: Apple M4
0.00.743.700 I ggml_metal_init: picking default device: Apple M4
0.00.745.565 I ggml_metal_init: using embedded metal library
0.00.752.266 I ggml_metal_init: GPU name:   Apple M4
0.00.752.271 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.752.272 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.752.273 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.752.273 I ggml_metal_init: simdgroup reduction   = true
0.00.752.273 I ggml_metal_init: simdgroup matrix mul. = true
0.00.752.274 I ggml_metal_init: has residency sets    = true
0.00.752.274 I ggml_metal_init: has bfloat            = true
0.00.752.274 I ggml_metal_init: use bfloat            = true
0.00.752.275 I ggml_metal_init: hasUnifiedMemory      = true
0.00.752.277 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.770.107 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.773.657 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.773.661 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.773.690 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.777.140 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.777.142 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.777.143 I llama_init_from_model: graph nodes  = 967
0.00.777.143 I llama_init_from_model: graph splits = 2
0.00.777.146 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.777.147 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.034 I 
0.00.809.116 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.137 I perplexity: tokenizing the input ..
0.00.816.207 I perplexity: tokenization took 7.066 ms
0.00.816.228 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.962.513 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.963.849 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.963.865 I llama_perf_context_print:        load time =     799.11 ms
0.00.963.866 I llama_perf_context_print: prompt eval time =     145.87 ms /   128 tokens (    1.14 ms per token,   877.48 tokens per second)
0.00.963.867 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.963.867 I llama_perf_context_print:       total time =     154.83 ms /   129 tokens
0.00.964.248 I ggml_metal_free: deallocating

real	0m0.979s
user	0m0.079s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.912 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.827 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.832 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.834 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.835 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.841 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.842 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.842 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.843 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.843 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.845 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.845 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.845 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.846 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.846 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.848 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.848 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.848 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.789 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.791 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.791 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.791 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.792 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.793 I llama_model_loader: - type  f32:  194 tensors
0.00.024.793 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.793 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.794 I print_info: file format = GGUF V3 (latest)
0.00.024.798 I print_info: file type   = Q5_1
0.00.024.800 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.912 I load: special tokens cache size = 25
0.00.039.111 I load: token to piece cache size = 0.2984 MB
0.00.039.114 I print_info: arch             = gptneox
0.00.039.114 I print_info: vocab_only       = 0
0.00.039.114 I print_info: n_ctx_train      = 2048
0.00.039.115 I print_info: n_embd           = 2048
0.00.039.115 I print_info: n_layer          = 24
0.00.039.118 I print_info: n_head           = 16
0.00.039.119 I print_info: n_head_kv        = 16
0.00.039.119 I print_info: n_rot            = 32
0.00.039.121 I print_info: n_swa            = 0
0.00.039.121 I print_info: n_embd_head_k    = 128
0.00.039.122 I print_info: n_embd_head_v    = 128
0.00.039.122 I print_info: n_gqa            = 1
0.00.039.123 I print_info: n_embd_k_gqa     = 2048
0.00.039.124 I print_info: n_embd_v_gqa     = 2048
0.00.039.124 I print_info: f_norm_eps       = 1.0e-05
0.00.039.125 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.125 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.125 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.125 I print_info: f_logit_scale    = 0.0e+00
0.00.039.126 I print_info: n_ff             = 8192
0.00.039.126 I print_info: n_expert         = 0
0.00.039.126 I print_info: n_expert_used    = 0
0.00.039.126 I print_info: causal attn      = 1
0.00.039.127 I print_info: pooling type     = 0
0.00.039.127 I print_info: rope type        = 2
0.00.039.127 I print_info: rope scaling     = linear
0.00.039.128 I print_info: freq_base_train  = 10000.0
0.00.039.128 I print_info: freq_scale_train = 1
0.00.039.128 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.132 I print_info: rope_finetuned   = unknown
0.00.039.132 I print_info: ssm_d_conv       = 0
0.00.039.132 I print_info: ssm_d_inner      = 0
0.00.039.132 I print_info: ssm_d_state      = 0
0.00.039.132 I print_info: ssm_dt_rank      = 0
0.00.039.132 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.133 I print_info: model type       = 1.4B
0.00.039.133 I print_info: model params     = 1.41 B
0.00.039.133 I print_info: general.name     = 1.4B
0.00.039.134 I print_info: vocab type       = BPE
0.00.039.134 I print_info: n_vocab          = 50304
0.00.039.134 I print_info: n_merges         = 50009
0.00.039.135 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.135 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.135 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.135 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.136 I print_info: LF token         = 187 'Ċ'
0.00.039.136 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.136 I print_info: max token length = 1024
0.00.039.138 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.633.663 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.676 I load_tensors: offloading output layer to GPU
0.00.633.677 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.706 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.633.708 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.635.177 I llama_init_from_model: n_seq_max     = 1
0.00.635.183 I llama_init_from_model: n_ctx         = 128
0.00.635.183 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.184 I llama_init_from_model: n_batch       = 128
0.00.635.184 I llama_init_from_model: n_ubatch      = 128
0.00.635.185 I llama_init_from_model: flash_attn    = 0
0.00.635.186 I llama_init_from_model: freq_base     = 10000.0
0.00.635.186 I llama_init_from_model: freq_scale    = 1
0.00.635.187 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.190 I ggml_metal_init: allocating
0.00.635.243 I ggml_metal_init: found device: Apple M4
0.00.635.256 I ggml_metal_init: picking default device: Apple M4
0.00.637.059 I ggml_metal_init: using embedded metal library
0.00.643.640 I ggml_metal_init: GPU name:   Apple M4
0.00.643.644 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.645 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.645 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.646 I ggml_metal_init: simdgroup reduction   = true
0.00.643.646 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.646 I ggml_metal_init: has residency sets    = true
0.00.643.647 I ggml_metal_init: has bfloat            = true
0.00.643.647 I ggml_metal_init: use bfloat            = true
0.00.643.648 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.649 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.822 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.278 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.664.286 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.664.317 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.667.576 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.667.578 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.667.579 I llama_init_from_model: graph nodes  = 967
0.00.667.579 I llama_init_from_model: graph splits = 2
0.00.667.582 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.667.583 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.328 I 
0.00.699.410 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.434 I perplexity: tokenizing the input ..
0.00.706.768 I perplexity: tokenization took 7.331 ms
0.00.706.788 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.854.492 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.855.815 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.855.832 I llama_perf_context_print:        load time =     690.41 ms
0.00.855.832 I llama_perf_context_print: prompt eval time =     146.91 ms /   128 tokens (    1.15 ms per token,   871.30 tokens per second)
0.00.855.833 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.855.834 I llama_perf_context_print:       total time =     156.51 ms /   129 tokens
0.00.856.268 I ggml_metal_free: deallocating

real	0m0.870s
user	0m0.080s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.676 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.683 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.685 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.690 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.691 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.691 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.692 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.693 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.694 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.695 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.695 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.695 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.696 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.697 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.698 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.698 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.528 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.434 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.436 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.436 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.437 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.437 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.437 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.438 I llama_model_loader: - type  f32:  194 tensors
0.00.025.438 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.438 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.439 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.439 I print_info: file format = GGUF V3 (latest)
0.00.025.440 I print_info: file type   = Q2_K - Medium
0.00.025.441 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.523 I load: special tokens cache size = 25
0.00.039.731 I load: token to piece cache size = 0.2984 MB
0.00.039.734 I print_info: arch             = gptneox
0.00.039.734 I print_info: vocab_only       = 0
0.00.039.735 I print_info: n_ctx_train      = 2048
0.00.039.735 I print_info: n_embd           = 2048
0.00.039.735 I print_info: n_layer          = 24
0.00.039.739 I print_info: n_head           = 16
0.00.039.740 I print_info: n_head_kv        = 16
0.00.039.740 I print_info: n_rot            = 32
0.00.039.740 I print_info: n_swa            = 0
0.00.039.741 I print_info: n_embd_head_k    = 128
0.00.039.741 I print_info: n_embd_head_v    = 128
0.00.039.742 I print_info: n_gqa            = 1
0.00.039.743 I print_info: n_embd_k_gqa     = 2048
0.00.039.744 I print_info: n_embd_v_gqa     = 2048
0.00.039.744 I print_info: f_norm_eps       = 1.0e-05
0.00.039.745 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.745 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.745 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.745 I print_info: f_logit_scale    = 0.0e+00
0.00.039.746 I print_info: n_ff             = 8192
0.00.039.746 I print_info: n_expert         = 0
0.00.039.746 I print_info: n_expert_used    = 0
0.00.039.747 I print_info: causal attn      = 1
0.00.039.747 I print_info: pooling type     = 0
0.00.039.747 I print_info: rope type        = 2
0.00.039.747 I print_info: rope scaling     = linear
0.00.039.749 I print_info: freq_base_train  = 10000.0
0.00.039.750 I print_info: freq_scale_train = 1
0.00.039.750 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.751 I print_info: rope_finetuned   = unknown
0.00.039.751 I print_info: ssm_d_conv       = 0
0.00.039.751 I print_info: ssm_d_inner      = 0
0.00.039.751 I print_info: ssm_d_state      = 0
0.00.039.751 I print_info: ssm_dt_rank      = 0
0.00.039.751 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.752 I print_info: model type       = 1.4B
0.00.039.752 I print_info: model params     = 1.41 B
0.00.039.752 I print_info: general.name     = 1.4B
0.00.039.753 I print_info: vocab type       = BPE
0.00.039.753 I print_info: n_vocab          = 50304
0.00.039.753 I print_info: n_merges         = 50009
0.00.039.758 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.758 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.758 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.758 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.759 I print_info: LF token         = 187 'Ċ'
0.00.039.760 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.760 I print_info: max token length = 1024
0.00.039.760 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.331.839 I load_tensors: offloading 24 repeating layers to GPU
0.00.331.855 I load_tensors: offloading output layer to GPU
0.00.331.855 I load_tensors: offloaded 25/25 layers to GPU
0.00.331.888 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.331.889 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.333.574 I llama_init_from_model: n_seq_max     = 1
0.00.333.577 I llama_init_from_model: n_ctx         = 128
0.00.333.577 I llama_init_from_model: n_ctx_per_seq = 128
0.00.333.578 I llama_init_from_model: n_batch       = 128
0.00.333.578 I llama_init_from_model: n_ubatch      = 128
0.00.333.579 I llama_init_from_model: flash_attn    = 0
0.00.333.581 I llama_init_from_model: freq_base     = 10000.0
0.00.333.581 I llama_init_from_model: freq_scale    = 1
0.00.333.582 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.333.585 I ggml_metal_init: allocating
0.00.333.662 I ggml_metal_init: found device: Apple M4
0.00.333.676 I ggml_metal_init: picking default device: Apple M4
0.00.335.453 I ggml_metal_init: using embedded metal library
0.00.340.920 I ggml_metal_init: GPU name:   Apple M4
0.00.340.938 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.340.938 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.340.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.340.940 I ggml_metal_init: simdgroup reduction   = true
0.00.340.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.340.940 I ggml_metal_init: has residency sets    = true
0.00.340.941 I ggml_metal_init: has bfloat            = true
0.00.340.941 I ggml_metal_init: use bfloat            = true
0.00.340.943 I ggml_metal_init: hasUnifiedMemory      = true
0.00.340.947 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.361.854 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.365.546 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.365.562 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.365.608 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.368.962 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.368.964 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.368.965 I llama_init_from_model: graph nodes  = 967
0.00.368.965 I llama_init_from_model: graph splits = 2
0.00.368.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.368.968 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.397.495 I 
0.00.397.577 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.397.597 I perplexity: tokenizing the input ..
0.00.404.806 I perplexity: tokenization took 7.205 ms
0.00.404.826 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.538.218 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.539.572 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.539.588 I llama_perf_context_print:        load time =     387.65 ms
0.00.539.588 I llama_perf_context_print: prompt eval time =     132.49 ms /   128 tokens (    1.04 ms per token,   966.10 tokens per second)
0.00.539.589 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.539.589 I llama_perf_context_print:       total time =     142.10 ms /   129 tokens
0.00.539.956 I ggml_metal_free: deallocating

real	0m0.555s
user	0m0.082s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.941 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.003 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.009 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.011 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.012 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.012 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.012 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.012 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.013 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.014 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.018 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.018 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.018 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.018 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.019 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.021 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.021 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.021 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.743 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.728 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.516 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.518 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.518 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.518 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.519 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.519 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.519 I llama_model_loader: - type  f32:  194 tensors
0.00.024.520 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.520 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.520 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.521 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.521 I print_info: file format = GGUF V3 (latest)
0.00.024.522 I print_info: file type   = Q3_K - Medium
0.00.024.523 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.806 I load: special tokens cache size = 25
0.00.038.749 I load: token to piece cache size = 0.2984 MB
0.00.038.753 I print_info: arch             = gptneox
0.00.038.753 I print_info: vocab_only       = 0
0.00.038.753 I print_info: n_ctx_train      = 2048
0.00.038.754 I print_info: n_embd           = 2048
0.00.038.754 I print_info: n_layer          = 24
0.00.038.758 I print_info: n_head           = 16
0.00.038.759 I print_info: n_head_kv        = 16
0.00.038.759 I print_info: n_rot            = 32
0.00.038.759 I print_info: n_swa            = 0
0.00.038.759 I print_info: n_embd_head_k    = 128
0.00.038.760 I print_info: n_embd_head_v    = 128
0.00.038.763 I print_info: n_gqa            = 1
0.00.038.764 I print_info: n_embd_k_gqa     = 2048
0.00.038.765 I print_info: n_embd_v_gqa     = 2048
0.00.038.765 I print_info: f_norm_eps       = 1.0e-05
0.00.038.766 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.766 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.768 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.768 I print_info: f_logit_scale    = 0.0e+00
0.00.038.769 I print_info: n_ff             = 8192
0.00.038.769 I print_info: n_expert         = 0
0.00.038.769 I print_info: n_expert_used    = 0
0.00.038.769 I print_info: causal attn      = 1
0.00.038.769 I print_info: pooling type     = 0
0.00.038.769 I print_info: rope type        = 2
0.00.038.770 I print_info: rope scaling     = linear
0.00.038.770 I print_info: freq_base_train  = 10000.0
0.00.038.770 I print_info: freq_scale_train = 1
0.00.038.770 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.771 I print_info: rope_finetuned   = unknown
0.00.038.771 I print_info: ssm_d_conv       = 0
0.00.038.771 I print_info: ssm_d_inner      = 0
0.00.038.771 I print_info: ssm_d_state      = 0
0.00.038.771 I print_info: ssm_dt_rank      = 0
0.00.038.771 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.772 I print_info: model type       = 1.4B
0.00.038.772 I print_info: model params     = 1.41 B
0.00.038.772 I print_info: general.name     = 1.4B
0.00.038.773 I print_info: vocab type       = BPE
0.00.038.773 I print_info: n_vocab          = 50304
0.00.038.773 I print_info: n_merges         = 50009
0.00.038.773 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.773 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: LF token         = 187 'Ċ'
0.00.038.774 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: max token length = 1024
0.00.038.775 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.432.054 I load_tensors: offloading 24 repeating layers to GPU
0.00.432.064 I load_tensors: offloading output layer to GPU
0.00.432.065 I load_tensors: offloaded 25/25 layers to GPU
0.00.432.096 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.432.102 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.433.733 I llama_init_from_model: n_seq_max     = 1
0.00.433.736 I llama_init_from_model: n_ctx         = 128
0.00.433.736 I llama_init_from_model: n_ctx_per_seq = 128
0.00.433.737 I llama_init_from_model: n_batch       = 128
0.00.433.737 I llama_init_from_model: n_ubatch      = 128
0.00.433.737 I llama_init_from_model: flash_attn    = 0
0.00.433.739 I llama_init_from_model: freq_base     = 10000.0
0.00.433.740 I llama_init_from_model: freq_scale    = 1
0.00.433.743 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.433.746 I ggml_metal_init: allocating
0.00.433.807 I ggml_metal_init: found device: Apple M4
0.00.433.820 I ggml_metal_init: picking default device: Apple M4
0.00.435.499 I ggml_metal_init: using embedded metal library
0.00.441.847 I ggml_metal_init: GPU name:   Apple M4
0.00.441.852 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.441.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.441.854 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.441.855 I ggml_metal_init: simdgroup reduction   = true
0.00.441.855 I ggml_metal_init: simdgroup matrix mul. = true
0.00.441.856 I ggml_metal_init: has residency sets    = true
0.00.441.856 I ggml_metal_init: has bfloat            = true
0.00.441.856 I ggml_metal_init: use bfloat            = true
0.00.441.857 I ggml_metal_init: hasUnifiedMemory      = true
0.00.441.860 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.460.730 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.464.308 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.464.315 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.464.351 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.467.749 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.467.751 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.467.751 I llama_init_from_model: graph nodes  = 967
0.00.467.752 I llama_init_from_model: graph splits = 2
0.00.467.755 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.467.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.493.181 I 
0.00.493.263 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.493.281 I perplexity: tokenizing the input ..
0.00.499.977 I perplexity: tokenization took 6.694 ms
0.00.499.989 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.631.276 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.632.613 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.632.628 I llama_perf_context_print:        load time =     484.23 ms
0.00.632.629 I llama_perf_context_print: prompt eval time =     131.06 ms /   128 tokens (    1.02 ms per token,   976.67 tokens per second)
0.00.632.630 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.632.630 I llama_perf_context_print:       total time =     139.45 ms /   129 tokens
0.00.633.050 I ggml_metal_free: deallocating

real	0m0.646s
user	0m0.078s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.834 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.091 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.104 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.104 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.105 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.106 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.106 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.109 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.109 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.110 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.111 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.112 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.913 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.774 I llama_model_loader: - type  f32:  194 tensors
0.00.024.774 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.774 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.775 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.775 I print_info: file format = GGUF V3 (latest)
0.00.024.779 I print_info: file type   = Q4_K - Medium
0.00.024.780 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.198 I load: special tokens cache size = 25
0.00.039.330 I load: token to piece cache size = 0.2984 MB
0.00.039.335 I print_info: arch             = gptneox
0.00.039.335 I print_info: vocab_only       = 0
0.00.039.335 I print_info: n_ctx_train      = 2048
0.00.039.335 I print_info: n_embd           = 2048
0.00.039.335 I print_info: n_layer          = 24
0.00.039.339 I print_info: n_head           = 16
0.00.039.340 I print_info: n_head_kv        = 16
0.00.039.340 I print_info: n_rot            = 32
0.00.039.341 I print_info: n_swa            = 0
0.00.039.341 I print_info: n_embd_head_k    = 128
0.00.039.341 I print_info: n_embd_head_v    = 128
0.00.039.342 I print_info: n_gqa            = 1
0.00.039.343 I print_info: n_embd_k_gqa     = 2048
0.00.039.343 I print_info: n_embd_v_gqa     = 2048
0.00.039.344 I print_info: f_norm_eps       = 1.0e-05
0.00.039.344 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.345 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.345 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.345 I print_info: f_logit_scale    = 0.0e+00
0.00.039.346 I print_info: n_ff             = 8192
0.00.039.346 I print_info: n_expert         = 0
0.00.039.346 I print_info: n_expert_used    = 0
0.00.039.346 I print_info: causal attn      = 1
0.00.039.346 I print_info: pooling type     = 0
0.00.039.346 I print_info: rope type        = 2
0.00.039.347 I print_info: rope scaling     = linear
0.00.039.350 I print_info: freq_base_train  = 10000.0
0.00.039.350 I print_info: freq_scale_train = 1
0.00.039.350 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.351 I print_info: rope_finetuned   = unknown
0.00.039.351 I print_info: ssm_d_conv       = 0
0.00.039.351 I print_info: ssm_d_inner      = 0
0.00.039.351 I print_info: ssm_d_state      = 0
0.00.039.351 I print_info: ssm_dt_rank      = 0
0.00.039.351 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.352 I print_info: model type       = 1.4B
0.00.039.354 I print_info: model params     = 1.41 B
0.00.039.354 I print_info: general.name     = 1.4B
0.00.039.354 I print_info: vocab type       = BPE
0.00.039.355 I print_info: n_vocab          = 50304
0.00.039.355 I print_info: n_merges         = 50009
0.00.039.355 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.355 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.355 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.355 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.356 I print_info: LF token         = 187 'Ċ'
0.00.039.356 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.356 I print_info: max token length = 1024
0.00.039.359 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.542.005 I load_tensors: offloading 24 repeating layers to GPU
0.00.542.018 I load_tensors: offloading output layer to GPU
0.00.542.018 I load_tensors: offloaded 25/25 layers to GPU
0.00.542.045 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.542.046 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.543.677 I llama_init_from_model: n_seq_max     = 1
0.00.543.682 I llama_init_from_model: n_ctx         = 128
0.00.543.682 I llama_init_from_model: n_ctx_per_seq = 128
0.00.543.683 I llama_init_from_model: n_batch       = 128
0.00.543.683 I llama_init_from_model: n_ubatch      = 128
0.00.543.684 I llama_init_from_model: flash_attn    = 0
0.00.543.685 I llama_init_from_model: freq_base     = 10000.0
0.00.543.685 I llama_init_from_model: freq_scale    = 1
0.00.543.686 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.543.689 I ggml_metal_init: allocating
0.00.543.759 I ggml_metal_init: found device: Apple M4
0.00.543.786 I ggml_metal_init: picking default device: Apple M4
0.00.545.478 I ggml_metal_init: using embedded metal library
0.00.552.288 I ggml_metal_init: GPU name:   Apple M4
0.00.552.293 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.552.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.552.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.552.295 I ggml_metal_init: simdgroup reduction   = true
0.00.552.296 I ggml_metal_init: simdgroup matrix mul. = true
0.00.552.296 I ggml_metal_init: has residency sets    = true
0.00.552.296 I ggml_metal_init: has bfloat            = true
0.00.552.297 I ggml_metal_init: use bfloat            = true
0.00.552.298 I ggml_metal_init: hasUnifiedMemory      = true
0.00.552.302 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.401 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.574.027 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.574.034 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.574.063 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.577.267 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.577.269 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.577.269 I llama_init_from_model: graph nodes  = 967
0.00.577.270 I llama_init_from_model: graph splits = 2
0.00.577.272 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.577.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.585 I 
0.00.607.657 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.675 I perplexity: tokenizing the input ..
0.00.614.830 I perplexity: tokenization took 7.153 ms
0.00.614.858 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.763.711 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.765.039 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.765.055 I llama_perf_context_print:        load time =     598.74 ms
0.00.765.056 I llama_perf_context_print: prompt eval time =     147.94 ms /   128 tokens (    1.16 ms per token,   865.19 tokens per second)
0.00.765.056 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.765.057 I llama_perf_context_print:       total time =     157.47 ms /   129 tokens
0.00.765.445 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.080s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.957 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.010 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.015 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.017 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.018 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.018 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.018 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.019 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.020 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.020 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.020 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.021 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.021 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.022 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.024 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.024 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.024 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.819 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.882 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.604 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.605 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.605 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.606 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.606 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.606 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.607 I llama_model_loader: - type  f32:  194 tensors
0.00.025.607 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.607 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.608 I print_info: file format = GGUF V3 (latest)
0.00.025.608 I print_info: file type   = Q5_K - Medium
0.00.025.609 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.418 I load: special tokens cache size = 25
0.00.039.420 I load: token to piece cache size = 0.2984 MB
0.00.039.423 I print_info: arch             = gptneox
0.00.039.423 I print_info: vocab_only       = 0
0.00.039.423 I print_info: n_ctx_train      = 2048
0.00.039.423 I print_info: n_embd           = 2048
0.00.039.423 I print_info: n_layer          = 24
0.00.039.427 I print_info: n_head           = 16
0.00.039.427 I print_info: n_head_kv        = 16
0.00.039.428 I print_info: n_rot            = 32
0.00.039.428 I print_info: n_swa            = 0
0.00.039.428 I print_info: n_embd_head_k    = 128
0.00.039.428 I print_info: n_embd_head_v    = 128
0.00.039.429 I print_info: n_gqa            = 1
0.00.039.430 I print_info: n_embd_k_gqa     = 2048
0.00.039.431 I print_info: n_embd_v_gqa     = 2048
0.00.039.432 I print_info: f_norm_eps       = 1.0e-05
0.00.039.433 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.433 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.433 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.433 I print_info: f_logit_scale    = 0.0e+00
0.00.039.434 I print_info: n_ff             = 8192
0.00.039.434 I print_info: n_expert         = 0
0.00.039.434 I print_info: n_expert_used    = 0
0.00.039.435 I print_info: causal attn      = 1
0.00.039.435 I print_info: pooling type     = 0
0.00.039.435 I print_info: rope type        = 2
0.00.039.435 I print_info: rope scaling     = linear
0.00.039.436 I print_info: freq_base_train  = 10000.0
0.00.039.436 I print_info: freq_scale_train = 1
0.00.039.436 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.436 I print_info: rope_finetuned   = unknown
0.00.039.436 I print_info: ssm_d_conv       = 0
0.00.039.437 I print_info: ssm_d_inner      = 0
0.00.039.437 I print_info: ssm_d_state      = 0
0.00.039.437 I print_info: ssm_dt_rank      = 0
0.00.039.437 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.437 I print_info: model type       = 1.4B
0.00.039.438 I print_info: model params     = 1.41 B
0.00.039.438 I print_info: general.name     = 1.4B
0.00.039.438 I print_info: vocab type       = BPE
0.00.039.438 I print_info: n_vocab          = 50304
0.00.039.439 I print_info: n_merges         = 50009
0.00.039.439 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.439 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.439 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.439 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.440 I print_info: LF token         = 187 'Ċ'
0.00.039.440 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.440 I print_info: max token length = 1024
0.00.039.441 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.594.990 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.996 I load_tensors: offloading output layer to GPU
0.00.594.997 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.022 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.595.026 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.596.541 I llama_init_from_model: n_seq_max     = 1
0.00.596.544 I llama_init_from_model: n_ctx         = 128
0.00.596.544 I llama_init_from_model: n_ctx_per_seq = 128
0.00.596.545 I llama_init_from_model: n_batch       = 128
0.00.596.545 I llama_init_from_model: n_ubatch      = 128
0.00.596.545 I llama_init_from_model: flash_attn    = 0
0.00.596.546 I llama_init_from_model: freq_base     = 10000.0
0.00.596.547 I llama_init_from_model: freq_scale    = 1
0.00.596.548 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.596.549 I ggml_metal_init: allocating
0.00.596.565 I ggml_metal_init: found device: Apple M4
0.00.596.575 I ggml_metal_init: picking default device: Apple M4
0.00.597.941 I ggml_metal_init: using embedded metal library
0.00.603.974 I ggml_metal_init: GPU name:   Apple M4
0.00.603.978 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.603.978 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.603.980 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.981 I ggml_metal_init: simdgroup reduction   = true
0.00.603.981 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.981 I ggml_metal_init: has residency sets    = true
0.00.603.981 I ggml_metal_init: has bfloat            = true
0.00.603.982 I ggml_metal_init: use bfloat            = true
0.00.603.983 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.984 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.943 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.427 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.624.433 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.624.466 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.627.591 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.627.593 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.627.593 I llama_init_from_model: graph nodes  = 967
0.00.627.593 I llama_init_from_model: graph splits = 2
0.00.627.596 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.627.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.518 I 
0.00.662.608 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.632 I perplexity: tokenizing the input ..
0.00.669.609 I perplexity: tokenization took 6.976 ms
0.00.669.622 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.590 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.810.919 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.810.935 I llama_perf_context_print:        load time =     652.55 ms
0.00.810.935 I llama_perf_context_print: prompt eval time =     139.74 ms /   128 tokens (    1.09 ms per token,   915.99 tokens per second)
0.00.810.936 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.937 I llama_perf_context_print:       total time =     148.42 ms /   129 tokens
0.00.811.285 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.077s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.723 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.729 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.730 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.735 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.736 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.736 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.737 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.738 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.738 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.738 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.740 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.741 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.741 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.743 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.747 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.747 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.533 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.528 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.341 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.343 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.343 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.343 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.343 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.344 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.344 I llama_model_loader: - type  f32:  194 tensors
0.00.024.345 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.345 I print_info: file format = GGUF V3 (latest)
0.00.024.346 I print_info: file type   = Q6_K
0.00.024.347 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.061 I load: special tokens cache size = 25
0.00.038.044 I load: token to piece cache size = 0.2984 MB
0.00.038.047 I print_info: arch             = gptneox
0.00.038.047 I print_info: vocab_only       = 0
0.00.038.047 I print_info: n_ctx_train      = 2048
0.00.038.047 I print_info: n_embd           = 2048
0.00.038.047 I print_info: n_layer          = 24
0.00.038.052 I print_info: n_head           = 16
0.00.038.052 I print_info: n_head_kv        = 16
0.00.038.053 I print_info: n_rot            = 32
0.00.038.053 I print_info: n_swa            = 0
0.00.038.053 I print_info: n_embd_head_k    = 128
0.00.038.053 I print_info: n_embd_head_v    = 128
0.00.038.054 I print_info: n_gqa            = 1
0.00.038.055 I print_info: n_embd_k_gqa     = 2048
0.00.038.058 I print_info: n_embd_v_gqa     = 2048
0.00.038.059 I print_info: f_norm_eps       = 1.0e-05
0.00.038.059 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.059 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.059 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.060 I print_info: f_logit_scale    = 0.0e+00
0.00.038.060 I print_info: n_ff             = 8192
0.00.038.060 I print_info: n_expert         = 0
0.00.038.061 I print_info: n_expert_used    = 0
0.00.038.061 I print_info: causal attn      = 1
0.00.038.061 I print_info: pooling type     = 0
0.00.038.063 I print_info: rope type        = 2
0.00.038.063 I print_info: rope scaling     = linear
0.00.038.063 I print_info: freq_base_train  = 10000.0
0.00.038.064 I print_info: freq_scale_train = 1
0.00.038.064 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.064 I print_info: rope_finetuned   = unknown
0.00.038.064 I print_info: ssm_d_conv       = 0
0.00.038.064 I print_info: ssm_d_inner      = 0
0.00.038.064 I print_info: ssm_d_state      = 0
0.00.038.065 I print_info: ssm_dt_rank      = 0
0.00.038.065 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.065 I print_info: model type       = 1.4B
0.00.038.065 I print_info: model params     = 1.41 B
0.00.038.065 I print_info: general.name     = 1.4B
0.00.038.066 I print_info: vocab type       = BPE
0.00.038.066 I print_info: n_vocab          = 50304
0.00.038.066 I print_info: n_merges         = 50009
0.00.038.070 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.070 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.070 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.070 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.071 I print_info: LF token         = 187 'Ċ'
0.00.038.071 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.071 I print_info: max token length = 1024
0.00.038.071 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.605.901 I load_tensors: offloading 24 repeating layers to GPU
0.00.605.904 I load_tensors: offloading output layer to GPU
0.00.605.905 I load_tensors: offloaded 25/25 layers to GPU
0.00.605.926 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.605.929 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.607.164 I llama_init_from_model: n_seq_max     = 1
0.00.607.166 I llama_init_from_model: n_ctx         = 128
0.00.607.166 I llama_init_from_model: n_ctx_per_seq = 128
0.00.607.167 I llama_init_from_model: n_batch       = 128
0.00.607.167 I llama_init_from_model: n_ubatch      = 128
0.00.607.168 I llama_init_from_model: flash_attn    = 0
0.00.607.168 I llama_init_from_model: freq_base     = 10000.0
0.00.607.169 I llama_init_from_model: freq_scale    = 1
0.00.607.170 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.607.171 I ggml_metal_init: allocating
0.00.607.198 I ggml_metal_init: found device: Apple M4
0.00.607.206 I ggml_metal_init: picking default device: Apple M4
0.00.608.660 I ggml_metal_init: using embedded metal library
0.00.614.778 I ggml_metal_init: GPU name:   Apple M4
0.00.614.781 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.782 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.783 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.784 I ggml_metal_init: simdgroup reduction   = true
0.00.614.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.785 I ggml_metal_init: has residency sets    = true
0.00.614.785 I ggml_metal_init: has bfloat            = true
0.00.614.785 I ggml_metal_init: use bfloat            = true
0.00.614.786 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.791 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.017 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.634.409 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.634.412 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.634.446 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.637.832 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.637.834 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.637.835 I llama_init_from_model: graph nodes  = 967
0.00.637.835 I llama_init_from_model: graph splits = 2
0.00.637.837 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.637.838 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.489 I 
0.00.672.567 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.585 I perplexity: tokenizing the input ..
0.00.679.253 I perplexity: tokenization took 6.665 ms
0.00.679.271 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.616 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.821.267 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.821.281 I llama_perf_context_print:        load time =     663.59 ms
0.00.821.283 I llama_perf_context_print: prompt eval time =     139.49 ms /   128 tokens (    1.09 ms per token,   917.65 tokens per second)
0.00.821.287 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.287 I llama_perf_context_print:       total time =     148.80 ms /   129 tokens
0.00.821.638 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.076s
sys	0m0.134s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.322 I build: 4689 (90e4dba4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.882 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.621 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.627 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.628 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.629 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.630 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.630 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.630 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.632 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.632 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.632 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.633 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.633 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.634 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.636 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.637 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.637 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.162 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.202 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.692 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.694 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.695 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.695 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.696 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.696 I llama_model_loader: - type  f32:  194 tensors
0.00.058.697 I llama_model_loader: - type  f16:   98 tensors
0.00.058.698 I print_info: file format = GGUF V3 (latest)
0.00.058.698 I print_info: file type   = all F32 (guessed)
0.00.058.700 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.071.251 I load: special tokens cache size = 25
0.00.079.631 I load: token to piece cache size = 0.2984 MB
0.00.079.636 I print_info: arch             = gptneox
0.00.079.636 I print_info: vocab_only       = 0
0.00.079.637 I print_info: n_ctx_train      = 2048
0.00.079.637 I print_info: n_embd           = 2048
0.00.079.637 I print_info: n_layer          = 24
0.00.079.640 I print_info: n_head           = 16
0.00.079.641 I print_info: n_head_kv        = 16
0.00.079.641 I print_info: n_rot            = 32
0.00.079.642 I print_info: n_swa            = 0
0.00.079.642 I print_info: n_embd_head_k    = 128
0.00.079.642 I print_info: n_embd_head_v    = 128
0.00.079.643 I print_info: n_gqa            = 1
0.00.079.643 I print_info: n_embd_k_gqa     = 2048
0.00.079.644 I print_info: n_embd_v_gqa     = 2048
0.00.079.645 I print_info: f_norm_eps       = 1.0e-05
0.00.079.645 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.645 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.645 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.646 I print_info: f_logit_scale    = 0.0e+00
0.00.079.647 I print_info: n_ff             = 8192
0.00.079.648 I print_info: n_expert         = 0
0.00.079.648 I print_info: n_expert_used    = 0
0.00.079.648 I print_info: causal attn      = 1
0.00.079.648 I print_info: pooling type     = 0
0.00.079.648 I print_info: rope type        = 2
0.00.079.649 I print_info: rope scaling     = linear
0.00.079.649 I print_info: freq_base_train  = 10000.0
0.00.079.649 I print_info: freq_scale_train = 1
0.00.079.650 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.650 I print_info: rope_finetuned   = unknown
0.00.079.650 I print_info: ssm_d_conv       = 0
0.00.079.650 I print_info: ssm_d_inner      = 0
0.00.079.651 I print_info: ssm_d_state      = 0
0.00.079.651 I print_info: ssm_dt_rank      = 0
0.00.079.651 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.651 I print_info: model type       = 1.4B
0.00.079.651 I print_info: model params     = 1.41 B
0.00.079.654 I print_info: general.name     = 1.4B
0.00.079.654 I print_info: vocab type       = BPE
0.00.079.655 I print_info: n_vocab          = 50304
0.00.079.656 I print_info: n_merges         = 50009
0.00.079.656 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.656 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.656 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.656 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.657 I print_info: LF token         = 187 'Ċ'
0.00.079.658 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.658 I print_info: max token length = 1024
0.00.079.659 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.374.885 I load_tensors: offloading 24 repeating layers to GPU
0.01.374.889 I load_tensors: offloading output layer to GPU
0.01.374.889 I load_tensors: offloaded 25/25 layers to GPU
0.01.374.913 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.374.915 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.375.772 I llama_init_from_model: n_seq_max     = 1
0.01.375.773 I llama_init_from_model: n_ctx         = 128
0.01.375.773 I llama_init_from_model: n_ctx_per_seq = 128
0.01.375.774 I llama_init_from_model: n_batch       = 128
0.01.375.774 I llama_init_from_model: n_ubatch      = 128
0.01.375.774 I llama_init_from_model: flash_attn    = 0
0.01.375.775 I llama_init_from_model: freq_base     = 10000.0
0.01.375.775 I llama_init_from_model: freq_scale    = 1
0.01.375.775 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.375.780 I ggml_metal_init: allocating
0.01.375.833 I ggml_metal_init: found device: Apple M4
0.01.375.842 I ggml_metal_init: picking default device: Apple M4
0.01.377.006 I ggml_metal_init: using embedded metal library
0.01.380.774 I ggml_metal_init: GPU name:   Apple M4
0.01.380.777 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.380.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.380.777 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.380.778 I ggml_metal_init: simdgroup reduction   = true
0.01.380.778 I ggml_metal_init: simdgroup matrix mul. = true
0.01.380.778 I ggml_metal_init: has residency sets    = true
0.01.380.778 I ggml_metal_init: has bfloat            = true
0.01.380.778 I ggml_metal_init: use bfloat            = true
0.01.380.779 I ggml_metal_init: hasUnifiedMemory      = true
0.01.380.780 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.391.351 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.393.066 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.393.068 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.393.082 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.394.704 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.394.706 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.394.706 I llama_init_from_model: graph nodes  = 967
0.01.394.706 I llama_init_from_model: graph splits = 2
0.01.394.708 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.394.708 I 
0.01.394.745 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.394.747 I compute_imatrix: tokenizing the input ..
0.01.398.741 I compute_imatrix: tokenization took 3.994 ms
0.01.398.743 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.664.909 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.667.433 I llama_perf_context_print:        load time =    1639.02 ms
0.01.667.434 I llama_perf_context_print: prompt eval time =     264.43 ms /   128 tokens (    2.07 ms per token,   484.06 tokens per second)
0.01.667.435 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.667.435 I llama_perf_context_print:       total time =    1641.54 ms /   129 tokens
0.01.667.940 I ggml_metal_free: deallocating

real	0m1.854s
user	0m0.128s
sys	0m0.251s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4689 (90e4dba4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147f09f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147f0a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147f0abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147f0b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147f0b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147f0bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147f0c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147f0c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147f0ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147f0d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147f0d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147f0dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147f0e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147f0efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147f0ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147f10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147f10d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147f11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147f11c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147f12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147f13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147f13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147f14150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147f14410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147f14a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147f15690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147f15bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147f15e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147f16330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147f165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147f16e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147f173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147f17680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127e070c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127e075e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127e07a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127e07ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127e08330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127e087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127e08c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127e09080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127e094f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127e09960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127e09dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127e0a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127e0a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127e0ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127e0b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127e0b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127e0bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127e0c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127e0c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127e0c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127e0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127e0d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127e0d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127e0dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127e0e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127e0e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127e0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127e0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127e0f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127e0f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127e0fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127e0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127e10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127e108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127e10d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127e11180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127e115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127e11a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127e11ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127e12340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127e127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127e12c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127e13090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127e13500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127e13970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127e13de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127e14250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127e14ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127e14fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127e15580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127e15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127e160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127e16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127e16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127e171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127e177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127e17d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127e18300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127e188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127e18e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127e19410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127e199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127e0a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127e1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127e1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127e1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127e1afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127e1b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127e1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127e1c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127e1c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127e1cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127e1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127e1d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127e1dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127e1e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127e1e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127e1ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127e1f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127e1f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127e1fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127e202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127e207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127e20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127e211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127e216f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127e21bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127e220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127e225f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127e22af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127e22ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127e234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127e239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127e23ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127e243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127e248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127e24df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127e252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127e257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127e25cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127e261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127e266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127e26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127e270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127e275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127e27af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127e27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127e284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127e289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127e28ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127e293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127e298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127e29df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127e2a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127e2a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127e2acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127e2b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127e2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127e2bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127e2c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127e2c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127e2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127e2cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127e2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127e2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127e2def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127e2e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127e2e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127e2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127e2f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127e2f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127e2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127e301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127e306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127e30bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127e310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127e315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127e31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127e31ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127e324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127e329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127e32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127e333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127e338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127e33df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127e342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127e347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127e34cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127e351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127e356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127e35bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127e360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127e365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127e36af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127e36ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127e374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127e379f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127e37ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127e383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127e389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127e38f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127e39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127e39ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127e3a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127e3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127e3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127e3b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127e3b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127e3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127e3c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127e3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127e3d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127e3d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127e3d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127e3de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127e3e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127e3eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127e3f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127e3f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127e3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127e40060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127e405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127e40b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127e41050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127e415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127e41af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127e42040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127e42590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127e42ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127e43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127e43580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127e43ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127e44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127e44570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127e44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127e45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127e45560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127e45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127e46000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127e46550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127e46aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127e46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127e47540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127e47a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127e47fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127e48530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127e48a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127e48fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127e49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127e49a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127e49fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127e4a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127e4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127e4afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127e4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127e4ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127e4bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127e4c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127e4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127e4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127e4d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127e4da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127e4df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127e4e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127e4ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127e4ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127e4f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127e4fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127e4ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127e504b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127e50a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127e50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127e513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127e51890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127e51d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127e521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127e52670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127e52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127e52fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127e53450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127e538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127e53d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127e54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127e546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127e54b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127e55010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127e554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127e55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127e56120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127e56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127e56f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127e57680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127e57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127e58130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127e583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127e58a00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.785.610 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.785.615 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107f04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107f05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107f056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107f05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107f05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107f06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107f06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107f06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107f07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107f075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107f07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107f08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107f08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107f093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107f09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107f0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107f0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107f0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107f0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107f0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107f0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107f0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107f0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107f0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107f0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107f0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107f0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107f0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107f0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107f0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107f0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107f10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107f106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107f10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107f10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107f11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107f118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107f11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107f12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107f12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107f12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107f12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107f13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107f137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107f13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107f140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107f14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107f14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107f14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107f15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107f156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107f15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107f15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107f16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107f16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107f16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107f17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107f17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107f17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107f18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107f184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107f18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107f18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107f19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107f19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107f19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107f19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107f1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107f1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107f1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107f1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107f1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107f1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107f1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107f1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107f1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107f1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107f1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107f1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107f1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107f1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107f1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107f1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107f1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107f1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107f1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107f1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107f1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107f20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107f20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107f209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107f20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107f212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107f21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107f21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107f22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107f22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107f228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107f22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107f231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107f23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107f23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107f23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107f24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107f24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107f24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107f250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107f25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107f259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107f25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107f262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107f26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107f26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107f26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107f27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107f278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107f27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107f281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107f28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107f28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107f28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107f29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107f297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107f29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107f2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107f2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107f2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107f2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107f2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107f2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107f2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107f2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107f2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107f2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107f2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107f2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107f2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107f2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107f2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107f2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107f2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107f2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107f2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107f2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107f2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107f2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107f30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107f306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107f30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107f30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107f31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107f31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107f31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107f32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107f325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107f32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107f32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107f33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107f337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107f33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107f34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107f344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107f34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107f34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107f35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107f35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107f36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107f363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107f36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107f36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107f37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107f375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107f37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107f37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107f38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107f38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107f38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107f39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107f394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107f39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107f39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107f3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107f3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107f3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107f3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107f3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107f3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107f3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107f3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107f3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107f3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107f3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107f3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107f3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107f3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107f3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107f3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107f3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107f3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107f3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107f3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107f3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107f400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107f40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107f409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107f40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107f41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107f417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107f41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107f42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107f42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107f430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107f43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107f43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107f441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107f447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107f44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107f45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107f458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107f45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107f46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107f46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107f46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107f475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107f47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107f48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107f486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107f48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107f49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107f49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107f49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107f4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107f4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107f4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107f4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107f4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107f4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107f4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107f4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107f4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107f4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107f4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107f4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107f4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107f4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107f4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107f4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107f4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107f50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107f50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107f510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107f516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107f51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107f52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107f527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107f52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107f53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107f53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107f53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107f544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107f54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107f55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107f555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107f55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107f56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107f56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107f56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107f571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107f576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107f57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107f580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107f585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107f58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107f58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107f594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107f599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107f59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107f5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107f5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107f5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107f5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107f5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107f5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107f5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107f5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107f5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107f5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107f5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107f5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107f5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1137044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x113704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x113704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x113705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1137056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x113705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x113705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1137063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x113706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x113706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x113707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1137078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1137083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x113708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x113709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x113709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11370a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11370a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11370b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11370b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11370bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11370c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11370cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11370d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11370db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11370de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11370e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11370e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11370e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11370ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11370f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11370f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11370fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11370ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x113710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1137107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x113710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1137110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x113711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1137119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x113711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x113712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x113712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x113712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x113712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x113713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1137138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x113713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1137141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x113714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x113714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x113714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x113715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1137157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x113715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1137160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x113716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x113716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x113716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x113717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x113717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x113717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x113718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1137185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x113718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x113718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x113719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x113719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x113719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11371a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11371a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11371a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11371adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11371b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11371b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11371bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11371bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11371c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11371c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11371ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11371d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11371d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11371da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11371de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11371e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11371e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11371ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11371f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11371f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11371f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11371fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x113720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x113720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x113720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x113720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1137213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x113721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x113721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x113722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x113722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1137229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x113722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1137232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x113723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x113723e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x113724290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x113724700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x113724b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x113724fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x113725450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1137258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x113725d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1137261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x113726610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x113726a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x113726ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x113727360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1137277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x113727c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1137280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x113728520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x113728990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x113728e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x113729270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1137296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x113729b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x113729fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11372a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11372a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11372ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11372b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11372b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11372ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11372bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11372c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11372c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11372cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11372d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11372d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11372d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11372dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11372e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11372e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11372eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11372efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11372f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11372f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11372fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x113730160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1137305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x113730a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x113730eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x113731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x113731790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x113731c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x113732070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1137324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x113732950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x113732dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x113733230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1137336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x113733b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x113733f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1137343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x113734860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x113734cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x113735140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1137355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x113735a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x113735e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x113736300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x113736770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x113736be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x113737050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1137374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x113737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x113737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x113738210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x113738680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x113738af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x113738f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1137393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x113739840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x113739cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11373a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11373a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11373aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11373ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11373b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11373b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11373bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11373c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11373c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11373c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11373cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11373d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11373d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11373dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11373df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11373e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11373e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11373ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11373f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11373f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11373f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11373fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1137402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x113740730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x113740ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x113741010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x113741b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x113741e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x113742110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x113742580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1137429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x113742e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1137432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x113743740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x113743bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x113744020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x113744490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x113744900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x113744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1137451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x113745650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x113745ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x113745f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1137463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x113746810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x113746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1137470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x113747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1137479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x113747e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1137482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x113748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x113748b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x113749000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x113749470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1137498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x113749d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11374a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11374a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11374aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11374af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11374b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11374b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11374bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11374c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11374c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11374c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11374ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11374d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11374d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11374db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11374dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11374e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11374e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11374ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11374f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11374f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11374fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11374fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x113750360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1137507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x113750c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1137510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x113751520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x113751990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x113751e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x113752270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1137526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x113752b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x113752fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x113753430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1137538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x113753d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x113754180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1137545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x113754a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x113754ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x113755340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1137557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x113756220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x113756940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x113757060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x113757780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x113757a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x113757eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1137584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x113758ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.842s
user	0m0.281s
sys	0m0.340s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4689 (90e4dba4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13210a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13210ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13210b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13210b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13210bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13210c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13210c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13210cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13210d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13210d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13210dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13210e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13210ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13210f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13210fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x132110420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x132110b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x132111260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132111980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132112150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132112870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132112f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1321136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132113f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132114670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132114930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132114f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x132115bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1321160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1321163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132116850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132116b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1321173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1321178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x132117ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x132118040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1321184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x132118980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x132118e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1321192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x132119760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x132119c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13211a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13211a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13211a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13211ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13211b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13211bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13211c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13211c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13211cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13211d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13211db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13211e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13211e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13211ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13211f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13211f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13211fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x132120390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x132120650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x132120af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x132120f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x132121430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1321218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x132121d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x132122210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1321226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x132122b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x132122ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x132123490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x132123930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x132123dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x132124320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x132124870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132124dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x132125310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x132125860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132125db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x132126300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132126850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132126da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1321272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x132127840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x132127d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1321282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x132128830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x132128d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1321292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x132129820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x132129d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13212a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13212a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13212ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13212b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13212b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13212bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13211ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13212c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13212c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13212cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13212d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13212d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13212deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13212e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13212e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13212eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13212f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13212f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13212fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1321303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x132130930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x132130e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x132131320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1321317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x132131c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x132132100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1321325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x132132a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x132132ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x132133380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x132133820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x132133cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x132134160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x132134600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x132134aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x132134f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1321353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x132135880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x132135d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1321361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x132136660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x132136b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x132136fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x132137440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1321378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x132137d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x132138220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1321386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x132138b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x132139000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1321394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x132139940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x132139de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13213a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13213a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13213abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13213b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13213b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13213b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13213be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13213c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13213c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13213cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13213d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13213d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13213da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13213dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13213e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13213e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13213ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13213f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13213f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13213fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13213ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1321403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x132140840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x132140ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132141180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x132141620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x132141ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132141f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x132142400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1321428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x132142d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1321431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132143680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x132143b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132143fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132144460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x132144900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132144da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132145240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1321456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132145b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x132146020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1321464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x132146960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132146e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1321472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x132147740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x132147be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x132148080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1321485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x132148b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x132149070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1321495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x132149880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x132149e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13214a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13214aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13214b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13214b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13214ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13214c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13214c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13214ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13214d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13214d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13214dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13214e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13214e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13214ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13214f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13214f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13214fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x132150380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1321508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x132150e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132151370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1321518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x132151e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132152360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1321528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x132152e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132153350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1321538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132153df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x132154340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x132154890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132154de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x132155330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132155880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132155dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132156320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132156870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132156dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x132157310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x132157860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x132157db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x132158300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x132158850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x132158da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1321592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x132159840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x132159d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13215a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13215a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13215ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13215b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13215b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13215bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13215c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13215c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13215cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13215d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13215d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13215dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13215e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13215e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13215ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13215f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13215f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13215fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x132160280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1321607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x132160d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1321611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x132161660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x132161b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x132161fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x132162440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1321628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x132162d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x132163220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1321636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132163b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132164000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1321644a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132164940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132164de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132165280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1321657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132165ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x132166610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x132166d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x132167450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x132167710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x132167f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1321681c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1321687d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.293 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.296 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x130e0b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x130e0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x130e0c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130e0c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x130e0cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130e0cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130e0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x130e0d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x130e0dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x130e0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130e0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130e0ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x130e0f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x130e0ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x130e107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x130e10ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x130e115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x130e11d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x130e12430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x130e12c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x130e13320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x130e13a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x130e14160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x130e14880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x130e14fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x130e15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x130e15520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x130e15990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x130e15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x130e16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x130e16770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x130e16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x130e170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130e173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x130e17820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x130e17c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130e181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130e186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x130e18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130e190f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x130e195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130e19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x130e19ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x130e1a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x130e1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x130e1ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130e1b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x130e1b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130e1bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x130e1c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x130e1c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x130e1c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x130e1cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x130e1d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x130e1d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x130e1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x130e1e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x130e1e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130e1eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130e1f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x130e1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x130e1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x130e20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x130e20600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x130e20aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x130e20f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x130e213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x130e21880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x130e21d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x130e221c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x130e22660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x130e22b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x130e22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x130e234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x130e23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x130e23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x130e244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x130e24a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x130e24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x130e254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x130e25a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x130e25f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x130e264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x130e26a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x130e26f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x130e274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x130e27a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x130e27f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x130e284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x130e289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130e28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130e29490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x130e299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x130e29f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x130e2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130e2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x130e2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130e2b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130e2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x130e2bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x130e2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x130e2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x130e2cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x130e2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130e2d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x130e2def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130e2e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x130e2e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x130e2eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x130e2f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130e2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x130e2fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x130e30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x130e308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x130e30d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x130e31200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x130e316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130e31b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x130e31fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x130e32480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130e32920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x130e32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x130e33260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x130e33700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x130e33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x130e34040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x130e344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x130e34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x130e34e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x130e352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x130e35760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x130e35c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x130e360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x130e36540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x130e369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x130e36e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x130e37320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x130e377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x130e37c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x130e38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x130e385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x130e38a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x130e38ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x130e39380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x130e39820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x130e39cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x130e3a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x130e3a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x130e3aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130e3af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x130e3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x130e3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x130e3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x130e3c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130e3c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x130e3cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x130e3cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x130e3d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x130e3d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x130e3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x130e3e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130e3e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130e3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130e3f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x130e3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130e3f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x130e3fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x130e40280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130e40720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x130e40bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130e41060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x130e41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x130e419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130e41e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x130e422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x130e42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x130e42c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x130e430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x130e43560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x130e43a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x130e43ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x130e44340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x130e447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x130e44c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x130e45120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x130e455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x130e45a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x130e45f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x130e463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x130e46840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x130e46ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x130e47180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x130e47620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x130e47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x130e480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x130e48610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x130e48b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x130e48e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x130e49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x130e49a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x130e4a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x130e4a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x130e4ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x130e4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x130e4b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130e4bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130e4c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130e4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130e4ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130e4d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x130e4d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130e4de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130e4e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x130e4e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130e4ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130e4f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130e4f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130e4fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x130e503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130e50910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130e50e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130e513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x130e51900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130e51e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130e523a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x130e528f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130e52e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130e53390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130e538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x130e53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130e54380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x130e548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130e54e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130e55370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130e558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130e55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130e56360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130e568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x130e56e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130e57350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x130e578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x130e57df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x130e58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x130e58890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x130e58de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x130e59330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x130e59880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x130e59dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x130e5a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x130e5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x130e5adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x130e5b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x130e5b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x130e5bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x130e5c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x130e5c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x130e5cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x130e5d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x130e5d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130e5dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130e5e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x130e5e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x130e5ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x130e5f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130e5f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130e5fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130e602c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130e60760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x130e60c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130e610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x130e61540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130e619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130e61e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130e62320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x130e627c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130e62c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130e63100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130e635a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130e63a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x130e63ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x130e64380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x130e64820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130e64d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130e65490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x130e65bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130e662d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x130e669f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x130e66cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130e674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130e67760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130e67d70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x130f08ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x130f09110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x130f09580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130f099f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x130f09e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130f0a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130f0a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x130f0abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x130f0b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x130f0b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130f0b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130f0c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x130f0cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x130f0d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x130f0dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x130f0e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x130f0e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x130f0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x130f0f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x130f0fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x130f105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x130f10cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x130f113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x130f11b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x130f12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x130f124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x130f127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x130f12c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x130f13090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x130f13500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x130f13970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x130f13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x130f14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130f145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x130f14a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x130f14eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130f15320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130f15790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x130f15c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130f16070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x130f164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130f16950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x130f16dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x130f17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x130f176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x130f17b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130f17f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x130f183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130f18860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x130f18cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x130f19140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x130f195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x130f19a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x130f19e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x130f1a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x130f1a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x130f1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x130f1b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130f1b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130f1bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x130f1bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x130f1c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x130f1c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x130f1cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x130f1d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x130f1d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x130f1d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x130f1de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x130f1e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x130f1e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x130f1eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x130f1f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x130f1f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x130f1f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x130f1fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x130f201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x130f20630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x130f20aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x130f20f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x130f21380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x130f217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x130f21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x130f220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x130f22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x130f229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x130f22e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x130f23290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x130f23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x130f23b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x130f23fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130f24450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130f248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x130f24d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x130f251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x130f25610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130f25a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x130f25ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130f26360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130f267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x130f26c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x130f270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x130f27520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x130f27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x130f28220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130f284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x130f28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130f28dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x130f29230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x130f296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x130f29b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130f29f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x130f2a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x130f2a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x130f2acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x130f2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x130f2b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x130f2ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130f2be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x130f2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x130f2c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130f2cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x130f2d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x130f2d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x130f2d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x130f2dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x130f2e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x130f2e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x130f2eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x130f2ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x130f2f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x130f2f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x130f2fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x130f30120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x130f30590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x130f30a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x130f30e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x130f312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x130f31750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x130f31bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x130f32030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x130f324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x130f32910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x130f32d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x130f331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x130f33660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x130f33ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x130f33f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x130f343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x130f34820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130f34c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x130f35100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x130f35570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x130f359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x130f35e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130f362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x130f36730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x130f36ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x130f37010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x130f37480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x130f378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x130f37d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130f381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130f38640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130f38ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x130f38f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130f39390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x130f39800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x130f39c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130f3a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x130f3a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130f3a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x130f3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x130f3b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130f3b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x130f3bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x130f3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x130f3c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x130f3c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x130f3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x130f3d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x130f3d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x130f3da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x130f3df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x130f3e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x130f3e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x130f3ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x130f3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x130f3f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x130f3f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x130f3fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x130f40280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x130f406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x130f40b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x130f40fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x130f41440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x130f418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x130f41d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x130f42190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x130f42600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x130f42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x130f42ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x130f43350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x130f437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x130f43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x130f440a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130f44510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130f44980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130f44df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130f45260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130f456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x130f46250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130f46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130f467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x130f46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130f470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130f47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130f47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130f47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x130f48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130f486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130f48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x130f49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130f498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130f49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x130f4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130f4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130f4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130f4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x130f4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130f4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x130f4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130f4c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130f4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130f4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130f4cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130f4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130f4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x130f4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130f4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x130f4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x130f4e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x130f4ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x130f4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x130f4f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x130f4fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x130f4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x130f50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x130f50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x130f50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x130f51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x130f514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x130f51950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x130f51dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x130f52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x130f526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x130f52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x130f52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x130f533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130f53860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130f53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x130f54140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x130f545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x130f54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130f54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130f55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130f55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130f55be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x130f56050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130f564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x130f56930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130f56da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130f57210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130f57680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x130f57af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130f57f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130f583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130f58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130f58cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x130f59120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x130f59590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x130f59a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130f59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130f5a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x130f5b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130f5b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x130f5be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x130f5c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130f5c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130f5cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130f5d180 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.944s
user	0m0.229s
sys	0m0.183s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
